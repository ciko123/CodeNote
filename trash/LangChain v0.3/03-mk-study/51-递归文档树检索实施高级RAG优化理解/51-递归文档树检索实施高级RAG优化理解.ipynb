{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.RAPTOR递归文档树优化策略.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@Time    : 2024/8/7 11:18\n",
    "@Author  : thezehui@gmail.com\n",
    "@File    : 16.RAPTOR递归文档树优化策略.py\n",
    "\"\"\"\n",
    "from typing import Optional\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import umap\n",
    "import weaviate\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_weaviate import WeaviateVectorStore\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from weaviate.auth import AuthApiKey\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 1.定义随机数种子、文本嵌入模型、大语言模型、向量数据库\n",
    "RANDOM_SEED = 224\n",
    "embd = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-small\",\n",
    "    cache_folder=\"./embeddings/\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "db = WeaviateVectorStore(\n",
    "    client=weaviate.connect_to_wcs(\n",
    "        cluster_url=\"https://mbakeruerziae6psyex7ng.c0.us-west3.gcp.weaviate.cloud\",\n",
    "        auth_credentials=AuthApiKey(\"ZltPVa9ZSOxUcfafelsggGyyH6tnTYQYJvBx\"),\n",
    "    ),\n",
    "    index_name=\"RaptorRAG\",\n",
    "    text_key=\"text\",\n",
    "    embedding=embd,\n",
    ")\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "        embeddings: np.ndarray, dim: int, n_neighbors: Optional[int] = None, metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    使用UMAP对传递嵌入向量进行全局降维\n",
    "\n",
    "    :param embeddings: 需要降维的嵌入向量\n",
    "    :param dim: 降低后的维度\n",
    "    :param n_neighbors: 每个向量需要考虑的邻居数量，如果没有提供默认为嵌入数量的开方\n",
    "    :param metric: 用于UMAP的距离度量，默认为余弦相似性\n",
    "    :return: 一个降维到指定维度的numpy嵌入数组\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(n_neighbors=n_neighbors, n_components=dim, metric=metric).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "        embeddings: np.ndarray, dim: int, n_neighbors: int = 10, metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    使用UMAP对嵌入进行局部降维处理，通常在全局聚类之后进行。\n",
    "\n",
    "    :param embeddings: 需要降维的嵌入向量\n",
    "    :param dim: 降低后的维度\n",
    "    :param n_neighbors: 每个向量需要考虑的邻居数量\n",
    "    :param metric: 用于UMAP的距离度量，默认为余弦相似性\n",
    "    :return: 一个降维到指定维度的numpy嵌入数组\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric,\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "        embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    使用高斯混合模型结合贝叶斯信息准则（BIC）确定最佳的聚类数目。\n",
    "\n",
    "    :param embeddings: 需要聚类的嵌入向量\n",
    "    :param max_clusters: 最大聚类数\n",
    "    :param random_state: 随机数\n",
    "    :return: 返回最优聚类数\n",
    "    \"\"\"\n",
    "    # 1.获取最大聚类树，最大聚类数不能超过嵌入向量的数量\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "\n",
    "    # 2.逐个设置聚类树并找出最优聚类数\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        # 3.创建高斯混合模型，并计算聚类结果\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def gmm_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0) -> tuple[list, int]:\n",
    "    \"\"\"\n",
    "    使用基于概率阈值的高斯混合模型（GMM）对嵌入进行聚类。\n",
    "\n",
    "    :param embeddings: 需要聚类的嵌入向量（降维）\n",
    "    :param threshold: 概率阈值\n",
    "    :param random_state: 用于可重现的随机性种子\n",
    "    :return: 包含聚类标签和确定聚类数目的元组\n",
    "    \"\"\"\n",
    "    # 1.获取最优聚类数\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "\n",
    "    # 2.创建高斯混合模型对象并嵌入数据\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "\n",
    "    # 3.预测每个样本属于各个聚类的概率\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "\n",
    "    # 4.根据概率阈值确定每个嵌入的聚类标签\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "\n",
    "    # 5.返回聚类标签和聚类数目\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(embeddings: np.ndarray, dim: int, threshold: float) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    对嵌入进行聚类，首先全局降维，然后使用高斯混合模型进行聚类，最后在每个全局聚类中进行局部聚类。\n",
    "\n",
    "    :param embeddings: 需要执行操作的嵌入向量列表\n",
    "    :param dim: 指定的降维维度\n",
    "    :param threshold: 概率阈值\n",
    "    :return: 包含每个嵌入的聚类ID的列表，每个数组代表一个嵌入的聚类标签。\n",
    "    \"\"\"\n",
    "    # 1.检测传入的嵌入向量，当数据量不足时不进行聚类\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # 2.调用函数进行全局降维\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "\n",
    "    # 3.对降维后的数据进行全局聚类\n",
    "    global_clusters, n_global_clusters = gmm_cluster(reduced_embeddings_global, threshold)\n",
    "\n",
    "    # 4.初始化一个空列表，用于存储所有嵌入的局部聚类标签\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # 5.遍历每个全局聚类以执行局部聚类\n",
    "    for i in range(n_global_clusters):\n",
    "        # 6.提取属于当前全局聚类的嵌入向量\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        # 7.如果当前全局聚类中没有嵌入向量则跳过循环\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "\n",
    "        # 8.如果当前全局聚类中的嵌入量很少，直接将它们分配到一个聚类中\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # 9.执行局部降维和聚类\n",
    "            reduced_embeddings_local = local_cluster_embeddings(global_cluster_embeddings_, dim)\n",
    "            local_clusters, n_local_clusters = gmm_cluster(reduced_embeddings_local, threshold)\n",
    "\n",
    "        # 10.分配局部聚类ID，调整已处理的总聚类数目\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(all_local_clusters[idx], j + total_clusters)\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "def embed(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    将传递的的文本列表转换成嵌入向量列表\n",
    "\n",
    "    :param texts: 需要转换的文本列表\n",
    "    :return: 生成的嵌入向量列表并转换成numpy数组\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    return np.array(text_embeddings)\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts: list[str]) -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    对文本列表进行嵌入和聚类,并返回一个包含文本、嵌入和聚类标签的数据框。\n",
    "    该函数将嵌入生成和聚类结合成一个步骤。\n",
    "\n",
    "    :param texts: 需要处理的文本列表\n",
    "    :return: 返回包含文本、嵌入和聚类标签的数据框\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)\n",
    "    cluster_labels = perform_clustering(text_embeddings_np, 10, 0.1)\n",
    "    df = pd.DataFrame()\n",
    "    df[\"text\"] = texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)\n",
    "    df[\"cluster\"] = cluster_labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    将数据框中的文本格式化成单个字符串\n",
    "\n",
    "    :param df: 需要处理的数据框，内部涵盖text、embd、cluster三个字段\n",
    "    :return: 返回合并格式化后的字符串\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- ---\".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(texts: list[str], level: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    对传入的文本列表进行嵌入、聚类和总结。\n",
    "    该函数首先问文本生成嵌入，基于相似性对他们进行聚类，扩展聚类分配以便处理，然后总结每个聚类中的内容。\n",
    "\n",
    "    :param texts: 需要处理的文本列表\n",
    "    :param level: 一个整数，可以定义处理的深度\n",
    "    :return: 包含两个数据框的元组\n",
    "    - 第一个 DataFrame (df_clusters) 包括原始文本、它们的嵌入以及聚类分配。\n",
    "    - 第二个 DataFrame (df_summary) 包含每个聚类的摘要信息、指定的处理级别以及聚类标识符。\n",
    "    \"\"\"\n",
    "    # 1.嵌入和聚类文本，生成包含text、embd、cluster的数据框\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # 2.定义变量，用于扩展数据框，以便更方便地操作聚类\n",
    "    expanded_list = []\n",
    "\n",
    "    # 3.扩展数据框条目，将文档和聚类配对，便于处理\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # 4.从扩展列表创建一个新的数据框\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # 5.获取唯一的聚类标识符以进行处理\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    # 6.创建汇总Prompt、汇总链\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Language doc. \n",
    "\n",
    "    LangChain Expression Language provides a way to compose chain in LangChain.\n",
    "\n",
    "    Give a detailed summary of the documentation provided.\n",
    "\n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # 7.格式化每个聚类中的文本以进行总结\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # 8.创建一个DataFrame来存储总结及其对应的聚类和级别\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "        texts: list[str], level: int = 1, n_levels: int = 3,\n",
    ") -> dict[int, tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    递归地嵌入、聚类和总结文本，直到达到指定的级别或唯一聚类数变为1，将结果存储在每个级别处。\n",
    "\n",
    "    :param texts: 要处理的文本列表\n",
    "    :param level: 当前递归级别（从1开始）\n",
    "    :param n_levels: 递归地最大深度（默认为3）\n",
    "    :return: 一个字典，其中键是递归级别，值是包含该级别处聚类DataFrame和总结DataFrame的元组。\n",
    "    \"\"\"\n",
    "    # 1.定义字典用于存储每个级别处的结果\n",
    "    results = {}\n",
    "\n",
    "    # 2.对当前级别执行嵌入、聚类和总结\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # 3.存储当前级别的结果\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # 4.确定是否可以继续递归并且有意义\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # 5.使用总结作为下一级递归的输入文本\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # 6.将下一级的结果合并到当前结果字典中\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 2.定义文档加载器、文本分割器(中英文场景)\n",
    "loaders = [\n",
    "    UnstructuredFileLoader(\"./流浪地球.txt\"),\n",
    "    UnstructuredFileLoader(\"./电商产品数据.txt\"),\n",
    "    UnstructuredFileLoader(\"./项目API文档.md\"),\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。|！|？\", \"\\.\\s|\\!\\s|\\?\\s\", \"；|;\\s\", \"，|,\\s\", \" \", \"\"],\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "\n",
    "# 3.循环分割并加载文本\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split(text_splitter))\n",
    "\n",
    "# 4.构建文档树，最多3层\n",
    "leaf_texts = [doc.page_content for doc in docs]\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)\n",
    "\n",
    "# 5.遍历文档树结果，从每个级别提取总结并将它们添加到all_texts中\n",
    "all_texts = leaf_texts.copy()\n",
    "for level in sorted(results.keys()):\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# 6.将all_texts添加到向量数据库\n",
    "db.add_texts(all_texts)\n",
    "\n",
    "# 7.执行相似性检索（折叠树）\n",
    "retriever = db.as_retriever(search_type=\"mmr\")\n",
    "search_docs = retriever.invoke(\"流浪地球中的人类花了多长时间才流浪到新的恒星系？\")\n",
    "\n",
    "print(search_docs)\n",
    "print(len(search_docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
