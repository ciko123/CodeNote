{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3fefce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93be844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",  # 使用 GPT-4o 模型进行测试\n",
    "        temperature=0.1,\n",
    "        max_tokens=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd0dca",
   "metadata": {},
   "source": [
    "### 1. RunnableParallel使用技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "905349a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': '为什么程序员总是分不清万圣节和圣诞节？  \\n因为 Oct 31 == Dec 25。', 'poem': '代码如诗，行行深，  \\n键盘敲响，梦成真。  \\n昼夜不分，屏幕前，  \\n心中逻辑，万物连。  \\n\\n调试千遍，心如镜，  \\n'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# 1.编排prompt\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"请讲一个关于{subject}的冷笑话，尽可能短一些\")\n",
    "poem_prompt = ChatPromptTemplate.from_template(\"请写一篇关于{subject}的诗，尽可能短一些\")\n",
    "\n",
    "# 3.创建输出解析器\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4.编排链\n",
    "joke_chain = joke_prompt | llm | parser\n",
    "poem_chain = poem_prompt | llm | parser\n",
    "\n",
    "# 5.并行链\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "# map_chain = RunnableParallel({\n",
    "#     \"joke\": joke_chain,\n",
    "#     \"poem\": poem_chain,\n",
    "# })\n",
    "\n",
    "res = map_chain.invoke({\"subject\": \"程序员\"})\n",
    "\n",
    "print(res)\n",
    "\n",
    "# > {'joke': '为什么程序员总是分不清万圣节和圣诞节？  \\n因为 Oct 31 == Dec 25。', 'poem': '代码如诗，行行心血，  \\n屏幕闪烁，昼夜不歇。  \\n逻辑如谜，解尽千结，  \\n键盘敲响，梦想直写。'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add93fb",
   "metadata": {},
   "source": [
    "### 2. RunnableParallel模拟检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def retrieval(query: str) -> str:\n",
    "    \"\"\"一个模拟的检索器函数\"\"\"\n",
    "    print(\"正在检索:\", query)\n",
    "    return \"我是慕小课\"\n",
    "\n",
    "\n",
    "# 1.编排prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"请根据用户的问题回答，可以参考对应的上下文进行生成。\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "用户的提问是: {query}\"\"\")\n",
    "\n",
    "# 3.输出解析器\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4.构建链\n",
    "chain = {\n",
    "            \"context\": lambda x: retrieval(x[\"query\"]),\n",
    "            \"query\": itemgetter(\"query\"),\n",
    "        } | prompt | llm | parser\n",
    "\n",
    "# 5.调用链\n",
    "content = chain.invoke({\"query\": \"你好，我是谁?\"})\n",
    "\n",
    "print(content)\n",
    "\n",
    "# > 正在检索: 你好，我是谁?\n",
    "# > 你好，你是慕小课！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4976c60",
   "metadata": {},
   "source": [
    "### 3. RunnablePassthrough简化invoke调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39294b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def retrieval(query: str) -> str:\n",
    "    \"\"\"一个模拟的检索器函数\"\"\"\n",
    "    print(\"正在检索:\", query)\n",
    "    return \"我是慕小课\"\n",
    "\n",
    "\n",
    "# 1.编排prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"请根据用户的问题回答，可以参考对应的上下文进行生成。\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "用户的提问是: {query}\"\"\")\n",
    "\n",
    "# 3.输出解析器\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4.构建链\n",
    "chain = RunnablePassthrough.assign(context=lambda x: retrieval(x[\"query\"])) | prompt | llm | parser\n",
    "\n",
    "# 5.调用链\n",
    "content = chain.invoke({\"query\": \"你好，我是谁?\"})\n",
    "\n",
    "print(content)\n",
    "\n",
    "# > 正在检索: 你好，我是谁?\n",
    "# > 你好，你是慕小课！\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
