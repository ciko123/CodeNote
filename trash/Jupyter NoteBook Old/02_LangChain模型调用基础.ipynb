{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. LangChain æ¨¡å‹è°ƒç”¨åŸºç¡€\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "æŒæ¡ LangChain ä¸­ä¸åŒæ¨¡å‹çš„è°ƒç”¨æ–¹æ³•ï¼ŒåŒ…æ‹¬å•è½®å¯¹è¯ã€å¤šè½®å¯¹è¯ã€æ–‡æœ¬ç”Ÿæˆç­‰åŸºç¡€æ“ä½œã€‚\n",
    "\n",
    "## ğŸ“‹ æ ¸å¿ƒçŸ¥è¯†ç‚¹\n",
    "- **ChatTongyi** - é€šä¹‰åƒé—®èŠå¤©æ¨¡å‹\n",
    "- **Tongyi** - é€šä¹‰åƒé—®æ–‡æœ¬ç”Ÿæˆæ¨¡å‹\n",
    "- **æ¶ˆæ¯ç±»å‹** - SystemMessage, HumanMessage, AIMessage\n",
    "- **å¯¹è¯ç®¡ç†** - å•è½® vs å¤šè½®å¯¹è¯\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š 2.1 ç¯å¢ƒå‡†å¤‡\n",
    "\n",
    "### å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥LangChainçš„é€šä¹‰åƒé—®èŠå¤©æ¨¡å‹\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "# å¯¼å…¥LangChainçš„é€šä¹‰åƒé—®æ–‡æœ¬ç”Ÿæˆæ¨¡å‹\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "# å¯¼å…¥dotenvç”¨äºåŠ è½½ç¯å¢ƒå˜é‡\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ä».envæ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# å¯¼å…¥LangChainçš„æ¶ˆæ¯ç±»å‹\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,  # ä»£è¡¨AIç”Ÿæˆçš„æ¶ˆæ¯\n",
    "    HumanMessage,  # ä»£è¡¨ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯\n",
    "    SystemMessage,  # ä»£è¡¨ç³»ç»Ÿç”Ÿæˆçš„æ¶ˆæ¯æˆ–æŒ‡ä»¤ï¼Œç”¨äºæŒ‡å¯¼AIçš„è¡Œä¸º\n",
    ")\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒé…ç½®å®Œæˆï¼\")\n",
    "print(\n",
    "    f\"ğŸ”‘ DASHSCOPE_API_KEY å·²é…ç½®: {'æ˜¯' if os.getenv('DASHSCOPE_API_KEY') else 'å¦'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¬ 2.2 å•è½®å¯¹è¯\n",
    "\n",
    "### æœ€ç®€å•çš„é—®ç­”äº¤äº’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„æ¨¡å—\n",
    "from langchain_community.llms import Tongyi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# ä».envæ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆ›å»ºé€šä¹‰åƒé—®èŠå¤©æ¨¡å‹å®ä¾‹\n",
    "# model: ä½¿ç”¨çš„æ¨¡å‹åç§° (qwen-turbo)\n",
    "# temperature: æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼Œ0.1è¡¨ç¤ºè¾ƒä½éšæœºæ€§ï¼Œè¾“å‡ºæ›´ç¨³å®š\n",
    "llm_basic = Tongyi(model=\"qwen-flash\")\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹å‘é€é—®é¢˜å¹¶æ‰“å°å›å¤å†…å®¹\n",
    "question = \"ä¸‰å›½å¿—çš„ä½œè€…æ˜¯è°ï¼Ÿ\"\n",
    "response = llm_basic.invoke(question)\n",
    "\n",
    "print(f\"â“ é—®é¢˜ï¼š{question}\")\n",
    "print(f\"ğŸ¤– å›ç­”ï¼š{response}\")\n",
    "print(f\"ğŸ“Š æ¨¡å‹ï¼š{llm_basic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” å•è½®å¯¹è¯ç‰¹ç‚¹\n",
    "\n",
    "- **ç®€å•ç›´æ¥**ï¼šä¸€ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªå›ç­”\n",
    "- **æ— ä¸Šä¸‹æ–‡**ï¼šæ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹çš„\n",
    "- **é€‚åˆåœºæ™¯**ï¼šé—®ç­”ã€ç¿»è¯‘ã€æ‘˜è¦ç­‰ä¸€æ¬¡æ€§ä»»åŠ¡\n",
    "\n",
    "### ğŸ’¡ ä½¿ç”¨æŠ€å·§\n",
    "\n",
    "```python\n",
    "# å•è½®å¯¹è¯çš„æœ€ä½³å®è·µ\n",
    "1. é—®é¢˜è¦æ¸…æ™°æ˜ç¡®\n",
    "2. é¿å…æ¨¡ç³Šçš„æŒ‡ä»£\n",
    "3. ä¸€æ¬¡æ€§æä¾›å®Œæ•´ä¿¡æ¯\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—£ï¸ 2.3 å¤šè½®å¯¹è¯\n",
    "\n",
    "### å¸¦ä¸Šä¸‹æ–‡çš„è¿ç»­å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—£ï¸ å¤šè½®å¯¹è¯ç¤ºä¾‹ï¼š\n",
      "========================================\n",
      "â“ æœ€åé—®é¢˜ï¼šçº¢æ¥¼æ¢¦å‘¢ï¼Ÿ\n",
      "ğŸ¤– AI å›ç­”ï¼šã€Šçº¢æ¥¼æ¢¦ã€‹ä½œè€…æ˜¯æ¸…ä»£ä½œå®¶æ›¹é›ªèŠ¹ã€‚\n",
      "\n",
      "ğŸ’¡ æ³¨æ„ï¼šAIç†è§£äº†'çº¢æ¥¼æ¢¦å‘¢ï¼Ÿ'æ˜¯åœ¨é—®çº¢æ¥¼æ¢¦çš„ä½œè€…\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„æ¨¡å—\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "# å¯¼å…¥LangChainçš„æ¶ˆæ¯ç±»å‹\n",
    "from langchain.schema import (\n",
    "    AIMessage,  # ä»£è¡¨AIç”Ÿæˆçš„æ¶ˆæ¯\n",
    "    HumanMessage,  # ä»£è¡¨ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯\n",
    "    SystemMessage,  # ä»£è¡¨ç³»ç»Ÿç”Ÿæˆçš„æ¶ˆæ¯æˆ–æŒ‡ä»¤ï¼Œç”¨äºæŒ‡å¯¼AIçš„è¡Œä¸º\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatTongyi(\n",
    "    model=\"qwen-flash\",\n",
    "    base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
    "    api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# æ„å»ºå¤šè½®å¯¹è¯çš„æ¶ˆæ¯åˆ—è¡¨\n",
    "messages = [\n",
    "    # ç³»ç»Ÿæ¶ˆæ¯ï¼šè®¾å®šAIè§’è‰²å’Œè¡Œä¸ºæ¨¡å¼\n",
    "    SystemMessage(\n",
    "        content=\"ä½ æ˜¯çŸ¥è¯†æ¸Šåšçš„ä¸“å®¶ï¼ŒçŸ¥é“å¾ˆå¤šè‘—åä¹¦ç±ç›¸å…³çŸ¥è¯†ï¼Œè¯·ç®€æ´çš„ç”¨20ä¸ªå­—å›ç­”é—®é¢˜\"\n",
    "    ),\n",
    "    # ç”¨æˆ·æ¶ˆæ¯ï¼šè‡ªæˆ‘ä»‹ç»\n",
    "    HumanMessage(content=\"æˆ‘çš„èº«ä»½æ˜¯å­¦å‘˜ï¼Œåå­—å«å°é¡¾\"),\n",
    "    # AIæ¶ˆæ¯ï¼šå›åº”é—®å€™\n",
    "    AIMessage(content=\"æ¬¢è¿ï¼Œæœ‰ä»€ä¹ˆéœ€è¦å’¨è¯¢çš„?\"),\n",
    "    # ç”¨æˆ·æ¶ˆæ¯ï¼šè¯¢é—®ä¸‰å›½å¿—ä½œè€…\n",
    "    HumanMessage(content=\"ä¸‰å›½å¿—ä½œè€…çš„æ˜¯è°ï¼Ÿ\"),\n",
    "    # AIæ¶ˆæ¯ï¼šå›ç­”ä¸‰å›½å¿—ä½œè€…\n",
    "    AIMessage(content=\"ã€Šä¸‰å›½å¿—ã€‹çš„ä½œè€…æ˜¯ä¸‰å›½æ—¶æœŸçš„å†å²å­¦å®¶é™ˆå¯¿ã€‚\"),\n",
    "    # ç”¨æˆ·æ¶ˆæ¯ï¼šè¯¢é—®çº¢æ¥¼æ¢¦ä½œè€…ï¼ˆè¿™é‡Œä¼šç”¨åˆ°ä¸Šä¸‹æ–‡ï¼‰\n",
    "    HumanMessage(content=\"çº¢æ¥¼æ¢¦å‘¢ï¼Ÿ\"),\n",
    "]\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹å¤„ç†å¤šè½®å¯¹è¯å¹¶è·å–å›å¤\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(\"ğŸ—£ï¸ å¤šè½®å¯¹è¯ç¤ºä¾‹ï¼š\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"â“ æœ€åé—®é¢˜ï¼šçº¢æ¥¼æ¢¦å‘¢ï¼Ÿ\")\n",
    "print(f\"ğŸ¤– AI å›ç­”ï¼š{response.content}\")\n",
    "print(f\"\\nğŸ’¡ æ³¨æ„ï¼šAIç†è§£äº†'çº¢æ¥¼æ¢¦å‘¢ï¼Ÿ'æ˜¯åœ¨é—®çº¢æ¥¼æ¢¦çš„ä½œè€…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” å¤šè½®å¯¹è¯ç‰¹ç‚¹\n",
    "\n",
    "- **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šèƒ½ç†è§£å¯¹è¯å†å²\n",
    "- **è§’è‰²ä¸€è‡´æ€§**ï¼šä¿æŒç³»ç»Ÿè®¾å®šçš„è§’è‰²\n",
    "- **é€‚åˆåœºæ™¯**ï¼šå®¢æœã€æ•™å­¦ã€å¤æ‚é—®é¢˜åˆ†è§£\n",
    "\n",
    "### ğŸ’¡ æ¶ˆæ¯ç±»å‹è¯´æ˜\n",
    "\n",
    "| æ¶ˆæ¯ç±»å‹ | ç”¨é€” | ç¤ºä¾‹ |\n",
    "|---------|------|------|\n",
    "| **SystemMessage** | è®¾å®šAIè§’è‰²å’Œè¡Œä¸º | \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹\" |\n",
    "| **HumanMessage** | ç”¨æˆ·è¾“å…¥çš„é—®é¢˜ | \"å¦‚ä½•å­¦ä¹ Pythonï¼Ÿ\" |\n",
    "| **AIMessage** | AIçš„å†å²å›ç­” | \"å»ºè®®ä»åŸºç¡€è¯­æ³•å¼€å§‹\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 2.4 æ–‡æœ¬ç”Ÿæˆ (LLM)\n",
    "\n",
    "### åŸºç¡€æ–‡æœ¬è¡¥å…¨åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ï¼š\n",
      "==============================\n",
      "ğŸ“‹ æç¤ºè¯ï¼šè¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­åŠ›å­¦\n",
      "ğŸ¤– ç”Ÿæˆç»“æœï¼šé‡å­åŠ›å­¦æ˜¯ç ”ç©¶å¾®è§‚ç²’å­ï¼ˆå¦‚ç”µå­ã€å…‰å­ï¼‰è¡Œä¸ºåŠå…¶ä¸èƒ½é‡ç›¸äº’ä½œç”¨çš„ç‰©ç†ç†è®ºï¼Œæ­ç¤ºäº†ç‰©è´¨åœ¨æå°å°ºåº¦ä¸‹çš„æ³¢ç²’äºŒè±¡æ€§ã€ä¸ç¡®å®šæ€§ä¸æ¦‚ç‡æ€§è§„å¾‹ã€‚\n",
      "\n",
      "ğŸ“Š æ¨¡å‹ï¼š\u001b[1mTongyi\u001b[0m\n",
      "Params: {'model_name': 'qwen-flash'}\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„æ¨¡å—\n",
    "from langchain_community.llms import Tongyi\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆ›å»ºé€šä¹‰åƒé—®èŠå¤©æ¨¡å‹å®ä¾‹\n",
    "# model: ä½¿ç”¨çš„æ¨¡å‹åç§° (qwen-turbo)\n",
    "# temperature: æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼Œ0.1è¡¨ç¤ºè¾ƒä½éšæœºæ€§ï¼Œè¾“å‡ºæ›´ç¨³å®š\n",
    "llm = Tongyi(model=\"qwen-flash\")\n",
    "\n",
    "# å®šä¹‰æç¤ºè¯\n",
    "prompt = \"è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­åŠ›å­¦\"\n",
    "# è°ƒç”¨LLMç”Ÿæˆæ–‡æœ¬\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(\"ğŸ“ æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ï¼š\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"ğŸ“‹ æç¤ºè¯ï¼š{prompt}\")\n",
    "print(f\"ğŸ¤– ç”Ÿæˆç»“æœï¼š{response}\")\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹ï¼š{llm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Chat vs LLM å¯¹æ¯”\n",
    "\n",
    "#### **ChatTongyi (èŠå¤©æ¨¡å‹)**\n",
    "```python\n",
    "# ç‰¹ç‚¹ï¼š\n",
    "- ä¸“ä¸ºå¯¹è¯ä¼˜åŒ–\n",
    "- æ”¯æŒæ¶ˆæ¯æ ¼å¼ (System/Human/AIMessage)\n",
    "- ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å¼º\n",
    "- é€‚åˆå¤šè½®å¯¹è¯\n",
    "\n",
    "# ä½¿ç”¨åœºæ™¯ï¼š\n",
    "- æ™ºèƒ½å®¢æœ\n",
    "- æ•™å­¦åŠ©æ‰‹\n",
    "- è§’è‰²æ‰®æ¼”\n",
    "```\n",
    "\n",
    "#### **Tongyi (æ–‡æœ¬ç”Ÿæˆæ¨¡å‹)**\n",
    "```python\n",
    "# ç‰¹ç‚¹ï¼š\n",
    "- ä¸“ä¸ºæ–‡æœ¬ç”Ÿæˆä¼˜åŒ–\n",
    "- æ¥å—çº¯æ–‡æœ¬è¾“å…¥\n",
    "- ç”Ÿæˆè¿ç»­æ–‡æœ¬\n",
    "- é€‚åˆè¡¥å…¨ä»»åŠ¡\n",
    "\n",
    "# ä½¿ç”¨åœºæ™¯ï¼š\n",
    "- æ–‡ç« å†™ä½œ\n",
    "- ä»£ç ç”Ÿæˆ\n",
    "- æ–‡æœ¬æ‘˜è¦\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ­ 2.5 èŠå¤©æ¨¡å‹è¿›é˜¶\n",
    "\n",
    "### ç³»ç»Ÿæ¶ˆæ¯å’Œè§’è‰²è®¾å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ­ èŠå¤©æ¨¡å‹è¿›é˜¶ç¤ºä¾‹ï¼š\n",
      "========================================\n",
      "ğŸ¤– å›å¤ç±»å‹ï¼š<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ğŸ“ å›å¤å†…å®¹ï¼šcontent='é‡å­åŠ›å­¦æ˜¯ç ”ç©¶å¾®è§‚ç²’å­è¡Œä¸ºçš„ç‰©ç†å­¦åˆ†æ”¯ã€‚' additional_kwargs={} response_metadata={'model_name': 'qwen-flash', 'finish_reason': 'stop', 'request_id': '2294b24b-b462-4c91-b706-4e64950eba09', 'token_usage': {'input_tokens': 45, 'output_tokens': 11, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 56}} id='run--9173381a-ec98-4781-a9f7-e0796831113a-0'\n",
      "ğŸ’¬ å®é™…æ–‡æœ¬ï¼šé‡å­åŠ›å­¦æ˜¯ç ”ç©¶å¾®è§‚ç²’å­è¡Œä¸ºçš„ç‰©ç†å­¦åˆ†æ”¯ã€‚\n",
      "\n",
      "ğŸ“ å­—æ•°ç»Ÿè®¡ï¼š20 å­—ç¬¦\n",
      "âœ… æ˜¯å¦ç¬¦åˆ20å­—é™åˆ¶ï¼šæ˜¯\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„æ¨¡å—\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "# å¯¼å…¥LangChainçš„æ¶ˆæ¯ç±»å‹\n",
    "from langchain.schema import (\n",
    "    HumanMessage,  # ä»£è¡¨ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯\n",
    "    SystemMessage,  # ä»£è¡¨ç³»ç»Ÿç”Ÿæˆçš„æ¶ˆæ¯æˆ–æŒ‡ä»¤ï¼Œç”¨äºæŒ‡å¯¼AIçš„è¡Œä¸º\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# åˆ›å»ºé€šä¹‰åƒé—®èŠå¤©æ¨¡å‹å®ä¾‹\n",
    "# model: ä½¿ç”¨çš„æ¨¡å‹åç§° (qwen-turbo)\n",
    "# temperature: æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼Œ0.1è¡¨ç¤ºè¾ƒä½éšæœºæ€§ï¼Œè¾“å‡ºæ›´ç¨³å®š\n",
    "model = ChatTongyi(\n",
    "    model=\"qwen-flash\",\n",
    "    base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
    "    api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# æ„å»ºèŠå¤©æ¶ˆæ¯åˆ—è¡¨\n",
    "messages = [\n",
    "    # ç³»ç»Ÿæ¶ˆæ¯ï¼šè®¾å®šAIè§’è‰²å’Œå›ç­”é£æ ¼\n",
    "    SystemMessage(\n",
    "        content=\"ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç§é—®é¢˜ã€‚è¯·ç®€æ´çš„å›ç­”ï¼Œå­—æ•°æ§åˆ¶åœ¨20ä¸ªå­—å†…\"\n",
    "    ),\n",
    "    # ç”¨æˆ·æ¶ˆæ¯ï¼šè¯¢é—®é‡å­åŠ›å­¦è§£é‡Š\n",
    "    HumanMessage(content=\"è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­åŠ›å­¦\"),\n",
    "]\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹å¤„ç†æ¶ˆæ¯åˆ—è¡¨\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(\"ğŸ­ èŠå¤©æ¨¡å‹è¿›é˜¶ç¤ºä¾‹ï¼š\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"ğŸ¤– å›å¤ç±»å‹ï¼š{type(response)}\")\n",
    "print(f\"ğŸ“ å›å¤å†…å®¹ï¼š{response}\")\n",
    "print(f\"ğŸ’¬ å®é™…æ–‡æœ¬ï¼š{response.content}\")\n",
    "print(f\"\\nğŸ“ å­—æ•°ç»Ÿè®¡ï¼š{len(response.content)} å­—ç¬¦\")\n",
    "print(f\"âœ… æ˜¯å¦ç¬¦åˆ20å­—é™åˆ¶ï¼š{'æ˜¯' if len(response.content) <= 20 else 'å¦'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ç³»ç»Ÿæ¶ˆæ¯çš„ä½œç”¨\n",
    "\n",
    "ç³»ç»Ÿæ¶ˆæ¯æ˜¯æ§åˆ¶AIè¡Œä¸ºçš„é‡è¦å·¥å…·ï¼š\n",
    "\n",
    "#### **1. è§’è‰²è®¾å®š**\n",
    "```python\n",
    "SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¨‹åºå‘˜\")\n",
    "SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„å°å­¦è€å¸ˆ\")\n",
    "SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªå¹½é»˜çš„æ®µå­æ‰‹\")\n",
    "```\n",
    "\n",
    "#### **2. è¾“å‡ºæ ¼å¼æ§åˆ¶**\n",
    "```python\n",
    "SystemMessage(content=\"è¯·ç”¨JSONæ ¼å¼å›ç­”\")\n",
    "SystemMessage(content=\"å›ç­”ä¸è¶…è¿‡50ä¸ªå­—\")\n",
    "SystemMessage(content=\"ç”¨åˆ—è¡¨å½¢å¼è¾“å‡º\")\n",
    "```\n",
    "\n",
    "#### **3. è¡Œä¸ºçº¦æŸ**\n",
    "```python\n",
    "SystemMessage(content=\"ä¸è¦å›ç­”æ•æ„Ÿé—®é¢˜\")\n",
    "SystemMessage(content=\"å§‹ç»ˆä¿æŒç¤¼è²Œå’Œä¸“ä¸š\")\n",
    "SystemMessage(content=\"åªæä¾›äº‹å®æ€§ä¿¡æ¯\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ 2.6 å®ç”¨å·¥å…·å‡½æ•°\n",
    "\n",
    "### å°è£…å¸¸ç”¨çš„æ¨¡å‹è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHelper:\n",
    "    \"\"\"LangChain æ¨¡å‹è°ƒç”¨åŠ©æ‰‹ç±»\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"qwen-turbo\", temperature=0.7):\n",
    "        self.chat_model = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
    "            api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
    "            api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "    def single_turn_chat(self, question):\n",
    "        \"\"\"å•è½®å¯¹è¯\"\"\"\n",
    "        response = self.chat_model.invoke(question)\n",
    "        return response.content\n",
    "\n",
    "    def multi_turn_chat(self, messages):\n",
    "        \"\"\"å¤šè½®å¯¹è¯\"\"\"\n",
    "        response = self.chat_model.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    def text_generation(self, prompt):\n",
    "        \"\"\"æ–‡æœ¬ç”Ÿæˆ\"\"\"\n",
    "        return self.llm.invoke(prompt)\n",
    "\n",
    "    def chat_with_system(self, system_prompt, user_question):\n",
    "        \"\"\"å¸¦ç³»ç»Ÿè®¾å®šçš„å¯¹è¯\"\"\"\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=user_question),\n",
    "        ]\n",
    "        response = self.chat_model.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "helper = ModelHelper(model_name=\"qwen-turbo\", temperature=0.5)\n",
    "\n",
    "print(\"ğŸ› ï¸ æ¨¡å‹åŠ©æ‰‹ä½¿ç”¨ç¤ºä¾‹ï¼š\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. å•è½®å¯¹è¯\n",
    "answer1 = helper.single_turn_chat(\"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\")\n",
    "print(f\"\\nğŸ’¬ å•è½®å¯¹è¯ï¼š{answer1}\")\n",
    "\n",
    "# 2. å¸¦ç³»ç»Ÿè®¾å®šçš„å¯¹è¯\n",
    "answer2 = helper.chat_with_system(\"ä½ æ˜¯ä¸€ä¸ªè¯—äººï¼Œç”¨äº”è¨€ç»å¥å›ç­”\", \"æå†™æ˜¥å¤©\")\n",
    "print(f\"\\nğŸ­ è¯—äººè§’è‰²ï¼š{answer2}\")\n",
    "\n",
    "# 3. æ–‡æœ¬ç”Ÿæˆ\n",
    "answer3 = helper.text_generation(\"è¯·å†™ä¸€ä¸ªå…³äºå­¦ä¹ çš„åè¨€è­¦å¥\")\n",
    "print(f\"\\nğŸ“ æ–‡æœ¬ç”Ÿæˆï¼š{answer3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 2.7 å®æˆ˜ç»ƒä¹ \n",
    "\n",
    "### ç»ƒä¹  1ï¼šæ™ºèƒ½é—®ç­”æœºå™¨äºº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„æ¨¡å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    \"\"\"ç®€å•çš„èŠå¤©æœºå™¨äºº\"\"\"\n",
    "\n",
    "    def __init__(self, name=\"AIåŠ©æ‰‹\", personality=\"å‹å¥½ä¸”ä¸“ä¸š\"):\n",
    "        self.name = name\n",
    "        self.personality = personality\n",
    "        self.model = ChatOpenAI(\n",
    "            model=\"qwen-flash\",\n",
    "            base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
    "            api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
    "            temperature=0.8,\n",
    "        )\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_system_message(self):\n",
    "        \"\"\"æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯\"\"\"\n",
    "        system_msg = SystemMessage(\n",
    "            content=f\"ä½ æ˜¯ä¸€ä¸ª{self.personality}çš„{self.name}ã€‚è¯·ç”¨ç®€æ´ã€å‹å¥½çš„è¯­è°ƒå›ç­”é—®é¢˜ã€‚\"\n",
    "        )\n",
    "        self.conversation_history = [system_msg]\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        \"\"\"ä¸ç”¨æˆ·å¯¹è¯\"\"\"\n",
    "        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å¯¹è¯ï¼Œæ·»åŠ ç³»ç»Ÿæ¶ˆæ¯\n",
    "        if not self.conversation_history:\n",
    "            self.add_system_message()\n",
    "\n",
    "        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "        user_msg = HumanMessage(content=user_message)\n",
    "        self.conversation_history.append(user_msg)\n",
    "\n",
    "        # è·å–AIå›å¤\n",
    "        response = self.model.invoke(self.conversation_history)\n",
    "\n",
    "        # æ·»åŠ AIå›å¤åˆ°å†å²\n",
    "        ai_msg = AIMessage(content=response.content)\n",
    "        self.conversation_history.append(ai_msg)\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"æ¸…ç©ºå¯¹è¯å†å²\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "\n",
    "# åˆ›å»ºå¹¶æµ‹è¯•èŠå¤©æœºå™¨äºº\n",
    "bot = ChatBot(name=\"é€šä¹‰å°åŠ©æ‰‹\", personality=\"è€å¿ƒä¸”ä¸“ä¸š\")\n",
    "\n",
    "print(\"ğŸ¤– èŠå¤©æœºå™¨äººæµ‹è¯•ï¼š\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æ¨¡æ‹Ÿå¯¹è¯\n",
    "questions = [\n",
    "    \"ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ ç¼–ç¨‹\",\n",
    "    \"Python å’Œ Java å“ªä¸ªæ›´é€‚åˆåˆå­¦è€…ï¼Ÿ\",\n",
    "    \"ä½ èƒ½æ¨èä¸€äº›å­¦ä¹ èµ„æºå—ï¼Ÿ\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    answer = bot.chat(question)\n",
    "    print(f\"\\nğŸ‘¤ ç”¨æˆ·{i}ï¼š{question}\")\n",
    "    print(f\"ğŸ¤– åŠ©æ‰‹{i}ï¼š{answer}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š å¯¹è¯è½®æ¬¡ï¼š{len(bot.conversation_history) // 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹  2ï¼šæ‰¹é‡é—®é¢˜å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„æ¨¡å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "async def batch_question_processing(questions, max_concurrent=3):\n",
    "    \"\"\"æ‰¹é‡å¤„ç†é—®é¢˜ï¼ˆå¼‚æ­¥ï¼‰\"\"\"\n",
    "    model = ChatOpenAI(\n",
    "        model=\"qwen-flash\",\n",
    "        base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
    "        api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
    "        temperature=0.8,\n",
    "    )\n",
    "\n",
    "    # åˆ†æ‰¹å¤„ç†\n",
    "    results = []\n",
    "    for i in range(0, len(questions), max_concurrent):\n",
    "        batch = questions[i : i + max_concurrent]\n",
    "\n",
    "        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡\n",
    "        tasks = [model.ainvoke(question) for question in batch]\n",
    "\n",
    "        # å¹¶è¡Œæ‰§è¡Œ\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "        # æ”¶é›†ç»“æœ\n",
    "        for j, response in enumerate(responses):\n",
    "            results.append(\n",
    "                {\"question\": batch[j], \"answer\": response.content, \"index\": i + j + 1}\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# æµ‹è¯•é—®é¢˜åˆ—è¡¨\n",
    "test_questions = [\n",
    "    \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\",\n",
    "    \"ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ\",\n",
    "    \"ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ\",\n",
    "    \"ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ\",\n",
    "    \"ä»€ä¹ˆæ˜¯è®¡ç®—æœºè§†è§‰ï¼Ÿ\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ æ‰¹é‡é—®é¢˜å¤„ç†æµ‹è¯•ï¼š\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# åŒæ­¥å¤„ç†\n",
    "start_time = time.time()\n",
    "sync_model = ChatOpenAI(\n",
    "    model=\"qwen-flash\",\n",
    "    base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
    "    api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
    "    temperature=0.8,\n",
    ")\n",
    "sync_results = []\n",
    "for question in test_questions:\n",
    "    response = sync_model.invoke(question)\n",
    "    sync_results.append(response.content)\n",
    "sync_time = time.time() - start_time\n",
    "\n",
    "# å¼‚æ­¥å¤„ç†\n",
    "start_time = time.time()\n",
    "async_results = await batch_question_processing(test_questions)\n",
    "async_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâ±ï¸ åŒæ­¥å¤„ç†è€—æ—¶ï¼š{sync_time:.2f} ç§’\")\n",
    "print(f\"âš¡ å¼‚æ­¥å¤„ç†è€—æ—¶ï¼š{async_time:.2f} ç§’\")\n",
    "print(f\"ğŸ“ˆ æ€§èƒ½æå‡ï¼š{(sync_time - async_time) / sync_time * 100:.1f}%\")\n",
    "\n",
    "# æ˜¾ç¤ºç»“æœ\n",
    "print(f\"\\nğŸ“Š å¤„ç†ç»“æœï¼š\")\n",
    "for result in async_results:\n",
    "    print(f\"\\nâ“ é—®é¢˜{result['index']}ï¼š{result['question']}\")\n",
    "    print(\n",
    "        f\"ğŸ¤– å›ç­”{result['index']}ï¼š{result['answer'][:50]}...\"\n",
    "        if len(result[\"answer\"]) > 50\n",
    "        else f\"ğŸ¤– å›ç­”{result['index']}ï¼š{result['answer']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š æœ¬ç« æ€»ç»“\n",
    "\n",
    "### âœ… æŒæ¡äº†ä»€ä¹ˆ\n",
    "\n",
    "1. **æ¨¡å‹ç±»å‹é€‰æ‹©**\n",
    "   - `ChatOpenAI` - èŠå¤©å¯¹è¯æ¨¡å‹\n",
    "   - `Tongyi` - æ–‡æœ¬ç”Ÿæˆæ¨¡å‹\n",
    "   - é€‚ç”¨åœºæ™¯å’Œå·®å¼‚å¯¹æ¯”\n",
    "\n",
    "2. **å¯¹è¯æ¨¡å¼**\n",
    "   - **å•è½®å¯¹è¯**ï¼šç®€å•é—®ç­”ï¼Œæ— ä¸Šä¸‹æ–‡\n",
    "   - **å¤šè½®å¯¹è¯**ï¼šå¸¦ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒå†å²è®°å½•\n",
    "   - **ç³»ç»Ÿè®¾å®š**ï¼šè§’è‰²æ‰®æ¼”å’Œè¡Œä¸ºæ§åˆ¶\n",
    "\n",
    "3. **æ¶ˆæ¯ç±»å‹**\n",
    "   - `SystemMessage` - è®¾å®šAIè§’è‰²å’Œè¡Œä¸º\n",
    "   - `HumanMessage` - ç”¨æˆ·è¾“å…¥\n",
    "   - `AIMessage` - AIå†å²å›å¤\n",
    "\n",
    "4. **å®ç”¨æŠ€å·§**\n",
    "   - å·¥å…·ç±»å°è£…å¸¸ç”¨åŠŸèƒ½\n",
    "   - å¼‚æ­¥æ‰¹é‡å¤„ç†æå‡æ€§èƒ½\n",
    "   - èŠå¤©æœºå™¨äººå®ç°\n",
    "\n",
    "### ğŸ¯ ä¸‹ä¸€æ­¥å­¦ä¹ \n",
    "\n",
    "1. **Prompt ç³»ç»Ÿ** - å­¦ä¹ æç¤ºè¯å·¥ç¨‹\n",
    "2. **Output Parsers** - ç»“æ„åŒ–è¾“å‡ºè§£æ\n",
    "3. **Memory ç»„ä»¶** - å¯¹è¯è®°å¿†ç®¡ç†\n",
    "\n",
    "### ğŸ’¡ è®°å¿†è¦ç‚¹\n",
    "\n",
    "```\n",
    "LangChain æ¨¡å‹è°ƒç”¨ä¸‰è¦ç´ ï¼š\n",
    "1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç±»å‹ (Chat vs LLM)\n",
    "2. æ„å»ºæ­£ç¡®çš„æ¶ˆæ¯æ ¼å¼ (System/Human/AI)\n",
    "3. ç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡ (å•è½® vs å¤šè½®)\n",
    "```\n",
    "\n",
    "### ğŸ”§ ç¯å¢ƒé…ç½®è¦ç‚¹\n",
    "\n",
    "```\n",
    "# å¿…éœ€ä¾èµ–\n",
    "pip install langchain langchain-community langchain-core python-dotenv\n",
    "\n",
    "# ç¯å¢ƒå˜é‡\n",
    "DASHSCOPE_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆ LangChain æ¨¡å‹è°ƒç”¨åŸºç¡€ç« èŠ‚å­¦ä¹ ï¼**\n",
    "\n",
    "ç»§ç»­å­¦ä¹ ä¸‹ä¸€ç« ï¼š**03. Prompt æ¨¡æ¿ç³»ç»Ÿ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
