{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain 通义千问集成示例\n",
        "\n",
        "本notebook展示了LangChain与通义千问的不同集成方式：\n",
        "1. 通义千问（OpenAI兼容接口）\n",
        "2. 通义千问（社区版）\n",
        "3. 通义千问基础配置示例\n",
        "4. 通义千问高级配置示例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 通义千问（OpenAI兼容接口）示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "你好！我是通义千问（Qwen），是阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我能够回答问题、创作文字，比如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。如果你有任何问题或需要帮助，欢迎随时告诉我！😊\n"
          ]
        }
      ],
      "source": [
        "# 导入LangChain的OpenAI兼容聊天模型\n",
        "from langchain_openai import ChatOpenAI\n",
        "# 导入操作系统模块，用于访问环境变量\n",
        "import os\n",
        "# 导入dotenv用于加载环境变量\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 从.env文件加载环境变量\n",
        "load_dotenv()\n",
        "\n",
        "# 创建通义千问聊天模型实例（使用OpenAI兼容接口）\n",
        "# model: 使用的模型名称 (qwen-max是通义千问最强模型)\n",
        "# base_url: 通义千问API的兼容模式端点地址\n",
        "# api_key: 从环境变量中获取通义千问API密钥\n",
        "# temperature: 控制输出随机性，0表示最确定性的输出\n",
        "llm = ChatOpenAI(\n",
        "    model=\"qwen-flash\",\n",
        "    base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
        "    api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# 调用模型，发送问题\n",
        "response = llm.invoke(\"你是谁？\")\n",
        "\n",
        "# 打印模型的回复内容\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 通义千问（社区版）示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "你好！我是通义千问（Qwen），是阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我能够回答问题、创作文字，比如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。如果你有任何问题或需要帮助，尽管告诉我，我会尽力提供支持！😊\n"
          ]
        }
      ],
      "source": [
        "# 导入操作系统模块，用于访问环境变量\n",
        "import os\n",
        "# 导入LangChain的通义千问聊天模型\n",
        "from langchain_community.chat_models.tongyi import ChatTongyi\n",
        "# 导入dotenv用于加载环境变量\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 从.env文件加载环境变量\n",
        "load_dotenv()\n",
        "\n",
        "# 创建通义千问聊天模型实例\n",
        "# model: 使用的模型名称 (qwen-max是通义千问最强模型)\n",
        "# api_key: 从环境变量中获取通义千问API密钥\n",
        "# base_url: 注释掉了自定义base_url，使用默认的DashScope端点\n",
        "tongyi_chat = ChatTongyi(\n",
        "    model=\"qwen-flash\",\n",
        "    api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
        "    # base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
        ")\n",
        "# 调用模型，发送问题\n",
        "response = tongyi_chat.invoke(\"你是谁？\")\n",
        "# 打印模型的回复内容\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 通义千问基础配置示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain 是一个开源的 Python 框架，旨在简化大型语言模型（LLM, Large Language Model）在实际应用中的集成与开发。它由 Harrison Chase 于 2022 年创建，迅速成为构建基于 LLM 的应用程序的主流工具之一。其核心目标是让开发者能够更高效地将语言模型与外部数据源、工具和系统结合，从而构建智能、上下文感知的应用程序。\n",
            "\n",
            "---\n",
            "\n",
            "### 🌟 核心理念\n",
            "LangChain 的核心思想是“**链式思维**”（Chain of Thought），即通过将多个组件（如提示词模板、数据检索、模型调用、工具调用等）串联成可复用的“链”（Chain），实现复杂任务的自动化处理。\n",
            "\n",
            "---\n",
            "\n",
            "### ✅ 主要功能与特性\n",
            "\n",
            "#### 1. **模块化设计**\n",
            "LangChain 提供了丰富的模块，支持灵活组合：\n",
            "- **Prompts（提示工程）**：支持动态提示生成，如 `PromptTemplate`、`FewShotPromptTemplate`。\n",
            "- **Models（模型接口）**：统一接口支持 OpenAI、Anthropic、Hugging Face、Google Gemini 等多种 LLM。\n",
            "- **Memory（记忆机制）**：支持短期记忆（如 `ConversationBufferMemory`）、长期记忆（如向量数据库存储对话历史）。\n",
            "- **Retrieval（检索增强生成，RAG）**：连接外部知识库（如 FAISS、Chroma、Elasticsearch），实现基于文档的问答。\n",
            "- **Agents（智能体）**：允许模型自主决定使用哪些工具（如计算器、搜索引擎、数据库查询）完成任务。\n",
            "- **Tools（工具）**：内置或自定义工具（如 Wikipedia 检索、代码执行、API 调用等）。\n",
            "- **Callbacks（回调机制）**：用于日志记录、性能监控、调试等。\n",
            "\n",
            "#### 2. **支持 RAG（Retrieval-Augmented Generation）**\n",
            "这是 LangChain 最重要的应用场景之一：\n",
            "- 将用户问题与从外部文档（如 PDF、网页、数据库）中检索到的相关内容结合，生成更准确的回答。\n",
            "- 支持多种向量数据库：FAISS、Chroma、Pinecone、Weaviate 等。\n",
            "\n",
            "#### 3. **Agent（智能体）能力**\n",
            "允许语言模型“思考并行动”：\n",
            "- 模型可以根据任务选择合适的工具（如搜索、计算、调用 API）。\n",
            "- 支持多步推理，适合复杂任务（如规划、决策、自动化流程）。\n",
            "\n",
            "#### 4. **易于扩展与自定义**\n",
            "- 可轻松自定义提示模板、记忆策略、工具逻辑。\n",
            "- 支持插件系统，社区贡献丰富。\n",
            "\n",
            "#### 5. **生态丰富**\n",
            "- 与 Hugging Face、OpenAI、LlamaIndex、Streamlit、FastAPI 等深度集成。\n",
            "- 社区活跃，提供大量示例和教程。\n",
            "\n",
            "---\n",
            "\n",
            "### 🔧 典型应用场景\n",
            "\n",
            "| 应用场景 | 说明 |\n",
            "|--------|------|\n",
            "| 智能客服机器人 | 结合企业文档，实现精准问答。 |\n",
            "| 个性化助手 | 基于用户历史行为提供定制建议。 |\n",
            "| 自动化报告生成 | 从数据库/文档中提取数据，生成自然语言报告。 |\n",
            "| 代码生成与解释 | 通过 RAG 提供上下文相关的代码建议。 |\n",
            "| 知识库问答系统 | 构建私有领域的 AI 助手（如法律、医疗知识库）。 |\n",
            "\n",
            "---\n",
            "\n",
            "### 📦 安装与快速入门\n",
            "\n",
            "```bash\n",
            "pip install langchain\n",
            "```\n",
            "\n",
            "#### 示例：简单问答（带检索）\n",
            "\n",
            "```python\n",
            "from langchain.chains import RetrievalQA\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.vectorstores import FAISS\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "from langchain.text_splitter import CharacterTextSplitter\n",
            "from langchain.document_loaders import TextLoader\n",
            "\n",
            "# 1. 加载文档\n",
            "loader = TextLoader(\"data.txt\")\n",
            "documents = loader.load()\n",
            "\n",
            "# 2. 分割文本\n",
            "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
            "docs = text_splitter.split_documents(documents)\n",
            "\n",
            "# 3. 创建向量存储\n",
            "embeddings = OpenAIEmbeddings()\n",
            "vectorstore = FAISS.from_documents(docs, embeddings)\n",
            "\n",
            "# 4. 构建检索问答链\n",
            "qa_chain = RetrievalQA.from_chain_type(\n",
            "    llm=OpenAI(temperature=0),\n",
            "    chain_type=\"stuff\",\n",
            "    retriever=vectorstore.as_retriever(),\n",
            "    input_key=\"query\"\n",
            ")\n",
            "\n",
            "# 5. 查询\n",
            "result = qa_chain({\"query\": \"什么是 LangChain？\"})\n",
            "print(result[\"result\"])\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### 🚀 优势总结\n",
            "\n",
            "- **降低开发门槛**：无需从零构建提示逻辑和流程。\n",
            "- **高度可扩展**：支持自定义组件与集成第三方服务。\n",
            "- **社区强大**：大量开源项目、模板和最佳实践。\n",
            "- **生产友好**：支持异步、缓存、流式输出、错误处理等。\n",
            "\n",
            "---\n",
            "\n",
            "### ⚠️ 注意事项\n",
            "\n",
            "- 对于敏感数据，需注意隐私与安全（如避免将私有数据暴露给公共 LLM）。\n",
            "- 需合理设计提示（prompt engineering），否则可能产生幻觉（hallucination）。\n",
            "- 性能受模型调用延迟影响，需考虑缓存与异步优化。\n",
            "\n",
            "---\n",
            "\n",
            "### 📚 学习资源\n",
            "\n",
            "- 官方文档：[https://python.langchain.com](https://python.langchain.com)\n",
            "- GitHub 仓库：[https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)\n",
            "- LangChain 教程（YouTube / Bilibili / Medium）\n",
            "- LangChain + Streamlit 快速搭建演示界面\n",
            "\n",
            "---\n",
            "\n",
            "### ✅ 总结\n",
            "\n",
            "> **LangChain 就是为大模型时代打造的“操作系统”** —— 它让开发者不再重复造轮子，而是专注于业务逻辑与用户体验，快速构建真正有用、智能的 AI 应用。\n",
            "\n",
            "无论你是初学者还是资深工程师，只要想用 LLM 做点实事，LangChain 都是一个值得掌握的强大工具。\n",
            "\n",
            "--- \n",
            "\n",
            "如果你有具体的应用场景（如想做智能客服、知识库、自动写文案等），我可以为你定制一个基于 LangChain 的实现方案！\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的模块\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# 加载环境变量\n",
        "load_dotenv()\n",
        "\n",
        "# 基础配置：使用通义千问qwen-turbo模型\n",
        "llm_basic = ChatOpenAI(\n",
        "    model=\"qwen-flash\",  # 使用较快的模型\n",
        "    base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
        "    api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
        "    temperature=0.7,  # 稍微增加一些随机性\n",
        ")\n",
        "\n",
        "# 测试基础功能\n",
        "response = llm_basic.invoke(\"请介绍一下LangChain框架\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 通义千问高级配置示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain是一个用于开发语言模型应用的框架，它提供了一系列工具和组件来帮助开发者更高效地构建基于大语言模型的应用程序。在LangChain中，“Chain”指的是将多个组件（如LLM、提示模板等）连接起来形成一个处理流程的概念。通过这种方式，可以创建复杂的逻辑流来完成特定任务，比如信息检索、文档问答等。\n",
            "\n",
            "### Chain的基本概念\n",
            "\n",
            "- **组件**：构成链条的基本单元，包括但不限于语言模型（LLMs）、索引器、向量数据库接口等。\n",
            "- **链接**：定义了数据如何从一个组件流向另一个组件的过程。每个链节可能包含输入转换、调用外部API或执行某些计算等功能。\n",
            "- **灵活性**：用户可以根据需要自定义链条结构，以适应不同的应用场景需求。\n",
            "\n",
            "### 例子：简单的问答系统\n",
            "\n",
            "假设我们要构建一个能够回答关于某个主题的问题的小型问答系统。这里我们可以使用到两个主要组件：一个是用来存储相关知识库的向量数据库；另一个是大型语言模型（LLM），负责生成最终的答案。\n",
            "\n",
            "1. **准备知识库**：首先，我们需要有一个已经填充好相关信息的知识库。这可以通过爬取网页内容、导入文档等方式实现，并将其转化为适合快速查询的形式存入向量数据库中。\n",
            "   \n",
            "2. **提问与检索**：当用户提出一个问题时，我们先利用该问题作为查询条件，在向量数据库中搜索最相关的文档片段。\n",
            "   \n",
            "3. **生成答案**：找到相关文档后，我们将这些文档连同原始问题一起传递给LLM。LLM会根据提供的上下文信息来生成一个准确且自然的回答。\n",
            "\n",
            "在这个过程中，“Chain”的作用就是将上述步骤串联起来：\n",
            "- 第一步可能是`EmbeddingChain`，负责将文本转换为向量形式；\n",
            "- 接着是`RetrievalChain`，基于用户的查询从向量数据库中获取相关信息；\n",
            "- 最后是`LLMChain`，接收检索结果并生成最终答案。\n",
            "\n",
            "通过这样的链条设计，即使面对复杂多变的需求，也能灵活调整各个部分的功能而不影响整体架构，从而使得开发更加模块化和可维护。\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的模块\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# 加载环境变量\n",
        "load_dotenv()\n",
        "\n",
        "# 高级配置：使用通义千问qwen-max模型\n",
        "llm_advanced = ChatOpenAI(\n",
        "    model=\"qwen-max\",  # 使用最强模型\n",
        "    base_url=os.environ.get(\"TONGYI_BASE_URL\"),\n",
        "    api_key=os.environ.get(\"TONGYI_API_KEY\"),\n",
        "    temperature=0.1,  # 低温度，更稳定的输出\n",
        "    max_tokens=1000,  # 限制最大输出token数\n",
        "    top_p=0.8,  # 核采样参数\n",
        ")\n",
        "\n",
        "# 使用消息格式进行对话\n",
        "messages = [\n",
        "    SystemMessage(content=\"你是一个专业的AI助手，专门帮助用户了解LangChain技术。\"),\n",
        "    HumanMessage(content=\"请详细解释LangChain中的Chain概念，并给出一个简单的例子。\")\n",
        "]\n",
        "\n",
        "# 调用模型\n",
        "response = llm_advanced.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "本notebook展示了通义千问的几种集成方式：\n",
        "\n",
        "1. **OpenAI兼容接口**: 使用`ChatOpenAI`类配合自定义base_url，最通用的方式\n",
        "2. **社区版接口**: 使用`ChatTongyi`类，通过langchain-community包\n",
        "3. **基础配置**: 使用qwen-turbo模型，适合快速响应场景\n",
        "4. **高级配置**: 使用qwen-max模型，支持更多参数调节\n",
        "\n",
        "### 环境配置要求\n",
        "在`.env`文件中需要配置：\n",
        "```\n",
        "TONGYI_API_KEY=your_api_key_here\n",
        "TONGYI_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1\n",
        "```\n",
        "\n",
        "### 模型选择\n",
        "- `qwen-turbo`: 快速响应，成本较低\n",
        "- `qwen-plus`: 平衡性能和成本\n",
        "- `qwen-max`: 最强性能，适合复杂任务"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
