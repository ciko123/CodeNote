{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - MessagesPlaceholder å†å²æ¶ˆæ¯\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹ ä½¿ç”¨ LangChain MessagesPlaceholder å¤„ç†å¯¹è¯å†å²æ¶ˆæ¯å ä½\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£å ä½ç¬¦ä½œç”¨æœºåˆ¶\n",
    "- æŒæ¡åŠ¨æ€å¡«å……å†å²æ¶ˆæ¯\n",
    "- èƒ½ä¸æ¶ˆæ¯å†å²ç»„ä»¶é›†æˆ\n",
    "- æ§åˆ¶å†å²æ¶ˆæ¯é•¿åº¦ä»¥èŠ‚çœæˆæœ¬\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MessagesPlaceholder åŸºç¡€ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MessagesPlaceholder æ¨¡æ¿åˆ›å»ºæˆåŠŸ\n",
      "æ¨¡æ¿æ¶ˆæ¯æ•°é‡: 3\n",
      "\n",
      "ğŸ“ æ ¼å¼åŒ–åçš„æ¶ˆæ¯:\n",
      "  1. System: ä½ æ˜¯ GPT-4o å¯¹è¯åŠ©æ‰‹ï¼Œæ ¹æ®å†å²ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n",
      "  2. Human: ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ\n",
      "  3. AI: LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚\n",
      "  4. Human: å®ƒæœ‰ä»€ä¹ˆä¸»è¦ç‰¹ç‚¹ï¼Ÿ\n",
      "\n",
      "ğŸ¤– GPT-4o-mini å›å¤: LangChain çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š\n",
      "\n",
      "1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šLangChain æä¾›äº†å¤šä¸ªå¯ç»„åˆçš„æ¨¡å—ï¼Œå…è®¸å¼€å‘è€…æ ¹æ®éœ€æ±‚çµæ´»æ„å»ºåº”ç”¨ã€‚\n",
      "\n",
      "2. **å¤šç§æ•°æ®æºæ”¯æŒ**ï¼šå®ƒæ”¯æŒä»ä¸åŒçš„æ•°æ®æºè·å–ä¿¡æ¯ï¼Œå¦‚ APIã€æ•°æ®åº“å’Œæ–‡æ¡£ç­‰ã€‚\n",
      "\n",
      "3. **é“¾å¼æ“ä½œ**ï¼šLangChain å…è®¸å°†å¤šä¸ªå¤„ç†æ­¥éª¤ä¸²è”åœ¨ä¸€èµ·ï¼Œå½¢æˆæ•°æ®å¤„ç†å’Œå†³ç­–çš„é“¾æ¡ã€‚\n",
      "\n",
      "4. **é›†æˆ\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šMessagesPlaceholder æ­£ç¡®å·¥ä½œ\n"
     ]
    }
   ],
   "source": [
    "# MessagesPlaceholder åŸºç¡€ä½¿ç”¨ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨è½»é‡æ¨¡å‹èŠ‚çœæˆæœ¬ï¼‰\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå¸¦å†å²æ¶ˆæ¯å ä½ç¬¦çš„æ¨¡æ¿\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ GPT-4o å¯¹è¯åŠ©æ‰‹ï¼Œæ ¹æ®å†å²ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{current_question}\")\n",
    "])\n",
    "\n",
    "print(\"âœ… MessagesPlaceholder æ¨¡æ¿åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"æ¨¡æ¿æ¶ˆæ¯æ•°é‡: {len(chat_template.messages)}\")\n",
    "\n",
    "# å‡†å¤‡å†å²æ¶ˆæ¯ï¼ˆé™åˆ¶æ•°é‡ä»¥æ§åˆ¶æˆæœ¬ï¼‰\n",
    "history_messages = [\n",
    "    HumanMessage(content=\"ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ\"),\n",
    "    AIMessage(content=\"LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚\")\n",
    "]\n",
    "\n",
    "# æµ‹è¯•æ¨¡æ¿\n",
    "current_question = \"å®ƒæœ‰ä»€ä¹ˆä¸»è¦ç‰¹ç‚¹ï¼Ÿ\"\n",
    "formatted_messages = chat_template.format_messages(\n",
    "    history=history_messages,\n",
    "    current_question=current_question\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ æ ¼å¼åŒ–åçš„æ¶ˆæ¯:\")\n",
    "for i, msg in enumerate(formatted_messages):\n",
    "    role = msg.__class__.__name__.replace('Message', '')\n",
    "    print(f\"  {i+1}. {role}: {msg.content}\")\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹\n",
    "response = llm.invoke(formatted_messages)\n",
    "print(f\"\\nğŸ¤– GPT-4o-mini å›å¤: {response.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šå ä½ç¬¦èƒ½è¢«å†å²æ¶ˆæ¯æ­£ç¡®å¡«å……\n",
    "assert len(formatted_messages) == 4, \"æ¶ˆæ¯æ•°é‡ä¸æ­£ç¡®\"  # system + 2 history + user\n",
    "assert \"å®ƒæœ‰ä»€ä¹ˆä¸»è¦ç‰¹ç‚¹ï¼Ÿ\" in formatted_messages[-1].content, \"å½“å‰é—®é¢˜æœªæ­£ç¡®å¡«å……\"\n",
    "assert len(response.content) > 0, \"å›å¤ä¸ºç©º\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šMessagesPlaceholder æ­£ç¡®å·¥ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŠ¨æ€å†å²æ¶ˆæ¯ç®¡ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åŠ¨æ€å†å²æ¶ˆæ¯æ¨¡æ¿åˆ›å»ºæˆåŠŸ\n",
      "\n",
      "ğŸ“š å½“å‰å†å²æ¶ˆæ¯æ•°é‡: 4\n",
      "\n",
      "ğŸ§ª æµ‹è¯• 1: æ¨èä¸€ä¸ªå­¦ä¹ èµ„æº\n",
      "   ä½¿ç”¨å†å²: 2 æ¡æ¶ˆæ¯\n",
      "   å›å¤: å¯ä»¥æ¨èã€ŠPython Crash Courseã€‹è¿™æœ¬ä¹¦ï¼Œé€‚åˆåˆå­¦è€…å¿«é€Ÿå…¥é—¨ã€‚ä¹Ÿå¯ä»¥è®¿é—® Coursera æˆ– edX ä¸Šçš„ Python è¯¾ç¨‹ã€‚\n",
      "\n",
      "ğŸ§ª æµ‹è¯• 2: æˆ‘è¯¥å¦‚ä½•å¼€å§‹ï¼Ÿ\n",
      "   ä½¿ç”¨å†å²: 3 æ¡æ¶ˆæ¯\n",
      "   å›å¤: ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤å¼€å§‹å­¦ä¹  Pythonï¼š\n",
      "\n",
      "1. **å®‰è£… Python**ï¼šè®¿é—® [Python å®˜ç½‘](https://www.python.org/) ä¸‹è½½å¹¶å®‰è£…æœ€æ–°ç‰ˆæœ¬ã€‚\n",
      "2. **é€‰æ‹©å­¦ä¹ èµ„æº**ï¼šå¯ä»¥ä½¿ç”¨åœ¨çº¿è¯¾ç¨‹ï¼ˆå¦‚ Courseraã€Codecademyï¼‰ã€ä¹¦ç±ï¼ˆå¦‚ã€ŠPython Crash Courseã€‹ï¼‰æˆ– YouTube æ•™ç¨‹ã€‚\n",
      "3. **å®è·µç¼–ç¨‹**ï¼šé€šè¿‡ç¼–\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šåŠ¨æ€å†å²æ¶ˆæ¯ç®¡ç†æ­£å¸¸å·¥ä½œ\n"
     ]
    }
   ],
   "source": [
    "# åŠ¨æ€å†å²æ¶ˆæ¯ç®¡ç† - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=80,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæ¨¡æ¿\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ GPT-4o åŠ©æ‰‹ï¼Œæ ¹æ®å¯¹è¯å†å²æä¾›ç®€æ´å›ç­”ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "print(\"âœ… åŠ¨æ€å†å²æ¶ˆæ¯æ¨¡æ¿åˆ›å»ºæˆåŠŸ\")\n",
    "\n",
    "# æ¨¡æ‹Ÿå¯¹è¯å†å²ï¼ˆé™åˆ¶é•¿åº¦æ§åˆ¶æˆæœ¬ï¼‰\n",
    "conversation_history = [\n",
    "    HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹  Pythonã€‚\"),\n",
    "    AIMessage(content=\"å¾ˆå¥½çš„é€‰æ‹©ï¼Python æ˜¯ä¸€é—¨æ˜“å­¦çš„ç¼–ç¨‹è¯­è¨€ã€‚\"),\n",
    "    HumanMessage(content=\"Python é€‚åˆåšä»€ä¹ˆï¼Ÿ\"),\n",
    "    AIMessage(content=\"Python é€‚åˆæ•°æ®åˆ†æã€Web å¼€å‘ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚\")\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ“š å½“å‰å†å²æ¶ˆæ¯æ•°é‡: {len(conversation_history)}\")\n",
    "\n",
    "# æµ‹è¯•ä¸åŒé•¿åº¦çš„å†å²æ¶ˆæ¯\n",
    "test_questions = [\n",
    "    \"æ¨èä¸€ä¸ªå­¦ä¹ èµ„æº\",\n",
    "    \"æˆ‘è¯¥å¦‚ä½•å¼€å§‹ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nğŸ§ª æµ‹è¯• {i}: {question}\")\n",
    "    \n",
    "    # ä½¿ç”¨éƒ¨åˆ†å†å²æ¶ˆæ¯ï¼ˆæ¨¡æ‹ŸåŠ¨æ€ç®¡ç†ï¼‰\n",
    "    recent_history = conversation_history[-(i+1):]  # åŠ¨æ€è°ƒæ•´å†å²é•¿åº¦\n",
    "    \n",
    "    formatted_messages = template.format_messages(\n",
    "        chat_history=recent_history,\n",
    "        input=question\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(formatted_messages)\n",
    "    \n",
    "    print(f\"   ä½¿ç”¨å†å²: {len(recent_history)} æ¡æ¶ˆæ¯\")\n",
    "    print(f\"   å›å¤: {response.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šåŠ¨æ€å†å²æ¶ˆæ¯ç®¡ç†\n",
    "assert len(conversation_history) == 4, \"å†å²æ¶ˆæ¯æ•°é‡ä¸æ­£ç¡®\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šåŠ¨æ€å†å²æ¶ˆæ¯ç®¡ç†æ­£å¸¸å·¥ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å†å²æ¶ˆæ¯é•¿åº¦ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’° å†å²æ¶ˆæ¯é•¿åº¦ä¼˜åŒ–æµ‹è¯•:\n",
      "========================================\n",
      "\n",
      "ğŸ“ æµ‹è¯•å†å²é•¿åº¦: 0\n",
      "   å›å¤: å½“ç„¶ï¼Œå¯ä»¥è¯·æ‚¨æä¾›è®¨è®ºçš„å…·ä½“å†…å®¹æˆ–ä¸»é¢˜å—ï¼Ÿè¿™æ ·æˆ‘èƒ½æ›´å¥½åœ°ä¸ºæ‚¨æ€»ç»“ã€‚\n",
      "   é•¿åº¦: 33 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ“ æµ‹è¯•å†å²é•¿åº¦: 2\n",
      "   å›å¤: è¯·æä¾›è®¨è®ºçš„å…·ä½“å†…å®¹ï¼Œæˆ‘å°†ä¸ºæ‚¨æ€»ç»“ä¸»è¦è¦ç‚¹ã€‚\n",
      "   é•¿åº¦: 22 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ“ æµ‹è¯•å†å²é•¿åº¦: 4\n",
      "   å›å¤: è¯·æä¾›è®¨è®ºçš„å…·ä½“å†…å®¹æˆ–ä¸»é¢˜ï¼Œä»¥ä¾¿æˆ‘ä¸ºæ‚¨æ€»ç»“ä¸»è¦å†…å®¹ã€‚\n",
      "   é•¿åº¦: 26 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ“ æµ‹è¯•å†å²é•¿åº¦: 6\n",
      "   å›å¤: è¯·æä¾›è®¨è®ºçš„å…·ä½“å†…å®¹æˆ–ä¸»é¢˜ï¼Œæˆ‘å°†ä¸ºæ‚¨æ€»ç»“ä¸»è¦å†…å®¹ã€‚\n",
      "   é•¿åº¦: 25 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ“Š å†å²æ¶ˆæ¯é•¿åº¦ä¼˜åŒ–æ€»ç»“:\n",
      "âœ… æˆåŠŸæµ‹è¯•çš„é•¿åº¦: 4/4\n",
      "\n",
      "ğŸ“ˆ ä¸åŒå†å²é•¿åº¦å›å¤å¯¹æ¯”:\n",
      "   å†å² 0 æ¡: 33 å­—ç¬¦\n",
      "   å†å² 2 æ¡: 22 å­—ç¬¦\n",
      "   å†å² 4 æ¡: 26 å­—ç¬¦\n",
      "   å†å² 6 æ¡: 25 å­—ç¬¦\n",
      "\n",
      "ğŸ’¡ æˆæœ¬ä¼˜åŒ–å»ºè®®:\n",
      "   - é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡ï¼ˆå»ºè®® 2-4 æ¡ï¼‰\n",
      "   - ä½¿ç”¨æ¶ˆæ¯æ‘˜è¦æŠ€æœ¯å‹ç¼©é•¿å†å²\n",
      "   - æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å†å²é•¿åº¦\n",
      "   - å®šæœŸæ¸…ç†ä¸ç›¸å…³çš„å†å²æ¶ˆæ¯\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šå†å²æ¶ˆæ¯é•¿åº¦ä¼˜åŒ–æœ‰æ•ˆ\n"
     ]
    }
   ],
   "source": [
    "# å†å²æ¶ˆæ¯é•¿åº¦ä¼˜åŒ– - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ’° å†å²æ¶ˆæ¯é•¿åº¦ä¼˜åŒ–æµ‹è¯•:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=60,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæ¨¡æ¿\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ç®€æ´çš„ GPT-4o åŠ©æ‰‹ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡å›ç­”ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# ç”Ÿæˆä¸åŒé•¿åº¦çš„å†å²æ¶ˆæ¯\n",
    "def generate_history(length):\n",
    "    \"\"\"ç”ŸæˆæŒ‡å®šé•¿åº¦çš„å†å²æ¶ˆæ¯\"\"\"\n",
    "    history = []\n",
    "    for i in range(length):\n",
    "        if i % 2 == 0:\n",
    "            history.append(HumanMessage(content=f\"é—®é¢˜ {i//2 + 1}\"))\n",
    "        else:\n",
    "            history.append(AIMessage(content=f\"å›ç­” {i//2 + 1}\"))\n",
    "    return history\n",
    "\n",
    "# æµ‹è¯•ä¸åŒå†å²é•¿åº¦çš„å½±å“\n",
    "history_lengths = [0, 2, 4, 6]  # é™åˆ¶æœ€å¤§é•¿åº¦æ§åˆ¶æˆæœ¬\n",
    "test_question = \"æ€»ç»“ä¸€ä¸‹è®¨è®ºçš„ä¸»è¦å†…å®¹\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "for length in history_lengths:\n",
    "    print(f\"\\nğŸ“ æµ‹è¯•å†å²é•¿åº¦: {length}\")\n",
    "    \n",
    "    try:\n",
    "        # ç”Ÿæˆå†å²æ¶ˆæ¯\n",
    "        history = generate_history(length)\n",
    "        \n",
    "        # æ ¼å¼åŒ–æ¶ˆæ¯\n",
    "        if length == 0:\n",
    "            # æ— å†å²æ¶ˆæ¯çš„æƒ…å†µ\n",
    "            messages = [\n",
    "                {\"type\": \"system\", \"content\": \"ä½ æ˜¯ç®€æ´çš„ GPT-4o åŠ©æ‰‹ã€‚\"},\n",
    "                {\"type\": \"user\", \"content\": test_question}\n",
    "            ]\n",
    "            formatted_messages = [msg for msg in messages]  # ç®€åŒ–å¤„ç†\n",
    "        else:\n",
    "            formatted_messages = template.format_messages(\n",
    "                history=history,\n",
    "                question=test_question\n",
    "            )\n",
    "        \n",
    "        # è°ƒç”¨æ¨¡å‹\n",
    "        response = llm.invoke(formatted_messages)\n",
    "        \n",
    "        # è®°å½•ç»“æœ\n",
    "        results[length] = {\n",
    "            \"response\": response.content,\n",
    "            \"length\": len(response.content),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"   å›å¤: {response.content}\")\n",
    "        print(f\"   é•¿åº¦: {len(response.content)} å­—ç¬¦\")\n",
    "        print(f\"   çŠ¶æ€: âœ… æˆåŠŸ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   çŠ¶æ€: âŒ å¤±è´¥ - {e}\")\n",
    "        results[length] = {\n",
    "            \"response\": None,\n",
    "            \"length\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# é•¿åº¦ä¼˜åŒ–æ€»ç»“\n",
    "print(f\"\\nğŸ“Š å†å²æ¶ˆæ¯é•¿åº¦ä¼˜åŒ–æ€»ç»“:\")\n",
    "successful_lengths = [length for length, result in results.items() if result[\"success\"]]\n",
    "print(f\"âœ… æˆåŠŸæµ‹è¯•çš„é•¿åº¦: {len(successful_lengths)}/{len(history_lengths)}\")\n",
    "\n",
    "if len(successful_lengths) >= 2:\n",
    "    print(\"\\nğŸ“ˆ ä¸åŒå†å²é•¿åº¦å›å¤å¯¹æ¯”:\")\n",
    "    for length in successful_lengths:\n",
    "        result = results[length]\n",
    "        print(f\"   å†å² {length} æ¡: {result['length']} å­—ç¬¦\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ æˆæœ¬ä¼˜åŒ–å»ºè®®:\")\n",
    "    print(\"   - é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡ï¼ˆå»ºè®® 2-4 æ¡ï¼‰\")\n",
    "    print(\"   - ä½¿ç”¨æ¶ˆæ¯æ‘˜è¦æŠ€æœ¯å‹ç¼©é•¿å†å²\")\n",
    "    print(\"   - æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å†å²é•¿åº¦\")\n",
    "    print(\"   - å®šæœŸæ¸…ç†ä¸ç›¸å…³çš„å†å²æ¶ˆæ¯\")\n",
    "    print(\"âœ… éªŒè¯é€šè¿‡ï¼šå†å²æ¶ˆæ¯é•¿åº¦ä¼˜åŒ–æœ‰æ•ˆ\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Šï¼šå†å²é•¿åº¦æµ‹è¯•å¤±è´¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. é“¾å¼è°ƒç”¨é›†æˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MessagesPlaceholder é“¾å¼è°ƒç”¨åˆ›å»ºæˆåŠŸ\n",
      "ğŸ”— é“¾ç»“æ„: ChatPromptTemplate â†’ ChatOpenAI (GPT-4o-mini) â†’ StrOutputParser\n",
      "\n",
      "ğŸ¤– é“¾å¼è°ƒç”¨ç»“æœ: å½“ç„¶å¯ä»¥ï¼ä¸€ä¸ªå¸¸è§çš„æœºå™¨å­¦ä¹ ä¾‹å­æ˜¯åƒåœ¾é‚®ä»¶è¿‡æ»¤ã€‚ \n",
      "\n",
      "åœ¨è¿™ä¸ªåº”ç”¨ä¸­ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•ä¼šåˆ†æå¤§é‡çš„ç”µå­é‚®ä»¶ï¼Œå­¦ä¹ å“ªäº›ç‰¹å¾é€šå¸¸å‡ºç°åœ¨åƒåœ¾é‚®ä»¶ä¸­ï¼ˆæ¯”å¦‚ç‰¹å®šçš„å…³é”®è¯ã€å‘ä»¶äººåœ°å€æˆ–é‚®ä»¶æ ¼å¼ï¼‰ã€‚é€šè¿‡å¯¹è¿™äº›ç‰¹å¾çš„å­¦ä¹ ï¼Œç®—æ³•èƒ½å¤Ÿæ ¹æ®æ–°çš„é‚®ä»¶å†…å®¹åˆ¤æ–­å®ƒä»¬æ˜¯å¦å¯èƒ½æ˜¯åƒåœ¾é‚®ä»¶ã€‚å½“\n",
      "ğŸ“Š ç»“æœç±»å‹: <class 'str'>\n",
      "ğŸ“Š ç»“æœé•¿åº¦: 127 å­—ç¬¦\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šMessagesPlaceholder é“¾å¼è°ƒç”¨æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "# é“¾å¼è°ƒç”¨é›†æˆ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=80,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå¸¦å†å²æ¶ˆæ¯çš„æ¨¡æ¿\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ GPT-4o åŠ©æ‰‹ï¼Œæ ¹æ®å¯¹è¯å†å²æä¾›è¿è´¯å›ç­”ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# åˆ›å»º LCEL é“¾ï¼šchat_template | llm | parser\n",
    "chain = chat_template | llm | StrOutputParser()\n",
    "\n",
    "print(\"âœ… MessagesPlaceholder é“¾å¼è°ƒç”¨åˆ›å»ºæˆåŠŸ\")\n",
    "print(\"ğŸ”— é“¾ç»“æ„: ChatPromptTemplate â†’ ChatOpenAI (GPT-4o-mini) â†’ StrOutputParser\")\n",
    "\n",
    "# å‡†å¤‡å†å²æ¶ˆæ¯\n",
    "history = [\n",
    "    HumanMessage(content=\"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\"),\n",
    "    AIMessage(content=\"æœºå™¨å­¦ä¹ æ˜¯è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ çš„æŠ€æœ¯ã€‚\")\n",
    "]\n",
    "\n",
    "# æµ‹è¯•é“¾å¼è°ƒç”¨\n",
    "result = chain.invoke({\n",
    "    \"history\": history,\n",
    "    \"input\": \"èƒ½ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ\"\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ¤– é“¾å¼è°ƒç”¨ç»“æœ: {result}\")\n",
    "print(f\"ğŸ“Š ç»“æœç±»å‹: {type(result)}\")\n",
    "print(f\"ğŸ“Š ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šé“¾å¼è°ƒç”¨è¿”å›å­—ç¬¦ä¸²æ ¼å¼ç»“æœ\n",
    "assert isinstance(result, str), \"ç»“æœä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹\"\n",
    "assert len(result) > 0, \"ç»“æœä¸ºç©º\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šMessagesPlaceholder é“¾å¼è°ƒç”¨æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ MessagesPlaceholder å­¦ä¹ æ€»ç»“:\n",
      "========================================\n",
      "âœ… åŸºç¡€å ä½ç¬¦ï¼šhistory å˜é‡åŠ¨æ€å¡«å……\n",
      "âœ… åŠ¨æ€å†å²ç®¡ç†ï¼šå¯å˜é•¿åº¦å†å²æ¶ˆæ¯\n",
      "âœ… é•¿åº¦ä¼˜åŒ–ï¼šæˆæœ¬æ§åˆ¶ç­–ç•¥\n",
      "âœ… é“¾å¼è°ƒç”¨ï¼šLCEL è¯­æ³•é›†æˆ\n",
      "\n",
      "ğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: 4/4 é¡¹\n",
      "\n",
      "ğŸ’¡ MessagesPlaceholder æœ€ä½³å®è·µ:\n",
      "1. å†å²é•¿åº¦ï¼šé™åˆ¶åœ¨ 2-6 æ¡æ¶ˆæ¯æ§åˆ¶æˆæœ¬\n",
      "2. æ¶ˆæ¯æ ¼å¼ï¼šä½¿ç”¨ HumanMessage/AIMessage æ ‡å‡†æ ¼å¼\n",
      "3. åŠ¨æ€ç®¡ç†ï¼šæ ¹æ®ä¸Šä¸‹æ–‡ç›¸å…³æ€§è°ƒæ•´å†å²\n",
      "4. æˆæœ¬æ§åˆ¶ï¼šä¼˜å…ˆä½¿ç”¨ gpt-4o-mini æ¨¡å‹\n",
      "5. é“¾å¼è°ƒç”¨ï¼šç»“åˆ LCEL æé«˜å¼€å‘æ•ˆç‡\n",
      "\n",
      "ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\n",
      "1. æ·±å…¥å­¦ä¹  Structured Output ç»“æ„åŒ–è¾“å‡º\n",
      "2. æŒæ¡ ConversationBufferMemory è®°å¿†ç®¡ç†\n",
      "3. å­¦ä¹  ConversationSummaryMemory æ‘˜è¦è®°å¿†\n",
      "4. æ¢ç´¢ RunnableWithMessageHistory å†å²é›†æˆ\n",
      "5. å®è·µå¤šè½®å¯¹è¯çŠ¶æ€ç®¡ç†\n",
      "\n",
      "ğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: æˆ‘å­¦ä¼šäº†ä½¿ç”¨ MessagesPlaceholder æ¥å¤„ç†å’Œæ›¿æ¢æ¶ˆæ¯ä¸­çš„å ä½ç¬¦ã€‚\n",
      "\n",
      "âœ… MessagesPlaceholder å­¦ä¹ å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ MessagesPlaceholder å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… åŸºç¡€å ä½ç¬¦ï¼šhistory å˜é‡åŠ¨æ€å¡«å……\",\n",
    "    \"âœ… åŠ¨æ€å†å²ç®¡ç†ï¼šå¯å˜é•¿åº¦å†å²æ¶ˆæ¯\",\n",
    "    \"âœ… é•¿åº¦ä¼˜åŒ–ï¼šæˆæœ¬æ§åˆ¶ç­–ç•¥\",\n",
    "    \"âœ… é“¾å¼è°ƒç”¨ï¼šLCEL è¯­æ³•é›†æˆ\",\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/4 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ MessagesPlaceholder æœ€ä½³å®è·µ:\")\n",
    "print(\"1. å†å²é•¿åº¦ï¼šé™åˆ¶åœ¨ 2-6 æ¡æ¶ˆæ¯æ§åˆ¶æˆæœ¬\")\n",
    "print(\"2. æ¶ˆæ¯æ ¼å¼ï¼šä½¿ç”¨ HumanMessage/AIMessage æ ‡å‡†æ ¼å¼\")\n",
    "print(\"3. åŠ¨æ€ç®¡ç†ï¼šæ ¹æ®ä¸Šä¸‹æ–‡ç›¸å…³æ€§è°ƒæ•´å†å²\")\n",
    "print(\"4. æˆæœ¬æ§åˆ¶ï¼šä¼˜å…ˆä½¿ç”¨ gpt-4o-mini æ¨¡å‹\")\n",
    "print(\"5. é“¾å¼è°ƒç”¨ï¼šç»“åˆ LCEL æé«˜å¼€å‘æ•ˆç‡\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  Structured Output ç»“æ„åŒ–è¾“å‡º\")\n",
    "print(\"2. æŒæ¡ ConversationBufferMemory è®°å¿†ç®¡ç†\")\n",
    "print(\"3. å­¦ä¹  ConversationSummaryMemory æ‘˜è¦è®°å¿†\")\n",
    "print(\"4. æ¢ç´¢ RunnableWithMessageHistory å†å²é›†æˆ\")\n",
    "print(\"5. å®è·µå¤šè½®å¯¹è¯çŠ¶æ€ç®¡ç†\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ MessagesPlaceholder åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=40,\n",
    "    )\n",
    "    \n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"ä½ æ˜¯ç®€æ´çš„ GPT-4o åŠ©æ‰‹ã€‚\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"user\", \"ç”¨ä¸€å¥è¯è¯´æ˜ä½ å­¦ä¼šäº† {skill}\")\n",
    "    ])\n",
    "    \n",
    "    history = [HumanMessage(content=\"ä½ å¥½\")]\n",
    "    response = llm.invoke(template.format_messages(\n",
    "        history=history,\n",
    "        skill=\"MessagesPlaceholder\"\n",
    "    ))\n",
    "    \n",
    "    print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: {response.content}\")\n",
    "    print(\"\\nâœ… MessagesPlaceholder å­¦ä¹ å®Œæˆï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
