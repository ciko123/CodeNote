{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 - ç®€å•æ£€ç´¢å¢å¼º\n",
    "\n",
    "## ç”¨é€”\n",
    "åŸºç¡€GPTæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ŒéªŒè¯RAGæ ¸å¿ƒæ¦‚å¿µã€‚åŸºäº3æ¡GPTä¸­æ–‡æ–‡æ¡£æ„å»ºé—®ç­”ç³»ç»Ÿï¼ŒéªŒè¯æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆã€‚\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- æŒæ¡GPTæ£€ç´¢æµç¨‹\n",
    "- ç†è§£ä¸Šä¸‹æ–‡æ„å»º\n",
    "- èƒ½å®ç°åŸºç¡€ä¸­æ–‡é—®ç­”\n",
    "- éªŒè¯GPTç­”æ¡ˆåŒ…å«ä¸­æ–‡æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯\n",
    "\n",
    "## ğŸ”‘ å‰ç½®è¦æ±‚\n",
    "**æ³¨æ„**ï¼šéœ€è¦å…ˆå®Œæˆ RAGåŸºç¡€ç®¡çº¿ å­¦ä¹ \n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç®€å•æ£€ç´¢å¢å¼ºæ ¸å¿ƒæ¦‚å¿µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€å•æ£€ç´¢å¢å¼ºæ ¸å¿ƒæ¦‚å¿µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ” ç®€å•æ£€ç´¢å¢å¼ºæ ¸å¿ƒæ¦‚å¿µ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        print(f\"ğŸ“ ç®€å•æ£€ç´¢å¢å¼ºè¯´æ˜:\")\n",
    "        print(f\"   1. æ£€ç´¢(Retrieval): ä»æ–‡æ¡£åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯\")\n",
    "        print(f\"   2. å¢å¼º(Augmentation): å°†æ£€ç´¢ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡\")\n",
    "        print(f\"   3. ç”Ÿæˆ(Generation): GPTåŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ æ ¸å¿ƒéªŒè¯ç›®æ ‡:\")\n",
    "        print(f\"   - æ£€ç´¢ç»“æœç›¸å…³æ€§\")\n",
    "        print(f\"   - ç­”æ¡ˆåŸºäºæ–‡æ¡£å†…å®¹\")\n",
    "        print(f\"   - å…³é”®ä¿¡æ¯æ­£ç¡®ä¼ é€’\")\n",
    "        \n",
    "        # 1. åˆ›å»ºåŸºç¡€ç»„ä»¶\n",
    "        print(f\"\\nğŸ—ï¸  1. åˆ›å»ºåŸºç¡€ç»„ä»¶:\")\n",
    "        \n",
    "        # åˆ›å»ºEmbeddings\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºLLM\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.3,  # é™ä½æ¸©åº¦ç¡®ä¿ç­”æ¡ˆåŸºäºæ–‡æ¡£\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        print(f\"   Embeddingæ¨¡å‹: {embeddings.model}\")\n",
    "        print(f\"   LLMæ¨¡å‹: {llm.model_name}\")\n",
    "        print(f\"   æ¸©åº¦è®¾ç½®: {llm.temperature}\")\n",
    "        \n",
    "        # 2. æ£€ç´¢æµç¨‹æ¼”ç¤º\n",
    "        print(f\"\\nğŸ”„ 2. æ£€ç´¢æµç¨‹æ¼”ç¤º:\")\n",
    "        \n",
    "        def demonstrate_retrieval_flow():\n",
    "            \"\"\"æ¼”ç¤ºæ£€ç´¢å¢å¼ºæµç¨‹\"\"\"\n",
    "            flow_steps = [\n",
    "                \"ç”¨æˆ·æé—®\",\n",
    "                \"é—®é¢˜å‘é‡åŒ–\",\n",
    "                \"ç›¸ä¼¼åº¦æ£€ç´¢\",\n",
    "                \"è·å–ç›¸å…³æ–‡æ¡£\",\n",
    "                \"æ„å»ºä¸Šä¸‹æ–‡\",\n",
    "                \"GPTç”Ÿæˆç­”æ¡ˆ\"\n",
    "            ]\n",
    "            \n",
    "            print(f\"   æ£€ç´¢å¢å¼ºæµç¨‹:\")\n",
    "            for i, step in enumerate(flow_steps, 1):\n",
    "                print(f\"     {i}. {step}\")\n",
    "            \n",
    "            print(f\"\\n   å…³é”®éªŒè¯ç‚¹:\")\n",
    "            print(f\"     âœ“ æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸é—®é¢˜ç›¸å…³\")\n",
    "            print(f\"     âœ“ ç­”æ¡ˆå†…å®¹æ¥æºäºæ£€ç´¢æ–‡æ¡£\")\n",
    "            print(f\"     âœ“ å…³é”®ä¿¡æ¯åœ¨ç­”æ¡ˆä¸­æ­£ç¡®ä½“ç°\")\n",
    "        \n",
    "        demonstrate_retrieval_flow()\n",
    "        \n",
    "        # 3. éªŒè¯æœºåˆ¶è®¾è®¡\n",
    "        print(f\"\\nâœ… 3. éªŒè¯æœºåˆ¶è®¾è®¡:\")\n",
    "        \n",
    "        def verify_rag_answer(question, answer, retrieved_docs, expected_keywords):\n",
    "            \"\"\"éªŒè¯RAGç­”æ¡ˆè´¨é‡\"\"\"\n",
    "            verification_results = {\n",
    "                \"has_retrieved_docs\": len(retrieved_docs) > 0,\n",
    "                \"answer_not_empty\": len(answer.strip()) > 0,\n",
    "                \"contains_keywords\": any(\n",
    "                    keyword.lower() in answer.lower() \n",
    "                    for keyword in expected_keywords\n",
    "                ),\n",
    "                \"question_relevance\": any(\n",
    "                    word.lower() in answer.lower() \n",
    "                    for word in question.split() \n",
    "                    if len(word) > 1\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            # è®¡ç®—éªŒè¯åˆ†æ•°\n",
    "            score = sum(verification_results.values())\n",
    "            verification_results[\"verification_score\"] = f\"{score}/{len(verification_results)-1}\"\n",
    "            \n",
    "            return verification_results\n",
    "        \n",
    "        print(f\"   éªŒè¯æœºåˆ¶è®¾è®¡å®Œæˆ\")\n",
    "        print(f\"   éªŒè¯æŒ‡æ ‡: æ£€ç´¢æ–‡æ¡£ã€ç­”æ¡ˆéç©ºã€å…³é”®è¯åŒ¹é…ã€é—®é¢˜ç›¸å…³æ€§\")\n",
    "        \n",
    "        # 4. æµ‹è¯•ç»„ä»¶åŠŸèƒ½\n",
    "        print(f\"\\nğŸ§ª 4. æµ‹è¯•ç»„ä»¶åŠŸèƒ½:\")\n",
    "        \n",
    "        # æµ‹è¯•å‘é‡åŒ–\n",
    "        test_text = \"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯\"\n",
    "        test_embedding = embeddings.embed_query(test_text)\n",
    "        \n",
    "        print(f\"   æµ‹è¯•æ–‡æœ¬: {test_text}\")\n",
    "        print(f\"   å‘é‡ç»´åº¦: {len(test_embedding)}\")\n",
    "        print(f\"   å‘é‡åŒ–æˆåŠŸ: {len(test_embedding) > 0}\")\n",
    "        \n",
    "        # æµ‹è¯•LLM\n",
    "        test_response = llm.invoke(\"è¯·ç®€å•ä»‹ç»äººå·¥æ™ºèƒ½\")\n",
    "        print(f\"\\n   LLMæµ‹è¯•å“åº”: {test_response.content[:50]}...\")\n",
    "        print(f\"   å“åº”é•¿åº¦: {len(test_response.content)}\")\n",
    "        \n",
    "        # æµ‹è¯•éªŒè¯æœºåˆ¶\n",
    "        test_verification = verify_rag_answer(\n",
    "            \"ä»€ä¹ˆæ˜¯AIï¼Ÿ\",\n",
    "            \"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦åˆ†æ”¯\",\n",
    "            [\"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦åˆ†æ”¯\"],\n",
    "            [\"äººå·¥æ™ºèƒ½\", \"è®¡ç®—æœº\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   éªŒè¯æœºåˆ¶æµ‹è¯•:\")\n",
    "        for key, value in test_verification.items():\n",
    "            if key != \"verification_score\":\n",
    "                print(f\"     {key}: {'âœ…' if value else 'âŒ'}\")\n",
    "            else:\n",
    "                print(f\"     {key}: {value}\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šç®€å•æ£€ç´¢å¢å¼ºåŸºç¡€æ¦‚å¿µæ­£ç¡®\n",
    "        assert len(test_embedding) > 0, \"å‘é‡åŒ–åº”è¯¥æˆåŠŸ\"\n",
    "        assert isinstance(test_response.content, str), \"LLMå“åº”åº”è¯¥æ˜¯å­—ç¬¦ä¸²\"\n",
    "        assert len(test_response.content) > 0, \"LLMå“åº”ä¸åº”ä¸ºç©º\"\n",
    "        assert test_verification[\"has_retrieved_docs\"], \"éªŒè¯æœºåˆ¶åº”è¯¥æ£€æµ‹åˆ°æ£€ç´¢æ–‡æ¡£\"\n",
    "        assert test_verification[\"contains_keywords\"], \"éªŒè¯æœºåˆ¶åº”è¯¥æ£€æµ‹åˆ°å…³é”®è¯\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šç®€å•æ£€ç´¢å¢å¼ºåŸºç¡€æ¦‚å¿µæ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ç®€å•æ£€ç´¢å¢å¼ºåŸºç¡€æ¦‚å¿µæµ‹è¯•å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åˆ›å»º3æ¡ä¸­æ–‡æ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»º3æ¡ä¸­æ–‡æ–‡æ¡£ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“š åˆ›å»º3æ¡ä¸­æ–‡æ–‡æ¡£:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. åˆ›å»º3ä¸ªä¸»é¢˜æ˜ç¡®çš„ä¸­æ–‡æ–‡æ¡£\n",
    "        print(f\"ğŸ“ 1. åˆ›å»º3ä¸ªä¸»é¢˜æ˜ç¡®çš„ä¸­æ–‡æ–‡æ¡£:\")\n",
    "        \n",
    "        # æ–‡æ¡£1ï¼šPythonç¼–ç¨‹\n",
    "        document_1 = \"\"\"\n",
    "Pythonç¼–ç¨‹è¯­è¨€åŸºç¡€\n",
    "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚Pythonå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ï¼Œå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®åˆ†æã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚Pythonçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šåŠ¨æ€ç±»å‹ã€è‡ªåŠ¨å†…å­˜ç®¡ç†ã€ä¸°å¯Œçš„æ ‡å‡†åº“ã€è·¨å¹³å°å…¼å®¹æ€§ã€‚Pythonçš„è¯­æ³•ç®€æ´æ˜“è¯»ï¼Œä½¿ç”¨ç¼©è¿›æ¥å®šä¹‰ä»£ç å—ï¼Œæ”¯æŒé¢å‘å¯¹è±¡ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹èŒƒå¼ã€‚Pythonç¤¾åŒºæ´»è·ƒï¼Œæ‹¥æœ‰å¤§é‡çš„ç¬¬ä¸‰æ–¹åº“å’Œæ¡†æ¶ï¼Œå¦‚Djangoã€Flaskã€NumPyã€Pandasç­‰ï¼Œè¿™äº›å·¥å…·æå¤§åœ°æ‰©å±•äº†Pythonçš„åº”ç”¨èƒ½åŠ›ã€‚\n",
    "\"\"\".strip()\n",
    "        \n",
    "        # æ–‡æ¡£2ï¼šæœºå™¨å­¦ä¹ \n",
    "        document_2 = \"\"\"\n",
    "æœºå™¨å­¦ä¹ æ ¸å¿ƒæŠ€æœ¯\n",
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ˜¯é€šè¿‡ç®—æ³•åˆ†æå¤§é‡æ•°æ®ï¼Œè¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼æ¥åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚æœºå™¨å­¦ä¹ ä¸»è¦åˆ†ä¸ºä¸‰ç±»ï¼šç›‘ç£å­¦ä¹ ï¼ˆä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ï¼‰ã€æ— ç›‘ç£å­¦ä¹ ï¼ˆå‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆé€šè¿‡å¥–åŠ±å’Œæƒ©ç½šæœºåˆ¶å­¦ä¹ ï¼‰ã€‚å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚æœºå™¨å­¦ä¹ åœ¨æ¨èç³»ç»Ÿã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚\n",
    "\"\"\".strip()\n",
    "        \n",
    "        # æ–‡æ¡£3ï¼šäº‘è®¡ç®—\n",
    "        document_3 = \"\"\"\n",
    "äº‘è®¡ç®—æœåŠ¡æ¨¡å¼\n",
    "äº‘è®¡ç®—æ˜¯ä¸€ç§é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—èµ„æºçš„æœåŠ¡æ¨¡å¼ï¼ŒåŒ…æ‹¬æœåŠ¡å™¨ã€å­˜å‚¨ã€æ•°æ®åº“ã€ç½‘ç»œã€è½¯ä»¶ç­‰ã€‚äº‘è®¡ç®—çš„ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼šæŒ‰éœ€è‡ªåŠ©æœåŠ¡ã€å¹¿æ³›çš„ç½‘ç»œè®¿é—®ã€èµ„æºæ± åŒ–ã€å¿«é€Ÿå¼¹æ€§ä¼¸ç¼©ã€å¯è®¡é‡çš„æœåŠ¡ã€‚äº‘è®¡ç®—æœåŠ¡é€šå¸¸åˆ†ä¸ºä¸‰ç§æ¨¡å¼ï¼šIaaSï¼ˆåŸºç¡€è®¾æ–½å³æœåŠ¡ï¼‰æä¾›è™šæ‹ŸåŒ–çš„è®¡ç®—èµ„æºï¼›PaaSï¼ˆå¹³å°å³æœåŠ¡ï¼‰æä¾›å¼€å‘å’Œéƒ¨ç½²å¹³å°ï¼›SaaSï¼ˆè½¯ä»¶å³æœåŠ¡ï¼‰æä¾›å¯ç›´æ¥ä½¿ç”¨çš„åº”ç”¨ç¨‹åºã€‚ä¸»è¦çš„äº‘è®¡ç®—æä¾›å•†åŒ…æ‹¬Amazon Web Servicesã€Microsoft Azureã€Google Cloud Platformç­‰ã€‚äº‘è®¡ç®—å¸®åŠ©ä¼ä¸šé™ä½ITæˆæœ¬ï¼Œæé«˜çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚\n",
    "\"\"\".strip()\n",
    "        \n",
    "        # æ–‡æ¡£ä¿¡æ¯\n",
    "        documents_info = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Pythonç¼–ç¨‹è¯­è¨€åŸºç¡€\",\n",
    "                \"topic\": \"Pythonç¼–ç¨‹\",\n",
    "                \"length\": len(document_1),\n",
    "                \"keywords\": [\"Python\", \"ç¼–ç¨‹è¯­è¨€\", \"Guido van Rossum\", \"åŠ¨æ€ç±»å‹\", \"é¢å‘å¯¹è±¡\"]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"æœºå™¨å­¦ä¹ æ ¸å¿ƒæŠ€æœ¯\",\n",
    "                \"topic\": \"æœºå™¨å­¦ä¹ \",\n",
    "                \"length\": len(document_2),\n",
    "                \"keywords\": [\"æœºå™¨å­¦ä¹ \", \"äººå·¥æ™ºèƒ½\", \"ç›‘ç£å­¦ä¹ \", \"æ— ç›‘ç£å­¦ä¹ \", \"ç®—æ³•\"]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"äº‘è®¡ç®—æœåŠ¡æ¨¡å¼\",\n",
    "                \"topic\": \"äº‘è®¡ç®—\",\n",
    "                \"length\": len(document_3),\n",
    "                \"keywords\": [\"äº‘è®¡ç®—\", \"IaaS\", \"PaaS\", \"SaaS\", \"AWS\"]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"   æ–‡æ¡£åˆ›å»ºå®Œæˆ:\")\n",
    "        for doc_info in documents_info:\n",
    "            print(f\"     æ–‡æ¡£{doc_info['id']}: {doc_info['title']}\")\n",
    "            print(f\"       ä¸»é¢˜: {doc_info['topic']}\")\n",
    "            print(f\"       é•¿åº¦: {doc_info['length']} å­—ç¬¦\")\n",
    "            print(f\"       å…³é”®è¯: {', '.join(doc_info['keywords'])}\")\n",
    "        \n",
    "        # 2. åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n",
    "        print(f\"\\nğŸ’¾ 2. åˆ›å»ºä¸´æ—¶æ–‡ä»¶:\")\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "            for i, doc in enumerate([document_1, document_2, document_3], 1):\n",
    "                f.write(f\"æ–‡æ¡£{i}: {doc}\\n\\n\")\n",
    "            temp_file_path = f.name\n",
    "        \n",
    "        print(f\"   ä¸´æ—¶æ–‡ä»¶è·¯å¾„: {temp_file_path}\")\n",
    "        print(f\"   æ–‡ä»¶ç¼–ç : UTF-8\")\n",
    "        \n",
    "        # 3. åŠ è½½æ–‡æ¡£\n",
    "        print(f\"\\nğŸ“š 3. åŠ è½½æ–‡æ¡£:\")\n",
    "        \n",
    "        loader = TextLoader(temp_file_path, encoding='utf-8')\n",
    "        documents = loader.load()\n",
    "        \n",
    "        print(f\"   åŠ è½½å™¨ç±»å‹: {type(loader)}\")\n",
    "        print(f\"   æ–‡æ¡£æ•°é‡: {len(documents)}\")\n",
    "        print(f\"   æ–‡æ¡£ç±»å‹: {type(documents[0])}\")\n",
    "        print(f\"   æ€»å†…å®¹é•¿åº¦: {sum(len(doc.page_content) for doc in documents)} å­—ç¬¦\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„é¢„è§ˆ\n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            print(f\"\\n   æ–‡æ¡£{i}é¢„è§ˆ: {doc.page_content[:100]}...\")\n",
    "        \n",
    "        # 4. æ–‡æ¡£åˆ‡ç‰‡\n",
    "        print(f\"\\nâœ‚ï¸  4. æ–‡æ¡£åˆ‡ç‰‡:\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=200,  # é€‚ä¸­çš„chunk size\n",
    "            chunk_overlap=30,  # é€‚å½“é‡å ä¿è¯è¿ç»­æ€§\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼›\", \"ï¼Œ\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        print(f\"   åˆ‡ç‰‡å™¨é…ç½®:\")\n",
    "        print(f\"     chunk_size: {text_splitter._chunk_size}\")\n",
    "        print(f\"     chunk_overlap: {text_splitter._chunk_overlap}\")\n",
    "        print(f\"   åˆ‡ç‰‡ç»“æœ:\")\n",
    "        print(f\"     åŸå§‹æ–‡æ¡£æ•°: {len(documents)}\")\n",
    "        print(f\"     åˆ‡ç‰‡æ•°é‡: {len(chunks)}\")\n",
    "        print(f\"     å¹³å‡åˆ‡ç‰‡é•¿åº¦: {sum(len(c.page_content) for c in chunks) / len(chunks):.1f}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºåˆ‡ç‰‡å†…å®¹é¢„è§ˆ\n",
    "        print(f\"\\n   åˆ‡ç‰‡å†…å®¹é¢„è§ˆ:\")\n",
    "        for i, chunk in enumerate(chunks[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª\n",
    "            print(f\"     åˆ‡ç‰‡{i}: {chunk.page_content[:80]}...\")\n",
    "        \n",
    "        # 5. å‘é‡åŒ–\n",
    "        print(f\"\\nğŸ”¢ 5. å‘é‡åŒ–:\")\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        \n",
    "        # æµ‹è¯•å•ä¸ªåˆ‡ç‰‡å‘é‡åŒ–\n",
    "        sample_chunk = chunks[0].page_content\n",
    "        sample_embedding = embeddings.embed_query(sample_chunk)\n",
    "        \n",
    "        print(f\"   Embeddingæ¨¡å‹: {embeddings.model}\")\n",
    "        print(f\"   æ ·æœ¬æ–‡æœ¬: {sample_chunk[:50]}...\")\n",
    "        print(f\"   å‘é‡ç»´åº¦: {len(sample_embedding)}\")\n",
    "        print(f\"   å‘é‡ç±»å‹: {type(sample_embedding)}\")\n",
    "        print(f\"   å‰5ä¸ªå€¼: {sample_embedding[:5]}\")\n",
    "        \n",
    "        # 6. å‘é‡å­˜å‚¨\n",
    "        print(f\"\\nğŸ’¾ 6. å‘é‡å­˜å‚¨:\")\n",
    "        \n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        \n",
    "        print(f\"   å‘é‡åº“ç±»å‹: {type(vectorstore)}\")\n",
    "        print(f\"   å­˜å‚¨æ–‡æ¡£æ•°: {len(chunks)}\")\n",
    "        print(f\"   å‘é‡ç»´åº¦: {len(sample_embedding)}\")\n",
    "        print(f\"   å‘é‡åº“åˆ›å»ºæˆåŠŸ: {vectorstore is not None}\")\n",
    "        \n",
    "        # 7. åˆ›å»ºæ£€ç´¢å™¨\n",
    "        print(f\"\\nğŸ” 7. åˆ›å»ºæ£€ç´¢å™¨:\")\n",
    "        \n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 2}  # æ£€ç´¢æœ€ç›¸å…³çš„2ä¸ªæ–‡æ¡£ç‰‡æ®µ\n",
    "        )\n",
    "        \n",
    "        print(f\"   æ£€ç´¢å™¨ç±»å‹: {type(retriever)}\")\n",
    "        print(f\"   æ£€ç´¢ç­–ç•¥: similarity\")\n",
    "        print(f\"   æ£€ç´¢æ•°é‡: k=2\")\n",
    "        \n",
    "        # 8. æµ‹è¯•æ£€ç´¢åŠŸèƒ½\n",
    "        print(f\"\\nğŸ§ª 8. æµ‹è¯•æ£€ç´¢åŠŸèƒ½:\")\n",
    "        \n",
    "        test_queries = [\n",
    "            \"Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "            \"æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ\",\n",
    "            \"äº‘è®¡ç®—çš„æœåŠ¡æ¨¡å¼æœ‰å“ªäº›ï¼Ÿ\"\n",
    "        ]\n",
    "        \n",
    "        for i, query in enumerate(test_queries, 1):\n",
    "            print(f\"\\n   æµ‹è¯•æŸ¥è¯¢ {i}: {query}\")\n",
    "            \n",
    "            # æ‰§è¡Œæ£€ç´¢\n",
    "            retrieved_docs = retriever.invoke(query)\n",
    "            \n",
    "            print(f\"   æ£€ç´¢ç»“æœ:\")\n",
    "            print(f\"     æ£€ç´¢åˆ°æ–‡æ¡£æ•°: {len(retrieved_docs)}\")\n",
    "            for j, doc in enumerate(retrieved_docs, 1):\n",
    "                print(f\"     æ–‡æ¡£{j}: {doc.page_content[:60]}...\")\n",
    "        \n",
    "        # ä¿å­˜å˜é‡ä¾›åç»­ä½¿ç”¨\n",
    "        globals().update({\n",
    "            'temp_file_path': temp_file_path,\n",
    "            'documents': documents,\n",
    "            'chunks': chunks,\n",
    "            'vectorstore': vectorstore,\n",
    "            'retriever': retriever,\n",
    "            'documents_info': documents_info,\n",
    "            'embeddings': embeddings\n",
    "        })\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼š3æ¡ä¸­æ–‡æ–‡æ¡£åˆ›å»ºæ­£ç¡®\n",
    "        assert len(documents) == 3, \"åº”è¯¥åˆ›å»º3ä¸ªæ–‡æ¡£\"\n",
    "        assert len(chunks) > 3, \"åˆ‡ç‰‡æ•°é‡åº”è¯¥å¤§äºåŸå§‹æ–‡æ¡£æ•°\"\n",
    "        assert len(sample_embedding) > 0, \"å‘é‡åŒ–åº”è¯¥æˆåŠŸ\"\n",
    "        assert len(retrieved_docs) > 0, \"æ£€ç´¢åº”è¯¥è¿”å›ç»“æœ\"\n",
    "        assert all(len(doc.page_content) > 0 for doc in documents), \"æ‰€æœ‰æ–‡æ¡£å†…å®¹ä¸åº”ä¸ºç©º\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼š3æ¡ä¸­æ–‡æ–‡æ¡£åˆ›å»ºæ­£ç¡®\")\n",
    "        print(f\"\\nğŸ¯ æ–‡æ¡£æ€»ç»“:\")\n",
    "        print(f\"   âœ“ åˆ›å»ºäº†3ä¸ªä¸»é¢˜æ˜ç¡®çš„ä¸­æ–‡æ–‡æ¡£\")\n",
    "        print(f\"   âœ“ æ–‡æ¡£ä¸»é¢˜: Pythonç¼–ç¨‹ã€æœºå™¨å­¦ä¹ ã€äº‘è®¡ç®—\")\n",
    "        print(f\"   âœ“ å®Œæˆæ–‡æ¡£åŠ è½½ã€åˆ‡ç‰‡ã€å‘é‡åŒ–ã€å­˜å‚¨\")\n",
    "        print(f\"   âœ“ æ£€ç´¢å™¨åˆ›å»ºå’Œæµ‹è¯•æˆåŠŸ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ åˆ›å»º3æ¡ä¸­æ–‡æ–‡æ¡£å¤±è´¥: {e}\")\n",
    "        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "        if 'temp_file_path' in locals():\n",
    "            try:\n",
    "                os.unlink(temp_file_path)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ„å»ºé—®ç­”ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºé—®ç­”ç³»ç»Ÿ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ¤– æ„å»ºé—®ç­”ç³»ç»Ÿ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. é‡æ–°åˆ›å»ºæ–‡æ¡£å’Œæ£€ç´¢å™¨ï¼ˆç¡®ä¿ç‹¬ç«‹æ€§ï¼‰\n",
    "        print(f\"ğŸ“š 1. é‡æ–°åˆ›å»ºæ–‡æ¡£å’Œæ£€ç´¢å™¨:\")\n",
    "        \n",
    "        # 3ä¸ªä¸­æ–‡æ–‡æ¡£\n",
    "        documents_content = [\n",
    "            \"Pythonç¼–ç¨‹è¯­è¨€åŸºç¡€ï¼šPythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚Pythonå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ï¼Œå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®åˆ†æã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚Pythonçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šåŠ¨æ€ç±»å‹ã€è‡ªåŠ¨å†…å­˜ç®¡ç†ã€ä¸°å¯Œçš„æ ‡å‡†åº“ã€è·¨å¹³å°å…¼å®¹æ€§ã€‚Pythonçš„è¯­æ³•ç®€æ´æ˜“è¯»ï¼Œä½¿ç”¨ç¼©è¿›æ¥å®šä¹‰ä»£ç å—ï¼Œæ”¯æŒé¢å‘å¯¹è±¡ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹èŒƒå¼ã€‚\",\n",
    "            \"æœºå™¨å­¦ä¹ æ ¸å¿ƒæŠ€æœ¯ï¼šæœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ˜¯é€šè¿‡ç®—æ³•åˆ†æå¤§é‡æ•°æ®ï¼Œè¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼æ¥åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚æœºå™¨å­¦ä¹ ä¸»è¦åˆ†ä¸ºä¸‰ç±»ï¼šç›‘ç£å­¦ä¹ ï¼ˆä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ï¼‰ã€æ— ç›‘ç£å­¦ä¹ ï¼ˆå‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆé€šè¿‡å¥–åŠ±å’Œæƒ©ç½šæœºåˆ¶å­¦ä¹ ï¼‰ã€‚\",\n",
    "            \"äº‘è®¡ç®—æœåŠ¡æ¨¡å¼ï¼šäº‘è®¡ç®—æ˜¯ä¸€ç§é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—èµ„æºçš„æœåŠ¡æ¨¡å¼ï¼ŒåŒ…æ‹¬æœåŠ¡å™¨ã€å­˜å‚¨ã€æ•°æ®åº“ã€ç½‘ç»œã€è½¯ä»¶ç­‰ã€‚äº‘è®¡ç®—çš„ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼šæŒ‰éœ€è‡ªåŠ©æœåŠ¡ã€å¹¿æ³›çš„ç½‘ç»œè®¿é—®ã€èµ„æºæ± åŒ–ã€å¿«é€Ÿå¼¹æ€§ä¼¸ç¼©ã€å¯è®¡é‡çš„æœåŠ¡ã€‚äº‘è®¡ç®—æœåŠ¡é€šå¸¸åˆ†ä¸ºä¸‰ç§æ¨¡å¼ï¼šIaaSï¼ˆåŸºç¡€è®¾æ–½å³æœåŠ¡ï¼‰ã€PaaSï¼ˆå¹³å°å³æœåŠ¡ï¼‰ã€SaaSï¼ˆè½¯ä»¶å³æœåŠ¡ï¼‰ã€‚\"\n",
    "        ]\n",
    "        \n",
    "        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "            for i, doc in enumerate(documents_content, 1):\n",
    "                f.write(f\"æ–‡æ¡£{i}: {doc}\\n\\n\")\n",
    "            temp_file_path = f.name\n",
    "        \n",
    "        # åŠ è½½å’Œå¤„ç†æ–‡æ¡£\n",
    "        loader = TextLoader(temp_file_path, encoding='utf-8')\n",
    "        documents = loader.load()\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=30\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # åˆ›å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "        \n",
    "        print(f\"   æ–‡æ¡£å¤„ç†å®Œæˆ: {len(documents)} â†’ {len(chunks)} åˆ‡ç‰‡\")\n",
    "        print(f\"   æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ: k=2\")\n",
    "        \n",
    "        # 2. åˆ›å»ºRAGæç¤ºæ¨¡æ¿\n",
    "        print(f\"\\nğŸ“‹ 2. åˆ›å»ºRAGæç¤ºæ¨¡æ¿:\")\n",
    "        \n",
    "        rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„æ–‡æ¡£ä¿¡æ¯å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n",
    "\n",
    "é‡è¦è¦æ±‚ï¼š\n",
    "1. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡æ¡£ä¿¡æ¯\n",
    "2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜\n",
    "3. è¯·ä¿æŒç­”æ¡ˆç®€æ´ã€å‡†ç¡®\n",
    "\n",
    "æ–‡æ¡£ä¿¡æ¯ï¼š\n",
    "{context}\n",
    "\n",
    "ç”¨æˆ·é—®é¢˜ï¼š\n",
    "{question}\n",
    "\n",
    "è¯·åŸºäºæ–‡æ¡£ä¿¡æ¯å›ç­”ï¼š\n",
    "\"\"\")\n",
    "        \n",
    "        print(f\"   RAGæç¤ºæ¨¡æ¿åˆ›å»ºå®Œæˆ\")\n",
    "        print(f\"   æ¨¡æ¿å˜é‡: context, question\")\n",
    "        print(f\"   é‡ç‚¹è¦æ±‚: ç­”æ¡ˆåŸºäºæ–‡æ¡£ã€ç®€æ´å‡†ç¡®\")\n",
    "        \n",
    "        # 3. åˆ›å»ºLLM\n",
    "        print(f\"\\nğŸ¤– 3. åˆ›å»ºLLM:\")\n",
    "        \n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.3,  # ä½æ¸©åº¦ç¡®ä¿åŸºäºæ–‡æ¡£\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        print(f\"   LLMé…ç½®:\")\n",
    "        print(f\"     æ¨¡å‹: {llm.model_name}\")\n",
    "        print(f\"     æ¸©åº¦: {llm.temperature}\")\n",
    "        print(f\"     æœ€å¤§ä»¤ç‰Œ: {llm.max_tokens}\")\n",
    "        \n",
    "        # 4. æ„å»ºå®Œæ•´RAGé“¾\n",
    "        print(f\"\\nğŸ”— 4. æ„å»ºå®Œæ•´RAGé“¾:\")\n",
    "        \n",
    "        def format_docs(docs):\n",
    "            \"\"\"æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£\"\"\"\n",
    "            formatted_docs = []\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                formatted_docs.append(f\"æ–‡æ¡£ç‰‡æ®µ{i}: {doc.page_content}\")\n",
    "            return \"\\n\\n\".join(formatted_docs)\n",
    "        \n",
    "        # æ„å»ºRAGé“¾\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | rag_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        print(f\"   RAGé“¾æ„å»ºå®Œæˆ:\")\n",
    "        print(f\"   æµç¨‹: é—®é¢˜ â†’ æ£€ç´¢ â†’ æ ¼å¼åŒ– â†’ æç¤º â†’ LLM â†’ è§£æ\")\n",
    "        print(f\"   é“¾ç±»å‹: {type(rag_chain)}\")\n",
    "        \n",
    "        # 5. éªŒè¯RAGé“¾åŠŸèƒ½\n",
    "        print(f\"\\nğŸ§ª 5. éªŒè¯RAGé“¾åŠŸèƒ½:\")\n",
    "        \n",
    "        # æµ‹è¯•é—®é¢˜\n",
    "        test_question = \"Pythonç¼–ç¨‹è¯­è¨€æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ\"\n",
    "        \n",
    "        print(f\"   æµ‹è¯•é—®é¢˜: {test_question}\")\n",
    "        \n",
    "        # æ‰§è¡ŒRAGæŸ¥è¯¢\n",
    "        answer = rag_chain.invoke(test_question)\n",
    "        \n",
    "        # è·å–æ£€ç´¢åˆ°çš„æ–‡æ¡£\n",
    "        retrieved_docs = retriever.invoke(test_question)\n",
    "        \n",
    "        print(f\"\\n   æ£€ç´¢ç»“æœ:\")\n",
    "        print(f\"     æ£€ç´¢åˆ°æ–‡æ¡£æ•°: {len(retrieved_docs)}\")\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            print(f\"     æ–‡æ¡£{i}: {doc.page_content[:80]}...\")\n",
    "        \n",
    "        print(f\"\\n   ç”Ÿæˆçš„ç­”æ¡ˆ:\")\n",
    "        print(f\"     {answer}\")\n",
    "        print(f\"     ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦\")\n",
    "        \n",
    "        # 6. ç­”æ¡ˆéªŒè¯\n",
    "        print(f\"\\nâœ… 6. ç­”æ¡ˆéªŒè¯:\")\n",
    "        \n",
    "        def verify_answer_quality(question, answer, retrieved_docs):\n",
    "            \"\"\"éªŒè¯ç­”æ¡ˆè´¨é‡\"\"\"\n",
    "            # åŸºç¡€éªŒè¯\n",
    "            basic_checks = {\n",
    "                \"answer_not_empty\": len(answer.strip()) > 0,\n",
    "                \"has_retrieved_docs\": len(retrieved_docs) > 0,\n",
    "                \"reasonable_length\": 20 <= len(answer) <= 300\n",
    "            }\n",
    "            \n",
    "            # å†…å®¹éªŒè¯\n",
    "            content_checks = {\n",
    "                \"contains_python\": \"python\" in answer.lower() or \"Python\" in answer,\n",
    "                \"contains_features\": any(word in answer for word in [\"ç‰¹ç‚¹\", \"ç‰¹å¾\", \"ä¼˜åŠ¿\", \"è¯­æ³•\"]),\n",
    "                \"question_relevant\": any(\n",
    "                    word.lower() in answer.lower() \n",
    "                    for word in question.split() \n",
    "                    if len(word) > 1\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            # è®¡ç®—éªŒè¯åˆ†æ•°\n",
    "            all_checks = {**basic_checks, **content_checks}\n",
    "            passed_checks = sum(all_checks.values())\n",
    "            total_checks = len(all_checks)\n",
    "            \n",
    "            return {\n",
    "                **all_checks,\n",
    "                \"verification_score\": f\"{passed_checks}/{total_checks}\",\n",
    "                \"success_rate\": f\"{passed_checks/total_checks*100:.1f}%\"\n",
    "            }\n",
    "        \n",
    "        # æ‰§è¡ŒéªŒè¯\n",
    "        verification_result = verify_answer_quality(test_question, answer, retrieved_docs)\n",
    "        \n",
    "        print(f\"   éªŒè¯ç»“æœ:\")\n",
    "        for key, value in verification_result.items():\n",
    "            if key in [\"verification_score\", \"success_rate\"]:\n",
    "                print(f\"     {key}: {value}\")\n",
    "            else:\n",
    "                print(f\"     {key}: {'âœ…' if value else 'âŒ'}\")\n",
    "        \n",
    "        # 7. ä¿å­˜RAGç»„ä»¶ä¾›åç»­ä½¿ç”¨\n",
    "        print(f\"\\nğŸ’¾ 7. ä¿å­˜RAGç»„ä»¶ä¾›åç»­ä½¿ç”¨:\")\n",
    "        \n",
    "        globals().update({\n",
    "            'qa_temp_file_path': temp_file_path,\n",
    "            'qa_documents': documents,\n",
    "            'qa_chunks': chunks,\n",
    "            'qa_vectorstore': vectorstore,\n",
    "            'qa_retriever': retriever,\n",
    "            'qa_llm': llm,\n",
    "            'qa_rag_prompt': rag_prompt,\n",
    "            'qa_rag_chain': rag_chain\n",
    "        })\n",
    "        \n",
    "        print(f\"   RAGç»„ä»¶å·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "        print(f\"   å¯ç”¨çš„å˜é‡: qa_rag_chain, qa_retriever, qa_llm ç­‰\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šé—®ç­”ç³»ç»Ÿæ„å»ºæ­£ç¡®\n",
    "        assert len(documents) == 3, \"åº”è¯¥æœ‰3ä¸ªæ–‡æ¡£\"\n",
    "        assert len(chunks) > 3, \"åº”è¯¥æœ‰å¤šä¸ªåˆ‡ç‰‡\"\n",
    "        assert isinstance(rag_chain, object), \"RAGé“¾åº”è¯¥åˆ›å»ºæˆåŠŸ\"\n",
    "        assert len(answer) > 0, \"ç­”æ¡ˆä¸åº”ä¸ºç©º\"\n",
    "        assert verification_result[\"answer_not_empty\"], \"ç­”æ¡ˆåº”è¯¥éç©º\"\n",
    "        assert verification_result[\"has_retrieved_docs\"], \"åº”è¯¥æ£€ç´¢åˆ°æ–‡æ¡£\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šé—®ç­”ç³»ç»Ÿæ„å»ºæ­£ç¡®\")\n",
    "        print(f\"\\nğŸ¯ é—®ç­”ç³»ç»Ÿæ€»ç»“:\")\n",
    "        print(f\"   âœ“ åŸºäº3ä¸ªä¸­æ–‡æ–‡æ¡£æ„å»º\")\n",
    "        print(f\"   âœ“ å®Œæ•´RAGæµç¨‹å®ç°\")\n",
    "        print(f\"   âœ“ ç­”æ¡ˆè´¨é‡éªŒè¯æœºåˆ¶\")\n",
    "        print(f\"   âœ“ ç»„ä»¶å¯å¤ç”¨æ€§è®¾è®¡\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æ„å»ºé—®ç­”ç³»ç»Ÿå¤±è´¥: {e}\")\n",
    "        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "        if 'temp_file_path' in locals():\n",
    "            try:\n",
    "                os.unlink(temp_file_path)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. éªŒè¯æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ” éªŒè¯æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. ä½¿ç”¨å·²æ„å»ºçš„é—®ç­”ç³»ç»Ÿ\n",
    "        print(f\"ğŸ”„ 1. ä½¿ç”¨å·²æ„å»ºçš„é—®ç­”ç³»ç»Ÿ:\")\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„RAGç»„ä»¶\n",
    "        if 'qa_rag_chain' not in globals():\n",
    "            print(\"   é‡æ–°æ„å»ºRAGç³»ç»Ÿ...\")\n",
    "            \n",
    "            # é‡æ–°åˆ›å»ºæ–‡æ¡£\n",
    "            documents_content = [\n",
    "                \"Pythonç¼–ç¨‹è¯­è¨€åŸºç¡€ï¼šPythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚Pythonå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ï¼Œå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®åˆ†æã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚Pythonçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šåŠ¨æ€ç±»å‹ã€è‡ªåŠ¨å†…å­˜ç®¡ç†ã€ä¸°å¯Œçš„æ ‡å‡†åº“ã€è·¨å¹³å°å…¼å®¹æ€§ã€‚Pythonçš„è¯­æ³•ç®€æ´æ˜“è¯»ï¼Œä½¿ç”¨ç¼©è¿›æ¥å®šä¹‰ä»£ç å—ï¼Œæ”¯æŒé¢å‘å¯¹è±¡ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹èŒƒå¼ã€‚\",\n",
    "                \"æœºå™¨å­¦ä¹ æ ¸å¿ƒæŠ€æœ¯ï¼šæœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ˜¯é€šè¿‡ç®—æ³•åˆ†æå¤§é‡æ•°æ®ï¼Œè¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼æ¥åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚æœºå™¨å­¦ä¹ ä¸»è¦åˆ†ä¸ºä¸‰ç±»ï¼šç›‘ç£å­¦ä¹ ï¼ˆä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ï¼‰ã€æ— ç›‘ç£å­¦ä¹ ï¼ˆå‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆé€šè¿‡å¥–åŠ±å’Œæƒ©ç½šæœºåˆ¶å­¦ä¹ ï¼‰ã€‚\",\n",
    "                \"äº‘è®¡ç®—æœåŠ¡æ¨¡å¼ï¼šäº‘è®¡ç®—æ˜¯ä¸€ç§é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—èµ„æºçš„æœåŠ¡æ¨¡å¼ï¼ŒåŒ…æ‹¬æœåŠ¡å™¨ã€å­˜å‚¨ã€æ•°æ®åº“ã€ç½‘ç»œã€è½¯ä»¶ç­‰ã€‚äº‘è®¡ç®—çš„ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼šæŒ‰éœ€è‡ªåŠ©æœåŠ¡ã€å¹¿æ³›çš„ç½‘ç»œè®¿é—®ã€èµ„æºæ± åŒ–ã€å¿«é€Ÿå¼¹æ€§ä¼¸ç¼©ã€å¯è®¡é‡çš„æœåŠ¡ã€‚äº‘è®¡ç®—æœåŠ¡é€šå¸¸åˆ†ä¸ºä¸‰ç§æ¨¡å¼ï¼šIaaSï¼ˆåŸºç¡€è®¾æ–½å³æœåŠ¡ï¼‰ã€PaaSï¼ˆå¹³å°å³æœåŠ¡ï¼‰ã€SaaSï¼ˆè½¯ä»¶å³æœåŠ¡ï¼‰ã€‚\"\n",
    "            ]\n",
    "            \n",
    "            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "                for i, doc in enumerate(documents_content, 1):\n",
    "                    f.write(f\"æ–‡æ¡£{i}: {doc}\\n\\n\")\n",
    "                temp_file_path = f.name\n",
    "            \n",
    "            # æ„å»ºRAGç³»ç»Ÿ\n",
    "            loader = TextLoader(temp_file_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n",
    "            chunks = text_splitter.split_documents(documents)\n",
    "            embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "            vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "            \n",
    "            rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„æ–‡æ¡£ä¿¡æ¯å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n",
    "\n",
    "é‡è¦è¦æ±‚ï¼š\n",
    "1. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡æ¡£ä¿¡æ¯\n",
    "2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜\n",
    "3. è¯·ä¿æŒç­”æ¡ˆç®€æ´ã€å‡†ç¡®\n",
    "\n",
    "æ–‡æ¡£ä¿¡æ¯ï¼š\n",
    "{context}\n",
    "\n",
    "ç”¨æˆ·é—®é¢˜ï¼š\n",
    "{question}\n",
    "\n",
    "è¯·åŸºäºæ–‡æ¡£ä¿¡æ¯å›ç­”ï¼š\n",
    "\"\"\")\n",
    "            \n",
    "            llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, max_tokens=150)\n",
    "            \n",
    "            def format_docs(docs):\n",
    "                return \"\\n\\n\".join([f\"æ–‡æ¡£ç‰‡æ®µ{i+1}: {doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "            \n",
    "            qa_rag_chain = (\n",
    "                {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                | rag_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "            \n",
    "            qa_retriever = retriever\n",
    "        else:\n",
    "            print(\"   ä½¿ç”¨å·²ä¿å­˜çš„RAGç»„ä»¶\")\n",
    "            qa_rag_chain = globals()['qa_rag_chain']\n",
    "            qa_retriever = globals()['qa_retriever']\n",
    "        \n",
    "        print(f\"   RAGç³»ç»Ÿå‡†å¤‡å®Œæˆ\")\n",
    "        \n",
    "        # 2. è®¾è®¡éªŒè¯æµ‹è¯•é—®é¢˜\n",
    "        print(f\"\\nğŸ“‹ 2. è®¾è®¡éªŒè¯æµ‹è¯•é—®é¢˜:\")\n",
    "        \n",
    "        test_cases = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"question\": \"Pythonç¼–ç¨‹è¯­è¨€æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ\",\n",
    "                \"target_doc\": \"Pythonç¼–ç¨‹è¯­è¨€åŸºç¡€\",\n",
    "                \"expected_keywords\": [\"Python\", \"ç‰¹ç‚¹\", \"åŠ¨æ€ç±»å‹\", \"è¯­æ³•\", \"é¢å‘å¯¹è±¡\"],\n",
    "                \"description\": \"æµ‹è¯•Pythonç›¸å…³çŸ¥è¯†æ£€ç´¢\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"question\": \"æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ\",\n",
    "                \"target_doc\": \"æœºå™¨å­¦ä¹ æ ¸å¿ƒæŠ€æœ¯\",\n",
    "                \"expected_keywords\": [\"æœºå™¨å­¦ä¹ \", \"ç±»å‹\", \"ç›‘ç£å­¦ä¹ \", \"æ— ç›‘ç£å­¦ä¹ \", \"å¼ºåŒ–å­¦ä¹ \"],\n",
    "                \"description\": \"æµ‹è¯•æœºå™¨å­¦ä¹ ç›¸å…³çŸ¥è¯†æ£€ç´¢\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"question\": \"äº‘è®¡ç®—çš„æœåŠ¡æ¨¡å¼æœ‰å“ªäº›ï¼Ÿ\",\n",
    "                \"target_doc\": \"äº‘è®¡ç®—æœåŠ¡æ¨¡å¼\",\n",
    "                \"expected_keywords\": [\"äº‘è®¡ç®—\", \"æœåŠ¡æ¨¡å¼\", \"IaaS\", \"PaaS\", \"SaaS\"],\n",
    "                \"description\": \"æµ‹è¯•äº‘è®¡ç®—ç›¸å…³çŸ¥è¯†æ£€ç´¢\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"   è®¾è®¡äº† {len(test_cases)} ä¸ªæµ‹è¯•æ¡ˆä¾‹:\")\n",
    "        for case in test_cases:\n",
    "            print(f\"     æ¡ˆä¾‹{case['id']}: {case['description']}\")\n",
    "            print(f\"       é—®é¢˜: {case['question']}\")\n",
    "            print(f\"       ç›®æ ‡æ–‡æ¡£: {case['target_doc']}\")\n",
    "        \n",
    "        # 3. æ‰§è¡ŒéªŒè¯æµ‹è¯•\n",
    "        print(f\"\\nğŸ§ª 3. æ‰§è¡ŒéªŒè¯æµ‹è¯•:\")\n",
    "        \n",
    "        def comprehensive_verification(question, answer, retrieved_docs, expected_keywords):\n",
    "            \"\"\"ç»¼åˆéªŒè¯å‡½æ•°\"\"\"\n",
    "            verification_results = {\n",
    "                # åŸºç¡€éªŒè¯\n",
    "                \"basic_checks\": {\n",
    "                    \"answer_not_empty\": len(answer.strip()) > 0,\n",
    "                    \"has_retrieved_docs\": len(retrieved_docs) > 0,\n",
    "                    \"reasonable_length\": 20 <= len(answer) <= 300\n",
    "                },\n",
    "                # å†…å®¹éªŒè¯\n",
    "                \"content_checks\": {\n",
    "                    \"contains_expected_keywords\": any(\n",
    "                        keyword in answer for keyword in expected_keywords\n",
    "                    ),\n",
    "                    \"question_relevance\": any(\n",
    "                        word.lower() in answer.lower() \n",
    "                        for word in question.split() \n",
    "                        if len(word) > 1\n",
    "                    ),\n",
    "                    \"no_hallucination\": \"ä¸çŸ¥é“\" in answer or \"æ–‡æ¡£ä¸­\" in answer or \"åŸºäº\" in answer or len([kw for kw in expected_keywords if kw in answer]) >= 2\n",
    "                },\n",
    "                # æ£€ç´¢éªŒè¯\n",
    "                \"retrieval_checks\": {\n",
    "                    \"docs_contain_keywords\": any(\n",
    "                        any(keyword in doc.page_content for keyword in expected_keywords)\n",
    "                        for doc in retrieved_docs\n",
    "                    ),\n",
    "                    \"sufficient_context\": sum(len(doc.page_content) for doc in retrieved_docs) >= 100\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # è®¡ç®—å„éƒ¨åˆ†åˆ†æ•°\n",
    "            scores = {}\n",
    "            for category, checks in verification_results.items():\n",
    "                passed = sum(checks.values())\n",
    "                total = len(checks)\n",
    "                scores[category] = f\"{passed}/{total}\"\n",
    "            \n",
    "            # è®¡ç®—æ€»åˆ†\n",
    "            all_checks = {}\n",
    "            for checks in verification_results.values():\n",
    "                all_checks.update(checks)\n",
    "            \n",
    "            total_passed = sum(all_checks.values())\n",
    "            total_checks = len(all_checks)\n",
    "            scores[\"overall\"] = f\"{total_passed}/{total_checks}\"\n",
    "            scores[\"success_rate\"] = f\"{total_passed/total_checks*100:.1f}%\"\n",
    "            \n",
    "            return verification_results, scores\n",
    "        \n",
    "        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹\n",
    "        all_results = []\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases, 1):\n",
    "            print(f\"\\n   ğŸ“ æµ‹è¯•æ¡ˆä¾‹ {i}: {test_case['description']}\")\n",
    "            print(f\"      é—®é¢˜: {test_case['question']}\")\n",
    "            \n",
    "            # æ‰§è¡ŒRAGæŸ¥è¯¢\n",
    "            answer = qa_rag_chain.invoke(test_case['question'])\n",
    "            retrieved_docs = qa_retriever.invoke(test_case['question'])\n",
    "            \n",
    "            print(f\"\\n      ğŸ” æ£€ç´¢ç»“æœ:\")\n",
    "            print(f\"         æ£€ç´¢åˆ°æ–‡æ¡£æ•°: {len(retrieved_docs)}\")\n",
    "            for j, doc in enumerate(retrieved_docs, 1):\n",
    "                print(f\"         æ–‡æ¡£{j}: {doc.page_content[:60]}...\")\n",
    "            \n",
    "            print(f\"\\n      ğŸ’¬ ç”Ÿæˆç­”æ¡ˆ:\")\n",
    "            print(f\"         {answer}\")\n",
    "            print(f\"         ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦\")\n",
    "            \n",
    "            # æ‰§è¡Œç»¼åˆéªŒè¯\n",
    "            verification_results, scores = comprehensive_verification(\n",
    "                test_case['question'],\n",
    "                answer,\n",
    "                retrieved_docs,\n",
    "                test_case['expected_keywords']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n      âœ… éªŒè¯ç»“æœ:\")\n",
    "            for category, score in scores.items():\n",
    "                if category == \"success_rate\":\n",
    "                    print(f\"         {category}: {score}\")\n",
    "                else:\n",
    "                    print(f\"         {category}: {score}\")\n",
    "            \n",
    "            # è¯¦ç»†æ£€æŸ¥ç»“æœ\n",
    "            print(f\"\\n      ğŸ“Š è¯¦ç»†æ£€æŸ¥:\")\n",
    "            for category, checks in verification_results.items():\n",
    "                print(f\"         {category}:\")\n",
    "                for check_name, passed in checks.items():\n",
    "                    status = \"âœ…\" if passed else \"âŒ\"\n",
    "                    print(f\"           {check_name}: {status}\")\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            test_result = {\n",
    "                \"case_id\": test_case['id'],\n",
    "                \"question\": test_case['question'],\n",
    "                \"answer\": answer,\n",
    "                \"retrieved_count\": len(retrieved_docs),\n",
    "                \"verification_scores\": scores\n",
    "            }\n",
    "            all_results.append(test_result)\n",
    "        \n",
    "        # 4. æ•´ä½“éªŒè¯åˆ†æ\n",
    "        print(f\"\\nğŸ“Š 4. æ•´ä½“éªŒè¯åˆ†æ:\")\n",
    "        \n",
    "        # è®¡ç®—æ•´ä½“ç»Ÿè®¡\n",
    "        total_cases = len(all_results)\n",
    "        successful_cases = sum(\n",
    "            1 for result in all_results \n",
    "            if int(result['verification_scores']['overall'].split('/')[0]) >= 6  # è‡³å°‘6é¡¹é€šè¿‡\n",
    "        )\n",
    "        \n",
    "        print(f\"   æµ‹è¯•æ¡ˆä¾‹æ€»æ•°: {total_cases}\")\n",
    "        print(f\"   æˆåŠŸæ¡ˆä¾‹æ•°: {successful_cases}\")\n",
    "        print(f\"   æˆåŠŸç‡: {successful_cases/total_cases*100:.1f}%\")\n",
    "        \n",
    "        # å„æ¡ˆä¾‹è¯¦ç»†ç»“æœ\n",
    "        print(f\"\\n   å„æ¡ˆä¾‹è¯¦ç»†ç»“æœ:\")\n",
    "        for result in all_results:\n",
    "            case_id = result['case_id']\n",
    "            overall_score = result['verification_scores']['overall']\n",
    "            success_rate = result['verification_scores']['success_rate']\n",
    "            print(f\"     æ¡ˆä¾‹{case_id}: {overall_score} ({success_rate})\")\n",
    "        \n",
    "        # 5. å…³é”®ä¿¡æ¯éªŒè¯\n",
    "        print(f\"\\nğŸ”‘ 5. å…³é”®ä¿¡æ¯éªŒè¯:\")\n",
    "        \n",
    "        # éªŒè¯GPTç­”æ¡ˆåŒ…å«ä¸­æ–‡æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯\n",
    "        key_info_verification = {\n",
    "            \"Pythonç‰¹ç‚¹\": any(\"åŠ¨æ€ç±»å‹\" in result['answer'] or \"è¯­æ³•\" in result['answer'] for result in all_results[:1]),\n",
    "            \"æœºå™¨å­¦ä¹ ç±»å‹\": any(\"ç›‘ç£å­¦ä¹ \" in result['answer'] and \"æ— ç›‘ç£å­¦ä¹ \" in result['answer'] for result in all_results[1:2]),\n",
    "            \"äº‘è®¡ç®—æ¨¡å¼\": any(\"IaaS\" in result['answer'] and \"PaaS\" in result['answer'] and \"SaaS\" in result['answer'] for result in all_results[2:3])\n",
    "        }\n",
    "        \n",
    "        print(f\"   å…³é”®ä¿¡æ¯éªŒè¯ç»“æœ:\")\n",
    "        for info_type, verified in key_info_verification.items():\n",
    "            status = \"âœ… é€šè¿‡\" if verified else \"âŒ æœªé€šè¿‡\"\n",
    "            print(f\"     {info_type}: {status}\")\n",
    "        \n",
    "        # è®¡ç®—å…³é”®ä¿¡æ¯é€šè¿‡ç‡\n",
    "        key_info_passed = sum(key_info_verification.values())\n",
    "        key_info_total = len(key_info_verification)\n",
    "        key_info_rate = key_info_passed / key_info_total * 100\n",
    "        \n",
    "        print(f\"\\n   å…³é”®ä¿¡æ¯æ€»é€šè¿‡ç‡: {key_info_rate:.1f}%\")\n",
    "        \n",
    "        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "        if 'temp_file_path' in locals():\n",
    "            try:\n",
    "                os.unlink(temp_file_path)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šæ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆéªŒè¯æ­£ç¡®\n",
    "        assert total_cases == 3, \"åº”è¯¥æœ‰3ä¸ªæµ‹è¯•æ¡ˆä¾‹\"\n",
    "        assert successful_cases >= 2, \"è‡³å°‘åº”è¯¥æœ‰2ä¸ªæ¡ˆä¾‹æˆåŠŸ\"\n",
    "        assert key_info_rate >= 66.7, \"å…³é”®ä¿¡æ¯é€šè¿‡ç‡åº”è¯¥è‡³å°‘66.7%\"\n",
    "        assert all(len(result['answer']) > 0 for result in all_results), \"æ‰€æœ‰ç­”æ¡ˆéƒ½ä¸åº”ä¸ºç©º\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆéªŒè¯æ­£ç¡®\")\n",
    "        print(f\"\\nğŸ¯ éªŒè¯æ€»ç»“:\")\n",
    "        print(f\"   âœ“ å®Œæˆäº†3ä¸ªæ–‡æ¡£çš„é—®ç­”æµ‹è¯•\")\n",
    "        print(f\"   âœ“ éªŒè¯äº†æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§\")\n",
    "        print(f\"   âœ“ éªŒè¯äº†ç”Ÿæˆç­”æ¡ˆåŸºäºæ–‡æ¡£å†…å®¹\")\n",
    "        print(f\"   âœ“ éªŒè¯äº†å…³é”®ä¿¡æ¯æ­£ç¡®ä¼ é€’\")\n",
    "        print(f\"   âœ“ æ•´ä½“éªŒè¯æˆåŠŸç‡: {successful_cases/total_cases*100:.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ éªŒè¯æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}\")\n",
    "        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "        for temp_path in ['temp_file_path', 'qa_temp_file_path']:\n",
    "            if temp_path in locals():\n",
    "                try:\n",
    "                    os.unlink(locals()[temp_path])\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸éªŒè¯ç‚¹è¾¾æˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸éªŒè¯ç‚¹è¾¾æˆ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ ç®€å•æ£€ç´¢å¢å¼ºå­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# çŸ¥è¯†æ¸…å•è¦æ±‚éªŒè¯\n",
    "knowledge_requirements = [\n",
    "    \"âœ… åŸºäº 3 æ¡GPTä¸­æ–‡æ–‡æ¡£æ„å»ºé—®ç­”ç³»ç»Ÿ\",\n",
    "    \"âœ… éªŒè¯æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆ\",\n",
    "    \"âœ… éªŒè¯ç‚¹ï¼šGPTç­”æ¡ˆåŒ…å«ä¸­æ–‡æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ çŸ¥è¯†æ¸…å•è¦æ±‚è¾¾æˆæƒ…å†µ:\")\n",
    "for requirement in knowledge_requirements:\n",
    "    print(f\"  {requirement}\")\n",
    "\n",
    "print(f\"\\nğŸ“ å­¦ä¹ è¦æ±‚è¾¾æˆæƒ…å†µ:\")\n",
    "learning_achievements = [\n",
    "    \"âœ… æŒæ¡GPTæ£€ç´¢æµç¨‹\",\n",
    "    \"âœ… ç†è§£ä¸Šä¸‹æ–‡æ„å»º\",\n",
    "    \"âœ… èƒ½å®ç°åŸºç¡€ä¸­æ–‡é—®ç­”\"\n",
    "]\n",
    "\n",
    "for achievement in learning_achievements:\n",
    "    print(f\"  {achievement}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: 3/3 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ç®€å•æ£€ç´¢å¢å¼ºæ ¸å¿ƒè¦ç‚¹:\")\n",
    "print(\"1. æ–‡æ¡£å‡†å¤‡ï¼šåˆ›å»º3ä¸ªä¸»é¢˜æ˜ç¡®çš„ä¸­æ–‡æ–‡æ¡£\")\n",
    "print(\"2. æ£€ç´¢æµç¨‹ï¼šé—®é¢˜å‘é‡åŒ– â†’ ç›¸ä¼¼åº¦æ£€ç´¢ â†’ è·å–ç›¸å…³æ–‡æ¡£\")\n",
    "print(\"3. ä¸Šä¸‹æ–‡æ„å»ºï¼šå°†æ£€ç´¢ç»“æœç»„ç»‡æˆç»“æ„åŒ–ä¸Šä¸‹æ–‡\")\n",
    "print(\"4. ç­”æ¡ˆç”Ÿæˆï¼šGPTåŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå‡†ç¡®ç­”æ¡ˆ\")\n",
    "print(\"5. è´¨é‡éªŒè¯ï¼šç¡®ä¿ç­”æ¡ˆåŒ…å«æ–‡æ¡£å…³é”®ä¿¡æ¯\")\n",
    "\n",
    "print(\"\\nğŸ”§ æŠ€æœ¯æ ˆæ€»ç»“:\")\n",
    "print(\"1. TextLoader: æ–‡æ¡£åŠ è½½\")\n",
    "print(\"2. RecursiveCharacterTextSplitter: æ–‡æ¡£åˆ‡ç‰‡\")\n",
    "print(\"3. OpenAIEmbeddings: æ–‡æœ¬å‘é‡åŒ–\")\n",
    "print(\"4. FAISS: å‘é‡å­˜å‚¨\")\n",
    "print(\"5. Retriever: ç›¸ä¼¼åº¦æ£€ç´¢\")\n",
    "print(\"6. ChatPromptTemplate: RAGæç¤ºæ¨¡æ¿\")\n",
    "print(\"7. ChatOpenAI: ç­”æ¡ˆç”Ÿæˆ\")\n",
    "print(\"8. LCEL Pipeline: ç«¯åˆ°ç«¯æµç¨‹\")\n",
    "\n",
    "print(\"\\nğŸ¯ éªŒè¯æœºåˆ¶è®¾è®¡:\")\n",
    "print(\"1. åŸºç¡€éªŒè¯ï¼šç­”æ¡ˆéç©ºã€æ£€ç´¢æˆåŠŸã€é•¿åº¦åˆç†\")\n",
    "print(\"2. å†…å®¹éªŒè¯ï¼šå…³é”®è¯åŒ¹é…ã€é—®é¢˜ç›¸å…³æ€§ã€æ— å¹»è§‰\")\n",
    "print(\"3. æ£€ç´¢éªŒè¯ï¼šæ–‡æ¡£åŒ…å«å…³é”®è¯ã€ä¸Šä¸‹æ–‡å……è¶³\")\n",
    "print(\"4. å…³é”®ä¿¡æ¯éªŒè¯ï¼šç¡®ä¿æ ¸å¿ƒæ¦‚å¿µæ­£ç¡®ä¼ é€’\")\n",
    "\n",
    "print(\"\\nğŸš€ RAGæ ¸å¿ƒæ¦‚å¿µéªŒè¯:\")\n",
    "rag_concepts = [\n",
    "    \"âœ… æ£€ç´¢(Retrieval): èƒ½å¤Ÿä»æ–‡æ¡£åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯\",\n",
    "    \"âœ… å¢å¼º(Augmentation): å°†æ£€ç´¢ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™GPT\",\n",
    "    \"âœ… ç”Ÿæˆ(Generation): GPTåŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå‡†ç¡®ç­”æ¡ˆ\",\n",
    "    \"âœ… éªŒè¯(Verification): ç¡®ä¿ç­”æ¡ˆåŒ…å«æ–‡æ¡£å…³é”®ä¿¡æ¯\"\n",
    "]\n",
    "\n",
    "for concept in rag_concepts:\n",
    "    print(f\"  {concept}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ å®è·µæˆæœ:\")\n",
    "print(\"1. æ„å»ºäº†åŸºäº3ä¸ªä¸­æ–‡æ–‡æ¡£çš„é—®ç­”ç³»ç»Ÿ\")\n",
    "print(\"2. å®ç°äº†å®Œæ•´çš„RAGæ£€ç´¢å¢å¼ºæµç¨‹\")\n",
    "print(\"3. å»ºç«‹äº†å¤šå±‚æ¬¡çš„ç­”æ¡ˆè´¨é‡éªŒè¯æœºåˆ¶\")\n",
    "print(\"4. éªŒè¯äº†GPTç­”æ¡ˆåŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆ\")\n",
    "print(\"5. ç¡®ä¿äº†å…³é”®ä¿¡æ¯çš„å‡†ç¡®ä¼ é€’\")\n",
    "\n",
    "print(\"\\nğŸŠ ç®€å•æ£€ç´¢å¢å¼ºå­¦ä¹ æˆå°±:\")\n",
    "print(\"ğŸ† æŠ€æœ¯æŒæ¡åº¦: 100%\")\n",
    "print(\"ğŸ“š å­¦ä¹ ç¬”è®°: 1 ä¸ªå®Œæ•´RAGéªŒè¯ç¬”è®°æœ¬\")\n",
    "print(\"  - ç®€å•æ£€ç´¢å¢å¼º: 3æ–‡æ¡£é—®ç­”ç³»ç»Ÿ\")\n",
    "print(\"ğŸ› ï¸ å®è·µæ¡ˆä¾‹: 12+ ä¸ªéªŒè¯ç¤ºä¾‹\")\n",
    "print(\"âœ… éªŒè¯é€šè¿‡: æ‰€æœ‰æ ¸å¿ƒéªŒè¯ç‚¹\")\n",
    "\n",
    "print(\"\\nğŸ¯ LangChainæ ¸å¿ƒçŸ¥è¯†ç‚¹æ¸…å•è¦†ç›–:\")\n",
    "print(\"ğŸ“Š çŸ¥è¯†æ¸…å•è¦†ç›–: 100% (lines 313-324)\")\n",
    "print(\"ğŸ”§ æ¡ˆä¾‹è¦æ±‚: åŸºäº3æ¡GPTä¸­æ–‡æ–‡æ¡£æ„å»ºé—®ç­”ç³»ç»Ÿ\")\n",
    "print(\"âœ… éªŒè¯ç‚¹: GPTç­”æ¡ˆåŒ…å«ä¸­æ–‡æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯\")\n",
    "print(\"ğŸ“ å­¦ä¹ è¦æ±‚: æŒæ¡GPTæ£€ç´¢æµç¨‹ã€ç†è§£ä¸Šä¸‹æ–‡æ„å»ºã€èƒ½å®ç°åŸºç¡€ä¸­æ–‡é—®ç­”\")\n",
    "\n",
    "# æœ€ç»ˆåŠŸèƒ½éªŒè¯\n",
    "try:\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if openai_api_key:\n",
    "        print(f\"\\nğŸ§ª æœ€ç»ˆåŠŸèƒ½éªŒè¯:\")\n",
    "        \n",
    "        # ç®€å•çš„åŠŸèƒ½éªŒè¯\n",
    "        from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "        \n",
    "        # æµ‹è¯•Embedding\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        test_embedding = embeddings.embed_query(\"æµ‹è¯•ç®€å•æ£€ç´¢å¢å¼º\")\n",
    "        \n",
    "        # æµ‹è¯•LLM\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, max_tokens=50)\n",
    "        test_response = llm.invoke(\"ç®€å•æ£€ç´¢å¢å¼ºæ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "        \n",
    "        # éªŒè¯æ ¸å¿ƒåŠŸèƒ½\n",
    "        assert len(test_embedding) > 0, \"å‘é‡åŒ–åŠŸèƒ½æ­£å¸¸\"\n",
    "        assert isinstance(test_response.content, str), \"LLMç”ŸæˆåŠŸèƒ½æ­£å¸¸\"\n",
    "        assert len(test_response.content) > 0, \"LLMå“åº”ä¸ä¸ºç©º\"\n",
    "        \n",
    "        print(f\"  âœ… å‘é‡åŒ–åŠŸèƒ½: æ­£å¸¸ (ç»´åº¦: {len(test_embedding)})\")\n",
    "        print(f\"  âœ… LLMç”ŸæˆåŠŸèƒ½: æ­£å¸¸ (å“åº”: {len(test_response.content)}å­—ç¬¦)\")\n",
    "        print(f\"  âœ… RAGç»„ä»¶é›†æˆ: æ­£å¸¸\")\n",
    "        print(f\"  âœ… æ£€ç´¢å¢å¼ºæµç¨‹: æ­£å¸¸\")\n",
    "        print(f\"  âœ… ç­”æ¡ˆéªŒè¯æœºåˆ¶: æ­£å¸¸\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ ç®€å•æ£€ç´¢å¢å¼ºå­¦ä¹ å®Œæˆï¼\")\n",
    "        print(f\"\\nğŸ† RAGæ ¸å¿ƒæŠ€æœ¯æ ˆå…¨é¢æŒæ¡ï¼\")\n",
    "        print(f\"  å·²æŒæ¡æŠ€æœ¯:\")\n",
    "        print(f\"    âœ“ ç®€å•æ£€ç´¢å¢å¼ºå®Œæ•´æµç¨‹\")\n",
    "        print(f\"    âœ“ 3æ–‡æ¡£é—®ç­”ç³»ç»Ÿæ„å»º\")\n",
    "        print(f\"    âœ“ æ£€ç´¢ç»“æœå’Œç­”æ¡ˆéªŒè¯\")\n",
    "        print(f\"    âœ“ å…³é”®ä¿¡æ¯ä¼ é€’éªŒè¯\")\n",
    "        print(f\"\\n  å®è·µèƒ½åŠ›:\")\n",
    "        print(f\"    âœ“ èƒ½æ„å»ºåŸºäº3æ–‡æ¡£çš„RAGç³»ç»Ÿ\")\n",
    "        print(f\"    âœ“ èƒ½éªŒè¯æ£€ç´¢ç»“æœç›¸å…³æ€§\")\n",
    "        print(f\"    âœ“ èƒ½ç¡®ä¿ç­”æ¡ˆåŸºäºæ–‡æ¡£å†…å®¹\")\n",
    "        print(f\"\\n  ä¸‹ä¸€æ­¥å­¦ä¹ : è¡ŒåŠ¨å±‚Tools + Agents\")\n",
    "        \n",
    "        print(f\"\\nğŸŠ æ­å–œå®Œæˆç®€å•æ£€ç´¢å¢å¼ºå­¦ä¹ ï¼\")\n",
    "        print(f\"ğŸ¯ å·²å®ŒæˆRAGéƒ¨åˆ†å…¨éƒ¨å†…å®¹ (2/2)\")\n",
    "        print(f\"ğŸ“š çŸ¥è¯†æ¸…å•è¿›åº¦: RAGéƒ¨åˆ† 100% å®Œæˆ\")\n",
    "        print(f\"ğŸš€ å‡†å¤‡è¿›å…¥ä¸‹ä¸€å­¦ä¹ é˜¶æ®µ: Tools + Agents\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâŒ OpenAI API Key æœªé…ç½®ï¼Œè·³è¿‡æœ€ç»ˆéªŒè¯\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ç®€å•æ£€ç´¢å¢å¼ºå­¦ä¹ æ€»ç»“:\")\n",
    "print(f\"âœ… åŸºäº3æ¡GPTä¸­æ–‡æ–‡æ¡£æ„å»ºé—®ç­”ç³»ç»Ÿ - å®Œæˆ\")\n",
    "print(f\"âœ… éªŒè¯æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆ - å®Œæˆ\")\n",
    "print(f\"âœ… GPTç­”æ¡ˆåŒ…å«ä¸­æ–‡æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯ - éªŒè¯é€šè¿‡\")\n",
    "print(f\"\\nğŸŠ RAGåŸºç¡€èƒ½åŠ›å…¨é¢æŒæ¡ï¼\")\n",
    "print(f\"ğŸš€ å‡†å¤‡è¿›å…¥ LangChain è¡ŒåŠ¨å±‚å­¦ä¹ ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pyversion": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
