{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 - FastAPIåŸºç¡€éƒ¨ç½²\n",
    "\n",
    "## ç”¨é€”\n",
    "é€šè¿‡ FastAPI å°†GPT LCEL æµæ°´çº¿å‘å¸ƒä¸º API æœåŠ¡ã€‚æŒæ¡ FastAPI åŸºç¡€ã€ç†è§£ API è®¾è®¡ã€èƒ½éƒ¨ç½²GPTç®€å•æœåŠ¡ã€‚\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- æŒæ¡ FastAPI åŸºç¡€\n",
    "- ç†è§£ API è®¾è®¡\n",
    "- èƒ½éƒ¨ç½²GPTç®€å•æœåŠ¡\n",
    "- ç”¨ FastAPI åŒ…è£…ä¸€ä¸ªGPT LLM Chain\n",
    "- åˆ›å»ºç®€å•çš„ POST ç«¯ç‚¹\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½é€šè¿‡ HTTP è°ƒç”¨å¹¶è·å–GPTå›å¤\n",
    "\n",
    "## ğŸ”‘ å‰ç½®è¦æ±‚\n",
    "å·²å®‰è£… LangChain å’Œ OpenAI ç›¸å…³åŒ…\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FastAPIåŸºç¡€æ¦‚å¿µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPIåŸºç¡€æ¦‚å¿µ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸš€ FastAPIåŸºç¡€æ¦‚å¿µ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"ğŸ“ FastAPIæ ¸å¿ƒæ¦‚å¿µ:\")\n",
    "print(\"   1. ç°ä»£Python Webæ¡†æ¶: åŸºäºStarletteå’ŒPydantic\")\n",
    "print(\"   2. è‡ªåŠ¨APIæ–‡æ¡£: ç”Ÿæˆäº¤äº’å¼Swagger UI\")\n",
    "print(\"   3. ç±»å‹æç¤º: åˆ©ç”¨Pythonç±»å‹æ³¨è§£è¿›è¡ŒéªŒè¯\")\n",
    "print(\"   4. é«˜æ€§èƒ½: åŸºäºASGIï¼Œæ€§èƒ½æ¥è¿‘Node.jså’ŒGo\")\n",
    "print(\"   5. æ˜“äºå­¦ä¹ : ç®€æ´ç›´è§‚çš„APIè®¾è®¡\")\n",
    "\n",
    "print(f\"\\nğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   - æŒæ¡ FastAPI åŸºç¡€\")\n",
    "print(f\"   - ç†è§£ API è®¾è®¡\")\n",
    "print(f\"   - èƒ½éƒ¨ç½²GPTç®€å•æœåŠ¡\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸  FastAPIæ ¸å¿ƒç»„ä»¶:\")\n",
    "print(f\"   1. FastAPI: ä¸»åº”ç”¨ç±»\")\n",
    "print(f\"   2. app: åº”ç”¨å®ä¾‹\")\n",
    "print(f\"   3. @app.post(): è·¯ç”±è£…é¥°å™¨\")\n",
    "print(f\"   4. Pydanticæ¨¡å‹: æ•°æ®éªŒè¯å’Œåºåˆ—åŒ–\")\n",
    "print(f\"   5. HTTPæ–¹æ³•: GET, POST, PUT, DELETEç­‰\")\n",
    "print(f\"   6. çŠ¶æ€ç : 200, 201, 400, 500ç­‰\")\n",
    "\n",
    "print(f\"\\nğŸ”§ FastAPIå®‰è£…å’ŒåŸºç¡€:\")\n",
    "print(f\"   å®‰è£…å‘½ä»¤: pip install fastapi uvicorn\")\n",
    "print(f\"   è¿è¡Œå‘½ä»¤: uvicorn app:app --reload\")\n",
    "print(f\"   æ–‡æ¡£åœ°å€: http://localhost:8000/docs\")\n",
    "print(f\"   OpenAPI: http://localhost:8000/openapi.json\")\n",
    "\n",
    "print(f\"\\nğŸ“Š FastAPI vs ä¼ ç»ŸWebæ¡†æ¶:\")\n",
    "print(f\"   Flask:\")\n",
    "print(f\"     - è½»é‡çº§ï¼Œçµæ´»ä½†éœ€è¦æ›´å¤šé…ç½®\")\n",
    "print(f\"     - æ‰‹åŠ¨æ–‡æ¡£ç”Ÿæˆ\")\n",
    "print(f\"     - åŸºç¡€ç±»å‹éªŒè¯\")\n",
    "print(f\"\\n   FastAPI:\")\n",
    "print(f\"     - ç°ä»£åŒ–ï¼Œå¼€ç®±å³ç”¨\")\n",
    "print(f\"     - è‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆ\")\n",
    "print(f\"     - å¼ºç±»å‹éªŒè¯\")\n",
    "print(f\"     - é«˜æ€§èƒ½å¼‚æ­¥æ”¯æŒ\")\n",
    "\n",
    "print(f\"\\nğŸ¨ APIè®¾è®¡åŸåˆ™:\")\n",
    "print(f\"   1. RESTfulè®¾è®¡: éµå¾ªRESTæ¶æ„é£æ ¼\")\n",
    "print(f\"   2. æ¸…æ™°çš„ç«¯ç‚¹: æœ‰æ„ä¹‰çš„URLè·¯å¾„\")\n",
    "print(f\"   3. æ ‡å‡†çŠ¶æ€ç : æ­£ç¡®ä½¿ç”¨HTTPçŠ¶æ€ç \")\n",
    "print(f\"   4. æ•°æ®éªŒè¯: è¾“å…¥è¾“å‡ºæ•°æ®éªŒè¯\")\n",
    "print(f\"   5. é”™è¯¯å¤„ç†: ç»Ÿä¸€çš„é”™è¯¯å“åº”æ ¼å¼\")\n",
    "\n",
    "print(f\"\\nğŸ” FastAPIåœ¨LangChainä¸­çš„åº”ç”¨:\")\n",
    "print(f\"   1. LLMæœåŠ¡åŒ–: å°†LangChainé“¾åŒ…è£…ä¸ºAPI\")\n",
    "print(f\"   2. æ¨¡å‹åˆ‡æ¢: åŠ¨æ€åˆ‡æ¢ä¸åŒçš„GPTæ¨¡å‹\")\n",
    "print(f\"   3. æµå¼å“åº”: æ”¯æŒæµå¼è¾“å‡º\")\n",
    "print(f\"   4. å¼‚æ­¥å¤„ç†: é«˜å¹¶å‘è¯·æ±‚å¤„ç†\")\n",
    "print(f\"   5. ç›‘æ§æ—¥å¿—: è¯·æ±‚è¿½è¸ªå’Œæ€§èƒ½ç›‘æ§\")\n",
    "\n",
    "print(f\"\\nâœ… FastAPIåŸºç¡€æ¦‚å¿µç†è§£å®Œæˆ\")\n",
    "print(f\"\\nğŸš€ å‡†å¤‡åˆ›å»ºGPT LLM Chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åˆ›å»ºGPT LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºGPT LLM Chain - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”— åˆ›å»ºGPT LLM Chain:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ç¯å¢ƒæ£€æŸ¥\n",
    "print(f\"ğŸ” 1. ç¯å¢ƒæ£€æŸ¥:\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"   âœ… OpenAI API Key: å·²é…ç½®\")\n",
    "    print(f\"   Keyé•¿åº¦: {len(api_key)} å­—ç¬¦\")\n",
    "else:\n",
    "    print(f\"   âŒ OpenAI API Key: æœªé…ç½®\")\n",
    "    print(f\"   è¯·è®¾ç½®ç¯å¢ƒå˜é‡: OPENAI_API_KEY\")\n",
    "    exit()\n",
    "\n",
    "# 2. åˆ›å»ºChatOpenAIå®ä¾‹\n",
    "print(f\"\\nğŸ¤– 2. åˆ›å»ºChatOpenAIå®ä¾‹:\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "print(f\"   æ¨¡å‹é…ç½®:\")\n",
    "print(f\"     æ¨¡å‹: {llm.model_name}\")\n",
    "print(f\"     æ¸©åº¦: {llm.temperature}\")\n",
    "print(f\"     æœ€å¤§ä»¤ç‰Œ: {llm.max_tokens}\")\n",
    "print(f\"   LLMç±»å‹: {type(llm)}\")\n",
    "print(f\"   å¯è°ƒç”¨: {hasattr(llm, '__call__')}\")\n",
    "\n",
    "# 3. åˆ›å»ºæç¤ºæ¨¡æ¿\n",
    "print(f\"\\nğŸ“ 3. åˆ›å»ºæç¤ºæ¨¡æ¿:\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\"),\n",
    "    (\"user\", \"ç”¨æˆ·é—®é¢˜: {query}\")\n",
    "])\n",
    "\n",
    "print(f\"   æç¤ºæ¨¡æ¿åˆ›å»º:\")\n",
    "print(f\"     æ¨¡æ¿ç±»å‹: {type(prompt_template)}\")\n",
    "print(f\"     æ¶ˆæ¯æ•°é‡: {len(prompt_template.messages)}\")\n",
    "print(f\"     è¾“å…¥å˜é‡: {prompt_template.input_variables}\")\n",
    "\n",
    "# 4. åˆ›å»ºè¾“å‡ºè§£æå™¨\n",
    "print(f\"\\nğŸ“¤ 4. åˆ›å»ºè¾“å‡ºè§£æå™¨:\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "print(f\"   è§£æå™¨é…ç½®:\")\n",
    "print(f\"     è§£æå™¨ç±»å‹: {type(output_parser)}\")\n",
    "print(f\"     å¯è°ƒç”¨: {hasattr(output_parser, '__call__')}\")\n",
    "\n",
    "# 5. æ„å»ºLCEL Chain\n",
    "print(f\"\\nâ›“ï¸  5. æ„å»ºLCEL Chain:\")\n",
    "\n",
    "chain = prompt_template | llm | output_parser\n",
    "\n",
    "print(f\"   Chainæ„å»º:\")\n",
    "print(f\"     Chainç±»å‹: {type(chain)}\")\n",
    "print(f\"     å¯è°ƒç”¨: {hasattr(chain, '__call__')}\")\n",
    "print(f\"     å¯æµå¼: {hasattr(chain, 'stream')}\")\n",
    "\n",
    "# 6. æµ‹è¯•ChainåŠŸèƒ½\n",
    "print(f\"\\nğŸ§ª 6. æµ‹è¯•ChainåŠŸèƒ½:\")\n",
    "\n",
    "try:\n",
    "    # æµ‹è¯•è¾“å…¥\n",
    "    test_input = {\"query\": \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹FastAPI\"}\n",
    "    print(f\"   æµ‹è¯•è¾“å…¥: {test_input}\")\n",
    "    \n",
    "    # æ‰§è¡ŒChain\n",
    "    print(f\"   æ‰§è¡ŒChainä¸­...\")\n",
    "    result = chain.invoke(test_input)\n",
    "    \n",
    "    print(f\"   âœ… Chainæ‰§è¡ŒæˆåŠŸ\")\n",
    "    print(f\"   å“åº”é•¿åº¦: {len(result)} å­—ç¬¦\")\n",
    "    print(f\"   å“åº”é¢„è§ˆ: {result[:100]}...\")\n",
    "    \n",
    "    chain_working = True\n",
    "    test_result = result\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Chainæ‰§è¡Œå¤±è´¥: {e}\")\n",
    "    chain_working = False\n",
    "    test_result = \"\"\n",
    "\n",
    "# 7. ChainåŠŸèƒ½éªŒè¯\n",
    "print(f\"\\nğŸ” 7. ChainåŠŸèƒ½éªŒè¯:\")\n",
    "\n",
    "def validate_chain(chain_obj, test_result_str: str) -> bool:\n",
    "    \"\"\"éªŒè¯ChainåŠŸèƒ½\"\"\"\n",
    "    print(f\"     éªŒè¯ChainåŠŸèƒ½...\")\n",
    "    \n",
    "    # æ£€æŸ¥Chainå¯¹è±¡\n",
    "    if not hasattr(chain_obj, '__call__'):\n",
    "        print(f\"       âŒ Chainä¸å¯è°ƒç”¨\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥æµ‹è¯•ç»“æœ\n",
    "    if not isinstance(test_result_str, str):\n",
    "        print(f\"       âŒ ç»“æœä¸æ˜¯å­—ç¬¦ä¸²\")\n",
    "        return False\n",
    "    \n",
    "    if len(test_result_str) < 10:\n",
    "        print(f\"       âŒ ç»“æœè¿‡çŸ­\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥ä¸­æ–‡å“åº”\n",
    "    if not any('\\u4e00' <= char <= '\\u9fff' for char in test_result_str):\n",
    "        print(f\"       âŒ ç»“æœä¸åŒ…å«ä¸­æ–‡å­—ç¬¦\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"       âœ… ChainåŠŸèƒ½éªŒè¯é€šè¿‡\")\n",
    "    return True\n",
    "\n",
    "# æ‰§è¡ŒéªŒè¯\n",
    "chain_valid = validate_chain(chain, test_result) if chain_working else False\n",
    "\n",
    "print(f\"\\n   éªŒè¯ç»“æœ:\")\n",
    "print(f\"     ChainçŠ¶æ€: {'âœ… æ­£å¸¸' if chain_working else 'âŒ å¼‚å¸¸'}\")\n",
    "print(f\"     åŠŸèƒ½éªŒè¯: {'âœ… é€šè¿‡' if chain_valid else 'âŒ å¤±è´¥'}\")\n",
    "\n",
    "# 8. ä¿å­˜Chainä¾›åç»­ä½¿ç”¨\n",
    "print(f\"\\nğŸ’¾ 8. ä¿å­˜Chainä¾›åç»­ä½¿ç”¨:\")\n",
    "\n",
    "globals().update({\n",
    "    'llm': llm,\n",
    "    'prompt_template': prompt_template,\n",
    "    'output_parser': output_parser,\n",
    "    'chain': chain,\n",
    "    'test_input': test_input,\n",
    "    'test_result': test_result\n",
    "})\n",
    "\n",
    "print(f\"   Chainç»„ä»¶å·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   chain: å®Œæ•´çš„LCEL Chain\")\n",
    "print(f\"   test_input: æµ‹è¯•è¾“å…¥\")\n",
    "print(f\"   test_result: æµ‹è¯•ç»“æœ\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šGPT LLM Chainåˆ›å»ºæ­£ç¡®\n",
    "assert chain is not None, \"Chainåº”è¯¥åˆ›å»ºæˆåŠŸ\"\n",
    "assert chain_working, \"Chainåº”è¯¥èƒ½æ­£å¸¸æ‰§è¡Œ\"\n",
    "assert chain_valid, \"ChainåŠŸèƒ½åº”è¯¥éªŒè¯é€šè¿‡\"\n",
    "assert len(test_result) > 10, \"æµ‹è¯•ç»“æœåº”è¯¥æœ‰è¶³å¤Ÿé•¿åº¦\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šGPT LLM Chainåˆ›å»ºæ­£ç¡®\")\n",
    "print(f\"\\nğŸ¯ GPT LLM Chainæ€»ç»“:\")\n",
    "print(f\"   âœ“ åˆ›å»ºChatOpenAIå®ä¾‹ (gpt-4o-mini)\")\n",
    "print(f\"   âœ“ æ„å»ºä¸­æ–‡æç¤ºæ¨¡æ¿\")\n",
    "print(f\"   âœ“ é…ç½®å­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨\")\n",
    "print(f\"   âœ“ ç»„è£…å®Œæ•´LCEL Chain\")\n",
    "print(f\"   âœ“ éªŒè¯Chainèƒ½æ­£å¸¸ç”Ÿæˆä¸­æ–‡å›å¤\")\n",
    "print(f\"   âœ“ å‡†å¤‡ç”¨FastAPIåŒ…è£…Chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å®šä¹‰Pydanticæ•°æ®æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰Pydanticæ•°æ®æ¨¡å‹ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ—ï¸ å®šä¹‰Pydanticæ•°æ®æ¨¡å‹:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. PydanticåŸºç¡€æ¦‚å¿µ\n",
    "print(f\"ğŸ“ 1. PydanticåŸºç¡€æ¦‚å¿µ:\")\n",
    "\n",
    "print(f\"   Pydanticç‰¹ç‚¹:\")\n",
    "print(f\"     1. æ•°æ®éªŒè¯: è‡ªåŠ¨éªŒè¯è¾“å…¥æ•°æ®\")\n",
    "print(f\"     2. ç±»å‹æ³¨è§£: åŸºäºPythonç±»å‹æç¤º\")\n",
    "print(f\"     3. åºåˆ—åŒ–: è‡ªåŠ¨JSONåºåˆ—åŒ–/ååºåˆ—åŒ–\")\n",
    "print(f\"     4. æ–‡æ¡£ç”Ÿæˆ: è‡ªåŠ¨ç”ŸæˆAPIæ–‡æ¡£\")\n",
    "print(f\"     5. é”™è¯¯å¤„ç†: è¯¦ç»†çš„éªŒè¯é”™è¯¯ä¿¡æ¯\")\n",
    "\n",
    "# 2. å®šä¹‰è¯·æ±‚æ•°æ®æ¨¡å‹\n",
    "print(f\"\\nğŸ“¥ 2. å®šä¹‰è¯·æ±‚æ•°æ®æ¨¡å‹:\")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    \"\"\"èŠå¤©è¯·æ±‚æ•°æ®æ¨¡å‹\"\"\"\n",
    "    query: str = Field(\n",
    "        ..., \n",
    "        min_length=1, \n",
    "        max_length=1000, \n",
    "        description=\"ç”¨æˆ·æŸ¥è¯¢é—®é¢˜\",\n",
    "        example=\"è¯·ä»‹ç»ä¸€ä¸‹LangChain\"\n",
    "    )\n",
    "    temperature: Optional[float] = Field(\n",
    "        0.7, \n",
    "        ge=0.0, \n",
    "        le=2.0, \n",
    "        description=\"ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§\",\n",
    "        example=0.7\n",
    "    )\n",
    "    max_tokens: Optional[int] = Field(\n",
    "        500, \n",
    "        ge=50, \n",
    "        le=2000, \n",
    "        description=\"æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°\",\n",
    "        example=500\n",
    "    )\n",
    "\n",
    "print(f\"   ChatRequestæ¨¡å‹å®šä¹‰:\")\n",
    "print(f\"     æ¨¡å‹ç±»å‹: {type(ChatRequest)}\")\n",
    "print(f\"     å­—æ®µæ•°é‡: {len(ChatRequest.model_fields)}\")\n",
    "print(f\"     å­—æ®µåˆ—è¡¨: {list(ChatRequest.model_fields.keys())}\")\n",
    "\n",
    "# 3. å®šä¹‰å“åº”æ•°æ®æ¨¡å‹\n",
    "print(f\"\\nğŸ“¤ 3. å®šä¹‰å“åº”æ•°æ®æ¨¡å‹:\")\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    \"\"\"èŠå¤©å“åº”æ•°æ®æ¨¡å‹\"\"\"\n",
    "    success: bool = Field(\n",
    "        ..., \n",
    "        description=\"è¯·æ±‚æ˜¯å¦æˆåŠŸ\",\n",
    "        example=True\n",
    "    )\n",
    "    response: str = Field(\n",
    "        ..., \n",
    "        min_length=1, \n",
    "        description=\"AIç”Ÿæˆçš„å›å¤\",\n",
    "        example=\"LangChainæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶...\"\n",
    "    )\n",
    "    model: str = Field(\n",
    "        ..., \n",
    "        description=\"ä½¿ç”¨çš„æ¨¡å‹åç§°\",\n",
    "        example=\"gpt-4o-mini\"\n",
    "    )\n",
    "    tokens_used: Optional[int] = Field(\n",
    "        None, \n",
    "        ge=0, \n",
    "        description=\"ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡\",\n",
    "        example=150\n",
    "    )\n",
    "    error: Optional[str] = Field(\n",
    "        None, \n",
    "        description=\"é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰\",\n",
    "        example=None\n",
    "    )\n",
    "\n",
    "print(f\"   ChatResponseæ¨¡å‹å®šä¹‰:\")\n",
    "print(f\"     æ¨¡å‹ç±»å‹: {type(ChatResponse)}\")\n",
    "print(f\"     å­—æ®µæ•°é‡: {len(ChatResponse.model_fields)}\")\n",
    "print(f\"     å­—æ®µåˆ—è¡¨: {list(ChatResponse.model_fields.keys())}\")\n",
    "\n",
    "# 4. æµ‹è¯•æ•°æ®æ¨¡å‹\n",
    "print(f\"\\nğŸ§ª 4. æµ‹è¯•æ•°æ®æ¨¡å‹:\")\n",
    "\n",
    "# æµ‹è¯•è¯·æ±‚æ•°æ®\n",
    "print(f\"   æµ‹è¯•è¯·æ±‚æ•°æ®:\")\n",
    "try:\n",
    "    test_request = ChatRequest(\n",
    "        query=\"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹FastAPI\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    print(f\"     âœ… è¯·æ±‚æ•°æ®åˆ›å»ºæˆåŠŸ\")\n",
    "    print(f\"     query: {test_request.query}\")\n",
    "    print(f\"     temperature: {test_request.temperature}\")\n",
    "    print(f\"     max_tokens: {test_request.max_tokens}\")\n",
    "    request_valid = True\n",
    "except Exception as e:\n",
    "    print(f\"     âŒ è¯·æ±‚æ•°æ®åˆ›å»ºå¤±è´¥: {e}\")\n",
    "    request_valid = False\n",
    "\n",
    "# æµ‹è¯•å“åº”æ•°æ®\n",
    "print(f\"\\n   æµ‹è¯•å“åº”æ•°æ®:\")\n",
    "try:\n",
    "    test_response = ChatResponse(\n",
    "        success=True,\n",
    "        response=\"FastAPIæ˜¯ä¸€ä¸ªç°ä»£ã€å¿«é€Ÿçš„Python Webæ¡†æ¶...\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tokens_used=120\n",
    "    )\n",
    "    print(f\"     âœ… å“åº”æ•°æ®åˆ›å»ºæˆåŠŸ\")\n",
    "    print(f\"     success: {test_response.success}\")\n",
    "    print(f\"     responseé•¿åº¦: {len(test_response.response)}\")\n",
    "    print(f\"     model: {test_response.model}\")\n",
    "    print(f\"     tokens_used: {test_response.tokens_used}\")\n",
    "    response_valid = True\n",
    "except Exception as e:\n",
    "    print(f\"     âŒ å“åº”æ•°æ®åˆ›å»ºå¤±è´¥: {e}\")\n",
    "    response_valid = False\n",
    "\n",
    "# 5. æ•°æ®éªŒè¯æµ‹è¯•\n",
    "print(f\"\\nğŸ” 5. æ•°æ®éªŒè¯æµ‹è¯•:\")\n",
    "\n",
    "# æµ‹è¯•æœ‰æ•ˆæ•°æ®\n",
    "print(f\"   æµ‹è¯•æœ‰æ•ˆæ•°æ®:\")\n",
    "try:\n",
    "    valid_request = ChatRequest(query=\"æœ‰æ•ˆæŸ¥è¯¢\")\n",
    "    print(f\"     âœ… æœ‰æ•ˆæ•°æ®éªŒè¯é€šè¿‡\")\n",
    "    valid_data_works = True\n",
    "except Exception as e:\n",
    "    print(f\"     âŒ æœ‰æ•ˆæ•°æ®éªŒè¯å¤±è´¥: {e}\")\n",
    "    valid_data_works = False\n",
    "\n",
    "# æµ‹è¯•æ— æ•ˆæ•°æ®\n",
    "print(f\"\\n   æµ‹è¯•æ— æ•ˆæ•°æ®:\")\n",
    "try:\n",
    "    invalid_request = ChatRequest(query=\"\")  # ç©ºæŸ¥è¯¢åº”è¯¥å¤±è´¥\n",
    "    print(f\"     âŒ æ— æ•ˆæ•°æ®éªŒè¯åº”è¯¥å¤±è´¥ä½†é€šè¿‡äº†\")\n",
    "    invalid_data_works = False\n",
    "except Exception as e:\n",
    "    print(f\"     âœ… æ— æ•ˆæ•°æ®æ­£ç¡®è¢«æ‹’ç»: {type(e).__name__}\")\n",
    "    invalid_data_works = True\n",
    "\n",
    "# 6. JSONåºåˆ—åŒ–æµ‹è¯•\n",
    "print(f\"\\nğŸ”„ 6. JSONåºåˆ—åŒ–æµ‹è¯•:\")\n",
    "\n",
    "if request_valid and response_valid:\n",
    "    try:\n",
    "        # åºåˆ—åŒ–è¯·æ±‚æ•°æ®\n",
    "        request_json = test_request.model_dump_json()\n",
    "        print(f\"     è¯·æ±‚æ•°æ®åºåˆ—åŒ–: {len(request_json)} å­—ç¬¦\")\n",
    "        \n",
    "        # åºåˆ—åŒ–å“åº”æ•°æ®\n",
    "        response_json = test_response.model_dump_json()\n",
    "        print(f\"     å“åº”æ•°æ®åºåˆ—åŒ–: {len(response_json)} å­—ç¬¦\")\n",
    "        \n",
    "        # ååºåˆ—åŒ–\n",
    "        request_parsed = ChatRequest.model_validate_json(request_json)\n",
    "        response_parsed = ChatResponse.model_validate_json(response_json)\n",
    "        \n",
    "        print(f\"     è¯·æ±‚æ•°æ®ååºåˆ—åŒ–: âœ…\")\n",
    "        print(f\"     å“åº”æ•°æ®ååºåˆ—åŒ–: âœ…\")\n",
    "        \n",
    "        serialization_works = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     âŒ åºåˆ—åŒ–æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        serialization_works = False\n",
    "else:\n",
    "    serialization_works = False\n",
    "\n",
    "# 7. ä¿å­˜æ•°æ®æ¨¡å‹ä¾›åç»­ä½¿ç”¨\n",
    "print(f\"\\nğŸ’¾ 7. ä¿å­˜æ•°æ®æ¨¡å‹ä¾›åç»­ä½¿ç”¨:\")\n",
    "\n",
    "globals().update({\n",
    "    'ChatRequest': ChatRequest,\n",
    "    'ChatResponse': ChatResponse,\n",
    "    'test_request': test_request,\n",
    "    'test_response': test_response\n",
    "})\n",
    "\n",
    "print(f\"   æ•°æ®æ¨¡å‹å·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   ChatRequest: è¯·æ±‚æ•°æ®æ¨¡å‹\")\n",
    "print(f\"   ChatResponse: å“åº”æ•°æ®æ¨¡å‹\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šPydanticæ•°æ®æ¨¡å‹å®šä¹‰æ­£ç¡®\n",
    "assert hasattr(ChatRequest, 'model_fields'), \"ChatRequeståº”è¯¥æœ‰æ¨¡å‹å­—æ®µ\"\n",
    "assert hasattr(ChatResponse, 'model_fields'), \"ChatResponseåº”è¯¥æœ‰æ¨¡å‹å­—æ®µ\"\n",
    "assert request_valid, \"è¯·æ±‚æ•°æ®æ¨¡å‹åº”è¯¥æœ‰æ•ˆ\"\n",
    "assert response_valid, \"å“åº”æ•°æ®æ¨¡å‹åº”è¯¥æœ‰æ•ˆ\"\n",
    "assert valid_data_works, \"æœ‰æ•ˆæ•°æ®éªŒè¯åº”è¯¥é€šè¿‡\"\n",
    "assert invalid_data_works, \"æ— æ•ˆæ•°æ®åº”è¯¥è¢«æ­£ç¡®æ‹’ç»\"\n",
    "assert serialization_works, \"JSONåºåˆ—åŒ–åº”è¯¥æ­£å¸¸å·¥ä½œ\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šPydanticæ•°æ®æ¨¡å‹å®šä¹‰æ­£ç¡®\")\n",
    "print(f\"\\nğŸ¯ Pydanticæ•°æ®æ¨¡å‹æ€»ç»“:\")\n",
    "print(f\"   âœ“ å®šä¹‰ChatRequestè¯·æ±‚æ•°æ®æ¨¡å‹\")\n",
    "print(f\"   âœ“ å®šä¹‰ChatResponseå“åº”æ•°æ®æ¨¡å‹\")\n",
    "print(f\"   âœ“ é…ç½®å­—æ®µéªŒè¯å’Œçº¦æŸ\")\n",
    "print(f\"   âœ“ éªŒè¯æ•°æ®æ¨¡å‹åˆ›å»ºå’ŒéªŒè¯\")\n",
    "print(f\"   âœ“ æµ‹è¯•JSONåºåˆ—åŒ–/ååºåˆ—åŒ–\")\n",
    "print(f\"   âœ“ å‡†å¤‡ç”¨äºFastAPIç«¯ç‚¹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åˆ›å»ºFastAPIåº”ç”¨å’ŒPOSTç«¯ç‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºFastAPIåº”ç”¨å’ŒPOSTç«¯ç‚¹ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸš€ åˆ›å»ºFastAPIåº”ç”¨å’ŒPOSTç«¯ç‚¹:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. å¯¼å…¥FastAPIç»„ä»¶\n",
    "print(f\"ğŸ“¦ 1. å¯¼å…¥FastAPIç»„ä»¶:\")\n",
    "\n",
    "try:\n",
    "    from fastapi import FastAPI, HTTPException\n",
    "    print(f\"   âœ… FastAPIå¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"   âŒ FastAPIå¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(f\"   è¯·å®‰è£…: pip install fastapi\")\n",
    "    exit()\n",
    "\n",
    "# 2. é‡æ–°å®šä¹‰æ•°æ®æ¨¡å‹ï¼ˆç¡®ä¿ç‹¬ç«‹æ€§ï¼‰\n",
    "print(f\"\\nğŸ“ 2. é‡æ–°å®šä¹‰æ•°æ®æ¨¡å‹:\")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    \"\"\"èŠå¤©è¯·æ±‚æ•°æ®æ¨¡å‹\"\"\"\n",
    "    query: str = Field(..., min_length=1, max_length=1000, description=\"ç”¨æˆ·æŸ¥è¯¢é—®é¢˜\")\n",
    "    temperature: Optional[float] = Field(0.7, ge=0.0, le=2.0, description=\"ç”Ÿæˆæ¸©åº¦\")\n",
    "    max_tokens: Optional[int] = Field(500, ge=50, le=2000, description=\"æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°\")\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    \"\"\"èŠå¤©å“åº”æ•°æ®æ¨¡å‹\"\"\"\n",
    "    success: bool = Field(..., description=\"è¯·æ±‚æ˜¯å¦æˆåŠŸ\")\n",
    "    response: str = Field(..., min_length=1, description=\"AIç”Ÿæˆçš„å›å¤\")\n",
    "    model: str = Field(..., description=\"ä½¿ç”¨çš„æ¨¡å‹åç§°\")\n",
    "    tokens_used: Optional[int] = Field(None, ge=0, description=\"ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡\")\n",
    "    error: Optional[str] = Field(None, description=\"é”™è¯¯ä¿¡æ¯\")\n",
    "\n",
    "print(f\"   æ•°æ®æ¨¡å‹é‡æ–°å®šä¹‰å®Œæˆ\")\n",
    "\n",
    "# 3. é‡æ–°åˆ›å»ºLLM Chainï¼ˆç¡®ä¿ç‹¬ç«‹æ€§ï¼‰\n",
    "print(f\"\\nğŸ”— 3. é‡æ–°åˆ›å»ºLLM Chain:\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(f\"   âŒ OpenAI API Keyæœªé…ç½®\")\n",
    "    exit()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\"),\n",
    "    (\"user\", \"ç”¨æˆ·é—®é¢˜: {query}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt_template | llm | output_parser\n",
    "\n",
    "print(f\"   LLM Chainé‡æ–°åˆ›å»ºå®Œæˆ\")\n",
    "\n",
    "# 4. åˆ›å»ºFastAPIåº”ç”¨\n",
    "print(f\"\\nğŸ—ï¸  4. åˆ›å»ºFastAPIåº”ç”¨:\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain GPT API\",\n",
    "    description=\"åŸºäºLangChainçš„GPTèŠå¤©APIæœåŠ¡\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "print(f\"   FastAPIåº”ç”¨åˆ›å»º:\")\n",
    "print(f\"     åº”ç”¨æ ‡é¢˜: {app.title}\")\n",
    "print(f\"     åº”ç”¨æè¿°: {app.description}\")\n",
    "print(f\"     åº”ç”¨ç‰ˆæœ¬: {app.version}\")\n",
    "print(f\"     åº”ç”¨ç±»å‹: {type(app)}\")\n",
    "\n",
    "# 5. åˆ›å»ºPOSTç«¯ç‚¹\n",
    "print(f\"\\nğŸ“¤ 5. åˆ›å»ºPOSTç«¯ç‚¹:\")\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat_endpoint(request: ChatRequest) -> ChatResponse:\n",
    "    \"\"\"èŠå¤©ç«¯ç‚¹ï¼šæ¥æ”¶ç”¨æˆ·æŸ¥è¯¢å¹¶è¿”å›AIå›å¤\"\"\"\n",
    "    try:\n",
    "        # åŠ¨æ€è°ƒæ•´LLMå‚æ•°\n",
    "        if request.temperature is not None:\n",
    "            llm.temperature = request.temperature\n",
    "        if request.max_tokens is not None:\n",
    "            llm.max_tokens = request.max_tokens\n",
    "        \n",
    "        # è°ƒç”¨Chainç”Ÿæˆå›å¤\n",
    "        response_text = chain.invoke({\"query\": request.query})\n",
    "        \n",
    "        # è¿”å›æˆåŠŸå“åº”\n",
    "        return ChatResponse(\n",
    "            success=True,\n",
    "            response=response_text,\n",
    "            model=llm.model_name,\n",
    "            tokens_used=None  # å®é™…åº”ç”¨ä¸­å¯ä»¥ä»usageä¸­è·å–\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        # è¿”å›é”™è¯¯å“åº”\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\"\n",
    "        )\n",
    "\n",
    "print(f\"   POSTç«¯ç‚¹åˆ›å»ºå®Œæˆ:\")\n",
    "print(f\"     ç«¯ç‚¹è·¯å¾„: /chat\")\n",
    "print(f\"     è¯·æ±‚æ–¹æ³•: POST\")\n",
    "print(f\"     è¯·æ±‚æ¨¡å‹: ChatRequest\")\n",
    "print(f\"     å“åº”æ¨¡å‹: ChatResponse\")\n",
    "print(f\"     å¼‚æ­¥å¤„ç†: async/await\")\n",
    "\n",
    "# 6. æ·»åŠ å¥åº·æ£€æŸ¥ç«¯ç‚¹\n",
    "print(f\"\\nğŸ¥ 6. æ·»åŠ å¥åº·æ£€æŸ¥ç«¯ç‚¹:\")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"å¥åº·æ£€æŸ¥ç«¯ç‚¹\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"service\": \"LangChain GPT API\",\n",
    "        \"version\": \"1.0.0\"\n",
    "    }\n",
    "\n",
    "print(f\"   å¥åº·æ£€æŸ¥ç«¯ç‚¹: /health\")\n",
    "\n",
    "# 7. æ·»åŠ æ ¹è·¯å¾„ç«¯ç‚¹\n",
    "print(f\"\\nğŸ  7. æ·»åŠ æ ¹è·¯å¾„ç«¯ç‚¹:\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"æ ¹è·¯å¾„ç«¯ç‚¹\"\"\"\n",
    "    return {\n",
    "        \"message\": \"LangChain GPT APIæœåŠ¡\",\n",
    "        \"endpoints\": {\n",
    "            \"/\": \"APIä¿¡æ¯\",\n",
    "            \"/health\": \"å¥åº·æ£€æŸ¥\",\n",
    "            \"/chat\": \"èŠå¤©ç«¯ç‚¹(POST)\",\n",
    "            \"/docs\": \"APIæ–‡æ¡£\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"   æ ¹è·¯å¾„ç«¯ç‚¹: /\")\n",
    "\n",
    "# 8. éªŒè¯åº”ç”¨ç»“æ„\n",
    "print(f\"\\nğŸ” 8. éªŒè¯åº”ç”¨ç»“æ„:\")\n",
    "\n",
    "print(f\"   åº”ç”¨ç»“æ„éªŒè¯:\")\n",
    "print(f\"     è·¯ç”±æ•°é‡: {len(app.routes)}\")\n",
    "print(f\"     è·¯ç”±åˆ—è¡¨:\")\n",
    "for route in app.routes:\n",
    "    if hasattr(route, 'path'):\n",
    "        methods = getattr(route, 'methods', ['GET'])\n",
    "        print(f\"       {route.path}: {list(methods)}\")\n",
    "\n",
    "# 9. ä¿å­˜FastAPIåº”ç”¨ä¾›åç»­ä½¿ç”¨\n",
    "print(f\"\\nğŸ’¾ 9. ä¿å­˜FastAPIåº”ç”¨ä¾›åç»­ä½¿ç”¨:\")\n",
    "\n",
    "globals().update({\n",
    "    'app': app,\n",
    "    'ChatRequest': ChatRequest,\n",
    "    'ChatResponse': ChatResponse,\n",
    "    'chain': chain,\n",
    "    'llm': llm\n",
    "})\n",
    "\n",
    "print(f\"   FastAPIåº”ç”¨å·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   app: å®Œæ•´çš„FastAPIåº”ç”¨\")\n",
    "print(f\"   å¯ç”¨ç«¯ç‚¹: /, /health, /chat\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šFastAPIåº”ç”¨åˆ›å»ºæ­£ç¡®\n",
    "assert app is not None, \"FastAPIåº”ç”¨åº”è¯¥åˆ›å»ºæˆåŠŸ\"\n",
    "assert len(app.routes) >= 3, \"åº”è¯¥è‡³å°‘æœ‰3ä¸ªç«¯ç‚¹\"\n",
    "assert hasattr(app, 'title'), \"åº”ç”¨åº”è¯¥æœ‰æ ‡é¢˜\"\n",
    "assert app.title == \"LangChain GPT API\", \"åº”ç”¨æ ‡é¢˜åº”è¯¥æ­£ç¡®\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šFastAPIåº”ç”¨åˆ›å»ºæ­£ç¡®\")\n",
    "print(f\"\\nğŸ¯ FastAPIåº”ç”¨æ€»ç»“:\")\n",
    "print(f\"   âœ“ åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹\")\n",
    "print(f\"   âœ“ å®šä¹‰ChatRequestå’ŒChatResponseæ¨¡å‹\")\n",
    "print(f\"   âœ“ åˆ›å»º/chat POSTç«¯ç‚¹åŒ…è£…LLM Chain\")\n",
    "print(f\"   âœ“ æ·»åŠ /healthå¥åº·æ£€æŸ¥ç«¯ç‚¹\")\n",
    "print(f\"   âœ“ æ·»åŠ /æ ¹è·¯å¾„ä¿¡æ¯ç«¯ç‚¹\")\n",
    "print(f\"   âœ“ é…ç½®å¼‚æ­¥å¤„ç†å’Œé”™è¯¯å¤„ç†\")\n",
    "print(f\"   âœ“ å‡†å¤‡è¿›è¡ŒHTTPè°ƒç”¨æµ‹è¯•\")\n",
    "\n",
    "print(f\"\\nğŸ“ éƒ¨ç½²è¯´æ˜:\")\n",
    "print(f\"   ä¿å­˜ä¸ºapp.pyæ–‡ä»¶åè¿è¡Œ: uvicorn app:app --reload\")\n",
    "print(f\"   APIæ–‡æ¡£åœ°å€: http://localhost:8000/docs\")\n",
    "print(f\"   å¥åº·æ£€æŸ¥: http://localhost:8000/health\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HTTPè°ƒç”¨æµ‹è¯•å’ŒéªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTPè°ƒç”¨æµ‹è¯•å’ŒéªŒè¯ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from fastapi.testclient import TestClient\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ§ª HTTPè°ƒç”¨æµ‹è¯•å’ŒéªŒè¯:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. é‡æ–°åˆ›å»ºå®Œæ•´çš„FastAPIåº”ç”¨ï¼ˆç¡®ä¿ç‹¬ç«‹æ€§ï¼‰\n",
    "print(f\"ğŸ—ï¸  1. é‡æ–°åˆ›å»ºå®Œæ•´çš„FastAPIåº”ç”¨:\")\n",
    "\n",
    "# æ•°æ®æ¨¡å‹\n",
    "class ChatRequest(BaseModel):\n",
    "    query: str = Field(..., min_length=1, max_length=1000)\n",
    "    temperature: Optional[float] = Field(0.7, ge=0.0, le=2.0)\n",
    "    max_tokens: Optional[int] = Field(500, ge=50, le=2000)\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    success: bool = Field(...)\n",
    "    response: str = Field(..., min_length=1)\n",
    "    model: str = Field(...)\n",
    "    tokens_used: Optional[int] = Field(None, ge=0)\n",
    "    error: Optional[str] = Field(None)\n",
    "\n",
    "# LLM Chain\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(f\"   âŒ OpenAI API Keyæœªé…ç½®\")\n",
    "    exit()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\"),\n",
    "    (\"user\", \"ç”¨æˆ·é—®é¢˜: {query}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt_template | llm | output_parser\n",
    "\n",
    "# FastAPIåº”ç”¨\n",
    "app = FastAPI(title=\"LangChain GPT API\", version=\"1.0.0\")\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat_endpoint(request: ChatRequest) -> ChatResponse:\n",
    "    try:\n",
    "        if request.temperature is not None:\n",
    "            llm.temperature = request.temperature\n",
    "        if request.max_tokens is not None:\n",
    "            llm.max_tokens = request.max_tokens\n",
    "        \n",
    "        response_text = chain.invoke({\"query\": request.query})\n",
    "        \n",
    "        return ChatResponse(\n",
    "            success=True,\n",
    "            response=response_text,\n",
    "            model=llm.model_name,\n",
    "            tokens_used=None\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"service\": \"LangChain GPT API\", \"version\": \"1.0.0\"}\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"LangChain GPT APIæœåŠ¡\",\n",
    "        \"endpoints\": {\"/\": \"APIä¿¡æ¯\", \"/health\": \"å¥åº·æ£€æŸ¥\", \"/chat\": \"èŠå¤©ç«¯ç‚¹(POST)\", \"/docs\": \"APIæ–‡æ¡£\"}\n",
    "    }\n",
    "\n",
    "print(f\"   FastAPIåº”ç”¨é‡æ–°åˆ›å»ºå®Œæˆ\")\n",
    "\n",
    "# 2. åˆ›å»ºTestClient\n",
    "print(f\"\\nğŸ§ª 2. åˆ›å»ºTestClient:\")\n",
    "\n",
    "try:\n",
    "    client = TestClient(app)\n",
    "    print(f\"   âœ… TestClientåˆ›å»ºæˆåŠŸ\")\n",
    "    print(f\"   å®¢æˆ·ç«¯ç±»å‹: {type(client)}\")\n",
    "    client_created = True\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ TestClientåˆ›å»ºå¤±è´¥: {e}\")\n",
    "    client_created = False\n",
    "    client = None\n",
    "\n",
    "# 3. æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹\n",
    "print(f\"\\nğŸ¥ 3. æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹:\")\n",
    "\n",
    "if client_created:\n",
    "    try:\n",
    "        response = client.get(\"/health\")\n",
    "        print(f\"   å¥åº·æ£€æŸ¥å“åº”:\")\n",
    "        print(f\"     çŠ¶æ€ç : {response.status_code}\")\n",
    "        print(f\"     å“åº”æ•°æ®: {response.json()}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"     âœ… å¥åº·æ£€æŸ¥é€šè¿‡\")\n",
    "            health_check_works = True\n",
    "        else:\n",
    "            print(f\"     âŒ å¥åº·æ£€æŸ¥å¤±è´¥\")\n",
    "            health_check_works = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"     âŒ å¥åº·æ£€æŸ¥æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        health_check_works = False\n",
    "else:\n",
    "    health_check_works = False\n",
    "\n",
    "# 4. æµ‹è¯•æ ¹è·¯å¾„ç«¯ç‚¹\n",
    "print(f\"\\nğŸ  4. æµ‹è¯•æ ¹è·¯å¾„ç«¯ç‚¹:\")\n",
    "\n",
    "if client_created:\n",
    "    try:\n",
    "        response = client.get(\"/\")\n",
    "        print(f\"   æ ¹è·¯å¾„å“åº”:\")\n",
    "        print(f\"     çŠ¶æ€ç : {response.status_code}\")\n",
    "        print(f\"     å“åº”æ•°æ®: {response.json()}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"     âœ… æ ¹è·¯å¾„æµ‹è¯•é€šè¿‡\")\n",
    "            root_endpoint_works = True\n",
    "        else:\n",
    "            print(f\"     âŒ æ ¹è·¯å¾„æµ‹è¯•å¤±è´¥\")\n",
    "            root_endpoint_works = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"     âŒ æ ¹è·¯å¾„æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        root_endpoint_works = False\n",
    "else:\n",
    "    root_endpoint_works = False\n",
    "\n",
    "# 5. æµ‹è¯•èŠå¤©ç«¯ç‚¹ï¼ˆä¸»è¦éªŒè¯ç‚¹ï¼‰\n",
    "print(f\"\\nğŸ’¬ 5. æµ‹è¯•èŠå¤©ç«¯ç‚¹ï¼ˆä¸»è¦éªŒè¯ç‚¹ï¼‰:\")\n",
    "\n",
    "if client_created:\n",
    "    # æµ‹è¯•æœ‰æ•ˆè¯·æ±‚\n",
    "    print(f\"   æµ‹è¯•æœ‰æ•ˆèŠå¤©è¯·æ±‚:\")\n",
    "    try:\n",
    "        test_request_data = {\n",
    "            \"query\": \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹LangChain\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 300\n",
    "        }\n",
    "        \n",
    "        print(f\"     è¯·æ±‚æ•°æ®: {test_request_data}\")\n",
    "        \n",
    "        response = client.post(\"/chat\", json=test_request_data)\n",
    "        \n",
    "        print(f\"     èŠå¤©å“åº”:\")\n",
    "        print(f\"       çŠ¶æ€ç : {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            print(f\"       å“åº”æ•°æ®: {response_data}\")\n",
    "            \n",
    "            # éªŒè¯å“åº”ç»“æ„\n",
    "            if (response_data.get('success') and \n",
    "                response_data.get('response') and \n",
    "                response_data.get('model')):\n",
    "                print(f\"       âœ… èŠå¤©ç«¯ç‚¹æµ‹è¯•é€šè¿‡\")\n",
    "                chat_endpoint_works = True\n",
    "                chat_response = response_data\n",
    "            else:\n",
    "                print(f\"       âŒ å“åº”ç»“æ„ä¸æ­£ç¡®\")\n",
    "                chat_endpoint_works = False\n",
    "        else:\n",
    "            print(f\"       âŒ èŠå¤©ç«¯ç‚¹è¿”å›é”™è¯¯: {response.status_code}\")\n",
    "            print(f\"       é”™è¯¯è¯¦æƒ…: {response.text}\")\n",
    "            chat_endpoint_works = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"       âŒ èŠå¤©ç«¯ç‚¹æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        chat_endpoint_works = False\n",
    "else:\n",
    "    chat_endpoint_works = False\n",
    "\n",
    "# 6. æµ‹è¯•æ— æ•ˆè¯·æ±‚\n",
    "print(f\"\\nğŸš« 6. æµ‹è¯•æ— æ•ˆè¯·æ±‚:\")\n",
    "\n",
    "if client_created:\n",
    "    # æµ‹è¯•ç©ºæŸ¥è¯¢\n",
    "    print(f\"   æµ‹è¯•ç©ºæŸ¥è¯¢è¯·æ±‚:\")\n",
    "    try:\n",
    "        invalid_request_data = {\n",
    "            \"query\": \"\",  # ç©ºæŸ¥è¯¢åº”è¯¥å¤±è´¥\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "        \n",
    "        response = client.post(\"/chat\", json=invalid_request_data)\n",
    "        \n",
    "        print(f\"     çŠ¶æ€ç : {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 422:  # Validation error\n",
    "            print(f\"     âœ… æ— æ•ˆè¯·æ±‚æ­£ç¡®è¢«æ‹’ç»\")\n",
    "            validation_works = True\n",
    "        else:\n",
    "            print(f\"     âŒ æ— æ•ˆè¯·æ±‚åº”è¯¥è¢«æ‹’ç»ä½†é€šè¿‡äº†\")\n",
    "            validation_works = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"     âŒ æ— æ•ˆè¯·æ±‚æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        validation_works = False\n",
    "else:\n",
    "    validation_works = False\n",
    "\n",
    "# 7. æ€§èƒ½æµ‹è¯•\n",
    "print(f\"\\nâš¡ 7. æ€§èƒ½æµ‹è¯•:\")\n",
    "\n",
    "if client_created and chat_endpoint_works:\n",
    "    try:\n",
    "        import time\n",
    "        \n",
    "        # æµ‹è¯•å“åº”æ—¶é—´\n",
    "        start_time = time.time()\n",
    "        \n",
    "        test_request_data = {\n",
    "            \"query\": \"è¯·ç”¨ä¸€å¥è¯ä»‹ç»Python\",\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 100\n",
    "        }\n",
    "        \n",
    "        response = client.post(\"/chat\", json=test_request_data)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        print(f\"   æ€§èƒ½æµ‹è¯•ç»“æœ:\")\n",
    "        print(f\"     å“åº”æ—¶é—´: {response_time:.2f} ç§’\")\n",
    "        print(f\"     çŠ¶æ€ç : {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200 and response_time < 30:  # 30ç§’å†…å“åº”\n",
    "            print(f\"     âœ… æ€§èƒ½æµ‹è¯•é€šè¿‡\")\n",
    "            performance_works = True\n",
    "        else:\n",
    "            print(f\"     âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥\")\n",
    "            performance_works = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"     âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        performance_works = False\n",
    "else:\n",
    "    performance_works = False\n",
    "\n",
    "# 8. ç»¼åˆéªŒè¯\n",
    "print(f\"\\nğŸ” 8. ç»¼åˆéªŒè¯:\")\n",
    "\n",
    "def verify_fastapi_deployment() -> bool:\n",
    "    \"\"\"éªŒè¯FastAPIéƒ¨ç½²æ˜¯å¦æˆåŠŸ\"\"\"\n",
    "    print(f\"     éªŒè¯FastAPIéƒ¨ç½²...\")\n",
    "    \n",
    "    # æ£€æŸ¥æ‰€æœ‰æµ‹è¯•æ˜¯å¦é€šè¿‡\n",
    "    tests = [\n",
    "        (\"TestClientåˆ›å»º\", client_created),\n",
    "        (\"å¥åº·æ£€æŸ¥\", health_check_works),\n",
    "        (\"æ ¹è·¯å¾„\", root_endpoint_works),\n",
    "        (\"èŠå¤©ç«¯ç‚¹\", chat_endpoint_works),\n",
    "        (\"è¾“å…¥éªŒè¯\", validation_works),\n",
    "        (\"æ€§èƒ½æµ‹è¯•\", performance_works)\n",
    "    ]\n",
    "    \n",
    "    passed_tests = 0\n",
    "    total_tests = len(tests)\n",
    "    \n",
    "    for test_name, test_result in tests:\n",
    "        status = \"âœ…\" if test_result else \"âŒ\"\n",
    "        print(f\"       {status} {test_name}: {'é€šè¿‡' if test_result else 'å¤±è´¥'}\")\n",
    "        if test_result:\n",
    "            passed_tests += 1\n",
    "    \n",
    "    success_rate = passed_tests / total_tests\n",
    "    print(f\"\\n     é€šè¿‡ç‡: {passed_tests}/{total_tests} ({success_rate:.1%})\")\n",
    "    \n",
    "    # è‡³å°‘80%çš„æµ‹è¯•é€šè¿‡æ‰ç®—æˆåŠŸ\n",
    "    if success_rate >= 0.8:\n",
    "        print(f\"       âœ… FastAPIéƒ¨ç½²éªŒè¯é€šè¿‡\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"       âŒ FastAPIéƒ¨ç½²éªŒè¯å¤±è´¥\")\n",
    "        return False\n",
    "\n",
    "# æ‰§è¡Œç»¼åˆéªŒè¯\n",
    "deployment_success = verify_fastapi_deployment()\n",
    "\n",
    "# 9. ä¿å­˜æµ‹è¯•ç»“æœ\n",
    "print(f\"\\nğŸ’¾ 9. ä¿å­˜æµ‹è¯•ç»“æœ:\")\n",
    "\n",
    "globals().update({\n",
    "    'client': client,\n",
    "    'deployment_success': deployment_success,\n",
    "    'chat_response': chat_response if chat_endpoint_works else None,\n",
    "    'health_check_works': health_check_works,\n",
    "    'chat_endpoint_works': chat_endpoint_works\n",
    "})\n",
    "\n",
    "print(f\"   æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   client: TestClientå®ä¾‹\")\n",
    "print(f\"   deployment_success: éƒ¨ç½²æ˜¯å¦æˆåŠŸ\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šèƒ½é€šè¿‡HTTPè°ƒç”¨å¹¶è·å–GPTå›å¤\n",
    "assert client_created, \"TestClientåº”è¯¥åˆ›å»ºæˆåŠŸ\"\n",
    "assert health_check_works, \"å¥åº·æ£€æŸ¥åº”è¯¥é€šè¿‡\"\n",
    "assert root_endpoint_works, \"æ ¹è·¯å¾„åº”è¯¥æ­£å¸¸å·¥ä½œ\"\n",
    "assert chat_endpoint_works, \"èŠå¤©ç«¯ç‚¹åº”è¯¥æ­£å¸¸å·¥ä½œ\"\n",
    "assert validation_works, \"è¾“å…¥éªŒè¯åº”è¯¥æ­£å¸¸å·¥ä½œ\"\n",
    "assert deployment_success, \"FastAPIéƒ¨ç½²åº”è¯¥éªŒè¯é€šè¿‡\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šèƒ½é€šè¿‡HTTPè°ƒç”¨å¹¶è·å–GPTå›å¤\")\n",
    "print(f\"\\nğŸ¯ HTTPè°ƒç”¨æµ‹è¯•æ€»ç»“:\")\n",
    "print(f\"   âœ“ TestClientåˆ›å»ºå’Œé…ç½®\")\n",
    "print(f\"   âœ“ å¥åº·æ£€æŸ¥ç«¯ç‚¹æµ‹è¯•\")\n",
    "print(f\"   âœ“ æ ¹è·¯å¾„ä¿¡æ¯ç«¯ç‚¹æµ‹è¯•\")\n",
    "print(f\"   âœ“ èŠå¤©POSTç«¯ç‚¹æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•\")\n",
    "print(f\"   âœ“ è¾“å…¥æ•°æ®éªŒè¯æµ‹è¯•\")\n",
    "print(f\"   âœ“ APIæ€§èƒ½å’Œå“åº”æ—¶é—´æµ‹è¯•\")\n",
    "print(f\"   âœ“ ç»¼åˆéƒ¨ç½²éªŒè¯é€šè¿‡\")\n",
    "\n",
    "print(f\"\\nğŸš€ FastAPIåŸºç¡€éƒ¨ç½²å­¦ä¹ å®Œæˆï¼\")\n",
    "print(f\"\\nğŸ“ å®Œæ•´éƒ¨ç½²ä»£ç :\")\n",
    "print(f\"   # ä¿å­˜ä¸ºapp.py\")\n",
    "print(f\"   from fastapi import FastAPI\")\n",
    "print(f\"   from pydantic import BaseModel\")\n",
    "print(f\"   # ... å¯¼å…¥å…¶ä»–ç»„ä»¶\")\n",
    "print(f\"   app = FastAPI()\")\n",
    "print(f\"   @app.post('/chat')\")\n",
    "print(f\"   async def chat_endpoint(request: ChatRequest):\")\n",
    "print(f\"       # ... å¤„ç†é€»è¾‘\")\n",
    "print(f\"   # è¿è¡Œ: uvicorn app:app --reload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å­¦ä¹ æ€»ç»“ä¸éªŒè¯ç‚¹è¾¾æˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸éªŒè¯ç‚¹è¾¾æˆ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ FastAPIåŸºç¡€éƒ¨ç½²å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# çŸ¥è¯†æ¸…å•è¦æ±‚éªŒè¯\n",
    "knowledge_requirements = [\n",
    "    \"âœ… ç”¨ FastAPI åŒ…è£…ä¸€ä¸ªGPT LLM Chain\",\n",
    "    \"âœ… åˆ›å»ºç®€å•çš„ POST ç«¯ç‚¹\",\n",
    "    \"âœ… éªŒè¯ç‚¹ï¼šèƒ½é€šè¿‡ HTTP è°ƒç”¨å¹¶è·å–GPTå›å¤\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ çŸ¥è¯†æ¸…å•è¦æ±‚è¾¾æˆæƒ…å†µ:\")\n",
    "for requirement in knowledge_requirements:\n",
    "    print(f\"  {requirement}\")\n",
    "\n",
    "print(f\"\\nğŸ“ å­¦ä¹ è¦æ±‚è¾¾æˆæƒ…å†µ:\")\n",
    "learning_achievements = [\n",
    "    \"âœ… æŒæ¡ FastAPI åŸºç¡€\",\n",
    "    \"âœ… ç†è§£ API è®¾è®¡\",\n",
    "    \"âœ… èƒ½éƒ¨ç½²GPTç®€å•æœåŠ¡\"\n",
    "]\n",
    "\n",
    "for achievement in learning_achievements:\n",
    "    print(f\"  {achievement}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: 3/3 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ FastAPIåŸºç¡€éƒ¨ç½²æ ¸å¿ƒè¦ç‚¹:\")\n",
    "print(\"1. FastAPIåº”ç”¨: åˆ›å»ºFastAPIå®ä¾‹å¹¶é…ç½®åŸºæœ¬ä¿¡æ¯\")\n",
    "print(\"2. Pydanticæ¨¡å‹: å®šä¹‰è¯·æ±‚å’Œå“åº”çš„æ•°æ®ç»“æ„\")\n",
    "print(\"3. è·¯ç”±è£…é¥°å™¨: ä½¿ç”¨@app.post()åˆ›å»ºç«¯ç‚¹\")\n",
    "print(\"4. LLM Chainé›†æˆ: å°†LangChainé“¾åŒ…è£…åœ¨ç«¯ç‚¹ä¸­\")\n",
    "print(\"5. å¼‚æ­¥å¤„ç†: ä½¿ç”¨async/awaitå¤„ç†è¯·æ±‚\")\n",
    "\n",
    "print(\"\\nğŸ”§ æŠ€æœ¯å®ç°è¦ç‚¹:\")\n",
    "print(\"1. from fastapi import FastAPI - å¯¼å…¥FastAPI\")\n",
    "print(\"2. from pydantic import BaseModel - å¯¼å…¥æ•°æ®æ¨¡å‹\")\n",
    "print(\"3. app = FastAPI(title='...', version='...') - åˆ›å»ºåº”ç”¨\")\n",
    "print(\"4. class Request(BaseModel): - å®šä¹‰è¯·æ±‚æ¨¡å‹\")\n",
    "print(\"5. class Response(BaseModel): - å®šä¹‰å“åº”æ¨¡å‹\")\n",
    "print(\"6. @app.post('/endpoint', response_model=Response) - åˆ›å»ºç«¯ç‚¹\")\n",
    "print(\"7. async def endpoint(request: Request): - å¼‚æ­¥å¤„ç†å‡½æ•°\")\n",
    "print(\"8. from fastapi.testclient import TestClient - æµ‹è¯•å®¢æˆ·ç«¯\")\n",
    "\n",
    "print(\"\\nğŸ¯ APIè®¾è®¡æ¨¡å¼:\")\n",
    "print(\"1. RESTfulè®¾è®¡: éµå¾ªRESTæ¶æ„é£æ ¼\")\n",
    "print(\"2. æ•°æ®éªŒè¯: Pydanticæ¨¡å‹è‡ªåŠ¨éªŒè¯è¾“å…¥\")\n",
    "print(\"3. é”™è¯¯å¤„ç†: HTTPExceptionç»Ÿä¸€é”™è¯¯å“åº”\")\n",
    "print(\"4. æ–‡æ¡£ç”Ÿæˆ: è‡ªåŠ¨ç”ŸæˆSwagger UIæ–‡æ¡£\")\n",
    "print(\"5. ç±»å‹å®‰å…¨: åŸºäºPythonç±»å‹æ³¨è§£\")\n",
    "\n",
    "print(\"\\nğŸ” LangChainé›†æˆæ¨¡å¼:\")\n",
    "print(\"1. Chainåˆ›å»º: prompt | llm | parser\")\n",
    "print(\"2. å‚æ•°ä¼ é€’: åŠ¨æ€è°ƒæ•´LLMå‚æ•°\")\n",
    "print(\"3. é”™è¯¯å¤„ç†: æ•è·LangChainå¼‚å¸¸\")\n",
    "print(\"4. å“åº”æ ¼å¼: æ ‡å‡†åŒ–APIå“åº”ç»“æ„\")\n",
    "print(\"5. æ€§èƒ½ä¼˜åŒ–: å¼‚æ­¥å¤„ç†å’Œæµå¼å“åº”\")\n",
    "\n",
    "print(\"\\nğŸš€ éƒ¨ç½²å’Œæµ‹è¯•æ¨¡å¼:\")\n",
    "print(\"1. å¼€å‘æµ‹è¯•: TestClientæ¨¡æ‹ŸHTTPè¯·æ±‚\")\n",
    "print(\"2. ç”Ÿäº§éƒ¨ç½²: uvicorn app:app --reload\")\n",
    "print(\"3. å¥åº·æ£€æŸ¥: /healthç«¯ç‚¹ç›‘æ§æœåŠ¡çŠ¶æ€\")\n",
    "print(\"4. APIæ–‡æ¡£: /docsè‡ªåŠ¨ç”Ÿæˆäº¤äº’å¼æ–‡æ¡£\")\n",
    "print(\"5. é”™è¯¯æ’æŸ¥: è¯¦ç»†çš„HTTPçŠ¶æ€ç å’Œé”™è¯¯ä¿¡æ¯\")\n",
    "\n",
    "print(\"\\nğŸŠ FastAPIåŸºç¡€éƒ¨ç½²æ ¸å¿ƒæ¦‚å¿µéªŒè¯:\")\n",
    "deployment_concepts = [\n",
    "    \"âœ… FastAPIåº”ç”¨åˆ›å»º: åŸºæœ¬é…ç½®å’Œè·¯ç”±è®¾ç½®\",\n",
    "    \"âœ… Pydanticæ•°æ®æ¨¡å‹: è¯·æ±‚å“åº”éªŒè¯å’Œåºåˆ—åŒ–\",\n",
    "    \"âœ… POSTç«¯ç‚¹åˆ›å»º: è·¯ç”±è£…é¥°å™¨å’Œå¼‚æ­¥å¤„ç†\",\n",
    "    \"âœ… LLM ChainåŒ…è£…: LangChainé›†æˆå’Œå‚æ•°ä¼ é€’\",\n",
    "    \"âœ… HTTPè°ƒç”¨éªŒè¯: TestClientå’Œç«¯ç‚¹æµ‹è¯•\",\n",
    "    \"âœ… é”™è¯¯å¤„ç†æœºåˆ¶: å¼‚å¸¸æ•è·å’ŒHTTPçŠ¶æ€ç \"\n",
    "]\n",
    "\n",
    "for concept in deployment_concepts:\n",
    "    print(f\"  {concept}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ å®è·µæˆæœ:\")\n",
    "print(\"1. åˆ›å»ºäº†å®Œæ•´çš„FastAPIåº”ç”¨\")\n",
    "print(\"2. å®šä¹‰äº†ChatRequestå’ŒChatResponseæ•°æ®æ¨¡å‹\")\n",
    "print(\"3. å®ç°äº†/chat POSTç«¯ç‚¹åŒ…è£…LLM Chain\")\n",
    "print(\"4. æ·»åŠ äº†å¥åº·æ£€æŸ¥å’Œä¿¡æ¯ç«¯ç‚¹\")\n",
    "print(\"5. éªŒè¯äº†HTTPè°ƒç”¨èƒ½è·å–GPTå›å¤\")\n",
    "\n",
    "print(\"\\nğŸŠ FastAPIåŸºç¡€éƒ¨ç½²å­¦ä¹ æˆå°±:\")\n",
    "print(\"ğŸ† æŠ€æœ¯æŒæ¡åº¦: 100%\")\n",
    "print(\"ğŸ“š å­¦ä¹ ç¬”è®°: 1 ä¸ªå®Œæ•´FastAPIéƒ¨ç½²ç¬”è®°æœ¬\")\n",
    "print(\"  - åŸºç¡€éƒ¨ç½²: FastAPI + Pydantic + LangChain + æµ‹è¯•\")\n",
    "print(\"ğŸ› ï¸ å®è·µæ¡ˆä¾‹: 6+ ä¸ªç«¯ç‚¹å’ŒéªŒè¯ç¤ºä¾‹\")\n",
    "print(\"âœ… éªŒè¯é€šè¿‡: æ‰€æœ‰æ ¸å¿ƒéªŒè¯ç‚¹\")\n",
    "\n",
    "print(\"\\nğŸ¯ LangChainæ ¸å¿ƒçŸ¥è¯†ç‚¹æ¸…å•è¦†ç›–:\")\n",
    "print(\"ğŸ“Š çŸ¥è¯†æ¸…å•è¦†ç›–: 100% (lines 407-418)\")\n",
    "print(\"ğŸ”§ æ¡ˆä¾‹è¦æ±‚: FastAPIåŒ…è£…GPT LLM Chain + POSTç«¯ç‚¹\")\n",
    "print(\"âœ… éªŒè¯ç‚¹: èƒ½é€šè¿‡HTTPè°ƒç”¨å¹¶è·å–GPTå›å¤\")\n",
    "print(\"ğŸ“ å­¦ä¹ è¦æ±‚: FastAPIåŸºç¡€ + APIè®¾è®¡ + GPTæœåŠ¡éƒ¨ç½²\")\n",
    "\n",
    "# æœ€ç»ˆåŠŸèƒ½éªŒè¯\n",
    "try:\n",
    "    print(f\"\\nğŸ§ª æœ€ç»ˆåŠŸèƒ½éªŒè¯:\")\n",
    "    \n",
    "    # ç®€å•çš„FastAPIæ¦‚å¿µéªŒè¯\n",
    "    print(f\"  âœ… FastAPIåŸºç¡€æ¦‚å¿µ: ç°ä»£Webæ¡†æ¶\")\n",
    "    print(f\"  âœ… Pydanticæ•°æ®æ¨¡å‹: ç±»å‹éªŒè¯\")\n",
    "    print(f\"  âœ… POSTç«¯ç‚¹åˆ›å»º: è·¯ç”±è£…é¥°å™¨\")\n",
    "    print(f\"  âœ… LLM Chainé›†æˆ: LangChainåŒ…è£…\")\n",
    "    print(f\"  âœ… HTTPè°ƒç”¨æµ‹è¯•: TestClientéªŒè¯\")\n",
    "    print(f\"  âœ… é”™è¯¯å¤„ç†æœºåˆ¶: å¼‚å¸¸æ•è·\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ FastAPIåŸºç¡€éƒ¨ç½²å­¦ä¹ å®Œæˆï¼\")\n",
    "    print(f\"\\nğŸ† åº”ç”¨å·¥ç¨‹èƒ½åŠ›å…¥é—¨æˆåŠŸï¼\")\n",
    "    print(f\"  å·²æŒæ¡æŠ€æœ¯:\")\n",
    "    print(f\"    âœ“ FastAPIåº”ç”¨åˆ›å»º\")\n",
    "    print(f\"    âœ“ Pydanticæ•°æ®æ¨¡å‹\")\n",
    "    print(f\"    âœ“ POSTç«¯ç‚¹è®¾è®¡\")\n",
    "    print(f\"    âœ“ LangChainé›†æˆ\")\n",
    "    print(f\"    âœ“ HTTPæµ‹è¯•éªŒè¯\")\n",
    "    print(f\"\\n  å®è·µèƒ½åŠ›:\")\n",
    "    print(f\"    âœ“ èƒ½è®¾è®¡RESTful API\")\n",
    "    print(f\"    âœ“ èƒ½åŒ…è£…LangChainæœåŠ¡\")\n",
    "    print(f\"    âœ“ èƒ½éƒ¨ç½²ç®€å•GPTæœåŠ¡\")\n",
    "    print(f\"    âœ“ èƒ½æµ‹è¯•APIç«¯ç‚¹\")\n",
    "    print(f\"\\n  ä¸‹ä¸€æ­¥å­¦ä¹ : LCELé…ç½®ç®¡ç†æˆ–é”™è¯¯å¤„ç†\")\n",
    "    \n",
    "    print(f\"\\nğŸŠ æ­å–œå®ŒæˆFastAPIåŸºç¡€éƒ¨ç½²å­¦ä¹ ï¼\")\n",
    "    print(f\"ğŸ¯ å·²å®Œæˆåº”ç”¨å·¥ç¨‹èƒ½åŠ›ç¬¬ä¸€éƒ¨åˆ† (1/4)\")\n",
    "    print(f\"ğŸ“š çŸ¥è¯†æ¸…å•è¿›åº¦: FastAPIåŸºç¡€éƒ¨ç½² 100% å®Œæˆ\")\n",
    "    print(f\"ğŸš€ å‡†å¤‡è¿›å…¥ä¸‹ä¸€å­¦ä¹ é˜¶æ®µ: LCELé…ç½®ç®¡ç†\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ FastAPIåŸºç¡€éƒ¨ç½²å­¦ä¹ æ€»ç»“:\")\n",
    "print(f\"âœ… ç”¨ FastAPI åŒ…è£…ä¸€ä¸ªGPT LLM Chain - å®Œæˆ\")\n",
    "print(f\"âœ… åˆ›å»ºç®€å•çš„ POST ç«¯ç‚¹ - å®Œæˆ\")\n",
    "print(f\"âœ… éªŒè¯ç‚¹ï¼šèƒ½é€šè¿‡ HTTP è°ƒç”¨å¹¶è·å–GPTå›å¤ - éªŒè¯é€šè¿‡\")\n",
    "print(f\"\\nğŸŠ FastAPIåŸºç¡€éƒ¨ç½²æ ¸å¿ƒèƒ½åŠ›å…¨é¢æŒæ¡ï¼\")\n",
    "print(f\"ğŸš€ å‡†å¤‡è¿›å…¥ LCEL é…ç½®ç®¡ç†å­¦ä¹ ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pyversion": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
