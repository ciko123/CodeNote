{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - LCEL ç®¡é“è¯­æ³•\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹  LangChain 1.0 é€šè¿‡ \"|\" æ“ä½œç¬¦åˆ›å»º LCEL æµæ°´çº¿ï¼Œè¿™æ˜¯æœ€é‡è¦å’Œæ¨èçš„å†™æ³•\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- æŒæ¡ç®¡é“æ“ä½œç¬¦ \"|\" è¯­æ³•\n",
    "- ç†è§£æ•°æ®æµå‘å’Œä¼ é€’æœºåˆ¶\n",
    "- èƒ½æ„å»ºå®Œæ•´çš„é€šä¹‰åƒé—®å¤„ç†é“¾\n",
    "- æŒæ¡ prompt|llm|parser æ ¸å¿ƒæ¨¡å¼\n",
    "\n",
    "## ğŸ”‘ å‰ç½®è¦æ±‚\n",
    "**æ³¨æ„**ï¼šéœ€è¦å…ˆå®Œæˆ Runnable åŸºç¡€æ¦‚å¿µå­¦ä¹ ï¼Œç†è§£é“¾å¼ç»„åˆåŸç†\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç®¡é“æ“ä½œç¬¦åŸºç¡€è¯­æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®¡é“æ“ä½œç¬¦åŸºç¡€è¯­æ³• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”§ ç®¡é“æ“ä½œç¬¦åŸºç¡€è¯­æ³•ç†è§£:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥ OpenAI API é…ç½®\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ æœªæ‰¾åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡\")\n",
    "    print(\"   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=your_key_here\")\n",
    "else:\n",
    "    print(f\"âœ… OpenAI API Key å·²é…ç½® (é•¿åº¦: {len(openai_api_key)})\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ LCEL ç®¡é“è¯­æ³•æ ¸å¿ƒæ¦‚å¿µ:\")\n",
    "    print(f\"   1. | æ“ä½œç¬¦ï¼šè¿æ¥ä¸åŒçš„ Runnable ç»„ä»¶\")\n",
    "    print(f\"   2. æ•°æ®æµï¼šå·¦åˆ°å³ä¼ é€’è¾“å‡ºç»“æœ\")\n",
    "    print(f\"   3. ç±»å‹å®‰å…¨ï¼šè‡ªåŠ¨ç±»å‹è½¬æ¢å’ŒéªŒè¯\")\n",
    "    print(f\"   4. å¯ç»„åˆæ€§ï¼šä»»æ„ç»„ä»¶éƒ½å¯ä»¥ç®¡é“è¿æ¥\")\n",
    "    \n",
    "    # åˆ›å»ºåŸºç¡€å¤„ç†å‡½æ•°\n",
    "    def step1_process(text: str) -> str:\n",
    "        return f\"æ­¥éª¤1å¤„ç†: {text}\"\n",
    "    \n",
    "    def step2_process(text: str) -> str:\n",
    "        return f\"æ­¥éª¤2å¤„ç†: {text}\"\n",
    "    \n",
    "    def step3_process(text: str) -> str:\n",
    "        return f\"æ­¥éª¤3å¤„ç†: {text}\"\n",
    "    \n",
    "    # åŒ…è£…ä¸º Runnable\n",
    "    step1 = RunnableLambda(step1_process)\n",
    "    step2 = RunnableLambda(step2_process)\n",
    "    step3 = RunnableLambda(step3_process)\n",
    "    \n",
    "    print(f\"\\nğŸ—ï¸  åˆ›å»ºçš„åŸºç¡€ç»„ä»¶:\")\n",
    "    print(f\"   1. æ­¥éª¤1: {type(step1).__name__}\")\n",
    "    print(f\"   2. æ­¥éª¤2: {type(step2).__name__}\")\n",
    "    print(f\"   3. æ­¥éª¤3: {type(step3).__name__}\")\n",
    "    \n",
    "    # åŸºç¡€ç®¡é“è¿æ¥\n",
    "    print(f\"\\nğŸ”— åŸºç¡€ç®¡é“è¿æ¥æµ‹è¯•:\")\n",
    "    \n",
    "    # ä¸¤æ­¥ç®¡é“\n",
    "    pipe_2step = step1 | step2\n",
    "    print(f\"   ä¸¤æ­¥ç®¡é“ç±»å‹: {type(pipe_2step)}\")\n",
    "    \n",
    "    result_2step = pipe_2step.invoke(\"æµ‹è¯•æ•°æ®\")\n",
    "    print(f\"   ä¸¤æ­¥ç®¡é“ç»“æœ: {result_2step}\")\n",
    "    \n",
    "    # ä¸‰æ­¥ç®¡é“\n",
    "    pipe_3step = step1 | step2 | step3\n",
    "    print(f\"\\n   ä¸‰æ­¥ç®¡é“ç±»å‹: {type(pipe_3step)}\")\n",
    "    \n",
    "    result_3step = pipe_3step.invoke(\"æµ‹è¯•æ•°æ®\")\n",
    "    print(f\"   ä¸‰æ­¥ç®¡é“ç»“æœ: {result_3step}\")\n",
    "    \n",
    "    # éªŒè¯ç‚¹ï¼šç®¡é“æ“ä½œç¬¦åŸºç¡€è¯­æ³•æ­£ç¡®\n",
    "    assert result_2step == \"æ­¥éª¤2å¤„ç†: æ­¥éª¤1å¤„ç†: æµ‹è¯•æ•°æ®\", \"ä¸¤æ­¥ç®¡é“ç»“æœé”™è¯¯\"\n",
    "    assert result_3step == \"æ­¥éª¤3å¤„ç†: æ­¥éª¤2å¤„ç†: æ­¥éª¤1å¤„ç†: æµ‹è¯•æ•°æ®\", \"ä¸‰æ­¥ç®¡é“ç»“æœé”™è¯¯\"\n",
    "    print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šç®¡é“æ“ä½œç¬¦åŸºç¡€è¯­æ³•ç†è§£æ­£ç¡®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. prompt | llm åŸºç¡€ç®¡é“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt | llm åŸºç¡€ç®¡é“ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ¤– prompt | llm åŸºç¡€ç®¡é“æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆ›å»º LLM\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=60\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»º PromptTemplate\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\"],\n",
    "            template=\"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ {topic}ï¼Œç”¨ä¸€å¥è¯æ¦‚æ‹¬ã€‚\"\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ“ åˆ›å»ºçš„ç»„ä»¶:\")\n",
    "        print(f\"   1. Prompt: {type(prompt).__name__}\")\n",
    "        print(f\"   2. LLM: {type(llm).__name__}\")\n",
    "        print(f\"   3. æ¨¡æ¿å˜é‡: {prompt.input_variables}\")\n",
    "        \n",
    "        # æ„å»º prompt | llm ç®¡é“\n",
    "        print(f\"\\nğŸ”— æ„å»º prompt | llm ç®¡é“:\")\n",
    "        \n",
    "        prompt_llm_chain = prompt | llm\n",
    "        \n",
    "        print(f\"   ç®¡é“ç±»å‹: {type(prompt_llm_chain)}\")\n",
    "        print(f\"   ç®¡é“å¯ç”¨: {prompt_llm_chain is not None}\")\n",
    "        \n",
    "        # æµ‹è¯•åŸºç¡€è°ƒç”¨\n",
    "        print(f\"\\nğŸ§ª æµ‹è¯•åŸºç¡€è°ƒç”¨:\")\n",
    "        \n",
    "        test_topics = [\n",
    "            \"äººå·¥æ™ºèƒ½\",\n",
    "            \"æœºå™¨å­¦ä¹ \",\n",
    "            \"æ·±åº¦å­¦ä¹ \"\n",
    "        ]\n",
    "        \n",
    "        for i, topic in enumerate(test_topics, 1):\n",
    "            print(f\"\\n   æµ‹è¯• {i}: ä¸»é¢˜ = {topic}\")\n",
    "            \n",
    "            # è°ƒç”¨ç®¡é“\n",
    "            result = prompt_llm_chain.invoke({\"topic\": topic})\n",
    "            \n",
    "            print(f\"   è¾“å…¥: {{'topic': '{topic}'}}\")\n",
    "            print(f\"   è¾“å‡ºç±»å‹: {type(result)}\")\n",
    "            print(f\"   è¾“å‡ºå†…å®¹: {result.content}\")\n",
    "            \n",
    "            # éªŒè¯è¾“å‡ºæ ¼å¼\n",
    "            assert hasattr(result, 'content'), \"è¾“å‡ºç¼ºå°‘ content å±æ€§\"\n",
    "            assert len(result.content) > 0, \"è¾“å‡ºå†…å®¹ä¸ºç©º\"\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šprompt | llm ç®¡é“æ­£ç¡®æ„å»º\n",
    "        final_test = prompt_llm_chain.invoke({\"topic\": \"Python\"})\n",
    "        assert hasattr(final_test, 'content'), \"æœ€ç»ˆæµ‹è¯•è¾“å‡ºæ ¼å¼é”™è¯¯\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šprompt | llm åŸºç¡€ç®¡é“æ„å»ºæ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ prompt | llm ç®¡é“æ„å»ºå¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. prompt | llm | parser å®Œæ•´é“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt | llm | parser å®Œæ•´é“¾ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”— prompt | llm | parser å®Œæ•´é“¾æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆ›å»º LLM\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=80\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºè§£æå™¨\n",
    "        str_parser = StrOutputParser()\n",
    "        json_parser = JsonOutputParser()\n",
    "        \n",
    "        print(f\"ğŸ“ åˆ›å»ºçš„ç»„ä»¶:\")\n",
    "        print(f\"   1. LLM: {type(llm).__name__}\")\n",
    "        print(f\"   2. å­—ç¬¦ä¸²è§£æå™¨: {type(str_parser).__name__}\")\n",
    "        print(f\"   3. JSONè§£æå™¨: {type(json_parser).__name__}\")\n",
    "        \n",
    "        # 1. å­—ç¬¦ä¸²è¾“å‡ºé“¾\n",
    "        print(f\"\\nğŸ”— 1. å­—ç¬¦ä¸²è¾“å‡ºé“¾ (prompt | llm | StrOutputParser):\")\n",
    "        \n",
    "        str_prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹çš„æ ¸å¿ƒè§‚ç‚¹ï¼š{text}\"\n",
    "        )\n",
    "        \n",
    "        str_chain = str_prompt | llm | str_parser\n",
    "        \n",
    "        print(f\"   é“¾ç±»å‹: {type(str_chain)}\")\n",
    "        \n",
    "        test_text = \"äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•ä¸æ–­çªç ´ï¼Œåº”ç”¨åœºæ™¯è¶Šæ¥è¶Šå¹¿æ³›ã€‚\"\n",
    "        str_result = str_chain.invoke({\"text\": test_text})\n",
    "        \n",
    "        print(f\"   è¾“å…¥: {test_text}\")\n",
    "        print(f\"   è¾“å‡ºç±»å‹: {type(str_result)}\")\n",
    "        print(f\"   è¾“å‡ºå†…å®¹: {str_result}\")\n",
    "        \n",
    "        # 2. JSON è¾“å‡ºé“¾\n",
    "        print(f\"\\nğŸ”— 2. JSON è¾“å‡ºé“¾ (prompt | llm | JsonOutputParser):\")\n",
    "        \n",
    "        json_prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"åˆ†æä»¥ä¸‹æ–‡æœ¬å¹¶è¿”å›JSONæ ¼å¼ï¼š{{\\\"summary\\\": \\\"æ‘˜è¦\\\", \\\"keywords\\\": [\\\"å…³é”®è¯1\\\", \\\"å…³é”®è¯2\\\"]}}\\næ–‡æœ¬ï¼š{text}\"\n",
    "        )\n",
    "        \n",
    "        json_chain = json_prompt | llm | json_parser\n",
    "        \n",
    "        print(f\"   é“¾ç±»å‹: {type(json_chain)}\")\n",
    "        \n",
    "        json_result = json_chain.invoke({\"text\": test_text})\n",
    "        \n",
    "        print(f\"   è¾“å…¥: {test_text}\")\n",
    "        print(f\"   è¾“å‡ºç±»å‹: {type(json_result)}\")\n",
    "        print(f\"   è¾“å‡ºå†…å®¹: {json.dumps(json_result, ensure_ascii=False, indent=6)}\")\n",
    "        \n",
    "        # 3. å¤æ‚é“¾ç»„åˆ\n",
    "        print(f\"\\nğŸ”— 3. å¤æ‚é“¾ç»„åˆæµ‹è¯•:\")\n",
    "        \n",
    "        # åˆ›å»ºå¤šæ­¥éª¤å¤„ç†é“¾\n",
    "        analysis_prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\"],\n",
    "            template=\"è¯·åˆ†æ {topic} çš„ä¸‰ä¸ªä¸»è¦ç‰¹ç‚¹ï¼Œç”¨é€—å·åˆ†éš”ã€‚\"\n",
    "        )\n",
    "        \n",
    "        def split_features(text: str) -> list:\n",
    "            \"\"\"åˆ†å‰²ç‰¹ç‚¹åˆ—è¡¨\"\"\"\n",
    "            return [feature.strip() for feature in text.split(',') if feature.strip()]\n",
    "        \n",
    "        from langchain_core.runnables import RunnableLambda\n",
    "        \n",
    "        feature_splitter = RunnableLambda(split_features)\n",
    "        complex_chain = analysis_prompt | llm | str_parser | feature_splitter\n",
    "        \n",
    "        complex_result = complex_chain.invoke({\"topic\": \"Pythonç¼–ç¨‹è¯­è¨€\"})\n",
    "        \n",
    "        print(f\"   å¤æ‚é“¾ç»“æœ: {complex_result}\")\n",
    "        print(f\"   ç»“æœç±»å‹: {type(complex_result)}\")\n",
    "        print(f\"   ç‰¹å¾æ•°é‡: {len(complex_result)}\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šå®Œæ•´é“¾æ­£ç¡®æ„å»º\n",
    "        assert isinstance(str_result, str), \"å­—ç¬¦ä¸²é“¾è¾“å‡ºç±»å‹é”™è¯¯\"\n",
    "        assert isinstance(json_result, dict), \"JSONé“¾è¾“å‡ºç±»å‹é”™è¯¯\"\n",
    "        assert isinstance(complex_result, list), \"å¤æ‚é“¾è¾“å‡ºç±»å‹é”™è¯¯\"\n",
    "        assert \"summary\" in json_result, \"JSONè¾“å‡ºç¼ºå°‘summaryå­—æ®µ\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šprompt | llm | parser å®Œæ•´é“¾æ„å»ºæ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ å®Œæ•´é“¾æ„å»ºå¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ•°æ®æµå‘å’Œç±»å‹ä¼ é€’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æµå‘å’Œç±»å‹ä¼ é€’ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“Š æ•°æ®æµå‘å’Œç±»å‹ä¼ é€’æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆ›å»º LLM\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºç±»å‹æ£€æŸ¥å‡½æ•°\n",
    "        def check_type_and_pass(data):\n",
    "            print(f\"   æ•°æ®ç±»å‹: {type(data)}\")\n",
    "            print(f\"   æ•°æ®å†…å®¹: {data}\")\n",
    "            return data\n",
    "        \n",
    "        type_checker = RunnableLambda(check_type_and_pass)\n",
    "        str_parser = StrOutputParser()\n",
    "        \n",
    "        print(f\"ğŸ“ æµ‹è¯•ç»„ä»¶:\")\n",
    "        print(f\"   1. ç±»å‹æ£€æŸ¥å™¨: {type(type_checker).__name__}\")\n",
    "        print(f\"   2. å­—ç¬¦ä¸²è§£æå™¨: {type(str_parser).__name__}\")\n",
    "        \n",
    "        # 1. åŸºç¡€æ•°æ®æµæµ‹è¯•\n",
    "        print(f\"\\nğŸ” 1. åŸºç¡€æ•°æ®æµæµ‹è¯•:\")\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"input\"],\n",
    "            template=\"å¤„ç†è¾“å…¥: {input}\"\n",
    "        )\n",
    "        \n",
    "        # æ„å»ºå¸¦ç±»å‹æ£€æŸ¥çš„é“¾\n",
    "        chain_with_checks = prompt | type_checker | llm | type_checker | str_parser | type_checker\n",
    "        \n",
    "        print(f\"   æ‰§è¡Œé“¾: prompt â†’ type_checker â†’ llm â†’ type_checker â†’ str_parser â†’ type_checker\")\n",
    "        \n",
    "        result = chain_with_checks.invoke({\"input\": \"æµ‹è¯•æ•°æ®æµ\"})\n",
    "        \n",
    "        print(f\"\\n   æœ€ç»ˆç»“æœ: {result}\")\n",
    "        print(f\"   æœ€ç»ˆç±»å‹: {type(result)}\")\n",
    "        \n",
    "        # 2. ç±»å‹è½¬æ¢æµ‹è¯•\n",
    "        print(f\"\\nğŸ” 2. ç±»å‹è½¬æ¢æµ‹è¯•:\")\n",
    "        \n",
    "        def dict_to_string(data: dict) -> str:\n",
    "            \"\"\"å­—å…¸è½¬å­—ç¬¦ä¸²\"\"\"\n",
    "            return f\"å­—ç¬¦ä¸²åŒ–: {data.get('text', 'æœªçŸ¥')}\"\n",
    "        \n",
    "        def string_to_dict(data: str) -> dict:\n",
    "            \"\"\"å­—ç¬¦ä¸²è½¬å­—å…¸\"\"\"\n",
    "            return {\"result\": data, \"length\": len(data)}\n",
    "        \n",
    "        dict_to_str_runnable = RunnableLambda(dict_to_string)\n",
    "        str_to_dict_runnable = RunnableLambda(string_to_dict)\n",
    "        \n",
    "        # ç±»å‹è½¬æ¢é“¾\n",
    "        type_conversion_chain = (\n",
    "            PromptTemplate(input_variables=[\"text\"], template=\"{text}\")\n",
    "            | dict_to_str_runnable\n",
    "            | llm\n",
    "            | str_parser\n",
    "            | str_to_dict_runnable\n",
    "        )\n",
    "        \n",
    "        conversion_result = type_conversion_chain.invoke({\"text\": \"ç±»å‹è½¬æ¢æµ‹è¯•\"})\n",
    "        \n",
    "        print(f\"   è½¬æ¢é“¾ç»“æœ: {conversion_result}\")\n",
    "        print(f\"   ç»“æœç±»å‹: {type(conversion_result)}\")\n",
    "        \n",
    "        # 3. æ•°æ®ä¿æŒæµ‹è¯•\n",
    "        print(f\"\\nğŸ” 3. æ•°æ®ä¿æŒæµ‹è¯• (RunnablePassthrough):\")\n",
    "        \n",
    "        # ä½¿ç”¨ RunnablePassthrough ä¿æŒåŸå§‹æ•°æ®\n",
    "        passthrough_chain = (\n",
    "            {\n",
    "                \"original\": RunnablePassthrough(),\n",
    "                \"processed\": PromptTemplate(input_variables=[\"text\"], template=\"å¤„ç†: {text}\") | llm | str_parser\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        passthrough_result = passthrough_chain.invoke({\"text\": \"æ•°æ®ä¿æŒæµ‹è¯•\"})\n",
    "        \n",
    "        print(f\"   åŸå§‹æ•°æ®: {passthrough_result['original']}\")\n",
    "        print(f\"   å¤„ç†æ•°æ®: {passthrough_result['processed']}\")\n",
    "        print(f\"   ç»“æœç±»å‹: {type(passthrough_result)}\")\n",
    "        \n",
    "        # 4. é”™è¯¯å¤„ç†æµ‹è¯•\n",
    "        print(f\"\\nğŸ” 4. é”™è¯¯å¤„ç†æµ‹è¯•:\")\n",
    "        \n",
    "        def error_handler(data):\n",
    "            \"\"\"é”™è¯¯å¤„ç†å‡½æ•°\"\"\"\n",
    "            try:\n",
    "                if \"error\" in str(data).lower():\n",
    "                    raise ValueError(\"æ£€æµ‹åˆ°é”™è¯¯å…³é”®è¯\")\n",
    "                return data\n",
    "            except Exception as e:\n",
    "                return f\"é”™è¯¯å¤„ç†: {str(e)}\"\n",
    "        \n",
    "        error_handler_runnable = RunnableLambda(error_handler)\n",
    "        \n",
    "        # å¸¦é”™è¯¯å¤„ç†çš„é“¾\n",
    "        safe_chain = (\n",
    "            PromptTemplate(input_variables=[\"text\"], template=\"{text}\")\n",
    "            | error_handler_runnable\n",
    "            | type_checker\n",
    "        )\n",
    "        \n",
    "        normal_result = safe_chain.invoke({\"text\": \"æ­£å¸¸æ•°æ®\"})\n",
    "        error_result = safe_chain.invoke({\"text\": \"åŒ…å«errorçš„æ•°æ®\"})\n",
    "        \n",
    "        print(f\"   æ­£å¸¸æ•°æ®å¤„ç†: {normal_result}\")\n",
    "        print(f\"   é”™è¯¯æ•°æ®å¤„ç†: {error_result}\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šæ•°æ®æµå‘å’Œç±»å‹ä¼ é€’æ­£ç¡®\n",
    "        assert isinstance(result, str), \"åŸºç¡€æ•°æ®æµæœ€ç»ˆç±»å‹é”™è¯¯\"\n",
    "        assert isinstance(conversion_result, dict), \"ç±»å‹è½¬æ¢ç»“æœç±»å‹é”™è¯¯\"\n",
    "        assert isinstance(passthrough_result, dict), \"æ•°æ®ä¿æŒç»“æœç±»å‹é”™è¯¯\"\n",
    "        assert \"original\" in passthrough_result, \"ç¼ºå°‘åŸå§‹æ•°æ®å­—æ®µ\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæ•°æ®æµå‘å’Œç±»å‹ä¼ é€’æ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æ•°æ®æµå‘æµ‹è¯•å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ LCEL ç®¡é“è¯­æ³•å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… ç®¡é“æ“ä½œç¬¦ï¼š| è¯­æ³•åŸºç¡€\",\n",
    "    \"âœ… prompt|llmï¼šåŸºç¡€ç®¡é“ç»„åˆ\",\n",
    "    \"âœ… prompt|llm|parserï¼šå®Œæ•´å¤„ç†é“¾\",\n",
    "    \"âœ… æ•°æ®æµå‘ï¼šç±»å‹ä¼ é€’å’Œè½¬æ¢\",\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/4 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ LCEL ç®¡é“è¯­æ³•æœ€ä½³å®è·µ:\")\n",
    "print(\"1. ç®¡é“ç»„åˆï¼šä½¿ç”¨ | æ“ä½œç¬¦è¿æ¥ç»„ä»¶\")\n",
    "print(\"2. æ•°æ®æµå‘ï¼šç†è§£å·¦åˆ°å³çš„ä¼ é€’æœºåˆ¶\")\n",
    "print(\"3. ç±»å‹å®‰å…¨ï¼šæ³¨æ„è¾“å…¥è¾“å‡ºç±»å‹åŒ¹é…\")\n",
    "print(\"4. è§£æå™¨ä½¿ç”¨ï¼šStrOutputParser å’Œ JsonOutputParser\")\n",
    "print(\"5. é”™è¯¯å¤„ç†ï¼šåœ¨é“¾ä¸­åŠ å…¥å¼‚å¸¸å¤„ç†é€»è¾‘\")\n",
    "\n",
    "print(\"\\nğŸ”§ å¸¸ç”¨ç®¡é“æ¨¡å¼:\")\n",
    "print(\"1. prompt | llm - åŸºç¡€ç”Ÿæˆ\")\n",
    "print(\"2. prompt | llm | StrOutputParser - å­—ç¬¦ä¸²è¾“å‡º\")\n",
    "print(\"3. prompt | llm | JsonOutputParser - ç»“æ„åŒ–è¾“å‡º\")\n",
    "print(\"4. prompt | llm | parser | RunnableLambda - åå¤„ç†\")\n",
    "print(\"5. RunnablePassthrough | prompt | llm - æ•°æ®ä¿æŒ\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  RunnablePassthrough æ•°æ®é€ä¼ \")\n",
    "print(\"2. æŒæ¡æµå¼è¾“å‡º Streaming æŠ€æœ¯\")\n",
    "print(\"3. å­¦ä¹  ChatMessageHistory æ¶ˆæ¯å†å²\")\n",
    "print(\"4. æ¢ç´¢æ›´å¤æ‚çš„ LCEL ç»„åˆæ¨¡å¼\")\n",
    "print(\"5. å®è·µç”Ÿäº§çº§ç®¡é“åº”ç”¨å¼€å‘\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ LCEL ç®¡é“è¯­æ³•åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    if openai_api_key:\n",
    "        # ç®€å•æµ‹è¯•\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=30)\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"æ€»ç»“: {text}\"\n",
    "        )\n",
    "        \n",
    "        # æ„å»º prompt | llm | parser é“¾\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        result = chain.invoke({\"text\": \"LCEL ç®¡é“è¯­æ³•æµ‹è¯•\"})\n",
    "        \n",
    "        print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ:\")\n",
    "        print(f\"   API çŠ¶æ€: å¯ç”¨\")\n",
    "        print(f\"   é“¾ç±»å‹: {type(chain).__name__}\")\n",
    "        print(f\"   è¾“å‡ºç±»å‹: {type(result)}\")\n",
    "        print(f\"   è¾“å‡ºå†…å®¹: {result}\")\n",
    "        print(\"\\nâœ… LCEL ç®¡é“è¯­æ³•å­¦ä¹ å®Œæˆï¼\")\n",
    "        \n",
    "        print(f\"\\nğŸŠ æ ¸å¿ƒç®¡é“æŠ€æœ¯æŒæ¡ï¼\")\n",
    "        print(f\"   å·²æŒæ¡: Runnable + RunnableMap + LCEL ç®¡é“è¯­æ³•\")\n",
    "        print(f\"   ä¸‹ä¸€æ­¥: å­¦ä¹ æµå¼è¾“å‡ºå’Œæ¶ˆæ¯å†å²æŠ€æœ¯\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  æœ€ç»ˆéªŒè¯è·³è¿‡: OpenAI API Key æœªé…ç½®\")\n",
    "        print(\"   è¯·é…ç½® OPENAI_API_KEY åé‡è¯•\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
