{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Document Loaders æ–‡æ¡£åŠ è½½\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹ ä½¿ç”¨ LangChain Document Loaders ä»æ•°æ®æºåŠ è½½æ–‡æ¡£ï¼ŒRAG ç³»ç»Ÿçš„èµ·ç‚¹\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- æŒæ¡åŸºç¡€ Loader ä½¿ç”¨æ–¹æ³•\n",
    "- ç†è§£æ–‡æ¡£åŠ è½½æœºåˆ¶å’Œç»“æ„\n",
    "- èƒ½å¤„ç†æœ¬åœ°ä¸­æ–‡æ–‡æ¡£\n",
    "- ç†Ÿæ‚‰ Document å¯¹è±¡ç»“æ„\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TextLoader åŸºç¡€ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextLoader åŸºç¡€ä½¿ç”¨ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# æ–‡æ¡£è·¯å¾„ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰\n",
    "file_path = \"../sample_chinese_text.txt\"\n",
    "\n",
    "print(\"ğŸ” TextLoader åŸºç¡€æµ‹è¯•:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"âœ… æ–‡ä»¶å­˜åœ¨: {file_path}\")\n",
    "    \n",
    "    # åˆ›å»º TextLoader\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nğŸ“ Loader åˆ›å»ºæˆåŠŸ\")\n",
    "    print(f\"   æ–‡ä»¶è·¯å¾„: {file_path}\")\n",
    "    print(f\"   ç¼–ç æ ¼å¼: utf-8\")\n",
    "    \n",
    "    # åŠ è½½æ–‡æ¡£\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"\\nğŸ“š æ–‡æ¡£åŠ è½½ç»“æœ:\")\n",
    "    print(f\"   æ–‡æ¡£æ•°é‡: {len(documents)}\")\n",
    "    \n",
    "    # åˆ†æç¬¬ä¸€ä¸ªæ–‡æ¡£\n",
    "    if documents:\n",
    "        doc = documents[0]\n",
    "        print(f\"   æ–‡æ¡£ç±»å‹: {type(doc)}\")\n",
    "        print(f\"   å†…å®¹é•¿åº¦: {len(doc.page_content)} å­—ç¬¦\")\n",
    "        print(f\"   å…ƒæ•°æ®: {doc.metadata}\")\n",
    "        print(f\"\\nğŸ“– æ–‡æ¡£å†…å®¹é¢„è§ˆ:\")\n",
    "        print(f\"   {doc.page_content[:100]}...\")\n",
    "    \n",
    "    # éªŒè¯ç‚¹ï¼šèƒ½æ­£å¸¸è¿”å›ä¸­æ–‡æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®\n",
    "    assert len(documents) > 0, \"æ–‡æ¡£åŠ è½½å¤±è´¥\"\n",
    "    assert isinstance(documents[0].page_content, str), \"æ–‡æ¡£å†…å®¹ä¸æ˜¯å­—ç¬¦ä¸²\"\n",
    "    assert 'source' in documents[0].metadata, \"ç¼ºå°‘æºæ–‡ä»¶å…ƒæ•°æ®\"\n",
    "    print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šTextLoader æ­£å¸¸å·¥ä½œ\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
    "    print(\"   è¯·ç¡®ä¿ sample_chinese_text.txt æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document å¯¹è±¡ç»“æ„åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document å¯¹è±¡ç»“æ„åˆ†æ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# æ–‡æ¡£è·¯å¾„\n",
    "file_path = \"../sample_chinese_text.txt\"\n",
    "\n",
    "print(\"ğŸ” Document å¯¹è±¡ç»“æ„åˆ†æ:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # åŠ è½½æ–‡æ¡£\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    if documents:\n",
    "        doc = documents[0]\n",
    "        \n",
    "        print(f\"ğŸ“‹ Document å¯¹è±¡è¯¦ç»†ä¿¡æ¯:\")\n",
    "        print(f\"   ç±»å‹: {type(doc)}\")\n",
    "        print(f\"   æ¨¡å—: {type(doc).__module__}\")\n",
    "        \n",
    "        # åˆ†æ page_content\n",
    "        content = doc.page_content\n",
    "        print(f\"\\nğŸ“ page_content åˆ†æ:\")\n",
    "        print(f\"   ç±»å‹: {type(content)}\")\n",
    "        print(f\"   é•¿åº¦: {len(content)} å­—ç¬¦\")\n",
    "        print(f\"   è¡Œæ•°: {len(content.splitlines())} è¡Œ\")\n",
    "        print(f\"   åŒ…å«ä¸­æ–‡: {'æ˜¯' if any('\\u4e00' <= char <= '\\u9fff' for char in content) else 'å¦'}\")\n",
    "        \n",
    "        # åˆ†æ metadata\n",
    "        metadata = doc.metadata\n",
    "        print(f\"\\nğŸ·ï¸  metadata åˆ†æ:\")\n",
    "        print(f\"   ç±»å‹: {type(metadata)}\")\n",
    "        print(f\"   é”®å€¼å¯¹: {list(metadata.items())}\")\n",
    "        \n",
    "        # æ£€æŸ¥å¯ç”¨çš„å±æ€§å’Œæ–¹æ³•\n",
    "        print(f\"\\nğŸ”§ å¯ç”¨å±æ€§:\")\n",
    "        for attr in dir(doc):\n",
    "            if not attr.startswith('_') and not callable(getattr(doc, attr)):\n",
    "                value = getattr(doc, attr)\n",
    "                print(f\"   {attr}: {type(value)} = {str(value)[:50]}...\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šDocument[] ç»“æ„æ­£ç¡®\n",
    "        assert hasattr(doc, 'page_content'), \"ç¼ºå°‘ page_content å±æ€§\"\n",
    "        assert hasattr(doc, 'metadata'), \"ç¼ºå°‘ metadata å±æ€§\"\n",
    "        assert isinstance(doc.metadata, dict), \"metadata ä¸æ˜¯å­—å…¸ç±»å‹\"\n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šDocument å¯¹è±¡ç»“æ„æ­£ç¡®\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å¤šæ–‡æ¡£åŠ è½½æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šæ–‡æ¡£åŠ è½½æµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "print(\"ğŸ“š å¤šæ–‡æ¡£åŠ è½½æµ‹è¯•:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶\n",
    "test_files = []\n",
    "test_content = [\n",
    "    \"è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚\\nåŒ…å«ä¸­æ–‡å†…å®¹å’ŒåŸºç¡€ä¿¡æ¯ã€‚\",\n",
    "    \"è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æ¡£ã€‚\\nåŒ…å«ä¸åŒçš„ä¸­æ–‡å†…å®¹å’Œè¯¦ç»†ä¿¡æ¯ã€‚\",\n",
    "    \"è¿™æ˜¯ç¬¬ä¸‰ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚\\nåŒ…å«è¡¥å……çš„ä¸­æ–‡å†…å®¹å’Œæ€»ç»“ä¿¡æ¯ã€‚\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    for i, content in enumerate(test_content, 1):\n",
    "        file_path = os.path.join(temp_dir, f\"test_doc_{i}.txt\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        test_files.append(file_path)\n",
    "    \n",
    "    print(f\"âœ… åˆ›å»º {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶\")\n",
    "    \n",
    "    # æµ‹è¯•å•ä¸ªæ–‡ä»¶åŠ è½½\n",
    "    print(f\"\\nğŸ” å•æ–‡ä»¶åŠ è½½æµ‹è¯•:\")\n",
    "    for i, file_path in enumerate(test_files, 1):\n",
    "        loader = TextLoader(file_path, encoding='utf-8')\n",
    "        docs = loader.load()\n",
    "        print(f\"   æ–‡ä»¶ {i}: {len(docs)} ä¸ªæ–‡æ¡£, {len(docs[0].page_content)} å­—ç¬¦\")\n",
    "    \n",
    "    # æµ‹è¯•æ‰¹é‡åŠ è½½\n",
    "    print(f\"\\nğŸ“¦ æ‰¹é‡åŠ è½½æµ‹è¯•:\")\n",
    "    all_documents = []\n",
    "    for file_path in test_files:\n",
    "        loader = TextLoader(file_path, encoding='utf-8')\n",
    "        all_documents.extend(loader.load())\n",
    "    \n",
    "    print(f\"   æ€»æ–‡æ¡£æ•°: {len(all_documents)}\")\n",
    "    print(f\"   æ€»å­—ç¬¦æ•°: {sum(len(doc.page_content) for doc in all_documents)}\")\n",
    "    \n",
    "    # åˆ†ææ–‡æ¡£æ¥æº\n",
    "    print(f\"\\nğŸ“Š æ–‡æ¡£æ¥æºåˆ†æ:\")\n",
    "    sources = [doc.metadata.get('source', 'unknown') for doc in all_documents]\n",
    "    for i, source in enumerate(sources, 1):\n",
    "        filename = os.path.basename(source)\n",
    "        print(f\"   æ–‡æ¡£ {i}: {filename}\")\n",
    "    \n",
    "    # éªŒè¯ç‚¹ï¼šå¤šæ–‡æ¡£åŠ è½½æ­£ç¡®\n",
    "    assert len(all_documents) == len(test_files), \"æ–‡æ¡£æ•°é‡ä¸åŒ¹é…\"\n",
    "    assert all(len(doc.page_content) > 0 for doc in all_documents), \"å­˜åœ¨ç©ºæ–‡æ¡£\"\n",
    "    print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šå¤šæ–‡æ¡£åŠ è½½æ­£å¸¸å·¥ä½œ\")\n",
    "    \n",
    "finally:\n",
    "    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "    import shutil\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"\\nğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. é”™è¯¯å¤„ç†å’Œç¼–ç æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é”™è¯¯å¤„ç†å’Œç¼–ç æµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "print(\"ğŸ” é”™è¯¯å¤„ç†å’Œç¼–ç æµ‹è¯•:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æµ‹è¯•åœºæ™¯\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"æ–‡ä»¶ä¸å­˜åœ¨\",\n",
    "        \"file_path\": \"nonexistent_file.txt\",\n",
    "        \"expected_error\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ç©ºæ–‡ä»¶\",\n",
    "        \"content\": \"\",\n",
    "        \"expected_error\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"çº¯ä¸­æ–‡å†…å®¹\",\n",
    "        \"content\": \"è¿™æ˜¯çº¯ä¸­æ–‡æµ‹è¯•å†…å®¹ï¼ŒåŒ…å«å„ç§æ±‰å­—å’Œæ ‡ç‚¹ç¬¦å·ã€‚\",\n",
    "        \"expected_error\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ä¸­è‹±æ–‡æ··åˆ\",\n",
    "        \"content\": \"This is English mixed with ä¸­æ–‡å†…å®¹ and 123 numbers.\",\n",
    "        \"expected_error\": False\n",
    "    }\n",
    "]\n",
    "\n",
    "results = {}\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "try:\n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\nğŸ§ª æµ‹è¯• {i}: {scenario['name']}\")\n",
    "        \n",
    "        try:\n",
    "            # å‡†å¤‡æ–‡ä»¶\n",
    "            if 'content' in scenario:\n",
    "                file_path = os.path.join(temp_dir, f\"test_{i}.txt\")\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(scenario['content'])\n",
    "            else:\n",
    "                file_path = scenario['file_path']\n",
    "            \n",
    "            # å°è¯•åŠ è½½\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # åˆ†æç»“æœ\n",
    "            if documents:\n",
    "                doc = documents[0]\n",
    "                print(f\"   æˆåŠŸ: {len(doc.page_content)} å­—ç¬¦\")\n",
    "                print(f\"   å†…å®¹é¢„è§ˆ: {doc.page_content[:50]}...\")\n",
    "                \n",
    "                # æ£€æŸ¥ç¼–ç \n",
    "                has_chinese = any('\\u4e00' <= char <= '\\u9fff' for char in doc.page_content)\n",
    "                has_english = any('a' <= char.lower() <= 'z' for char in doc.page_content)\n",
    "                print(f\"   åŒ…å«ä¸­æ–‡: {'æ˜¯' if has_chinese else 'å¦'}\")\n",
    "                print(f\"   åŒ…å«è‹±æ–‡: {'æ˜¯' if has_english else 'å¦'}\")\n",
    "                \n",
    "                results[scenario['name']] = {\n",
    "                    \"success\": True,\n",
    "                    \"length\": len(doc.page_content)\n",
    "                }\n",
    "            else:\n",
    "                print(f\"   æˆåŠŸ: ä½†æ— æ–‡æ¡£å†…å®¹\")\n",
    "                results[scenario['name']] = {\"success\": True, \"length\": 0}\n",
    "            \n",
    "        except Exception as e:\n",
    "            if scenario['expected_error']:\n",
    "                print(f\"   é¢„æœŸé”™è¯¯: {type(e).__name__}\")\n",
    "                results[scenario['name']] = {\"success\": True, \"expected_error\": True}\n",
    "            else:\n",
    "                print(f\"   æ„å¤–é”™è¯¯: {e}\")\n",
    "                results[scenario['name']] = {\"success\": False, \"error\": str(e)}\n",
    "    \n",
    "    # é”™è¯¯å¤„ç†æ€»ç»“\n",
    "    print(f\"\\nğŸ“Š é”™è¯¯å¤„ç†æµ‹è¯•æ€»ç»“:\")\n",
    "    successful_tests = sum(1 for result in results.values() if result[\"success\"])\n",
    "    print(f\"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}/{len(test_scenarios)}\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ é”™è¯¯å¤„ç†å»ºè®®:\")\n",
    "    print(\"   - ä½¿ç”¨ try-except æ•è·æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯\")\n",
    "    print(\"   - æŒ‡å®šæ­£ç¡®çš„ç¼–ç æ ¼å¼ï¼ˆutf-8 for ä¸­æ–‡ï¼‰\")\n",
    "    print(\"   - æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæƒé™\")\n",
    "    print(\"   - éªŒè¯æ–‡æ¡£å†…å®¹ä¸ä¸ºç©º\")\n",
    "    print(\"âœ… éªŒè¯é€šè¿‡ï¼šé”™è¯¯å¤„ç†æœºåˆ¶æœ‰æ•ˆ\")\n",
    "    \n",
    "finally:\n",
    "    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "    import shutil\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“‹ Document Loaders å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… TextLoader åŸºç¡€ä½¿ç”¨ï¼šåŠ è½½æœ¬åœ° txt æ–‡ä»¶\",\n",
    "    \"âœ… Document å¯¹è±¡ç»“æ„ï¼špage_content å’Œ metadata\",\n",
    "    \"âœ… å¤šæ–‡æ¡£åŠ è½½ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶\",\n",
    "    \"âœ… é”™è¯¯å¤„ç†ï¼šæ–‡ä»¶ä¸å­˜åœ¨å’Œç¼–ç é—®é¢˜\",\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/4 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Document Loaders æœ€ä½³å®è·µ:\")\n",
    "print(\"1. æ–‡ä»¶è·¯å¾„ï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„è¦æ˜ç¡®\")\n",
    "print(\"2. ç¼–ç æ ¼å¼ï¼šä¸­æ–‡æ–‡æ¡£ä½¿ç”¨ utf-8 ç¼–ç \")\n",
    "print(\"3. é”™è¯¯å¤„ç†ï¼šæ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§å’Œè¯»å–æƒé™\")\n",
    "print(\"4. æ‰¹é‡å¤„ç†ï¼šä½¿ç”¨å¾ªç¯å¤„ç†å¤šä¸ªæ–‡æ¡£\")\n",
    "print(\"5. å†…å®¹éªŒè¯ï¼šæ£€æŸ¥æ–‡æ¡£å†…å®¹é•¿åº¦å’Œæ ¼å¼\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  Text Splitters æ–‡æ¡£åˆ†å‰²\")\n",
    "print(\"2. æŒæ¡å…¶ä»– Loader ç±»å‹ï¼ˆPDFã€CSVã€JSONï¼‰\")\n",
    "print(\"3. å­¦ä¹  DirectoryLoader ç›®å½•æ‰¹é‡åŠ è½½\")\n",
    "print(\"4. æ¢ç´¢è‡ªå®šä¹‰ Loader å¼€å‘\")\n",
    "print(\"5. å®è·µå¤§æ–‡æ¡£å¤„ç†å’Œä¼˜åŒ–ç­–ç•¥\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ TextLoader åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    file_path = \"../sample_chinese_text.txt\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        loader = TextLoader(file_path, encoding='utf-8')\n",
    "        documents = loader.load()\n",
    "        \n",
    "        print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ:\")\n",
    "        print(f\"   åŠ è½½æ–‡ä»¶: {os.path.basename(file_path)}\")\n",
    "        print(f\"   æ–‡æ¡£æ•°é‡: {len(documents)}\")\n",
    "        print(f\"   å†…å®¹é•¿åº¦: {len(documents[0].page_content)} å­—ç¬¦\")\n",
    "        print(\"\\nâœ… Document Loaders å­¦ä¹ å®Œæˆï¼\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  æœ€ç»ˆéªŒè¯è·³è¿‡: æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
