{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Text Splitters æ–‡æœ¬åˆ†å‰²\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹ ä½¿ç”¨ LangChain RecursiveCharacterTextSplitter å°†æ–‡æ¡£æŒ‰è§„åˆ™åˆ‡åˆ†æˆ chunkï¼Œä¿è¯å‘é‡åŒ–è´¨é‡\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£æ–‡æœ¬åˆ‡åˆ†åŸç†å’Œç­–ç•¥\n",
    "- æŒæ¡åˆ‡åˆ†å‚æ•°è®¾ç½®å’Œä¼˜åŒ–\n",
    "- èƒ½å¯¹ä¸­æ–‡æ–‡æ¡£è¿›è¡Œæœ‰æ•ˆåˆ‡ç‰‡\n",
    "- æ§åˆ¶åˆ‡ç‰‡å¤§å°å’Œè´¨é‡\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RecursiveCharacterTextSplitter åŸºç¡€ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecursiveCharacterTextSplitter åŸºç¡€ä½¿ç”¨ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "file_path = \"../sample_chinese_text.txt\"\n",
    "\n",
    "print(\"âœ‚ï¸ RecursiveCharacterTextSplitter åŸºç¡€æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # åŠ è½½æ–‡æ¡£\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    if documents:\n",
    "        original_doc = documents[0]\n",
    "        print(f\"ğŸ“ åŸå§‹æ–‡æ¡£ä¿¡æ¯:\")\n",
    "        print(f\"   å†…å®¹é•¿åº¦: {len(original_doc.page_content)} å­—ç¬¦\")\n",
    "        print(f\"   è¡Œæ•°: {len(original_doc.page_content.splitlines())} è¡Œ\")\n",
    "        \n",
    "        # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,  # åˆ‡ç‰‡å¤§å°\n",
    "            chunk_overlap=50,  # é‡å å¤§å°\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼›\", \"ï¼Œ\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ‚ï¸ åˆ†å‰²å™¨é…ç½®:\")\n",
    "        print(f\"   chunk_size: 500 å­—ç¬¦\")\n",
    "        print(f\"   chunk_overlap: 50 å­—ç¬¦\")\n",
    "        print(f\"   åˆ†éš”ç¬¦: æ®µè½ > å¥å­ > åˆ†å¥ > é€—å· > ç©ºæ ¼ > å­—ç¬¦\")\n",
    "        \n",
    "        # æ‰§è¡Œåˆ†å‰²\n",
    "        chunks = text_splitter.split_documents([original_doc])\n",
    "        \n",
    "        print(f\"\\nğŸ“Š åˆ†å‰²ç»“æœ:\")\n",
    "        print(f\"   åˆ‡ç‰‡æ•°é‡: {len(chunks)}\")\n",
    "        print(f\"   å¹³å‡é•¿åº¦: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.1f} å­—ç¬¦\")\n",
    "        \n",
    "        # æ˜¾ç¤ºå‰å‡ ä¸ªåˆ‡ç‰‡\n",
    "        print(f\"\\nğŸ“‹ åˆ‡ç‰‡å†…å®¹é¢„è§ˆ:\")\n",
    "        for i, chunk in enumerate(chunks[:3], 1):\n",
    "            content = chunk.page_content\n",
    "            print(f\"   åˆ‡ç‰‡ {i}: {len(content)} å­—ç¬¦\")\n",
    "            print(f\"   å†…å®¹: {content[:80]}...\")\n",
    "            print(f\"   å…ƒæ•°æ®: {chunk.metadata}\")\n",
    "            print()\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šä¸­æ–‡æ–‡æ¡£è¢«æ­£ç¡®åˆ‡åˆ†æˆæŒ‡å®šå¤§å°çš„ç‰‡æ®µ\n",
    "        assert len(chunks) > 1, \"æ–‡æ¡£æœªè¢«åˆ†å‰²\"\n",
    "        assert all(len(chunk.page_content) <= 550 for chunk in chunks), \"å­˜åœ¨è¶…å¤§åˆ‡ç‰‡\"\n",
    "        assert all(len(chunk.page_content) > 0 for chunk in chunks), \"å­˜åœ¨ç©ºåˆ‡ç‰‡\"\n",
    "        print(f\"âœ… éªŒè¯é€šè¿‡ï¼šæ–‡æœ¬åˆ†å‰²æ­£å¸¸å·¥ä½œ\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ä¸åŒå‚æ•°é…ç½®å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸åŒå‚æ•°é…ç½®å¯¹æ¯” _ ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "file_path = \"../sample_chinese_text.txt\"\n",
    "\n",
    "print(\"ğŸ”§ ä¸åŒå‚æ•°é…ç½®å¯¹æ¯”æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    if documents:\n",
    "        original_doc = documents[0]\n",
    "        \n",
    "        # æµ‹è¯•ä¸åŒé…ç½®\n",
    "        configs = [\n",
    "            {\n",
    "                \"name\": \"å°åˆ‡ç‰‡\",\n",
    "                \"chunk_size\": 200,\n",
    "                \"chunk_overlap\": 20\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"æ ‡å‡†åˆ‡ç‰‡\",\n",
    "                \"chunk_size\": 500,\n",
    "                \"chunk_overlap\": 50\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"å¤§åˆ‡ç‰‡\",\n",
    "                \"chunk_size\": 800,\n",
    "                \"chunk_overlap\": 80\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for config in configs:\n",
    "            print(f\"\\nğŸ§ª æµ‹è¯•é…ç½®: {config['name']}\")\n",
    "            \n",
    "            # åˆ›å»ºåˆ†å‰²å™¨\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=config['chunk_size'],\n",
    "                chunk_overlap=config['chunk_overlap'],\n",
    "                length_function=len\n",
    "            )\n",
    "            \n",
    "            # æ‰§è¡Œåˆ†å‰²\n",
    "            chunks = splitter.split_documents([original_doc])\n",
    "            \n",
    "            # ç»Ÿè®¡ç»“æœ\n",
    "            chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "            \n",
    "            results[config['name']] = {\n",
    "                \"chunk_count\": len(chunks),\n",
    "                \"avg_length\": sum(chunk_lengths) / len(chunk_lengths),\n",
    "                \"max_length\": max(chunk_lengths),\n",
    "                \"min_length\": min(chunk_lengths),\n",
    "                \"chunks\": chunks\n",
    "            }\n",
    "            \n",
    "            print(f\"   åˆ‡ç‰‡æ•°é‡: {len(chunks)}\")\n",
    "            print(f\"   å¹³å‡é•¿åº¦: {results[config['name']]['avg_length']:.1f} å­—ç¬¦\")\n",
    "            print(f\"   é•¿åº¦èŒƒå›´: {results[config['name']]['min_length']}-{results[config['name']]['max_length']} å­—ç¬¦\")\n",
    "        \n",
    "        # é…ç½®å¯¹æ¯”æ€»ç»“\n",
    "        print(f\"\\nğŸ“Š é…ç½®å¯¹æ¯”æ€»ç»“:\")\n",
    "        print(f\"{'é…ç½®':<8} {'åˆ‡ç‰‡æ•°':<8} {'å¹³å‡é•¿åº¦':<10} {'é•¿åº¦èŒƒå›´':<15}\")\n",
    "        print(\"-\" * 45)\n",
    "        for name, result in results.items():\n",
    "            print(f\"{name:<8} {result['chunk_count']:<8} {result['avg_length']:<10.1f} {result['min_length']}-{result['max_length']:<10}\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ å‚æ•°é…ç½®å»ºè®®:\")\n",
    "        print(\"   - å°åˆ‡ç‰‡(200): é€‚åˆç²¾ç¡®æ£€ç´¢ï¼Œä½†å¯èƒ½ä¸¢å¤±ä¸Šä¸‹æ–‡\")\n",
    "        print(\"   - æ ‡å‡†åˆ‡ç‰‡(500): å¹³è¡¡ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ç²¾åº¦\")\n",
    "        print(\"   - å¤§åˆ‡ç‰‡(800): ä¿æŒæ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†æ£€ç´¢ç²’åº¦è¾ƒç²—\")\n",
    "        print(\"âœ… éªŒè¯é€šè¿‡ï¼šå‚æ•°é…ç½®å½±å“åˆ†å‰²æ•ˆæœ\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä¸­æ–‡æ–‡æœ¬åˆ†å‰²ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸­æ–‡æ–‡æœ¬åˆ†å‰²ä¼˜åŒ– _ ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "file_path = \"../sample_chinese_text.txt\"\n",
    "\n",
    "print(\"ğŸˆ¯ ä¸­æ–‡æ–‡æœ¬åˆ†å‰²ä¼˜åŒ–æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    if documents:\n",
    "        original_doc = documents[0]\n",
    "        \n",
    "        # æµ‹è¯•ä¸åŒåˆ†éš”ç¬¦ç­–ç•¥\n",
    "        separator_strategies = [\n",
    "            {\n",
    "                \"name\": \"é»˜è®¤åˆ†éš”ç¬¦\",\n",
    "                \"separators\": [\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼›\", \"ï¼Œ\", \" \", \"\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ä¸­æ–‡ä¼˜åŒ–\",\n",
    "                \"separators\": [\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"ï¼Œ\", \" \", \"\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"å¥å­ä¼˜å…ˆ\",\n",
    "                \"separators\": [\"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"\\n\\n\", \"\\n\", \"ï¼Œ\", \" \", \"\"]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for strategy in separator_strategies:\n",
    "            print(f\"\\nğŸ§ª æµ‹è¯•ç­–ç•¥: {strategy['name']}\")\n",
    "            \n",
    "            # åˆ›å»ºåˆ†å‰²å™¨\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=50,\n",
    "                length_function=len,\n",
    "                separators=strategy['separators']\n",
    "            )\n",
    "            \n",
    "            # æ‰§è¡Œåˆ†å‰²\n",
    "            chunks = splitter.split_documents([original_doc])\n",
    "            \n",
    "            # åˆ†æåˆ†å‰²è´¨é‡\n",
    "            chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "            \n",
    "            # æ£€æŸ¥å¥å­å®Œæ•´æ€§ï¼ˆç®€å•æ£€æŸ¥ï¼šä»¥å¥å·ç»“å°¾çš„æ¯”ä¾‹ï¼‰\n",
    "            complete_sentences = sum(1 for chunk in chunks \n",
    "                                    if chunk.page_content.strip().endswith('ã€‚') or \n",
    "                                       chunk.page_content.strip().endswith('ï¼') or\n",
    "                                       chunk.page_content.strip().endswith('ï¼Ÿ'))\n",
    "            \n",
    "            sentence_ratio = complete_sentences / len(chunks) if chunks else 0\n",
    "            \n",
    "            results[strategy['name']] = {\n",
    "                \"chunk_count\": len(chunks),\n",
    "                \"avg_length\": sum(chunk_lengths) / len(chunk_lengths),\n",
    "                \"sentence_ratio\": sentence_ratio,\n",
    "                \"chunks\": chunks\n",
    "            }\n",
    "            \n",
    "            print(f\"   åˆ‡ç‰‡æ•°é‡: {len(chunks)}\")\n",
    "            print(f\"   å¹³å‡é•¿åº¦: {results[strategy['name']]['avg_length']:.1f} å­—ç¬¦\")\n",
    "            print(f\"   å¥å­å®Œæ•´æ€§: {sentence_ratio:.1%}\")\n",
    "            \n",
    "            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªåˆ‡ç‰‡\n",
    "            if chunks:\n",
    "                print(f\"   é¦–ç‰‡é¢„è§ˆ: {chunks[0].page_content[:60]}...\")\n",
    "        \n",
    "        # åˆ†å‰²ç­–ç•¥å¯¹æ¯”\n",
    "        print(f\"\\nğŸ“Š åˆ†å‰²ç­–ç•¥å¯¹æ¯”:\")\n",
    "        print(f\"{'ç­–ç•¥':<12} {'åˆ‡ç‰‡æ•°':<8} {'å¹³å‡é•¿åº¦':<10} {'å¥å­å®Œæ•´æ€§':<12}\")\n",
    "        print(\"-\" * 45)\n",
    "        for name, result in results.items():\n",
    "            print(f\"{name:<12} {result['chunk_count']:<8} {result['avg_length']:<10.1f} {result['sentence_ratio']:<12.1%}\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ ä¸­æ–‡åˆ†å‰²ä¼˜åŒ–å»ºè®®:\")\n",
    "        print(\"   - ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ä½œä¸ºä¸»è¦åˆ†éš”ç¬¦\")\n",
    "        print(\"   - ä¼˜å…ˆä¿æŒå¥å­å®Œæ•´æ€§æé«˜è¯­ä¹‰è¿è´¯æ€§\")\n",
    "        print(\"   - æ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´åˆ†éš”ç¬¦ä¼˜å…ˆçº§\")\n",
    "        print(\"âœ… éªŒè¯é€šè¿‡ï¼šä¸­æ–‡åˆ†å‰²ç­–ç•¥ä¼˜åŒ–æœ‰æ•ˆ\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åˆ†å‰²è´¨é‡è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†å‰²è´¨é‡è¯„ä¼° _ ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import re\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "file_path = \"../sample_chinese_text.txt\"\n",
    "\n",
    "print(\"ğŸ“ åˆ†å‰²è´¨é‡è¯„ä¼°æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    if documents:\n",
    "        original_doc = documents[0]\n",
    "        \n",
    "        # åˆ›å»ºæ ‡å‡†åˆ†å‰²å™¨\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents([original_doc])\n",
    "        \n",
    "        print(f\"ğŸ“Š è´¨é‡è¯„ä¼°æŒ‡æ ‡:\")\n",
    "        \n",
    "        # 1. åŸºç¡€ç»Ÿè®¡\n",
    "        chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "        print(f\"\\nğŸ“ˆ åŸºç¡€ç»Ÿè®¡:\")\n",
    "        print(f\"   åˆ‡ç‰‡æ•°é‡: {len(chunks)}\")\n",
    "        print(f\"   å¹³å‡é•¿åº¦: {sum(chunk_lengths) / len(chunk_lengths):.1f} å­—ç¬¦\")\n",
    "        print(f\"   é•¿åº¦æ ‡å‡†å·®: {(sum((x - sum(chunk_lengths)/len(chunk_lengths))**2 for x in chunk_lengths) / len(chunk_lengths))**0.5:.1f} å­—ç¬¦\")\n",
    "        print(f\"   é•¿åº¦èŒƒå›´: {min(chunk_lengths)}-{max(chunk_lengths)} å­—ç¬¦\")\n",
    "        \n",
    "        # 2. è¯­ä¹‰å®Œæ•´æ€§\n",
    "        print(f\"\\nğŸ”¤ è¯­ä¹‰å®Œæ•´æ€§:\")\n",
    "        \n",
    "        # æ£€æŸ¥å¥å­å®Œæ•´æ€§\n",
    "        sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›']\n",
    "        complete_sentences = sum(1 for chunk in chunks \n",
    "                                if any(chunk.page_content.strip().endswith(end) for end in sentence_endings))\n",
    "        print(f\"   å®Œæ•´å¥å­æ¯”ä¾‹: {complete_sentences/len(chunks):.1%}\")\n",
    "        \n",
    "        # æ£€æŸ¥è¯æ±‡å®Œæ•´æ€§ï¼ˆé¿å…åœ¨è¯æ±‡ä¸­é—´åˆ†å‰²ï¼‰\n",
    "        incomplete_words = sum(1 for chunk in chunks \n",
    "                               if len(chunk.page_content) > 0 and \n",
    "                                  chunk.page_content[-1] not in [' ', '\\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼Œ'])\n",
    "        print(f\"   è¯æ±‡ä¸­æ–­æ¯”ä¾‹: {incomplete_words/len(chunks):.1%}\")\n",
    "        \n",
    "        # 3. å†…å®¹è¦†ç›–åº¦\n",
    "        print(f\"\\nğŸ“‹ å†…å®¹è¦†ç›–åº¦:\")\n",
    "        total_chars = sum(len(chunk.page_content) for chunk in chunks)\n",
    "        original_chars = len(original_doc.page_content)\n",
    "        coverage = total_chars / original_chars\n",
    "        print(f\"   åŸå§‹å­—ç¬¦æ•°: {original_chars}\")\n",
    "        print(f\"   åˆ‡ç‰‡å­—ç¬¦æ€»æ•°: {total_chars}\")\n",
    "        print(f\"   è¦†ç›–ç‡: {coverage:.1%} (åŒ…å«é‡å )\")\n",
    "        \n",
    "        # 4. é‡å åˆ†æ\n",
    "        print(f\"\\nğŸ”— é‡å åˆ†æ:\")\n",
    "        overlap_info = []\n",
    "        for i in range(len(chunks) - 1):\n",
    "            current = chunks[i].page_content\n",
    "            next_chunk = chunks[i + 1].page_content\n",
    "            \n",
    "            # ç®€å•é‡å æ£€æµ‹ï¼ˆæŸ¥æ‰¾å…±åŒçš„åç¼€/å‰ç¼€\n",
    "            max_overlap = min(50, len(current), len(next_chunk))\n",
    "            overlap_found = 0\n",
    "            \n",
    "            for j in range(1, max_overlap + 1):\n",
    "                if current[-j:] == next_chunk[:j]:\n",
    "                    overlap_found = j\n",
    "            \n",
    "            overlap_info.append(overlap_found)\n",
    "        \n",
    "        if overlap_info:\n",
    "            avg_overlap = sum(overlap_info) / len(overlap_info)\n",
    "            print(f\"   å¹³å‡é‡å é•¿åº¦: {avg_overlap:.1f} å­—ç¬¦\")\n",
    "            print(f\"   é‡å ä¸€è‡´æ€§: {'é«˜' if max(overlap_info) - min(overlap_info) <= 10 else 'ä¸­'}\")\n",
    "        \n",
    "        # 5. è´¨é‡è¯„åˆ†\n",
    "        print(f\"\\nâ­ è´¨é‡è¯„åˆ†:\")\n",
    "        \n",
    "        # ç®€å•è¯„åˆ†ç®—æ³•\n",
    "        length_score = 1.0 - abs(500 - sum(chunk_lengths) / len(chunk_lengths)) / 500  # é•¿åº¦ä¸€è‡´æ€§\n",
    "        sentence_score = complete_sentences / len(chunks)  # å¥å­å®Œæ•´æ€§\n",
    "        word_score = 1.0 - incomplete_words / len(chunks)  # è¯æ±‡å®Œæ•´æ€§\n",
    "        \n",
    "        overall_score = (length_score + sentence_score + word_score) / 3 * 100\n",
    "        \n",
    "        print(f\"   é•¿åº¦ä¸€è‡´æ€§: {length_score*100:.1f} åˆ†\")\n",
    "        print(f\"   å¥å­å®Œæ•´æ€§: {sentence_score*100:.1f} åˆ†\")\n",
    "        print(f\"   è¯æ±‡å®Œæ•´æ€§: {word_score*100:.1f} åˆ†\")\n",
    "        print(f\"   ç»¼åˆè¯„åˆ†: {overall_score:.1f} åˆ†\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ è´¨é‡ä¼˜åŒ–å»ºè®®:\")\n",
    "        if overall_score >= 80:\n",
    "            print(\"   - åˆ†å‰²è´¨é‡ä¼˜ç§€ï¼Œå»ºè®®ä¿æŒå½“å‰é…ç½®\")\n",
    "        elif overall_score >= 60:\n",
    "            print(\"   - åˆ†å‰²è´¨é‡è‰¯å¥½ï¼Œå¯å¾®è°ƒå‚æ•°ä¼˜åŒ–\")\n",
    "        else:\n",
    "            print(\"   - åˆ†å‰²è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®è°ƒæ•´åˆ†éš”ç¬¦å’Œå‚æ•°\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šåˆ†å‰²è´¨é‡è¯„ä¼°å®Œæˆ\n",
    "        assert len(chunks) > 0, \"æ— åˆ†å‰²ç»“æœ\"\n",
    "        assert overall_score > 0, \"è´¨é‡è¯„åˆ†å¼‚å¸¸\"\n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šåˆ†å‰²è´¨é‡è¯„ä¼°å®Œæˆ\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ _ ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“‹ Text Splitters å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… åŸºç¡€åˆ†å‰²ï¼šRecursiveCharacterTextSplitter ä½¿ç”¨\",\n",
    "    \"âœ… å‚æ•°é…ç½®ï¼šchunk_size å’Œ chunk_overlap ä¼˜åŒ–\",\n",
    "    \"âœ… ä¸­æ–‡ä¼˜åŒ–ï¼šåˆ†éš”ç¬¦ç­–ç•¥å’Œè¯­ä¹‰å®Œæ•´æ€§\",\n",
    "    \"âœ… è´¨é‡è¯„ä¼°ï¼šåˆ†å‰²æ•ˆæœé‡åŒ–åˆ†æ\",\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/4 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Text Splitters æœ€ä½³å®è·µ:\")\n",
    "print(\"1. å‚æ•°è®¾ç½®ï¼šchunk_size 300-800ï¼Œchunk_overlap 10-20%\")\n",
    "print(\"2. åˆ†éš”ç¬¦é€‰æ‹©ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹å’Œè¯­è¨€ç‰¹ç‚¹å®šåˆ¶\")\n",
    "print(\"3. ä¸­æ–‡å¤„ç†ï¼šä¼˜å…ˆä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ä¿æŒè¯­ä¹‰å®Œæ•´æ€§\")\n",
    "print(\"4. è´¨é‡ç›‘æ§ï¼šå®šæœŸè¯„ä¼°åˆ†å‰²æ•ˆæœå’Œè°ƒæ•´å‚æ•°\")\n",
    "print(\"5. æ€§èƒ½ä¼˜åŒ–ï¼šé¿å…è¿‡åº¦åˆ†å‰²å½±å“æ£€ç´¢æ•ˆç‡\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  Embedding æ–‡æœ¬å‘é‡åŒ–\")\n",
    "print(\"2. æŒæ¡ Vector Stores å‘é‡å­˜å‚¨\")\n",
    "print(\"3. å­¦ä¹  Retrievers æ£€ç´¢å™¨é…ç½®\")\n",
    "print(\"4. æ¢ç´¢å…¶ä»–åˆ†å‰²å™¨ç±»å‹\")\n",
    "print(\"5. å®è·µ RAG ç³»ç»Ÿå®Œæ•´æµç¨‹\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ TextSplitter åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    file_path = \"../sample_chinese_text.txt\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # åŠ è½½å’Œåˆ†å‰²\n",
    "        loader = TextLoader(file_path, encoding='utf-8')\n",
    "        documents = loader.load()\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(documents)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ:\")\n",
    "        print(f\"   å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}\")\n",
    "        print(f\"   åŸå§‹æ–‡æ¡£: {len(documents)} ä¸ª\")\n",
    "        print(f\"   åˆ†å‰²åˆ‡ç‰‡: {len(chunks)} ä¸ª\")\n",
    "        print(f\"   å¹³å‡é•¿åº¦: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.1f} å­—ç¬¦\")\n",
    "        print(\"\\nâœ… Text Splitters å­¦ä¹ å®Œæˆï¼\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  æœ€ç»ˆéªŒè¯è·³è¿‡: æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
