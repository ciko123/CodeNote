{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - ChatOpenAI GPT åŸºç¡€è°ƒç”¨\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹ ä½¿ç”¨ OpenAI æ¥å£è°ƒç”¨ GPT æ¨¡å‹çš„åŸºç¡€æ–¹æ³•\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- æŒæ¡ ChatOpenAI åŸºç¡€è°ƒç”¨\n",
    "- ç†è§£ base_url å’Œ api_key é…ç½®\n",
    "- èƒ½æ§åˆ¶è¾“å‡ºé£æ ¼\n",
    "- æŒæ¡æ ¸å¿ƒå‚æ•°ä½œç”¨\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ChatOpenAI åŸºç¡€åˆå§‹åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChatOpenAI (GPT-4o) åˆå§‹åŒ–æˆåŠŸ\n",
      "æ¨¡å‹é…ç½®: gpt-4o\n",
      "æ¸©åº¦å‚æ•°: 0.7\n",
      "æœ€å¤§ä»¤ç‰Œæ•°: 500\n",
      "API Base URL: https://api.jiekou.ai/openai\n"
     ]
    }
   ],
   "source": [
    "# ChatOpenAI åŸºç¡€åˆå§‹åŒ– - ç‹¬ç«‹ä»£ç å—\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ– ChatOpenAI æ¨¡å‹ï¼ˆä½¿ç”¨ GPT-4oï¼‰\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",           # GPT-4o æ¨èæ¨¡å‹ï¼Œé«˜è´¨é‡å“åº”\n",
    "    temperature=0.7,          # æ¨èå€¼ï¼šå¹³è¡¡éšæœºæ€§å’Œä¸€è‡´æ€§\n",
    "    max_tokens=500,           # å®ç”¨å€¼ï¼šæ§åˆ¶è¾“å‡ºé•¿åº¦\n",
    ")\n",
    "\n",
    "print(\"âœ… ChatOpenAI (GPT-4o) åˆå§‹åŒ–æˆåŠŸ\")\n",
    "print(f\"æ¨¡å‹é…ç½®: {llm.model_name}\")  # ä¿®å¤ï¼šä½¿ç”¨ model_name è€Œä¸æ˜¯ model\n",
    "print(f\"æ¸©åº¦å‚æ•°: {llm.temperature}\")\n",
    "print(f\"æœ€å¤§ä»¤ç‰Œæ•°: {llm.max_tokens}\")\n",
    "print(f\"API Base URL: {os.getenv('OPENAI_BASE_URL')}\")\n",
    "\n",
    "# å¯é€‰ï¼šæŸ¥çœ‹æ‰€æœ‰å¯ç”¨å±æ€§\n",
    "# print(f\"æ‰€æœ‰å±æ€§: {vars(llm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç®€å•å¯¹è¯æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– GPT-4o å›å¤:\n",
      "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªç”±äººå·¥æ™ºèƒ½é©±åŠ¨çš„è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥ç”¨å¤šç§è¯­è¨€ä¸æ‚¨äº¤æµã€‚æˆ‘å¯ä»¥å¸®åŠ©è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€ååŠ©å­¦ä¹ ã€åˆ›æ„å†™ä½œä»¥åŠè§£å†³å„ç§ä»»åŠ¡éœ€æ±‚ã€‚æˆ‘åŸºäºå¤§é‡çš„æ–‡æœ¬æ•°æ®è®­ç»ƒï¼Œä½†æˆ‘çš„çŸ¥è¯†ä»…æ›´æ–°è‡³2023å¹´10æœˆã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼ ğŸ˜Š\n",
      "\n",
      "ğŸ“Š å›å¤é•¿åº¦: 121 å­—ç¬¦\n",
      "ğŸ“Š Token ä½¿ç”¨æƒ…å†µ: {'input_tokens': 15, 'output_tokens': 78, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šæˆåŠŸè·å–å›å¤ä¸”é•¿åº¦å—æ§\n"
     ]
    }
   ],
   "source": [
    "# ç®€å•å¯¹è¯æµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ– ChatOpenAI æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# æµ‹è¯•ç®€å•å¯¹è¯\n",
    "response = llm.invoke(\"ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\")\n",
    "\n",
    "print(\"ğŸ¤– GPT-4o å›å¤:\")\n",
    "print(response.content)\n",
    "print(f\"\\nğŸ“Š å›å¤é•¿åº¦: {len(response.content)} å­—ç¬¦\")\n",
    "print(f\"ğŸ“Š Token ä½¿ç”¨æƒ…å†µ: {response.usage_metadata if hasattr(response, 'usage_metadata') else 'æ— æ³•è·å–'}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šèƒ½æˆåŠŸè·å– GPT-4o AI å›å¤å¹¶æ§åˆ¶è¾“å‡ºé•¿åº¦\n",
    "assert len(response.content) > 0, \"å›å¤å†…å®¹ä¸ºç©º\"\n",
    "assert len(response.content) <= 500, \"å›å¤é•¿åº¦è¶…è¿‡é™åˆ¶\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæˆåŠŸè·å–å›å¤ä¸”é•¿åº¦å—æ§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ ¸å¿ƒå‚æ•°æµ‹è¯• - æ¸©åº¦å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ GPT-4o ä½æ¸©åº¦ (0.1): å¥½çš„ï¼å¯ä»¥ç”¨ä¸€ä¸ªæ¯”å–»æ¥å½¢è±¡åœ°è§£é‡Šæœºå™¨å­¦ä¹ ï¼š\n",
      "\n",
      "**æœºå™¨å­¦ä¹ å°±åƒæ•™ä¸€ä¸ªå­©å­éª‘è‡ªè¡Œè½¦**ã€‚\n",
      "\n",
      "- **æ•°æ®æ˜¯è®­ç»ƒçš„åŸºç¡€**ï¼šä½ ç»™å­©å­ä¸€è¾†è‡ªè¡Œè½¦ï¼ˆæ•°æ®ï¼‰ï¼Œå¹¶è®©ä»–åå¤ç»ƒä¹ ã€‚æ¯æ¬¡éª‘è½¦æ—¶ï¼Œä»–éƒ½ä¼šæ‘”å€’ã€è°ƒæ•´å§¿åŠ¿ï¼Œæ…¢æ…¢å­¦ä¼šå¦‚ä½•ä¿æŒå¹³è¡¡ã€‚è¿™å°±åƒæœºå™¨å­¦ä¹ ä¸­çš„â€œè®­ç»ƒæ•°æ®â€ï¼Œé€šè¿‡ä¸æ–­å°è¯•å’Œåé¦ˆï¼Œæ¨¡å‹é€æ¸å­¦ä¼šå¦‚ä½•å®Œæˆä»»åŠ¡ã€‚\n",
      "\n",
      "- **ç®—æ³•æ˜¯å­¦ä¹ çš„è§„åˆ™**ï¼šä½ å¯èƒ½ä¼šå‘Šè¯‰å­©å­ä¸€äº›éª‘è½¦çš„æŠ€å·§ï¼Œæ¯”å¦‚â€œä¿æŒå¹³è¡¡â€ã€â€œç”¨è„šè¹¬è¸æ¿â€ã€â€œæ¡ç´§è½¦æŠŠâ€ã€‚è¿™äº›è§„åˆ™å°±åƒæœºå™¨å­¦ä¹ ä¸­çš„ç®—æ³•ï¼ŒæŒ‡å¯¼æ¨¡å‹å¦‚ä½•ä»æ•°æ®ä¸­å­¦ä¹ ã€‚\n",
      "\n",
      "- **æ¨¡å‹æ˜¯å­¦ä¼šéª‘è½¦çš„èƒ½åŠ›**ï¼šç»è¿‡å¤šæ¬¡ç»ƒä¹ ï¼Œå­©å­ç»ˆäºå­¦ä¼šäº†éª‘è½¦ã€‚è¿™å°±åƒæœºå™¨å­¦ä¹ æ¨¡å‹å®Œæˆäº†è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨æ–°æƒ…å†µä¸‹ï¼ˆæ¯”å¦‚\n",
      "\n",
      "\n",
      "ğŸ¨ GPT-4o é«˜æ¸©åº¦ (1.0): å¥½çš„ï¼æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªå¨å¸ˆå­¦ä¹ åšèœçš„æ¯”å–»æ¥è§£é‡Šæœºå™¨å­¦ä¹ ï¼š\n",
      "\n",
      "---\n",
      "\n",
      "æœºå™¨å­¦ä¹ å°±åƒä¸€ä¸ªå­¦å¾’å¨å¸ˆé€šè¿‡å°è¯•å’Œåé¦ˆä¸æ–­å­¦ä¹ å¦‚ä½•åˆ¶ä½œç¾å‘³ä½³è‚´ã€‚èµ·åˆï¼Œå­¦å¾’å¯èƒ½ä¸çŸ¥é“å¦‚ä½•æ­£ç¡®è°ƒæ–™ï¼Œä¹Ÿæœ‰å¯èƒ½åšçš„èœä¸å¤ªå¥½åƒã€‚ä½†éšç€æ—¶é—´æ¨ç§»ï¼Œä»–ä»ç»éªŒä¸­ç§¯ç´¯çŸ¥è¯†ï¼Œäº†è§£å“ªäº›è°ƒå‘³æ–™æ­é…å¾—å¥½ï¼Œå“ªäº›èœçƒ§å¾—å¤ªä¹…ä¼šä¸å¥½åƒç­‰ç­‰ã€‚é€šè¿‡ä¸æ–­è¯•éªŒå’Œäººä»¬çš„åé¦ˆï¼Œå­¦å¾’å¨å¸ˆæœ€ç»ˆèƒ½å¤Ÿåšå‡ºè¶Šæ¥è¶Šå¥½åƒçš„èœã€‚\n",
      "\n",
      "åœ¨è¿™ä¸ªæ¯”å–»ä¸­ï¼š\n",
      "- **æ•°æ®**æ˜¯é£Ÿæï¼šæœºå™¨å­¦ä¹ éœ€è¦é å¤§é‡çš„æ ·æœ¬æ•°æ®ï¼Œè€Œå¨å¸ˆéœ€è¦ä¸åŒç±»å‹çš„é£Ÿææ¥åšèœã€‚\n",
      "- **ç®—æ³•**æ˜¯é£Ÿè°±ï¼šæœºå™¨å­¦ä¹ çš„ç®—æ³•æä¾›äº†ä¸€å®šçš„è§„åˆ™ï¼Œå°±åƒå¨å¸ˆç”¨é£Ÿè°±æŒ‡å¯¼å¦‚ä½•ç»„åˆé£Ÿæä¸€æ ·ã€‚\n",
      "- **æ¨¡å‹**æ˜¯å¨å¸ˆçš„\n",
      "\n",
      "ğŸ“Š å‚æ•°å¯¹æ¯”åˆ†æ:\n",
      "ä½æ¸©åº¦å›å¤é•¿åº¦: 295 å­—ç¬¦\n",
      "é«˜æ¸©åº¦å›å¤é•¿åº¦: 283 å­—ç¬¦\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šæ¸©åº¦å‚æ•°å½±å“ GPT-4o è¾“å‡ºé£æ ¼\n"
     ]
    }
   ],
   "source": [
    "# æ ¸å¿ƒå‚æ•°æµ‹è¯• - æ¸©åº¦å¯¹æ¯” - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# æµ‹è¯•ä¸åŒæ¸©åº¦å‚æ•°çš„æ•ˆæœ\n",
    "test_prompt = \"è¯·ç”¨ä¸€ä¸ªæ¯”å–»è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ \"\n",
    "\n",
    "# ä½æ¸©åº¦ï¼ˆæ›´ç¡®å®šæ€§ï¼‰\n",
    "llm_low_temp = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "response_low = llm_low_temp.invoke(test_prompt)\n",
    "\n",
    "print(f\"ğŸ¯ GPT-4o ä½æ¸©åº¦ (0.1): {response_low.content}\")\n",
    "\n",
    "print()  # ç©ºè¡Œ\n",
    "print()  # ç©ºè¡Œ\n",
    "\n",
    "# é«˜æ¸©åº¦ï¼ˆæ›´åˆ›é€ æ€§ï¼‰\n",
    "llm_high_temp = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=1.0,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "response_high = llm_high_temp.invoke(test_prompt)\n",
    "print(f\"ğŸ¨ GPT-4o é«˜æ¸©åº¦ (1.0): {response_high.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šèƒ½æ ¹æ®å‚æ•°è°ƒèŠ‚ GPT-4o æ¨¡å‹è¾“å‡ºé£æ ¼\n",
    "\n",
    "print(\"\\nğŸ“Š å‚æ•°å¯¹æ¯”åˆ†æ:\")\n",
    "print(f\"ä½æ¸©åº¦å›å¤é•¿åº¦: {len(response_low.content)} å­—ç¬¦\")\n",
    "print(f\"é«˜æ¸©åº¦å›å¤é•¿åº¦: {len(response_high.content)} å­—ç¬¦\")\n",
    "print(\"âœ… éªŒè¯é€šè¿‡ï¼šæ¸©åº¦å‚æ•°å½±å“ GPT-4o è¾“å‡ºé£æ ¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ ¸å¿ƒå‚æ•°æµ‹è¯• - æ¨¡å‹å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ gpt-4o (æ¨è): LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨çš„æ¡†æ¶ï¼Œæ”¯æŒé“¾å¼è°ƒç”¨ã€å·¥å…·é›†æˆå’Œä¸Šä¸‹æ–‡ç®¡ç†ã€‚\n",
      "âš¡ gpt-4o-mini (è½»é‡): LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œæ”¯æŒå¤šç§æ•°æ®æºå’Œå·¥å…·é›†æˆï¼Œä¾¿äºå¼€å‘è€…åˆ›å»ºå¤æ‚çš„å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬å¤„ç†åº”ç”¨ã€‚\n",
      "ğŸ“Š gpt-4.1 (æ ‡å‡†): LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„å¼€æºå¼€å‘æ¡†æ¶ã€‚\n",
      "\n",
      "ğŸ“Š æ¨¡å‹å¯¹æ¯”åˆ†æ:\n",
      "gpt-4o å›å¤: 48 å­—ç¬¦\n",
      "gpt-4o-mini å›å¤: 62 å­—ç¬¦\n",
      "gpt-4.1 å›å¤: 34 å­—ç¬¦\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡å‹é€‰æ‹©å½±å“å“åº”ç‰¹å¾\n"
     ]
    }
   ],
   "source": [
    "# æ ¸å¿ƒå‚æ•°æµ‹è¯• - æ¨¡å‹å¯¹æ¯” - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# æµ‹è¯•ä¸åŒæ¨¡å‹çš„æ•ˆæœ\n",
    "test_prompt = \"ç”¨ä¸€å¥è¯ä»‹ç» LangChainï¼Œä¸è¶…è¿‡50å­—\"\n",
    "\n",
    "# GPT-4o ä¸»åŠ›æ¨¡å‹\n",
    "llm_gpt4o = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "response_gpt4o = llm_gpt4o.invoke(test_prompt)\n",
    "print(f\"ğŸš€ gpt-4o (æ¨è): {response_gpt4o.content}\")\n",
    "\n",
    "# GPT-4o-mini è½»é‡æ¨¡å‹\n",
    "llm_mini = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "response_mini = llm_mini.invoke(test_prompt)\n",
    "print(f\"âš¡ gpt-4o-mini (è½»é‡): {response_mini.content}\")\n",
    "\n",
    "# GPT-4.1 æ ‡å‡†æ¨¡å‹\n",
    "llm_41 = ChatOpenAI(\n",
    "    model=\"gpt-4.1\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "response_41 = llm_41.invoke(test_prompt)\n",
    "print(f\"ğŸ“Š gpt-4.1 (æ ‡å‡†): {response_41.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šä¸åŒæ¨¡å‹æä¾›ä¸åŒçš„å“åº”ç‰¹å¾\n",
    "print(\"\\nğŸ“Š æ¨¡å‹å¯¹æ¯”åˆ†æ:\")\n",
    "print(f\"gpt-4o å›å¤: {len(response_gpt4o.content)} å­—ç¬¦\")\n",
    "print(f\"gpt-4o-mini å›å¤: {len(response_mini.content)} å­—ç¬¦\")\n",
    "print(f\"gpt-4.1 å›å¤: {len(response_41.content)} å­—ç¬¦\")\n",
    "print(\"âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡å‹é€‰æ‹©å½±å“å“åº”ç‰¹å¾\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PromptTemplate åŸºç¡€ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PromptTemplate åˆ›å»ºæˆåŠŸ\n",
      "æ¨¡æ¿: è¯·ç”¨ GPT-4o çš„è§†è§’å†™ä¸€æ®µå…³äº {topic} çš„ä»‹ç»ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚\n",
      "\n",
      "ğŸ“ æ ¼å¼åŒ–åçš„æç¤ºè¯: è¯·ç”¨ GPT-4o çš„è§†è§’å†™ä¸€æ®µå…³äº äººå·¥æ™ºèƒ½ çš„ä»‹ç»ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚\n",
      "\n",
      "ğŸ¤– GPT-4o å›å¤: äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé¢†åŸŸï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»æ™ºèƒ½å®ç°å­¦ä¹ ã€æ¨ç†å’Œå†³ç­–ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªåŠ¨åŒ–ã€åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸï¼Œæ¨åŠ¨æŠ€æœ¯åˆ›æ–°ä¸ç¤¾ä¼šå‘å±•ã€‚\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡æ¿åŠ¨æ€ç”Ÿæˆå¹¶æˆåŠŸè°ƒç”¨\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate åŸºç¡€ä½¿ç”¨ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæç¤ºè¯æ¨¡æ¿\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"è¯·ç”¨ GPT-4o çš„è§†è§’å†™ä¸€æ®µå…³äº {topic} çš„ä»‹ç»ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚\"\n",
    ")\n",
    "\n",
    "print(\"âœ… PromptTemplate åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"æ¨¡æ¿: {prompt.template}\")\n",
    "\n",
    "# æµ‹è¯•æ¨¡æ¿\n",
    "topic = \"äººå·¥æ™ºèƒ½\"\n",
    "formatted_prompt = prompt.format(topic=topic)\n",
    "print(f\"\\nğŸ“ æ ¼å¼åŒ–åçš„æç¤ºè¯: {formatted_prompt}\")\n",
    "\n",
    "# ä½¿ç”¨æ¨¡æ¿è°ƒç”¨æ¨¡å‹\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(f\"\\nğŸ¤– GPT-4o å›å¤: {response.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šæ¨¡æ¿èƒ½åŠ¨æ€ç”Ÿæˆ prompt å¹¶ä¼ å…¥ GPT-4o\n",
    "assert \"äººå·¥æ™ºèƒ½\" in formatted_prompt, \"æ¨¡æ¿å˜é‡æœªæ­£ç¡®å¡«å……\"\n",
    "assert len(response.content) <= 100, \"å›å¤é•¿åº¦è¶…å‡ºé¢„æœŸ\"  # ç»™ä¸€äº›ç¼“å†²\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæ¨¡æ¿åŠ¨æ€ç”Ÿæˆå¹¶æˆåŠŸè°ƒç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ChatPromptTemplate å¤šè§’è‰²å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChatPromptTemplate åˆ›å»ºæˆåŠŸ\n",
      "\n",
      "ğŸ’¬ æ ¼å¼åŒ–åçš„æ¶ˆæ¯:\n",
      "1. [system]: ä½ æ˜¯ä¸€ä¸ªç²¾é€šä¸­æ–‡çš„ GPT-4o åŠ©æ‰‹ï¼Œä¸“é—¨è§£é‡ŠæŠ€æœ¯æ¦‚å¿µã€‚\n",
      "2. [human]: è¯·ç”¨ GPT-4o çš„èƒ½åŠ›è®²è§£ LangChain æ¡†æ¶ï¼Œç”¨ç®€å•çš„è¯­è¨€è¯´æ˜ã€‚\n",
      "\n",
      "ğŸ¤– GPT-4o å›å¤: å¥½çš„ï¼è®©æˆ‘ç”¨ç®€å•çš„è¯­è¨€æ¥è®²è§£ LangChain æ¡†æ¶ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ\n",
      "LangChain æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼Œæ¯”å¦‚ OpenAI çš„ GPT-4ï¼‰è®¾è®¡çš„å·¥å…·æ¡†æ¶ã€‚å®ƒçš„ç›®çš„æ˜¯è®©å¼€å‘è€…æ›´æ–¹ä¾¿åœ°å°†è¿™äº›è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®å’Œå¤æ‚çš„é€»è¾‘ç»“åˆèµ·æ¥ï¼Œä»è€Œæ„å»ºæ›´å¼ºå¤§çš„ AI åº”ç”¨ã€‚\n",
      "\n",
      "æ¢å¥è¯è¯´ï¼ŒLangChain å°±åƒæ˜¯ä¸€ä¸ªâ€œå·¥å…·ç®±â€ï¼Œå¸®åŠ©ä½ ç”¨è¯­è¨€æ¨¡å‹å»å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œæ¯”å¦‚å’Œæ•°æ®åº“äº¤äº’ã€å¤„ç†é•¿æ–‡æ¡£ã€ç®¡ç†å¯¹è¯æµç¨‹ç­‰ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### ä¸ºä»€ä¹ˆéœ€è¦ LangChainï¼Ÿ\n",
      "å¤§è¯­è¨€æ¨¡å‹æœ¬èº«å¾ˆå¼ºå¤§ï¼Œä½†å®ƒæœ‰ä¸€äº›é™åˆ¶ï¼Œæ¯”å¦‚ï¼š\n",
      "1. **æ— æ³•ç›´æ¥è®¿é—®å¤–éƒ¨æ•°æ®**ï¼šæ¨¡å‹ä¸ä¼šçŸ¥é“æ•°æ®åº“ã€API æˆ–æ–‡ä»¶ä¸­çš„ä¿¡æ¯ã€‚\n",
      "2. **å¤„ç†å¤æ‚é€»è¾‘å¾ˆéº»çƒ¦**ï¼šä¾‹å¦‚ï¼Œæ¨¡å‹æ— æ³•å¾ˆå¥½åœ°ç®¡ç†å¤šè½®å¯¹è¯æˆ–æ‰§è¡Œä¸€ç³»åˆ—çš„ä»»åŠ¡ã€‚\n",
      "3. **éš¾ä»¥æ‰©å±•å’Œç®¡ç†åŠŸèƒ½**ï¼šå½“ä½ æƒ³è¦è®©æ¨¡å‹æ‰§è¡Œå¤šä¸ªä»»åŠ¡æ—¶ï¼Œä»£ç å¯èƒ½ä¼šå˜å¾—å¤æ‚ã€‚\n",
      "\n",
      "LangChain é€šè¿‡æä¾›å·¥å…·å’Œæ¨¡å—ï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿæ›´è½»æ¾åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### LangChain çš„æ ¸å¿ƒæ¦‚å¿µ\n",
      "LangChain æœ‰å‡ ä¸ªé‡è¦çš„ç»„æˆéƒ¨åˆ†ï¼Œä¸‹é¢ç”¨ç®€å•çš„ä¾‹å­æ¥è¯´æ˜ï¼š\n",
      "\n",
      "1. **é“¾ï¼ˆChainï¼‰**ï¼š\n",
      "   - é“¾\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šå¤šè§’è‰²æ¨¡æ¿æ­£ç¡®ç”Ÿæˆæ¶ˆæ¯æ ¼å¼\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplate å¤šè§’è‰²å¯¹è¯ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå¤šè§’è‰²å¯¹è¯æ¨¡æ¿\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªç²¾é€šä¸­æ–‡çš„ GPT-4o åŠ©æ‰‹ï¼Œä¸“é—¨è§£é‡ŠæŠ€æœ¯æ¦‚å¿µã€‚\"),\n",
    "    (\"user\", \"è¯·ç”¨ GPT-4o çš„èƒ½åŠ›è®²è§£ {concept}ï¼Œç”¨ç®€å•çš„è¯­è¨€è¯´æ˜ã€‚\")\n",
    "])\n",
    "\n",
    "print(\"âœ… ChatPromptTemplate åˆ›å»ºæˆåŠŸ\")\n",
    "\n",
    "# æµ‹è¯•å¤šè§’è‰²æ¨¡æ¿\n",
    "concept = \"LangChain æ¡†æ¶\"\n",
    "formatted_messages = chat_prompt.format_messages(concept=concept)\n",
    "\n",
    "print(\"\\nğŸ’¬ æ ¼å¼åŒ–åçš„æ¶ˆæ¯:\")\n",
    "for i, message in enumerate(formatted_messages):\n",
    "    print(f\"{i+1}. [{message.type}]: {message.content}\")\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹\n",
    "response = llm.invoke(formatted_messages)\n",
    "print(f\"\\nğŸ¤– GPT-4o å›å¤: {response.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šèƒ½æ­£ç¡®å¡«å……å˜é‡å¹¶æŒ‰è§’è‰²ç”Ÿæˆ GPT-4o èŠå¤©æ¶ˆæ¯æ ¼å¼\n",
    "# æ³¨æ„ï¼š(\"user\", ...) ç”Ÿæˆçš„æ¶ˆæ¯ç±»å‹æ˜¯ \"human\"ï¼Œä¸æ˜¯ \"user\"\n",
    "assert len(formatted_messages) == 2, \"æ¶ˆæ¯æ•°é‡ä¸æ­£ç¡®\"\n",
    "assert formatted_messages[0].type == \"system\", \"ç³»ç»Ÿæ¶ˆæ¯ç±»å‹é”™è¯¯\"\n",
    "assert formatted_messages[1].type == \"human\", \"ç”¨æˆ·æ¶ˆæ¯ç±»å‹é”™è¯¯\"\n",
    "assert \"LangChain æ¡†æ¶\" in formatted_messages[1].content, \"å˜é‡æœªæ­£ç¡®å¡«å……\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šå¤šè§’è‰²æ¨¡æ¿æ­£ç¡®ç”Ÿæˆæ¶ˆæ¯æ ¼å¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LCEL é“¾å¼è°ƒç”¨ï¼ˆæ ¸å¿ƒè¯­æ³•ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LCEL é“¾åˆ›å»ºæˆåŠŸ\n",
      "ğŸ”— é“¾ç»“æ„: PromptTemplate â†’ ChatOpenAI (GPT-4o) â†’ StrOutputParser\n",
      "\n",
      "ğŸ¤– é“¾å¼è°ƒç”¨ç»“æœ: åŒºå—é“¾æ˜¯ä¸€ç§å»ä¸­å¿ƒåŒ–ã€åˆ†å¸ƒå¼çš„æ•°æ®å­˜å‚¨æŠ€æœ¯ï¼Œé€šè¿‡åŠ å¯†ç®—æ³•ç¡®ä¿æ•°æ®çš„å®‰å…¨æ€§ä¸ä¸å¯ç¯¡æ”¹æ€§ã€‚å…¶æ ¸å¿ƒæ˜¯å°†æ•°æ®ä»¥åŒºå—å½¢å¼æŒ‰æ—¶é—´é¡ºåºè¿æ¥ï¼Œå½¢æˆé“¾å¼ç»“æ„ã€‚åŒºå—é“¾å¹¿æ³›åº”ç”¨äºé‡‘èã€ä¾›åº”é“¾ã€æ™ºèƒ½åˆçº¦ç­‰é¢†åŸŸï¼Œå…·æœ‰é€æ˜ã€é«˜æ•ˆå’Œå®‰å…¨çš„ç‰¹ç‚¹ã€‚\n",
      "ğŸ“Š ç»“æœç±»å‹: <class 'str'>\n",
      "ğŸ“Š ç»“æœé•¿åº¦: 107 å­—ç¬¦\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šLCEL é“¾å¼è°ƒç”¨æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "# LCEL é“¾å¼è°ƒç”¨ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæç¤ºè¯æ¨¡æ¿\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"è¯·ç”¨ GPT-4o çš„è§†è§’ç®€è¦ä»‹ç» {topic}ï¼Œæ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚\"\n",
    ")\n",
    "\n",
    "# åˆ›å»º LCEL é“¾ï¼šprompt | llm | parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"âœ… LCEL é“¾åˆ›å»ºæˆåŠŸ\")\n",
    "print(\"ğŸ”— é“¾ç»“æ„: PromptTemplate â†’ ChatOpenAI (GPT-4o) â†’ StrOutputParser\")\n",
    "\n",
    "# æµ‹è¯•é“¾å¼è°ƒç”¨\n",
    "topic = \"åŒºå—é“¾æŠ€æœ¯\"\n",
    "result = chain.invoke(topic)  # ä¿®å¤ï¼šç›´æ¥ä¼ é€’å­—ç¬¦ä¸²å‚æ•°ï¼Œè€Œä¸æ˜¯å­—å…¸\n",
    "\n",
    "print(f\"\\nğŸ¤– é“¾å¼è°ƒç”¨ç»“æœ: {result}\")\n",
    "print(f\"ğŸ“Š ç»“æœç±»å‹: {type(result)}\")\n",
    "print(f\"ğŸ“Š ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šé“¾å¼è°ƒç”¨è¿”å› GPT-4o å­—ç¬¦ä¸²æ ¼å¼ç»“æœ\n",
    "assert isinstance(result, str), \"ç»“æœä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹\"\n",
    "assert len(result) > 0, \"ç»“æœä¸ºç©º\"\n",
    "assert \"åŒºå—é“¾\" in result, \"ç»“æœä¸åŒ…å«ç›¸å…³å†…å®¹\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šLCEL é“¾å¼è°ƒç”¨æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç»“æ„åŒ–è¾“å‡ºæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç»“æ„åŒ–è¾“å‡ºé“¾åˆ›å»ºæˆåŠŸ\n",
      "ğŸ“‹ è¾“å‡ºç»“æ„: title, summary, confidence\n",
      "\n",
      "ğŸ“Š ç»“æ„åŒ–è¾“å‡ºç»“æœ:\n",
      "æ ‡é¢˜: é‡å­è®¡ç®—æ¦‚è¿°\n",
      "æ‘˜è¦: é‡å­è®¡ç®—æ˜¯ä¸€ç§åŸºäºé‡å­åŠ›å­¦åŸç†çš„æ–°å‹è®¡ç®—æ–¹å¼ï¼Œåˆ©ç”¨é‡å­æ¯”ç‰¹å®ç°é«˜æ•ˆè®¡ç®—èƒ½åŠ›ï¼Œå¯¹å¯†ç å­¦ã€ä¼˜åŒ–é—®é¢˜åŠæœºå™¨å­¦ä¹ ç­‰é¢†åŸŸå…·æœ‰é‡è¦å½±å“ã€‚\n",
      "ç½®ä¿¡åº¦: 0.95\n",
      "ğŸ“Š ç»“æœç±»å‹: <class '__main__.GPTOutput'>\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šGPT-4o ç»“æ„åŒ–è¾“å‡ºç¬¦åˆ Pydantic æ¨¡å‹\n"
     ]
    }
   ],
   "source": [
    "# ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºç»“æ„\n",
    "class GPTOutput(BaseModel):\n",
    "    title: str = Field(description=\"å†…å®¹æ ‡é¢˜\")\n",
    "    summary: str = Field(description=\"å†…å®¹æ‘˜è¦\")\n",
    "    confidence: float = Field(description=\"ç½®ä¿¡åº¦ï¼Œ0-1ä¹‹é—´\")\n",
    "\n",
    "# åˆ›å»ºæç¤ºè¯æ¨¡æ¿ - æ˜ç¡®æŒ‡å®š JSON ç»“æ„\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"\"\"è¯·ä¸º {topic} ç”Ÿæˆå†…å®¹ï¼Œå¹¶ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¾“å‡ºï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n",
    "- title: å­—ç¬¦ä¸²ç±»å‹ï¼Œå†…å®¹æ ‡é¢˜\n",
    "- summary: å­—ç¬¦ä¸²ç±»å‹ï¼Œå†…å®¹æ‘˜è¦  \n",
    "- confidence: æµ®ç‚¹æ•°ç±»å‹ï¼Œç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´ï¼‰\n",
    "\n",
    "ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š\n",
    "{{\"title\": \"ç¤ºä¾‹æ ‡é¢˜\", \"summary\": \"ç¤ºä¾‹æ‘˜è¦\", \"confidence\": 0.85}}\n",
    "\n",
    "è¯·ä¸º {topic} ç”Ÿæˆç¬¦åˆä¸Šè¿° JSON ç»“æ„çš„å†…å®¹ï¼š\"\"\"\n",
    ")\n",
    "\n",
    "# åˆ›å»ºç»“æ„åŒ–è¾“å‡ºé“¾\n",
    "structured_llm = llm.with_structured_output(GPTOutput)\n",
    "structured_chain = prompt | structured_llm\n",
    "\n",
    "print(\"âœ… ç»“æ„åŒ–è¾“å‡ºé“¾åˆ›å»ºæˆåŠŸ\")\n",
    "print(\"ğŸ“‹ è¾“å‡ºç»“æ„: title, summary, confidence\")\n",
    "\n",
    "# æµ‹è¯•ç»“æ„åŒ–è¾“å‡º\n",
    "topic = \"é‡å­è®¡ç®—\"\n",
    "result = structured_chain.invoke({\"topic\": topic})\n",
    "\n",
    "print(f\"\\nğŸ“Š ç»“æ„åŒ–è¾“å‡ºç»“æœ:\")\n",
    "print(f\"æ ‡é¢˜: {result.title}\")\n",
    "print(f\"æ‘˜è¦: {result.summary}\")\n",
    "print(f\"ç½®ä¿¡åº¦: {result.confidence}\")\n",
    "print(f\"ğŸ“Š ç»“æœç±»å‹: {type(result)}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šGPT-4o è¾“å‡ºæ»¡è¶³æŒ‡å®š Pydantic æ¨¡å‹ç»“æ„\n",
    "assert isinstance(result, GPTOutput), \"ç»“æœç±»å‹ä¸æ­£ç¡®\"\n",
    "assert isinstance(result.title, str), \"æ ‡é¢˜ä¸æ˜¯å­—ç¬¦ä¸²\"\n",
    "assert isinstance(result.summary, str), \"æ‘˜è¦ä¸æ˜¯å­—ç¬¦ä¸²\"\n",
    "assert isinstance(result.confidence, float), \"ç½®ä¿¡åº¦ä¸æ˜¯æµ®ç‚¹æ•°\"\n",
    "assert 0 <= result.confidence <= 1, \"ç½®ä¿¡åº¦è¶…å‡ºèŒƒå›´\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šGPT-4o ç»“æ„åŒ–è¾“å‡ºç¬¦åˆ Pydantic æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. æµå¼è¾“å‡ºæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æµå¼è¾“å‡º ChatOpenAI (GPT-4o) åˆå§‹åŒ–æˆåŠŸ\n",
      "ğŸ”„ å¼€å§‹æµå¼è¾“å‡ºæµ‹è¯•...\n",
      "--------------------------------------------------\n",
      "LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œç”¨äºæ„å»ºåŸºäºè¯­è¨€æ¨¡å‹ (LLM) çš„åº”ç”¨ç¨‹åºï¼Œå…¶ä¸»è¦åŠŸèƒ½å¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹ä¸‰ä¸ªè¦ç‚¹ï¼š\n",
      "\n",
      "1. **æ¨¡å—åŒ–å’Œç»„ä»¶åŒ–è®¾è®¡**  \n",
      "   LangChain æä¾›äº†ä¸€ç³»åˆ—æ¨¡å—åŒ–ç»„ä»¶ï¼Œä¾‹å¦‚**æç¤ºæ¨¡æ¿**ï¼ˆPrompt Templatesï¼‰ã€**æ–‡æ¡£åŠ è½½å™¨**ï¼ˆDocument Loadersï¼‰ã€**è®°å¿†æ¨¡å—**ï¼ˆMemoryï¼‰ã€**å·¥å…·å’Œä»£ç†**ï¼ˆTools & Agentsï¼‰ï¼Œå¸®åŠ©å¼€å‘è€…è½»æ¾æ„å»ºå¤æ‚çš„è¯­è¨€æ¨¡å‹åº”ç”¨ã€‚é€šè¿‡è¿™äº›ç»„ä»¶ï¼Œå¼€å‘è€…å¯ä»¥å¿«é€Ÿå®ç°å¯¹è¯­è¨€æ¨¡å‹çš„è°ƒç”¨ã€ä¸Šä¸‹æ–‡ç®¡ç†å’Œä»»åŠ¡æ‰§è¡Œã€‚\n",
      "\n",
      "2. **ä¸Šä¸‹æ–‡å¢å¼ºä¸è®°å¿†èƒ½åŠ›**  \n",
      "   LangChain æ”¯æŒå°†å¤–éƒ¨æ•°æ®æ•´åˆåˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡é›†æˆæ•°æ®åº“ã€æ–‡æ¡£ã€API ç­‰æ•°æ®æºï¼ŒLangChain å¯ä»¥å®ç°åŸºäºå¤–éƒ¨çŸ¥è¯†çš„é—®ç­”ã€æœç´¢å’Œå¤æ‚æ¨ç†ã€‚åŒæ—¶ï¼Œå®ƒ\n",
      "--------------------------------------------------\n",
      "âœ… æµå¼è¾“å‡ºå®Œæˆ\n",
      "ğŸ“Š å®Œæ•´å›å¤é•¿åº¦: 382 å­—ç¬¦\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šGPT-4o æµå¼è¾“å‡ºåŠŸèƒ½æ­£å¸¸\n"
     ]
    }
   ],
   "source": [
    "# æµå¼è¾“å‡ºæµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¯ç”¨æµå¼è¾“å‡ºï¼‰\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    "    streaming=True,  # å¯ç”¨æµå¼è¾“å‡º\n",
    ")\n",
    "\n",
    "print(\"âœ… æµå¼è¾“å‡º ChatOpenAI (GPT-4o) åˆå§‹åŒ–æˆåŠŸ\")\n",
    "print(\"ğŸ”„ å¼€å§‹æµå¼è¾“å‡ºæµ‹è¯•...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# æµ‹è¯•æµå¼è¾“å‡º\n",
    "test_prompt = \"è¯·ç”¨ä¸‰ä¸ªè¦ç‚¹ä»‹ç» LangChain çš„ä¸»è¦åŠŸèƒ½\"\n",
    "full_response = \"\"\n",
    "\n",
    "try:\n",
    "    for chunk in llm.stream(test_prompt):\n",
    "        if hasattr(chunk, 'content') and chunk.content:\n",
    "            print(chunk.content, end='', flush=True)\n",
    "            full_response += chunk.content\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"âœ… æµå¼è¾“å‡ºå®Œæˆ\")\n",
    "    print(f\"ğŸ“Š å®Œæ•´å›å¤é•¿åº¦: {len(full_response)} å­—ç¬¦\")\n",
    "    \n",
    "    # éªŒè¯ç‚¹ï¼šæµå¼è¾“å‡ºèƒ½æ­£å¸¸å·¥ä½œ\n",
    "    assert len(full_response) > 0, \"æµå¼è¾“å‡ºç»“æœä¸ºç©º\"\n",
    "    print(\"âœ… éªŒè¯é€šè¿‡ï¼šGPT-4o æµå¼è¾“å‡ºåŠŸèƒ½æ­£å¸¸\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æµå¼è¾“å‡ºæµ‹è¯•å¤±è´¥: {e}\")\n",
    "    print(\"å¯èƒ½åŸå› ï¼š\")\n",
    "    print(\"1. å½“å‰ç¯å¢ƒä¸æ”¯æŒæµå¼è¾“å‡º\")\n",
    "    print(\"2. ç½‘ç»œè¿æ¥é—®é¢˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ‰¹é‡å¤„ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰¹é‡å¤„ç† ChatOpenAI (GPT-4o) åˆå§‹åŒ–æˆåŠŸ\n",
      "ğŸ“‹ æ‰¹é‡å¤„ç† 3 ä¸ªè¯·æ±‚...\n",
      "\n",
      "ğŸ”„ æ–¹æ³•1ï¼šé€ä¸ªè°ƒç”¨\n",
      "  1. æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡æ•°æ®å’Œç®—æ³•ä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œä»è€Œåœ¨æ— éœ€æ˜¾å¼ç¼–ç¨‹çš„æƒ…å†µä¸‹å®Œæˆä»»åŠ¡çš„æŠ€æœ¯ã€‚\n",
      "  2. æ·±åº¦å­¦ä¹ æ˜¯ä¸€ç§åŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„å¤§è§„æ¨¡æ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¤šå±‚éçº¿æ€§å˜æ¢ä»æ•°æ®ä¸­è‡ªåŠ¨æå–ç‰¹å¾å’Œæ¨¡å¼ï¼Œç”¨äºè§£å†³å¤æ‚çš„ä»»åŠ¡ã€‚\n",
      "  3. ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¯å‘çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡ç”±èŠ‚ç‚¹ï¼ˆç±»ä¼¼äºç¥ç»å…ƒï¼‰ç»„æˆçš„å±‚çº§ç»“æ„è¿›è¡Œä¿¡æ¯å¤„ç†å’Œç‰¹å¾æå–ï¼Œå¹¿æ³›åº”ç”¨äºæ¨¡å¼è¯†åˆ«ã€é¢„æµ‹å’Œåˆ†ç±»ç­‰ä»»åŠ¡ã€‚\n",
      "\n",
      "ğŸ”„ æ–¹æ³•2ï¼šä½¿ç”¨ batch æ–¹æ³•\n",
      "  1. æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡ç®—æ³•å’Œæ¨¡å‹è®©è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è§„å¾‹å¹¶è¿›è¡Œé¢„æµ‹æˆ–å†³ç­–çš„æŠ€æœ¯ã€‚\n",
      "  2. æ·±åº¦å­¦ä¹ æ˜¯ä¸€ç§åŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå¤šå±‚ç½‘ç»œç»“æ„è‡ªåŠ¨æå–å’Œå­¦ä¹ æ•°æ®çš„é«˜å±‚æ¬¡ç‰¹å¾ï¼Œä»è€Œå®ç°å¤æ‚é—®é¢˜çš„å»ºæ¨¡ä¸è§£å†³ã€‚\n",
      "  3. ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ‹Ÿç”±èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰å’Œè¿æ¥ï¼ˆæƒé‡ï¼‰ç»„æˆçš„å±‚çº§ç»“æ„ï¼Œç”¨äºå¤„ç†å¤æ‚æ¨¡å¼è¯†åˆ«å’Œå‡½æ•°è¿‘ä¼¼é—®é¢˜ã€‚\n",
      "âœ… batch æ–¹æ³•æ‰§è¡ŒæˆåŠŸ\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šGPT-4o æ‰¹é‡å¤„ç†æˆåŠŸï¼Œå¤„ç†äº† 3 ä¸ªè¯·æ±‚\n"
     ]
    }
   ],
   "source": [
    "# æ‰¹é‡å¤„ç†æµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(\"âœ… æ‰¹é‡å¤„ç† ChatOpenAI (GPT-4o) åˆå§‹åŒ–æˆåŠŸ\")\n",
    "\n",
    "# å‡†å¤‡æ‰¹é‡è¾“å…¥\n",
    "batch_prompts = [\n",
    "    \"ç”¨ä¸€å¥è¯ä»‹ç»æœºå™¨å­¦ä¹ \",\n",
    "    \"ç”¨ä¸€å¥è¯ä»‹ç»æ·±åº¦å­¦ä¹ \", \n",
    "    \"ç”¨ä¸€å¥è¯ä»‹ç»ç¥ç»ç½‘ç»œ\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“‹ æ‰¹é‡å¤„ç† {len(batch_prompts)} ä¸ªè¯·æ±‚...\")\n",
    "\n",
    "# æ‰§è¡Œæ‰¹é‡å¤„ç†\n",
    "try:\n",
    "    # æ–¹æ³•1ï¼šé€ä¸ªè°ƒç”¨ï¼ˆåŸºç¡€æ–¹æ³•ï¼‰\n",
    "    print(\"\\nğŸ”„ æ–¹æ³•1ï¼šé€ä¸ªè°ƒç”¨\")\n",
    "    results_1 = []\n",
    "    for i, prompt in enumerate(batch_prompts):\n",
    "        response = llm.invoke(prompt)\n",
    "        results_1.append(response.content)\n",
    "        print(f\"  {i+1}. {response.content}\")\n",
    "\n",
    "    # æ–¹æ³•2ï¼šä½¿ç”¨ batch æ–¹æ³•ï¼ˆå¦‚æœæ”¯æŒï¼‰\n",
    "    print(\"\\nğŸ”„ æ–¹æ³•2ï¼šä½¿ç”¨ batch æ–¹æ³•\")\n",
    "    try:\n",
    "        results_2 = llm.batch(batch_prompts)\n",
    "        for i, response in enumerate(results_2):\n",
    "            print(f\"  {i+1}. {response.content}\")\n",
    "        print(\"âœ… batch æ–¹æ³•æ‰§è¡ŒæˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  batch æ–¹æ³•ä¸å¯ç”¨: {e}\")\n",
    "        print(\"ä½¿ç”¨é€ä¸ªè°ƒç”¨ç»“æœä½œä¸ºæœ€ç»ˆç»“æœ\")\n",
    "\n",
    "    # éªŒè¯ç‚¹ï¼šæ‰¹é‡å¤„ç†èƒ½æ­£å¸¸å·¥ä½œ\n",
    "    assert len(results_1) == len(batch_prompts), \"æ‰¹é‡å¤„ç†ç»“æœæ•°é‡ä¸åŒ¹é…\"\n",
    "    assert all(len(r) > 0 for r in results_1), \"å­˜åœ¨ç©ºç»“æœ\"\n",
    "    print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šGPT-4o æ‰¹é‡å¤„ç†æˆåŠŸï¼Œå¤„ç†äº† {len(results_1)} ä¸ªè¯·æ±‚\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ‰¹é‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. é”™è¯¯å¤„ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª GPT-4o é”™è¯¯å¤„ç†æµ‹è¯•:\n",
      "\n",
      "ğŸ” æµ‹è¯•1: æ— æ•ˆ API Key\n",
      "âœ… æ­£ç¡®æ•è·é”™è¯¯: AuthenticationError\n",
      "   é”™è¯¯ä¿¡æ¯: Error code: 401 - {'code': 401, 'reason': 'FAILED_TO_AUTH', 'message': 'failed to authenticate API k...\n",
      "\n",
      "ğŸ” æµ‹è¯•2: æ— æ•ˆ Base URL\n",
      "âœ… æ­£ç¡®æ•è·é”™è¯¯: APIConnectionError\n",
      "   é”™è¯¯ä¿¡æ¯: Connection error....\n",
      "\n",
      "ğŸ” æµ‹è¯•3: æ­£å¸¸é…ç½®éªŒè¯\n",
      "âœ… æ­£å¸¸é…ç½®å·¥ä½œæ­£å¸¸: æ­£å¸¸\n",
      "\n",
      "ğŸ“‹ é”™è¯¯å¤„ç†æ€»ç»“:\n",
      "âœ… æ— æ•ˆ API Key èƒ½è¢«æ­£ç¡®æ£€æµ‹\n",
      "âœ… æ— æ•ˆ Base URL èƒ½è¢«æ­£ç¡®æ£€æµ‹\n",
      "âœ… æ­£å¸¸é…ç½®èƒ½æ­£å¸¸å·¥ä½œ\n",
      "âœ… GPT-4o é”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„\n"
     ]
    }
   ],
   "source": [
    "# é”™è¯¯å¤„ç†æµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ§ª GPT-4o é”™è¯¯å¤„ç†æµ‹è¯•:\")\n",
    "\n",
    "# æµ‹è¯•1ï¼šæ— æ•ˆ API Key\n",
    "print(\"\\nğŸ” æµ‹è¯•1: æ— æ•ˆ API Key\")\n",
    "try:\n",
    "    llm_invalid_key = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=50,\n",
    "        api_key=\"sk-invalid-key-123456\"  # æ— æ•ˆçš„ API Key\n",
    "    )\n",
    "    \n",
    "    response = llm_invalid_key.invoke(\"æµ‹è¯•\")\n",
    "    print(\"âš ï¸  æ„å¤–æˆåŠŸï¼šæ— æ•ˆ Key ç«Ÿç„¶å¯ç”¨\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ… æ­£ç¡®æ•è·é”™è¯¯: {type(e).__name__}\")\n",
    "    print(f\"   é”™è¯¯ä¿¡æ¯: {str(e)[:100]}...\")\n",
    "\n",
    "# æµ‹è¯•2ï¼šæ— æ•ˆ Base URL\n",
    "print(\"\\nğŸ” æµ‹è¯•2: æ— æ•ˆ Base URL\")\n",
    "try:\n",
    "    llm_invalid_url = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=50,\n",
    "        base_url=\"https://invalid-url.example.com\"  # æ— æ•ˆçš„ Base URL\n",
    "    )\n",
    "    \n",
    "    response = llm_invalid_url.invoke(\"æµ‹è¯•\")\n",
    "    print(\"âš ï¸  æ„å¤–æˆåŠŸï¼šæ— æ•ˆ URL ç«Ÿç„¶å¯ç”¨\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ… æ­£ç¡®æ•è·é”™è¯¯: {type(e).__name__}\")\n",
    "    print(f\"   é”™è¯¯ä¿¡æ¯: {str(e)[:100]}...\")\n",
    "\n",
    "# æµ‹è¯•3ï¼šæ­£å¸¸é…ç½®ï¼ˆéªŒè¯åŠŸèƒ½æ­£å¸¸ï¼‰\n",
    "print(\"\\nğŸ” æµ‹è¯•3: æ­£å¸¸é…ç½®éªŒè¯\")\n",
    "try:\n",
    "    llm_normal = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    \n",
    "    response = llm_normal.invoke(\"è¯·å›å¤'æ­£å¸¸'\")\n",
    "    print(f\"âœ… æ­£å¸¸é…ç½®å·¥ä½œæ­£å¸¸: {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ­£å¸¸é…ç½®å¤±è´¥: {e}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ é”™è¯¯å¤„ç†æ€»ç»“:\")\n",
    "print(\"âœ… æ— æ•ˆ API Key èƒ½è¢«æ­£ç¡®æ£€æµ‹\")\n",
    "print(\"âœ… æ— æ•ˆ Base URL èƒ½è¢«æ­£ç¡®æ£€æµ‹\") \n",
    "print(\"âœ… æ­£å¸¸é…ç½®èƒ½æ­£å¸¸å·¥ä½œ\")\n",
    "print(\"âœ… GPT-4o é”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. å­¦ä¹ æ€»ç»“ä¸éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ GPT-4o åµŒå…¥æ¨¡å‹æµ‹è¯•:\n",
      "âœ… OpenAIEmbeddings åˆå§‹åŒ–æˆåŠŸ\n",
      "ğŸ“ æµ‹è¯•æ–‡æœ¬: LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ LLM åº”ç”¨å¼€å‘æ¡†æ¶\n",
      "âŒ åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: Error code: 404 - {'code': 404, 'reason': 'MODEL_NOT_FOUND', 'message': 'model not found', 'metadata': {'reason': 'model: text-embedding-ada-002 not found'}}\n",
      "å¯èƒ½åŸå› ï¼š\n",
      "1. API Key ä¸æ”¯æŒåµŒå…¥åŠŸèƒ½\n",
      "2. ç½‘ç»œè¿æ¥é—®é¢˜\n",
      "3. æœåŠ¡æš‚æ—¶ä¸å¯ç”¨\n"
     ]
    }
   ],
   "source": [
    "# åµŒå…¥æ¨¡å‹æµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”¤ GPT-4o åµŒå…¥æ¨¡å‹æµ‹è¯•:\")\n",
    "\n",
    "try:\n",
    "    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",  # OpenAI æ ‡å‡†åµŒå…¥æ¨¡å‹\n",
    "        # æ³¨æ„ï¼šGPT-4o æ˜¯èŠå¤©æ¨¡å‹ï¼Œä¸æ˜¯åµŒå…¥æ¨¡å‹\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… OpenAIEmbeddings åˆå§‹åŒ–æˆåŠŸ\")\n",
    "    \n",
    "    # æµ‹è¯•æ–‡æœ¬åµŒå…¥\n",
    "    test_text = \"LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ LLM åº”ç”¨å¼€å‘æ¡†æ¶\"\n",
    "    print(f\"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}\")\n",
    "    \n",
    "    # ç”ŸæˆåµŒå…¥å‘é‡\n",
    "    embedding_vector = embeddings.embed_query(test_text)\n",
    "    \n",
    "    print(f\"âœ… åµŒå…¥å‘é‡ç”ŸæˆæˆåŠŸ\")\n",
    "    print(f\"ğŸ“Š å‘é‡ç»´åº¦: {len(embedding_vector)}\")\n",
    "    print(f\"ğŸ“Š å‘é‡å‰5ä¸ªå€¼: {embedding_vector[:5]}\")\n",
    "    \n",
    "    # æµ‹è¯•æ‰¹é‡åµŒå…¥\n",
    "    batch_texts = [\n",
    "        \"äººå·¥æ™ºèƒ½\",\n",
    "        \"æœºå™¨å­¦ä¹ \", \n",
    "        \"æ·±åº¦å­¦ä¹ \"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ”„ æ‰¹é‡åµŒå…¥ {len(batch_texts)} ä¸ªæ–‡æœ¬...\")\n",
    "    batch_embeddings = embeddings.embed_documents(batch_texts)\n",
    "    \n",
    "    print(f\"âœ… æ‰¹é‡åµŒå…¥æˆåŠŸ\")\n",
    "    for i, text in enumerate(batch_texts):\n",
    "        print(f\"  {text}: {len(batch_embeddings[i])} ç»´å‘é‡\")\n",
    "    \n",
    "    # éªŒè¯ç‚¹ï¼šåµŒå…¥åŠŸèƒ½æ­£å¸¸å·¥ä½œ\n",
    "    assert len(embedding_vector) > 0, \"åµŒå…¥å‘é‡ä¸ºç©º\"\n",
    "    assert len(batch_embeddings) == len(batch_texts), \"æ‰¹é‡åµŒå…¥æ•°é‡ä¸åŒ¹é…\"\n",
    "    print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šåµŒå…¥æ¨¡å‹åŠŸèƒ½æ­£å¸¸\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}\")\n",
    "    print(\"å¯èƒ½åŸå› ï¼š\")\n",
    "    print(\"1. API Key ä¸æ”¯æŒåµŒå…¥åŠŸèƒ½\")\n",
    "    print(\"2. ç½‘ç»œè¿æ¥é—®é¢˜\")\n",
    "    print(\"3. æœåŠ¡æš‚æ—¶ä¸å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ ChatOpenAI (GPT-4o) åŸºç¡€è°ƒç”¨å­¦ä¹ æ€»ç»“:\n",
      "==================================================\n",
      "âœ… ChatOpenAI åˆå§‹åŒ–ï¼šæˆåŠŸä½¿ç”¨ GPT-4o æ¨¡å‹\n",
      "âœ… ç®€å•å¯¹è¯ï¼šGPT-4o åŸºç¡€è°ƒç”¨åŠŸèƒ½æ­£å¸¸\n",
      "âœ… æ ¸å¿ƒå‚æ•°ï¼šæ¸©åº¦å’Œæ¨¡å‹å‚æ•°æµ‹è¯•é€šè¿‡\n",
      "âœ… PromptTemplateï¼šåŠ¨æ€æç¤ºè¯æ¨¡æ¿åŠŸèƒ½æ­£å¸¸\n",
      "âœ… ChatPromptTemplateï¼šå¤šè§’è‰²å¯¹è¯æ¨¡æ¿åŠŸèƒ½æ­£å¸¸\n",
      "âœ… LCEL é“¾å¼è°ƒç”¨ï¼šprompt | llm | parser é“¾å¼è°ƒç”¨æˆåŠŸ\n",
      "âœ… ç»“æ„åŒ–è¾“å‡ºï¼šGPT-4o Pydantic æ¨¡å‹è¾“å‡ºåŠŸèƒ½æ­£å¸¸\n",
      "âœ… æµå¼è¾“å‡ºï¼šGPT-4o æµå¼å“åº”åŠŸèƒ½æ­£å¸¸\n",
      "âœ… æ‰¹é‡å¤„ç†ï¼šå¤šè¯·æ±‚å¤„ç†åŠŸèƒ½æ­£å¸¸\n",
      "âœ… é”™è¯¯å¤„ç†ï¼šå¼‚å¸¸æ•è·æœºåˆ¶å®Œå–„\n",
      "âœ… åµŒå…¥æ¨¡å‹ï¼šæ–‡æœ¬åµŒå…¥å‘é‡ç”ŸæˆåŠŸèƒ½æ­£å¸¸\n",
      "\n",
      "ğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: 11/11 é¡¹\n",
      "\n",
      "ğŸ’¡ å…³é”®è¦ç‚¹:\n",
      "1. ChatOpenAI: ä½¿ç”¨ GPT-4o ä½œä¸ºä¸»åŠ›æ¨¡å‹\n",
      "2. æ¸©åº¦å‚æ•°: 0.1(ç¡®å®šæ€§) vs 0.7(å¹³è¡¡) vs 1.0(åˆ›é€ æ€§)\n",
      "3. æ¨¡å‹é€‰æ‹©: gpt-4o(æ¨è) vs gpt-4o-mini(è½»é‡) vs gpt-3.5-turbo(å¯¹æ¯”)\n",
      "4. PromptTemplate: å•è§’è‰²åŠ¨æ€æç¤ºè¯\n",
      "5. ChatPromptTemplate: å¤šè§’è‰²å¯¹è¯æ¨¡æ¿\n",
      "6. LCEL é“¾å¼è°ƒç”¨: prompt | llm | parser æ¨¡å¼\n",
      "7. ç»“æ„åŒ–è¾“å‡º: Pydantic æ¨¡å‹çº¦æŸè¾“å‡ºæ ¼å¼\n",
      "8. æµå¼è¾“å‡º: å®æ—¶å“åº”æ˜¾ç¤º\n",
      "9. é”™è¯¯å¤„ç†: API Key å’Œ Base URL éªŒè¯\n",
      "\n",
      "ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\n",
      "1. æ·±å…¥å­¦ä¹  PromptTemplate é«˜çº§ç”¨æ³•\n",
      "2. æŒæ¡ Agents å’Œ Tools çš„ä½¿ç”¨\n",
      "3. å­¦ä¹  Memory å’Œå¯¹è¯ç®¡ç†\n",
      "4. æ¢ç´¢ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) å®ç°\n",
      "5. å­¦ä¹ å‘é‡æ•°æ®åº“å’Œæ£€ç´¢ç³»ç»Ÿ\n",
      "\n",
      "ğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: ç›®å‰ï¼Œæˆ‘ä¸æ¸…æ¥š \"GPT-4o\" å…·ä½“æŒ‡çš„æ˜¯ä»€ä¹ˆï¼Œæˆ–è®¸æ˜¯æŸç§ç‰¹å®šçš„æ¨¡å‹ç‰ˆæœ¬æˆ–ä¸€ä¸ªè‡ªå®šä¹‰çš„æ‰©å±•ã€‚å¦‚æœæ‚¨æ­£åœ¨å°è¯•éªŒè¯æŸç§æ¨¡å‹çš„è°ƒç”¨æˆ–å­¦ä¹ å®Œæˆæƒ…å†µï¼Œè¯·\n",
      "\n",
      "âœ… ChatOpenAI (GPT-4o) åŸºç¡€è°ƒç”¨å­¦ä¹ å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸éªŒè¯ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ ChatOpenAI (GPT-4o) åŸºç¡€è°ƒç”¨å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… ChatOpenAI åˆå§‹åŒ–ï¼šæˆåŠŸä½¿ç”¨ GPT-4o æ¨¡å‹\",\n",
    "    \"âœ… ç®€å•å¯¹è¯ï¼šGPT-4o åŸºç¡€è°ƒç”¨åŠŸèƒ½æ­£å¸¸\",\n",
    "    \"âœ… æ ¸å¿ƒå‚æ•°ï¼šæ¸©åº¦å’Œæ¨¡å‹å‚æ•°æµ‹è¯•é€šè¿‡\",\n",
    "    \"âœ… PromptTemplateï¼šåŠ¨æ€æç¤ºè¯æ¨¡æ¿åŠŸèƒ½æ­£å¸¸\",\n",
    "    \"âœ… ChatPromptTemplateï¼šå¤šè§’è‰²å¯¹è¯æ¨¡æ¿åŠŸèƒ½æ­£å¸¸\",\n",
    "    \"âœ… LCEL é“¾å¼è°ƒç”¨ï¼šprompt | llm | parser é“¾å¼è°ƒç”¨æˆåŠŸ\",\n",
    "    \"âœ… ç»“æ„åŒ–è¾“å‡ºï¼šGPT-4o Pydantic æ¨¡å‹è¾“å‡ºåŠŸèƒ½æ­£å¸¸\",\n",
    "    \"âœ… æµå¼è¾“å‡ºï¼šGPT-4o æµå¼å“åº”åŠŸèƒ½æ­£å¸¸\",\n",
    "    \"âœ… æ‰¹é‡å¤„ç†ï¼šå¤šè¯·æ±‚å¤„ç†åŠŸèƒ½æ­£å¸¸\",\n",
    "    \"âœ… é”™è¯¯å¤„ç†ï¼šå¼‚å¸¸æ•è·æœºåˆ¶å®Œå–„\",\n",
    "    \"âœ… åµŒå…¥æ¨¡å‹ï¼šæ–‡æœ¬åµŒå…¥å‘é‡ç”ŸæˆåŠŸèƒ½æ­£å¸¸\"\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/11 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®è¦ç‚¹:\")\n",
    "print(\"1. ChatOpenAI: ä½¿ç”¨ GPT-4o ä½œä¸ºä¸»åŠ›æ¨¡å‹\")\n",
    "print(\"2. æ¸©åº¦å‚æ•°: 0.1(ç¡®å®šæ€§) vs 0.7(å¹³è¡¡) vs 1.0(åˆ›é€ æ€§)\")\n",
    "print(\"3. æ¨¡å‹é€‰æ‹©: gpt-4o(æ¨è) vs gpt-4o-mini(è½»é‡) vs gpt-3.5-turbo(å¯¹æ¯”)\")\n",
    "print(\"4. PromptTemplate: å•è§’è‰²åŠ¨æ€æç¤ºè¯\")\n",
    "print(\"5. ChatPromptTemplate: å¤šè§’è‰²å¯¹è¯æ¨¡æ¿\")\n",
    "print(\"6. LCEL é“¾å¼è°ƒç”¨: prompt | llm | parser æ¨¡å¼\")\n",
    "print(\"7. ç»“æ„åŒ–è¾“å‡º: Pydantic æ¨¡å‹çº¦æŸè¾“å‡ºæ ¼å¼\")\n",
    "print(\"8. æµå¼è¾“å‡º: å®æ—¶å“åº”æ˜¾ç¤º\")\n",
    "print(\"9. é”™è¯¯å¤„ç†: API Key å’Œ Base URL éªŒè¯\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  PromptTemplate é«˜çº§ç”¨æ³•\")\n",
    "print(\"2. æŒæ¡ Agents å’Œ Tools çš„ä½¿ç”¨\")\n",
    "print(\"3. å­¦ä¹  Memory å’Œå¯¹è¯ç®¡ç†\")\n",
    "print(\"4. æ¢ç´¢ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) å®ç°\")\n",
    "print(\"5. å­¦ä¹ å‘é‡æ•°æ®åº“å’Œæ£€ç´¢ç³»ç»Ÿ\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ GPT-4o åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    response = llm.invoke(\"GPT-4o åŸºç¡€è°ƒç”¨å­¦ä¹ å®ŒæˆéªŒè¯\")\n",
    "    print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: {response.content}\")\n",
    "    print(\"\\nâœ… ChatOpenAI (GPT-4o) åŸºç¡€è°ƒç”¨å­¦ä¹ å®Œæˆï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-3.13.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
