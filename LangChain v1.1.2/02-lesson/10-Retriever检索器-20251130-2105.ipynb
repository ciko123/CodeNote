{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Retriever æ£€ç´¢å™¨\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹ ä½¿ç”¨ LangChain Retriever å°è£…å‘é‡åº“æ£€ç´¢é€»è¾‘ï¼Œé€šè¿‡å‚æ•°æ§åˆ¶æ£€ç´¢ç»“æœæ•°é‡å’Œè´¨é‡\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£æ£€ç´¢å™¨é…ç½®å‚æ•°å’Œä½œç”¨\n",
    "- æŒæ¡æ£€ç´¢ç»“æœæ•°é‡æ§åˆ¶æ–¹æ³•\n",
    "- èƒ½è®¾ç½®æ£€ç´¢å‚æ•°ä¼˜åŒ–ç»“æœ\n",
    "- æŒæ¡ä¸åŒæ£€ç´¢ç­–ç•¥çš„ä½¿ç”¨\n",
    "\n",
    "## ğŸ”‘ å‰ç½®è¦æ±‚\n",
    "**æ³¨æ„**ï¼šéœ€è¦å…ˆå®Œæˆ Embedding + Vector Stores å­¦ä¹ ï¼Œé…ç½® DashScope API\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. åŸºç¡€ Retriever åˆ›å»ºå’Œé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€ Retriever åˆ›å»ºå’Œé…ç½® - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ” åŸºç¡€ Retriever åˆ›å»ºå’Œé…ç½®æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')\n",
    "if not dashscope_api_key:\n",
    "    print(\"âŒ DashScope API Key æœªé…ç½®\")\n",
    "    print(\"   è¯·å…ˆå®Œæˆ Embedding + Vector Stores å­¦ä¹ \")\n",
    "else:\n",
    "    try:\n",
    "        # åˆå§‹åŒ– Embeddings\n",
    "        embeddings = DashScopeEmbeddings(\n",
    "            model=\"text-embedding-v1\",\n",
    "            dashscope_api_key=dashscope_api_key\n",
    "        )\n",
    "        \n",
    "        # å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "        documents = [\n",
    "            \"LangChain æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œç”¨äºç®€åŒ– LLM åº”ç”¨å¼€å‘\",\n",
    "            \"Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“å­¦\",\n",
    "            \"æœºå™¨å­¦ä¹ é€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå®ç°æ™ºèƒ½å†³ç­–\",\n",
    "            \"å‘é‡åŒ–å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡ï¼Œä¾¿äºæ•°å­¦è®¡ç®—\",\n",
    "            \"FAISS æ˜¯ Facebook å¼€å‘çš„é«˜æ•ˆå‘é‡æ£€ç´¢åº“\",\n",
    "            \"RAG æŠ€æœ¯ç»“åˆæ£€ç´¢å’Œç”Ÿæˆï¼Œæé«˜å›ç­”å‡†ç¡®æ€§\",\n",
    "            \"å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–æ¬§æ°è·ç¦»\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"ğŸ“š å‡†å¤‡æ–‡æ¡£æ•°é‡: {len(documents)}\")\n",
    "        \n",
    "        # åˆ›å»ºå‘é‡åº“\n",
    "        vector_store = FAISS.from_texts(documents, embeddings)\n",
    "        print(f\"âœ… å‘é‡åº“åˆ›å»ºå®Œæˆ: {vector_store.index.ntotal} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        # åˆ›å»ºåŸºç¡€ Retriever\n",
    "        retriever = vector_store.as_retriever()\n",
    "        \n",
    "        print(f\"\\nğŸ”§ Retriever åŸºç¡€é…ç½®:\")\n",
    "        print(f\"   æ£€ç´¢å™¨ç±»å‹: {type(retriever)}\")\n",
    "        print(f\"   é»˜è®¤å‚æ•°: k=4 (é»˜è®¤æ£€ç´¢æ•°é‡)\")\n",
    "        \n",
    "        # æµ‹è¯•åŸºç¡€æ£€ç´¢\n",
    "        test_query = \"å¦‚ä½•å¼€å‘äººå·¥æ™ºèƒ½åº”ç”¨\"\n",
    "        print(f\"\\nğŸ§ª æµ‹è¯•æŸ¥è¯¢: {test_query}\")\n",
    "        \n",
    "        results = retriever.invoke(test_query)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š æ£€ç´¢ç»“æœ:\")\n",
    "        print(f\"   ç»“æœæ•°é‡: {len(results)}\")\n",
    "        \n",
    "        for i, doc in enumerate(results, 1):\n",
    "            print(f\"   ç»“æœ {i}: {doc.page_content}\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šåŸºç¡€ Retriever æ­£å¸¸å·¥ä½œ\n",
    "        assert len(results) > 0, \"æ£€ç´¢ç»“æœä¸ºç©º\"\n",
    "        assert all(hasattr(doc, 'page_content') for doc in results), \"ç»“æœç¼ºå°‘å†…å®¹\"\n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šåŸºç¡€ Retriever æ­£å¸¸å·¥ä½œ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Retriever åˆ›å»ºå¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ£€ç´¢å‚æ•° k çš„æ§åˆ¶æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€ç´¢å‚æ•° k çš„æ§åˆ¶æµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“Š æ£€ç´¢å‚æ•° k çš„æ§åˆ¶æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')\n",
    "if not dashscope_api_key:\n",
    "    print(\"âŒ DashScope API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆå§‹åŒ–ç»„ä»¶\n",
    "        embeddings = DashScopeEmbeddings(\n",
    "            model=\"text-embedding-v1\",\n",
    "            dashscope_api_key=dashscope_api_key\n",
    "        )\n",
    "        \n",
    "        documents = [\n",
    "            \"LangChain æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œç”¨äºç®€åŒ– LLM åº”ç”¨å¼€å‘\",\n",
    "            \"Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“å­¦\",\n",
    "            \"æœºå™¨å­¦ä¹ é€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå®ç°æ™ºèƒ½å†³ç­–\",\n",
    "            \"å‘é‡åŒ–å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡ï¼Œä¾¿äºæ•°å­¦è®¡ç®—\",\n",
    "            \"FAISS æ˜¯ Facebook å¼€å‘çš„é«˜æ•ˆå‘é‡æ£€ç´¢åº“\",\n",
    "            \"RAG æŠ€æœ¯ç»“åˆæ£€ç´¢å’Œç”Ÿæˆï¼Œæé«˜å›ç­”å‡†ç¡®æ€§\",\n",
    "            \"å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–æ¬§æ°è·ç¦»\"\n",
    "        ]\n",
    "        \n",
    "        vector_store = FAISS.from_texts(documents, embeddings)\n",
    "        \n",
    "        # æµ‹è¯•ä¸åŒçš„ k å€¼\n",
    "        k_values = [1, 2, 3, 5, 7]\n",
    "        test_query = \"æœºå™¨å­¦ä¹ å’Œç¼–ç¨‹\"\n",
    "        \n",
    "        print(f\"ğŸ§ª æµ‹è¯•æŸ¥è¯¢: {test_query}\")\n",
    "        print(f\"\\nğŸ“ˆ ä¸åŒ k å€¼çš„æ£€ç´¢ç»“æœ:\")\n",
    "        \n",
    "        results_summary = {}\n",
    "        \n",
    "        for k in k_values:\n",
    "            print(f\"\\nğŸ” k = {k}:\")\n",
    "            \n",
    "            # åˆ›å»ºæŒ‡å®š k å€¼çš„ Retriever\n",
    "            retriever = vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "            \n",
    "            # æ‰§è¡Œæ£€ç´¢\n",
    "            results = retriever.invoke(test_query)\n",
    "            \n",
    "            print(f\"   å®é™…ç»“æœæ•°: {len(results)}\")\n",
    "            \n",
    "            # è®°å½•ç»“æœ\n",
    "            results_summary[k] = {\n",
    "                \"count\": len(results),\n",
    "                \"contents\": [doc.page_content for doc in results]\n",
    "            }\n",
    "            \n",
    "            # æ˜¾ç¤ºç»“æœ\n",
    "            for i, doc in enumerate(results, 1):\n",
    "                print(f\"     {i}. {doc.page_content}\")\n",
    "        \n",
    "        # åˆ†æ k å€¼æ§åˆ¶æ•ˆæœ\n",
    "        print(f\"\\nğŸ“Š k å€¼æ§åˆ¶æ•ˆæœåˆ†æ:\")\n",
    "        print(f\"{'k å€¼':<6} {'ç»“æœæ•°':<8} {'æ§åˆ¶æ•ˆæœ':<12}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for k, summary in results_summary.items():\n",
    "            expected = k\n",
    "            actual = summary[\"count\"]\n",
    "            control_status = \"âœ… ç²¾ç¡®\" if actual == expected else f\"âš ï¸  {actual}/{expected}\"\n",
    "            print(f\"{k:<6} {actual:<8} {control_status:<12}\")\n",
    "        \n",
    "        # æµ‹è¯•è¾¹ç•Œæƒ…å†µ\n",
    "        print(f\"\\nğŸ§ª è¾¹ç•Œæƒ…å†µæµ‹è¯•:\")\n",
    "        \n",
    "        # k å¤§äºæ–‡æ¡£æ€»æ•°\n",
    "        large_k = len(documents) + 5\n",
    "        large_retriever = vector_store.as_retriever(search_kwargs={\"k\": large_k})\n",
    "        large_results = large_retriever.invoke(test_query)\n",
    "        \n",
    "        print(f\"   k = {large_k} (å¤§äºæ–‡æ¡£æ€»æ•°):\")\n",
    "        print(f\"   å®é™…ç»“æœæ•°: {len(large_results)} (æœ€å¤šå¯ç”¨æ–‡æ¡£æ•°)\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šèƒ½æ­£ç¡®æ§åˆ¶æ£€ç´¢ç»“æœæ•°é‡\n",
    "        for k in [1, 2, 3]:\n",
    "            retriever = vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "            results = retriever.invoke(test_query)\n",
    "            assert len(results) <= k, f\"k={k} æ—¶ç»“æœæ•°è¶…è¿‡é™åˆ¶\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæ£€ç´¢ç»“æœæ•°é‡æ§åˆ¶æ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ k å‚æ•°æ§åˆ¶æµ‹è¯•å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å¤šç§æ£€ç´¢ç­–ç•¥å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šç§æ£€ç´¢ç­–ç•¥å¯¹æ¯” - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”„ å¤šç§æ£€ç´¢ç­–ç•¥å¯¹æ¯”æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')\n",
    "if not dashscope_api_key:\n",
    "    print(\"âŒ DashScope API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆå§‹åŒ–ç»„ä»¶\n",
    "        embeddings = DashScopeEmbeddings(\n",
    "            model=\"text-embedding-v1\",\n",
    "            dashscope_api_key=dashscope_api_key\n",
    "        )\n",
    "        \n",
    "        documents = [\n",
    "            \"LangChain æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œç”¨äºç®€åŒ– LLM åº”ç”¨å¼€å‘\",\n",
    "            \"Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“å­¦\",\n",
    "            \"æœºå™¨å­¦ä¹ é€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå®ç°æ™ºèƒ½å†³ç­–\",\n",
    "            \"å‘é‡åŒ–å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡ï¼Œä¾¿äºæ•°å­¦è®¡ç®—\",\n",
    "            \"FAISS æ˜¯ Facebook å¼€å‘çš„é«˜æ•ˆå‘é‡æ£€ç´¢åº“\",\n",
    "            \"RAG æŠ€æœ¯ç»“åˆæ£€ç´¢å’Œç”Ÿæˆï¼Œæé«˜å›ç­”å‡†ç¡®æ€§\",\n",
    "            \"å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–æ¬§æ°è·ç¦»\"\n",
    "        ]\n",
    "        \n",
    "        vector_store = FAISS.from_texts(documents, embeddings)\n",
    "        test_query = \"äººå·¥æ™ºèƒ½ç¼–ç¨‹æŠ€æœ¯\"\n",
    "        \n",
    "        print(f\"ğŸ§ª æµ‹è¯•æŸ¥è¯¢: {test_query}\")\n",
    "        \n",
    "        # 1. ç›¸ä¼¼åº¦æ£€ç´¢ (é»˜è®¤)\n",
    "        print(f\"\\nğŸ“Š 1. ç›¸ä¼¼åº¦æ£€ç´¢ (Similarity Search):\")\n",
    "        similarity_retriever = vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "        similarity_results = similarity_retriever.invoke(test_query)\n",
    "        \n",
    "        for i, doc in enumerate(similarity_results, 1):\n",
    "            print(f\"   {i}. {doc.page_content}\")\n",
    "        \n",
    "        # 2. æœ€å¤§è¾¹é™…ç›¸å…³æ€§æ£€ç´¢ (MMR)\n",
    "        print(f\"\\nğŸ”„ 2. æœ€å¤§è¾¹é™…ç›¸å…³æ€§æ£€ç´¢ (MMR):\")\n",
    "        try:\n",
    "            mmr_results = vector_store.max_marginal_relevance_search(\n",
    "                test_query, \n",
    "                k=3, \n",
    "                fetch_k=5\n",
    "            )\n",
    "            \n",
    "            for i, doc in enumerate(mmr_results, 1):\n",
    "                print(f\"   {i}. {doc.page_content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   MMR æ£€ç´¢å¤±è´¥: {e}\")\n",
    "            mmr_results = []\n",
    "        \n",
    "        # 3. é˜ˆå€¼æ£€ç´¢\n",
    "        print(f\"\\nğŸ¯ 3. ç›¸ä¼¼åº¦é˜ˆå€¼æ£€ç´¢:\")\n",
    "        try:\n",
    "            threshold_results_with_scores = vector_store.similarity_search_with_score(\n",
    "                test_query, \n",
    "                k=5,\n",
    "                score_threshold=0.3\n",
    "            )\n",
    "            \n",
    "            for i, (doc, score) in enumerate(threshold_results_with_scores, 1):\n",
    "                print(f\"   {i}. {doc.page_content} (ç›¸ä¼¼åº¦: {score:.4f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   é˜ˆå€¼æ£€ç´¢å¤±è´¥: {e}\")\n",
    "        \n",
    "        # 4. ç»“æœå¯¹æ¯”åˆ†æ\n",
    "        print(f\"\\nğŸ“ˆ æ£€ç´¢ç­–ç•¥å¯¹æ¯”åˆ†æ:\")\n",
    "        print(f\"{'ç­–ç•¥':<15} {'ç»“æœæ•°':<8} {'ç‰¹ç‚¹':<20}\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        strategies = [\n",
    "            (\"ç›¸ä¼¼åº¦æ£€ç´¢\", len(similarity_results), \"æœ€ç›¸ä¼¼ï¼Œå¯èƒ½é‡å¤\"),\n",
    "            (\"MMR æ£€ç´¢\", len(mmr_results), \"å¤šæ ·æ€§ï¼Œå‡å°‘é‡å¤\"),\n",
    "            (\"é˜ˆå€¼æ£€ç´¢\", len(threshold_results_with_scores), \"è´¨é‡è¿‡æ»¤ï¼Œæ•°é‡ä¸å®š\")\n",
    "        ]\n",
    "        \n",
    "        for strategy, count, feature in strategies:\n",
    "            print(f\"{strategy:<15} {count:<8} {feature:<20}\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šä¸åŒæ£€ç´¢ç­–ç•¥å·¥ä½œæ­£å¸¸\n",
    "        assert len(similarity_results) > 0, \"ç›¸ä¼¼åº¦æ£€ç´¢æ— ç»“æœ\"\n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šå¤šç§æ£€ç´¢ç­–ç•¥æ­£å¸¸å·¥ä½œ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æ£€ç´¢ç­–ç•¥å¯¹æ¯”å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ£€ç´¢æ€§èƒ½å’Œè´¨é‡è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€ç´¢æ€§èƒ½å’Œè´¨é‡è¯„ä¼° - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“ æ£€ç´¢æ€§èƒ½å’Œè´¨é‡è¯„ä¼°æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')\n",
    "if not dashscope_api_key:\n",
    "    print(\"âŒ DashScope API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆå§‹åŒ–ç»„ä»¶\n",
    "        embeddings = DashScopeEmbeddings(\n",
    "            model=\"text-embedding-v1\",\n",
    "            dashscope_api_key=dashscope_api_key\n",
    "        )\n",
    "        \n",
    "        # æ‰©å±•æµ‹è¯•æ•°æ®é›†\n",
    "        documents = [\n",
    "            \"LangChain æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œç”¨äºç®€åŒ– LLM åº”ç”¨å¼€å‘\",\n",
    "            \"Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“å­¦\",\n",
    "            \"æœºå™¨å­¦ä¹ é€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå®ç°æ™ºèƒ½å†³ç­–\",\n",
    "            \"å‘é‡åŒ–å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡ï¼Œä¾¿äºæ•°å­¦è®¡ç®—\",\n",
    "            \"FAISS æ˜¯ Facebook å¼€å‘çš„é«˜æ•ˆå‘é‡æ£€ç´¢åº“\",\n",
    "            \"RAG æŠ€æœ¯ç»“åˆæ£€ç´¢å’Œç”Ÿæˆï¼Œæé«˜å›ç­”å‡†ç¡®æ€§\",\n",
    "            \"å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–æ¬§æ°è·ç¦»\",\n",
    "            \"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯\",\n",
    "            \"è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯ AI çš„æ ¸å¿ƒåº”ç”¨é¢†åŸŸ\",\n",
    "            \"æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ æµç¨‹çš„å…³é”®æ­¥éª¤\"\n",
    "        ]\n",
    "        \n",
    "        vector_store = FAISS.from_texts(documents, embeddings)\n",
    "        \n",
    "        # æµ‹è¯•æŸ¥è¯¢é›†\n",
    "        test_queries = [\n",
    "            \"äººå·¥æ™ºèƒ½ç¼–ç¨‹\",\n",
    "            \"æœºå™¨å­¦ä¹ ç®—æ³•\",\n",
    "            \"æ•°æ®å¤„ç†æŠ€æœ¯\",\n",
    "            \"å‘é‡æ£€ç´¢æ–¹æ³•\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"ğŸ“š æµ‹è¯•æ•°æ®: {len(documents)} ä¸ªæ–‡æ¡£\")\n",
    "        print(f\"ğŸ§ª æµ‹è¯•æŸ¥è¯¢: {len(test_queries)} ä¸ª\")\n",
    "        \n",
    "        # æ€§èƒ½æµ‹è¯•\n",
    "        print(f\"\\nâš¡ æ£€ç´¢æ€§èƒ½æµ‹è¯•:\")\n",
    "        \n",
    "        k_values = [1, 3, 5, 10]\n",
    "        performance_results = {}\n",
    "        \n",
    "        for k in k_values:\n",
    "            retriever = vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "            \n",
    "            # æµ‹è¯•å¤šæ¬¡å–å¹³å‡\n",
    "            times = []\n",
    "            for query in test_queries:\n",
    "                start_time = time.time()\n",
    "                results = retriever.invoke(query)\n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "            \n",
    "            avg_time = sum(times) / len(times)\n",
    "            performance_results[k] = {\n",
    "                \"avg_time\": avg_time,\n",
    "                \"result_count\": k\n",
    "            }\n",
    "            \n",
    "            print(f\"   k={k:<2}: å¹³å‡ {avg_time*1000:.1f} ms/æŸ¥è¯¢\")\n",
    "        \n",
    "        # è´¨é‡è¯„ä¼°\n",
    "        print(f\"\\nğŸ“Š æ£€ç´¢è´¨é‡è¯„ä¼°:\")\n",
    "        \n",
    "        # ä½¿ç”¨ k=3 è¿›è¡Œè´¨é‡æµ‹è¯•\n",
    "        retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "        \n",
    "        quality_metrics = {\n",
    "            \"total_queries\": len(test_queries),\n",
    "            \"total_results\": 0,\n",
    "            \"unique_results\": set(),\n",
    "            \"avg_result_length\": 0\n",
    "        }\n",
    "        \n",
    "        all_lengths = []\n",
    "        \n",
    "        for query in test_queries:\n",
    "            results = retriever.invoke(query)\n",
    "            quality_metrics[\"total_results\"] += len(results)\n",
    "            \n",
    "            for doc in results:\n",
    "                quality_metrics[\"unique_results\"].add(doc.page_content)\n",
    "                all_lengths.append(len(doc.page_content))\n",
    "        \n",
    "        quality_metrics[\"unique_count\"] = len(quality_metrics[\"unique_results\"])\n",
    "        quality_metrics[\"avg_result_length\"] = sum(all_lengths) / len(all_lengths) if all_lengths else 0\n",
    "        quality_metrics[\"diversity_rate\"] = quality_metrics[\"unique_count\"] / quality_metrics[\"total_results\"]\n",
    "        \n",
    "        print(f\"   æ€»æŸ¥è¯¢æ•°: {quality_metrics['total_queries']}\")\n",
    "        print(f\"   æ€»ç»“æœæ•°: {quality_metrics['total_results']}\")\n",
    "        print(f\"   å”¯ä¸€ç»“æœæ•°: {quality_metrics['unique_count']}\")\n",
    "        print(f\"   å¤šæ ·æ€§æ¯”ç‡: {quality_metrics['diversity_rate']:.1%}\")\n",
    "        print(f\"   å¹³å‡ç»“æœé•¿åº¦: {quality_metrics['avg_result_length']:.1f} å­—ç¬¦\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šæ£€ç´¢æ€§èƒ½å’Œè´¨é‡è¯„ä¼°å®Œæˆ\n",
    "        assert quality_metrics[\"total_results\"] > 0, \"æ— æ£€ç´¢ç»“æœ\"\n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæ£€ç´¢æ€§èƒ½å’Œè´¨é‡è¯„ä¼°å®Œæˆ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æ£€ç´¢æ€§èƒ½è¯„ä¼°å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ Retriever å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… åŸºç¡€æ£€ç´¢å™¨ï¼švector_store.as_retriever()\",\n",
    "    \"âœ… å‚æ•°æ§åˆ¶ï¼šsearch_kwargs={'k': N}\",\n",
    "    \"âœ… æ£€ç´¢ç­–ç•¥ï¼šç›¸ä¼¼åº¦ã€MMRã€é˜ˆå€¼\",\n",
    "    \"âœ… æ€§èƒ½è¯„ä¼°ï¼šé€Ÿåº¦ã€å¤šæ ·æ€§ã€ç›¸å…³æ€§\",\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/4 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Retriever æœ€ä½³å®è·µ:\")\n",
    "print(\"1. å‚æ•°è®¾ç½®ï¼šæ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„ k å€¼\")\n",
    "print(\"2. æ£€ç´¢ç­–ç•¥ï¼šç›¸ä¼¼åº¦æ£€ç´¢é€‚åˆç²¾ç¡®åŒ¹é…ï¼ŒMMR é€‚åˆå¤šæ ·æ€§\")\n",
    "print(\"3. æ€§èƒ½ä¼˜åŒ–ï¼šæ§åˆ¶ k å€¼å¤§å°ï¼Œé¿å…è¿‡åº¦æ£€ç´¢\")\n",
    "print(\"4. è´¨é‡ç›‘æ§ï¼šå®šæœŸè¯„ä¼°æ£€ç´¢æ•ˆæœå’Œè°ƒæ•´å‚æ•°\")\n",
    "print(\"5. æˆæœ¬æ§åˆ¶ï¼šåˆç†è®¾ç½®æ£€ç´¢æ•°é‡å‡å°‘åç»­å¤„ç†æˆæœ¬\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  RAG ç³»ç»Ÿå®Œæ•´æµç¨‹\")\n",
    "print(\"2. æŒæ¡ Chain ç±»å‹ï¼šRetrievalQAã€ConversationalRetrievalChain\")\n",
    "print(\"3. å­¦ä¹ è®°å¿†ç®¡ç†ï¼šConversationBufferMemory\")\n",
    "print(\"4. æ¢ç´¢é«˜çº§æ£€ç´¢ï¼šå¤šå‘é‡æ£€ç´¢ã€æ··åˆæ£€ç´¢\")\n",
    "print(\"5. å®è·µç”Ÿäº§çº§ RAG åº”ç”¨å¼€å‘\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ Retriever åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')\n",
    "    \n",
    "    if dashscope_api_key:\n",
    "        # ç®€å•æµ‹è¯•\n",
    "        embeddings = DashScopeEmbeddings(\n",
    "            model=\"text-embedding-v1\",\n",
    "            dashscope_api_key=dashscope_api_key\n",
    "        )\n",
    "        \n",
    "        test_docs = [\"æµ‹è¯•æ–‡æ¡£1\", \"æµ‹è¯•æ–‡æ¡£2\", \"æµ‹è¯•æ–‡æ¡£3\"]\n",
    "        vector_store = FAISS.from_texts(test_docs, embeddings)\n",
    "        retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "        \n",
    "        results = retriever.invoke(\"æµ‹è¯•\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ:\")\n",
    "        print(f\"   API çŠ¶æ€: å¯ç”¨\")\n",
    "        print(f\"   æ£€ç´¢ç»“æœ: {len(results)} ä¸ªæ–‡æ¡£\")\n",
    "        print(f\"   æ£€ç´¢å™¨ç±»å‹: {type(retriever).__name__}\")\n",
    "        print(\"\\nâœ… Retriever å­¦ä¹ å®Œæˆï¼\")\n",
    "        \n",
    "        print(f\"\\nğŸŠ RAG æ•°æ®å±‚å­¦ä¹ å®Œæˆï¼\")\n",
    "        print(f\"   å·²æŒæ¡: Document Loaders â†’ Text Splitters â†’ Embeddings â†’ Vector Stores â†’ Retrievers\")\n",
    "        print(f\"   ä¸‹ä¸€æ­¥: å­¦ä¹  RAG åº”ç”¨å±‚å’Œç¼–æ’å±‚æŠ€æœ¯\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  æœ€ç»ˆéªŒè¯è·³è¿‡: DashScope API Key æœªé…ç½®\")\n",
    "        print(\"   è¯·é…ç½® DASHSCOPE_API_KEY åé‡è¯•\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
