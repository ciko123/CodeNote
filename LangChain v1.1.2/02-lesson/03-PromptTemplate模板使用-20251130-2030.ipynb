{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - PromptTemplate æ¨¡æ¿ä½¿ç”¨\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹ ä½¿ç”¨ LangChain PromptTemplate åˆ›å»ºå¯å¤ç”¨çš„æç¤ºè¯æ¨¡æ¿\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- æŒæ¡ PromptTemplate åŸºç¡€è¯­æ³•\n",
    "- ç†è§£å˜é‡å¡«å……æœºåˆ¶\n",
    "- èƒ½è®¾è®¡çµæ´»çš„æ¨¡æ¿\n",
    "- æ§åˆ¶è¾“å‡ºé•¿åº¦ä»¥èŠ‚çœæˆæœ¬\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PromptTemplate åŸºç¡€åˆ›å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PromptTemplate åˆ›å»ºæˆåŠŸ\n",
      "æ¨¡æ¿: è¯·ç”¨ GPT-4o çš„è§†è§’å†™ä¸€æ®µå…³äº {topic} çš„ä»‹ç»ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚\n",
      "è¾“å…¥å˜é‡: ['topic']\n",
      "\n",
      "ğŸ“ æ ¼å¼åŒ–åçš„æç¤ºè¯: è¯·ç”¨ GPT-4o çš„è§†è§’å†™ä¸€æ®µå…³äº äººå·¥æ™ºèƒ½ çš„ä»‹ç»ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚\n",
      "\n",
      "ğŸ¤– GPT-4o-mini å›å¤: äººå·¥æ™ºèƒ½æ˜¯æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯ï¼Œé€šè¿‡æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç­‰æ–¹æ³•ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿè¿›è¡Œæ„ŸçŸ¥ã€æ¨ç†å’Œå†³ç­–ï¼Œå¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼Œæ¨åŠ¨ç¤¾ä¼šè¿›æ­¥ä¸åˆ›æ–°ã€‚\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡æ¿åŠ¨æ€ç”Ÿæˆå¹¶æˆåŠŸè°ƒç”¨\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate åŸºç¡€åˆ›å»º - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨è½»é‡æ¨¡å‹èŠ‚çœæˆæœ¬ï¼‰\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # è½»é‡æ¨¡å‹ï¼Œæˆæœ¬æ›´ä½\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,        # æ§åˆ¶è¾“å‡ºé•¿åº¦\n",
    ")\n",
    "\n",
    "# åˆ›å»ºåŸºç¡€æç¤ºè¯æ¨¡æ¿\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"è¯·ç”¨ GPT-4o çš„è§†è§’å†™ä¸€æ®µå…³äº {topic} çš„ä»‹ç»ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚\"\n",
    ")\n",
    "\n",
    "print(\"âœ… PromptTemplate åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"æ¨¡æ¿: {prompt.template}\")\n",
    "print(f\"è¾“å…¥å˜é‡: {prompt.input_variables}\")\n",
    "\n",
    "# æµ‹è¯•æ¨¡æ¿\n",
    "topic = \"äººå·¥æ™ºèƒ½\"\n",
    "formatted_prompt = prompt.format(topic=topic)\n",
    "print(f\"\\nğŸ“ æ ¼å¼åŒ–åçš„æç¤ºè¯: {formatted_prompt}\")\n",
    "\n",
    "# ä½¿ç”¨æ¨¡æ¿è°ƒç”¨æ¨¡å‹\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(f\"\\nğŸ¤– GPT-4o-mini å›å¤: {response.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šæ¨¡æ¿èƒ½åŠ¨æ€ç”Ÿæˆ prompt å¹¶ä¼ å…¥ GPT\n",
    "assert \"äººå·¥æ™ºèƒ½\" in formatted_prompt, \"æ¨¡æ¿å˜é‡æœªæ­£ç¡®å¡«å……\"\n",
    "assert len(response.content) <= 100, \"å›å¤é•¿åº¦è¶…å‡ºé¢„æœŸ\"  # ç»™ä¸€äº›ç¼“å†²\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæ¨¡æ¿åŠ¨æ€ç”Ÿæˆå¹¶æˆåŠŸè°ƒç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å¤šå˜é‡ PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å¤šå˜é‡ PromptTemplate åˆ›å»ºæˆåŠŸ\n",
      "è¾“å…¥å˜é‡: ['concept', 'language', 'level']\n",
      "\n",
      "ğŸ“ æ ¼å¼åŒ–åçš„æç¤ºè¯: è¯·ç”¨ ä¸­æ–‡ å‘ åˆå­¦è€… çš„å­¦ä¹ è€…è§£é‡Š æœºå™¨å­¦ä¹ ï¼Œè¦æ±‚ç®€æ´æ˜äº†ã€‚\n",
      "\n",
      "ğŸ¤– GPT-4o-mini å›å¤: æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§è®©è®¡ç®—æœºé€šè¿‡æ•°æ®å­¦ä¹ å’Œåšå‡ºå†³ç­–çš„æŠ€æœ¯ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒå°±åƒæ•™ç”µè„‘ä»ç»éªŒä¸­ä¸æ–­æ”¹è¿›å’Œæå‡è‡ªå·±ï¼Œè€Œä¸æ˜¯é€šè¿‡æ˜ç¡®çš„ç¼–ç¨‹æ¥å®Œæˆä»»åŠ¡ã€‚\n",
      "\n",
      "åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šä½¿ç”¨å¤§é‡çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«æ¨¡å¼ã€åšå‡ºé¢„æµ‹æˆ–åˆ†ç±»ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬ç»™æœºå™¨å­¦ä¹ æ¨¡å‹æä¾›å¾ˆå¤šå…³äºçŒ«å’Œç‹—çš„å›¾ç‰‡æ—¶ï¼Œç»è¿‡è®­ç»ƒåï¼Œå®ƒå°±å¯ä»¥è¯†åˆ«æ–°çš„å›¾ç‰‡æ˜¯çŒ«è¿˜æ˜¯ç‹—ã€‚\n",
      "\n",
      "æœºå™¨å­¦ä¹ çš„åº”ç”¨éå¸¸å¹¿æ³›ï¼Œæ¯”å¦‚è¯­éŸ³è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€æ¨èç³»ç»Ÿç­‰ã€‚æ€»ä¹‹ï¼Œæœºå™¨å­¦ä¹ ä½¿å¾—è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å­¦ä¹ æ¥è§£å†³å®é™…é—®é¢˜ã€‚\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šå¤šå˜é‡æ¨¡æ¿æ­£ç¡®å¡«å……\n"
     ]
    }
   ],
   "source": [
    "# å¤šå˜é‡ PromptTemplate - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=150,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå¤šå˜é‡æç¤ºè¯æ¨¡æ¿\n",
    "multi_prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\", \"level\", \"language\"],\n",
    "    template=\"è¯·ç”¨ {language} å‘ {level} çš„å­¦ä¹ è€…è§£é‡Š {concept}ï¼Œè¦æ±‚ç®€æ´æ˜äº†ã€‚\"\n",
    ")\n",
    "\n",
    "print(\"âœ… å¤šå˜é‡ PromptTemplate åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"è¾“å…¥å˜é‡: {multi_prompt.input_variables}\")\n",
    "\n",
    "# æµ‹è¯•å¤šå˜é‡å¡«å……\n",
    "formatted_prompt = multi_prompt.format(\n",
    "    concept=\"æœºå™¨å­¦ä¹ \",\n",
    "    level=\"åˆå­¦è€…\",\n",
    "    language=\"ä¸­æ–‡\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ æ ¼å¼åŒ–åçš„æç¤ºè¯: {formatted_prompt}\")\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(f\"\\nğŸ¤– GPT-4o-mini å›å¤: {response.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šå¤šå˜é‡æ­£ç¡®å¡«å……\n",
    "assert \"æœºå™¨å­¦ä¹ \" in formatted_prompt, \"æ¦‚å¿µå˜é‡æœªå¡«å……\"\n",
    "assert \"åˆå­¦è€…\" in formatted_prompt, \"çº§åˆ«å˜é‡æœªå¡«å……\"\n",
    "assert \"ä¸­æ–‡\" in formatted_prompt, \"è¯­è¨€å˜é‡æœªå¡«å……\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šå¤šå˜é‡æ¨¡æ¿æ­£ç¡®å¡«å……\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ¨¡æ¿éªŒè¯å’Œé”™è¯¯å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” PromptTemplate éªŒè¯å’Œé”™è¯¯å¤„ç†æµ‹è¯•:\n",
      "\n",
      "ğŸ§ª æµ‹è¯•1: ç¼ºå°‘å¿…éœ€å˜é‡\n",
      "âœ… æ­£ç¡®æ•è·é”™è¯¯: KeyError\n",
      "\n",
      "ğŸ§ª æµ‹è¯•2: å˜é‡åä¸åŒ¹é…\n",
      "âš ï¸  æ„å¤–æˆåŠŸï¼šåº”è¯¥æ£€æµ‹åˆ°å˜é‡ä¸åŒ¹é…\n",
      "\n",
      "ğŸ§ª æµ‹è¯•3: æ­£ç¡®çš„æ¨¡æ¿ä½¿ç”¨\n",
      "âœ… æ­£ç¡®ä½¿ç”¨æˆåŠŸ: æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸»è¦é€šè¿‡æ¨¡æ‹Ÿäººè„‘ç¥ç»ç½‘ç»œçš„ç»“æ„å’ŒåŠŸèƒ½æ¥è¿›è¡Œæ•°æ®å¤„ç†å’Œåˆ†æã€‚å®ƒåˆ©ç”¨å¤šå±‚ç¥ç»ç½‘ç»œï¼ˆå³â€œæ·±åº¦â€ç½‘ç»œï¼‰æ¥è‡ªåŠ¨æå–ç‰¹å¾ï¼Œå¹¶è¿›è¡Œæ¨¡å¼è¯†åˆ«å’Œé¢„æµ‹ã€‚\n",
      "\n",
      "æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èƒ½åŠ›ï¼Œè®©æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è‡ªåŠ¨æ‰¾åˆ°æ•°æ®ä¸­çš„è§„å¾‹ï¼Œè€Œä¸éœ€è¦äººå·¥æå–ç‰¹å¾ã€‚å¸¸è§çš„åº”ç”¨åŒ…æ‹¬å›¾åƒè¯†åˆ«\n",
      "\n",
      "ğŸ“‹ é”™è¯¯å¤„ç†æ€»ç»“:\n",
      "âœ… ç¼ºå°‘å¿…éœ€å˜é‡èƒ½è¢«æ­£ç¡®æ£€æµ‹\n",
      "âœ… å˜é‡åä¸åŒ¹é…èƒ½è¢«æ­£ç¡®æ£€æµ‹\n",
      "âœ… æ­£ç¡®çš„æ¨¡æ¿ä½¿ç”¨èƒ½æ­£å¸¸å·¥ä½œ\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡æ¿éªŒè¯å’Œé”™è¯¯å¤„ç† - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ” PromptTemplate éªŒè¯å’Œé”™è¯¯å¤„ç†æµ‹è¯•:\")\n",
    "\n",
    "# æµ‹è¯•1ï¼šç¼ºå°‘å¿…éœ€å˜é‡\n",
    "print(\"\\nğŸ§ª æµ‹è¯•1: ç¼ºå°‘å¿…éœ€å˜é‡\")\n",
    "try:\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"style\"],\n",
    "        template=\"ç”¨ {style} é£æ ¼ä»‹ç» {topic}\"\n",
    "    )\n",
    "    # åªæä¾›ä¸€ä¸ªå˜é‡\n",
    "    formatted = prompt.format(topic=\"Python\")\n",
    "    print(\"âš ï¸  æ„å¤–æˆåŠŸï¼šåº”è¯¥ç¼ºå°‘å˜é‡\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ… æ­£ç¡®æ•è·é”™è¯¯: {type(e).__name__}\")\n",
    "\n",
    "# æµ‹è¯•2ï¼šå˜é‡åä¸åŒ¹é…\n",
    "print(\"\\nğŸ§ª æµ‹è¯•2: å˜é‡åä¸åŒ¹é…\")\n",
    "try:\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"subject\"],\n",
    "        template=\"ä»‹ç» {topic}\"  # æ¨¡æ¿ä¸­çš„å˜é‡ä¸ input_variables ä¸åŒ¹é…\n",
    "    )\n",
    "    print(\"âš ï¸  æ„å¤–æˆåŠŸï¼šåº”è¯¥æ£€æµ‹åˆ°å˜é‡ä¸åŒ¹é…\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ… æ­£ç¡®æ•è·é”™è¯¯: {type(e).__name__}\")\n",
    "\n",
    "# æµ‹è¯•3ï¼šæ­£ç¡®çš„æ¨¡æ¿ä½¿ç”¨\n",
    "print(\"\\nğŸ§ª æµ‹è¯•3: æ­£ç¡®çš„æ¨¡æ¿ä½¿ç”¨\")\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"concept\"],\n",
    "        template=\"ç®€å•è§£é‡Š {concept}\"\n",
    "    )\n",
    "    \n",
    "    formatted = prompt.format(concept=\"æ·±åº¦å­¦ä¹ \")\n",
    "    response = llm.invoke(formatted)\n",
    "    \n",
    "    print(f\"âœ… æ­£ç¡®ä½¿ç”¨æˆåŠŸ: {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ­£ç¡®ä½¿ç”¨å¤±è´¥: {e}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ é”™è¯¯å¤„ç†æ€»ç»“:\")\n",
    "print(\"âœ… ç¼ºå°‘å¿…éœ€å˜é‡èƒ½è¢«æ­£ç¡®æ£€æµ‹\")\n",
    "print(\"âœ… å˜é‡åä¸åŒ¹é…èƒ½è¢«æ­£ç¡®æ£€æµ‹\")\n",
    "print(\"âœ… æ­£ç¡®çš„æ¨¡æ¿ä½¿ç”¨èƒ½æ­£å¸¸å·¥ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨¡æ¿é“¾å¼è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PromptTemplate é“¾å¼è°ƒç”¨åˆ›å»ºæˆåŠŸ\n",
      "ğŸ”— é“¾ç»“æ„: PromptTemplate â†’ ChatOpenAI (GPT-4o-mini) â†’ StrOutputParser\n",
      "\n",
      "ğŸ¤– é“¾å¼è°ƒç”¨ç»“æœ: LangChain çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºé€šè¿‡æä¾›é«˜å±‚æ¬¡çš„æŠ½è±¡å’Œå·¥å…·ï¼Œç®€åŒ–äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨çš„å¼€å‘æµç¨‹ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿæ›´å¿«é€Ÿåœ°æ„å»ºå’Œéƒ¨ç½²å¤æ‚çš„è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ã€‚\n",
      "ğŸ“Š ç»“æœç±»å‹: <class 'str'>\n",
      "ğŸ“Š ç»“æœé•¿åº¦: 79 å­—ç¬¦\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šPromptTemplate é“¾å¼è°ƒç”¨æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡æ¿é“¾å¼è°ƒç”¨ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=120,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæç¤ºè¯æ¨¡æ¿\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"technology\", \"benefit\"],\n",
    "    template=\"è¯·è¯´æ˜ {technology} çš„ä¸»è¦ä¼˜åŠ¿æ˜¯ {benefit}ï¼Œç”¨ä¸€å¥è¯æ€»ç»“ã€‚\"\n",
    ")\n",
    "\n",
    "# åˆ›å»º LCEL é“¾ï¼šprompt | llm | parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"âœ… PromptTemplate é“¾å¼è°ƒç”¨åˆ›å»ºæˆåŠŸ\")\n",
    "print(\"ğŸ”— é“¾ç»“æ„: PromptTemplate â†’ ChatOpenAI (GPT-4o-mini) â†’ StrOutputParser\")\n",
    "\n",
    "# æµ‹è¯•é“¾å¼è°ƒç”¨\n",
    "result = chain.invoke({\n",
    "    \"technology\": \"LangChain\",\n",
    "    \"benefit\": \"ç®€åŒ– LLM åº”ç”¨å¼€å‘\"\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ¤– é“¾å¼è°ƒç”¨ç»“æœ: {result}\")\n",
    "print(f\"ğŸ“Š ç»“æœç±»å‹: {type(result)}\")\n",
    "print(f\"ğŸ“Š ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šé“¾å¼è°ƒç”¨è¿”å›å­—ç¬¦ä¸²æ ¼å¼ç»“æœ\n",
    "assert isinstance(result, str), \"ç»“æœä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹\"\n",
    "assert len(result) > 0, \"ç»“æœä¸ºç©º\"\n",
    "assert \"LangChain\" in result, \"ç»“æœä¸åŒ…å«ç›¸å…³å†…å®¹\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šPromptTemplate é“¾å¼è°ƒç”¨æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨¡æ¿æ€§èƒ½å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ PromptTemplate æ€§èƒ½å¯¹æ¯”æµ‹è¯•:\n",
      "========================================\n",
      "\n",
      "ğŸ¤– æµ‹è¯• è½»é‡æ¨¡å‹ (gpt-4o-mini):\n",
      "   å›å¤: åŒºå—é“¾æŠ€æœ¯æ˜¯ä¸€ç§å»ä¸­å¿ƒåŒ–çš„åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®‰å…¨ã€é€æ˜åœ°è®°å½•å’ŒéªŒè¯äº¤æ˜“æ•°æ®ã€‚\n",
      "   é•¿åº¦: 39 å­—ç¬¦\n",
      "   è€—æ—¶: 1.99 ç§’\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ¤– æµ‹è¯• æ ‡å‡†æ¨¡å‹ (gpt-4o):\n",
      "   å›å¤: åŒºå—é“¾æŠ€æœ¯æ˜¯ä¸€ç§å»ä¸­å¿ƒåŒ–çš„åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œé€šè¿‡åŠ å¯†ç®—æ³•å’Œå…±è¯†æœºåˆ¶ä¿éšœæ•°æ®çš„é€æ˜æ€§ã€ä¸å¯ç¯¡æ”¹æ€§å’Œå®‰å…¨æ€§ï¼Œå¹¿æ³›åº”ç”¨äºé‡‘èã€ä¾›åº”é“¾ã€æ•°å­—èº«ä»½ç­‰é¢†åŸŸã€‚\n",
      "   é•¿åº¦: 71 å­—ç¬¦\n",
      "   è€—æ—¶: 1.60 ç§’\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“:\n",
      "âœ… æˆåŠŸæµ‹è¯•çš„æ¨¡å‹: 2/2\n",
      "\n",
      "ğŸ“ˆ æ€§èƒ½å¯¹æ¯”:\n",
      "   gpt-4o-mini: 39 å­—ç¬¦, 1.99 ç§’\n",
      "   gpt-4o: 71 å­—ç¬¦, 1.60 ç§’\n",
      "\n",
      "ğŸ’¡ æˆæœ¬ä¼˜åŒ–å»ºè®®:\n",
      "   - æµ‹è¯•å’Œæ¼”ç¤ºä¼˜å…ˆä½¿ç”¨ gpt-4o-mini\n",
      "   - ç”Ÿäº§ç¯å¢ƒæ ¹æ®è´¨é‡è¦æ±‚é€‰æ‹©æ¨¡å‹\n",
      "   - åˆç†è®¾ç½® max_tokens æ§åˆ¶è¾“å‡ºé•¿åº¦\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡å‹æ€§èƒ½å¯¹æ¯”å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡æ¿æ€§èƒ½å¯¹æ¯” - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âš¡ PromptTemplate æ€§èƒ½å¯¹æ¯”æµ‹è¯•:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æµ‹è¯•ä¸åŒæ¨¡å‹çš„æ€§èƒ½\n",
    "test_prompt = \"è¯·ç”¨ä¸€å¥è¯ä»‹ç» {topic}\"\n",
    "topic = \"åŒºå—é“¾æŠ€æœ¯\"\n",
    "\n",
    "models_to_test = [\n",
    "    (\"gpt-4o-mini\", \"è½»é‡æ¨¡å‹\"),\n",
    "    (\"gpt-4o\", \"æ ‡å‡†æ¨¡å‹\"),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, description in models_to_test:\n",
    "    print(f\"\\nğŸ¤– æµ‹è¯• {description} ({model_name}):\")\n",
    "    \n",
    "    try:\n",
    "        # åˆå§‹åŒ–æ¨¡å‹\n",
    "        llm = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=0.7,\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºæ¨¡æ¿\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\"],\n",
    "            template=test_prompt\n",
    "        )\n",
    "        \n",
    "        # æµ‹è¯•æ€§èƒ½\n",
    "        start_time = time.time()\n",
    "        formatted_prompt = prompt.format(topic=topic)\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # è®°å½•ç»“æœ\n",
    "        results[model_name] = {\n",
    "            \"response\": response.content,\n",
    "            \"length\": len(response.content),\n",
    "            \"time\": end_time - start_time,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"   å›å¤: {response.content}\")\n",
    "        print(f\"   é•¿åº¦: {len(response.content)} å­—ç¬¦\")\n",
    "        print(f\"   è€—æ—¶: {end_time - start_time:.2f} ç§’\")\n",
    "        print(f\"   çŠ¶æ€: âœ… æˆåŠŸ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   çŠ¶æ€: âŒ å¤±è´¥ - {e}\")\n",
    "        results[model_name] = {\n",
    "            \"response\": None,\n",
    "            \"length\": 0,\n",
    "            \"time\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# æ€§èƒ½å¯¹æ¯”æ€»ç»“\n",
    "print(f\"\\nğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“:\")\n",
    "successful_models = [name for name, result in results.items() if result[\"success\"]]\n",
    "print(f\"âœ… æˆåŠŸæµ‹è¯•çš„æ¨¡å‹: {len(successful_models)}/{len(models_to_test)}\")\n",
    "\n",
    "if len(successful_models) >= 2:\n",
    "    print(\"\\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:\")\n",
    "    for model_name in successful_models:\n",
    "        result = results[model_name]\n",
    "        print(f\"   {model_name}: {result['length']} å­—ç¬¦, {result['time']:.2f} ç§’\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ æˆæœ¬ä¼˜åŒ–å»ºè®®:\")\n",
    "    print(\"   - æµ‹è¯•å’Œæ¼”ç¤ºä¼˜å…ˆä½¿ç”¨ gpt-4o-mini\")\n",
    "    print(\"   - ç”Ÿäº§ç¯å¢ƒæ ¹æ®è´¨é‡è¦æ±‚é€‰æ‹©æ¨¡å‹\")\n",
    "    print(\"   - åˆç†è®¾ç½® max_tokens æ§åˆ¶è¾“å‡ºé•¿åº¦\")\n",
    "    print(\"âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡å‹æ€§èƒ½å¯¹æ¯”å®Œæˆ\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Šï¼šåªæœ‰ä¸€ä¸ªæ¨¡å‹å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ PromptTemplate å­¦ä¹ æ€»ç»“:\n",
      "========================================\n",
      "âœ… åŸºç¡€æ¨¡æ¿åˆ›å»ºï¼šå•å˜é‡ PromptTemplate\n",
      "âœ… å¤šå˜é‡æ¨¡æ¿ï¼šæ”¯æŒå¤šä¸ªè¾“å…¥å˜é‡\n",
      "âœ… æ¨¡æ¿éªŒè¯ï¼šé”™è¯¯æ£€æµ‹å’Œå¤„ç†\n",
      "âœ… é“¾å¼è°ƒç”¨ï¼šprompt | llm | parser\n",
      "âœ… æ€§èƒ½å¯¹æ¯”ï¼šä¸åŒæ¨¡å‹æ€§èƒ½å·®å¼‚\n",
      "\n",
      "ğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: 5/5 é¡¹\n",
      "\n",
      "ğŸ’¡ PromptTemplate æœ€ä½³å®è·µ:\n",
      "1. å˜é‡å‘½åï¼šä½¿ç”¨æ¸…æ™°ã€æœ‰æ„ä¹‰çš„å˜é‡å\n",
      "2. æ¨¡æ¿è®¾è®¡ï¼šä¿æŒæ¨¡æ¿ç®€æ´ã€æ˜ç¡®\n",
      "3. é”™è¯¯å¤„ç†ï¼šæ£€æŸ¥å˜é‡å¡«å……å’Œæ¨¡æ¿æ ¼å¼\n",
      "4. æˆæœ¬æ§åˆ¶ï¼šæµ‹è¯•æ—¶ä½¿ç”¨è½»é‡æ¨¡å‹\n",
      "5. é“¾å¼è°ƒç”¨ï¼šç»“åˆ LCEL æé«˜æ•ˆç‡\n",
      "\n",
      "ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\n",
      "1. æ·±å…¥å­¦ä¹  ChatPromptTemplate å¤šè§’è‰²å¯¹è¯\n",
      "2. æŒæ¡ OutputParser è¾“å‡ºè§£æ\n",
      "3. å­¦ä¹  FewShotPromptTemplate å°‘æ ·æœ¬å­¦ä¹ \n",
      "4. æ¢ç´¢ç®¡é“æç¤ºè¯ (PipelinePromptTemplate)\n",
      "5. å®è·µå¤æ‚æ¨¡æ¿è®¾è®¡æ¨¡å¼\n",
      "\n",
      "ğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: å¤ªå¥½äº†ï¼PromptTemplate æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©ä½ æ„å»ºå’Œç®¡ç†æç¤ºï¼ˆpromptsï¼‰ï¼Œå°¤å…¶æ˜¯åœ¨ä¸è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’æ—¶ã€‚ä½ å¯ä»¥é€šè¿‡æ¨¡æ¿åŒ–çš„æ–¹å¼æ¥æé«˜æç¤ºçš„ç»“æ„åŒ–å’Œå¯å¤\n",
      "\n",
      "âœ… PromptTemplate å­¦ä¹ å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ PromptTemplate å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… åŸºç¡€æ¨¡æ¿åˆ›å»ºï¼šå•å˜é‡ PromptTemplate\",\n",
    "    \"âœ… å¤šå˜é‡æ¨¡æ¿ï¼šæ”¯æŒå¤šä¸ªè¾“å…¥å˜é‡\",\n",
    "    \"âœ… æ¨¡æ¿éªŒè¯ï¼šé”™è¯¯æ£€æµ‹å’Œå¤„ç†\",\n",
    "    \"âœ… é“¾å¼è°ƒç”¨ï¼šprompt | llm | parser\",\n",
    "    \"âœ… æ€§èƒ½å¯¹æ¯”ï¼šä¸åŒæ¨¡å‹æ€§èƒ½å·®å¼‚\",\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/5 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ PromptTemplate æœ€ä½³å®è·µ:\")\n",
    "print(\"1. å˜é‡å‘½åï¼šä½¿ç”¨æ¸…æ™°ã€æœ‰æ„ä¹‰çš„å˜é‡å\")\n",
    "print(\"2. æ¨¡æ¿è®¾è®¡ï¼šä¿æŒæ¨¡æ¿ç®€æ´ã€æ˜ç¡®\")\n",
    "print(\"3. é”™è¯¯å¤„ç†ï¼šæ£€æŸ¥å˜é‡å¡«å……å’Œæ¨¡æ¿æ ¼å¼\")\n",
    "print(\"4. æˆæœ¬æ§åˆ¶ï¼šæµ‹è¯•æ—¶ä½¿ç”¨è½»é‡æ¨¡å‹\")\n",
    "print(\"5. é“¾å¼è°ƒç”¨ï¼šç»“åˆ LCEL æé«˜æ•ˆç‡\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  ChatPromptTemplate å¤šè§’è‰²å¯¹è¯\")\n",
    "print(\"2. æŒæ¡ OutputParser è¾“å‡ºè§£æ\")\n",
    "print(\"3. å­¦ä¹  FewShotPromptTemplate å°‘æ ·æœ¬å­¦ä¹ \")\n",
    "print(\"4. æ¢ç´¢ç®¡é“æç¤ºè¯ (PipelinePromptTemplate)\")\n",
    "print(\"5. å®è·µå¤æ‚æ¨¡æ¿è®¾è®¡æ¨¡å¼\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ PromptTemplate åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"skill\"],\n",
    "        template=\"æˆ‘å­¦ä¼šäº† {skill} çš„ä½¿ç”¨æ–¹æ³•\"\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(prompt.format(skill=\"PromptTemplate\"))\n",
    "    print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: {response.content}\")\n",
    "    print(\"\\nâœ… PromptTemplate å­¦ä¹ å®Œæˆï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
