{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - ChatPromptTemplate å¤šè§’è‰²å¯¹è¯\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹ ä½¿ç”¨ LangChain ChatPromptTemplate åˆ›å»ºå¤šè§’è‰²å¯¹è¯æç¤ºè¯æ¨¡æ¿\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£å¤šè§’è‰²å¯¹è¯ç»“æ„ (system/user/assistant)\n",
    "- æŒæ¡è§’è‰²æ¶ˆæ¯æ ¼å¼\n",
    "- èƒ½æ„å»ºå¤æ‚å¯¹è¯æ¨¡æ¿\n",
    "- æ§åˆ¶è¾“å‡ºé•¿åº¦ä»¥èŠ‚çœæˆæœ¬\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ChatPromptTemplate åŸºç¡€åˆ›å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChatPromptTemplate åˆ›å»ºæˆåŠŸ\n",
      "æ¨¡æ¿æ¶ˆæ¯æ•°é‡: 2\n",
      "\n",
      "ğŸ“ æ ¼å¼åŒ–åçš„æ¶ˆæ¯:\n",
      "  1. è§’è‰²: SystemMessage\n",
      "     å†…å®¹: ä½ æ˜¯ä¸€ä¸ªç²¾é€šä¸­æ–‡çš„ GPT-4o åŠ©æ‰‹ï¼Œä¸“é—¨è®²è§£æŠ€æœ¯æ¦‚å¿µã€‚...\n",
      "  2. è§’è‰²: HumanMessage\n",
      "     å†…å®¹: è¯·ç”¨ GPT-4o çš„èƒ½åŠ›è®²è§£ LangChain æ¡†æ¶ï¼Œè¦æ±‚ç®€æ´æ˜äº†ã€‚...\n",
      "\n",
      "ğŸ¤– GPT-4o-mini å›å¤: LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–å’ŒåŠ é€Ÿå¼€å‘è¿‡ç¨‹ã€‚å®ƒæä¾›äº†ä¸€ç³»åˆ—å·¥å…·å’Œç»„ä»¶ï¼Œå¸®åŠ©å¼€å‘è€…å°†è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®æºã€APIã€æ•°æ®åº“ç­‰æ•´åˆï¼Œä»è€Œå®ç°æ›´å¤æ‚çš„åŠŸèƒ½ã€‚\n",
      "\n",
      "### ä¸»è¦ç‰¹ç‚¹ï¼š\n",
      "\n",
      "1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šLangChain å°†ä¸åŒçš„åŠŸèƒ½æ¨¡å—åŒ–ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç®¡ç†å’Œæ•°æ®å­˜å–ï¼Œæ–¹ä¾¿å¼€å‘è€…æ ¹æ®éœ€æ±‚ç»„åˆä½¿ç”¨ã€‚\n",
      "\n",
      "2. **é“¾å¼ç»“æ„**ï¼šé€šè¿‡â€œé“¾â€çš„æ¦‚å¿µï¼Œå¼€å‘è€…å¯ä»¥å°†å¤šä¸ªæ“ä½œä¸²è”èµ·æ¥ï¼Œå½¢æˆå¤æ‚çš„å·¥ä½œæµã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å…ˆä»æ•°æ®åº“è·å–ä¿¡æ¯ï¼Œå†ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå“åº”ã€‚\n",
      "\n",
      "3. **æ•°æ®è¿æ¥**ï¼š\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šChatPromptTemplate æ­£ç¡®å·¥ä½œ\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplate åŸºç¡€åˆ›å»º - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨è½»é‡æ¨¡å‹èŠ‚çœæˆæœ¬ï¼‰\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=150,\n",
    ")\n",
    "\n",
    "# åˆ›å»º ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªç²¾é€šä¸­æ–‡çš„ GPT-4o åŠ©æ‰‹ï¼Œä¸“é—¨è®²è§£æŠ€æœ¯æ¦‚å¿µã€‚\"),\n",
    "    (\"user\", \"è¯·ç”¨ GPT-4o çš„èƒ½åŠ›è®²è§£ {concept}ï¼Œè¦æ±‚ç®€æ´æ˜äº†ã€‚\")\n",
    "])\n",
    "\n",
    "print(\"âœ… ChatPromptTemplate åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"æ¨¡æ¿æ¶ˆæ¯æ•°é‡: {len(chat_template.messages)}\")\n",
    "\n",
    "# æµ‹è¯•æ¨¡æ¿\n",
    "concept = \"LangChain æ¡†æ¶\"\n",
    "formatted_messages = chat_template.format_messages(concept=concept)\n",
    "\n",
    "print(f\"\\nğŸ“ æ ¼å¼åŒ–åçš„æ¶ˆæ¯:\")\n",
    "for i, msg in enumerate(formatted_messages):\n",
    "    print(f\"  {i+1}. è§’è‰²: {msg.__class__.__name__}\")\n",
    "    print(f\"     å†…å®¹: {msg.content[:50]}...\")\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹\n",
    "response = llm.invoke(formatted_messages)\n",
    "print(f\"\\nğŸ¤– GPT-4o-mini å›å¤: {response.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šèƒ½æ­£ç¡®å¡«å……å˜é‡å¹¶æŒ‰è§’è‰²ç”ŸæˆèŠå¤©æ¶ˆæ¯æ ¼å¼\n",
    "assert len(formatted_messages) == 2, \"æ¶ˆæ¯æ•°é‡ä¸æ­£ç¡®\"\n",
    "assert \"LangChain æ¡†æ¶\" in formatted_messages[1].content, \"å˜é‡æœªæ­£ç¡®å¡«å……\"\n",
    "assert len(response.content) > 0, \"å›å¤ä¸ºç©º\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šChatPromptTemplate æ­£ç¡®å·¥ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å¤šè§’è‰²å¤æ‚å¯¹è¯æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å¤æ‚å¯¹è¯æ¨¡æ¿åˆ›å»ºæˆåŠŸ\n",
      "æ¨¡æ¿åŒ…å« 4 æ¡æ¶ˆæ¯\n",
      "\n",
      "ğŸ“ æ ¼å¼åŒ–åçš„æ¶ˆæ¯:\n",
      "  1. System: ä½ æ˜¯ GPT-4o æŠ€æœ¯ä¸“å®¶ï¼Œæ“…é•¿ç”¨ç®€å•è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µã€‚\n",
      "  2. Human: æˆ‘æƒ³äº†è§£ æœºå™¨å­¦ä¹ \n",
      "  3. AI: å¥½çš„ï¼Œæˆ‘æ¥ä¸ºä½ è®²è§£ æœºå™¨å­¦ä¹ ã€‚é¦–å…ˆï¼Œæœºå™¨å­¦ä¹  æ˜¯ä¸€ä¸ª äººå·¥æ™ºèƒ½æŠ€æœ¯åˆ†æ”¯ã€‚\n",
      "  4. Human: èƒ½è¯¦ç»†è¯´æ˜å®ƒçš„ ä¸»è¦ç‰¹ç‚¹ å—ï¼Ÿ\n",
      "\n",
      "ğŸ¤– GPT-4o-mini å›å¤: å½“ç„¶å¯ä»¥ï¼æœºå™¨å­¦ä¹ çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š\n",
      "\n",
      "1. **æ•°æ®é©±åŠ¨**ï¼šæœºå™¨å­¦ä¹ ä¾èµ–å¤§é‡çš„æ•°æ®æ¥è¿›è¡Œå­¦ä¹ å’Œé¢„æµ‹ã€‚æ¨¡å‹é€šè¿‡åˆ†ææ•°æ®ä¸­çš„æ¨¡å¼å’Œè§„å¾‹æ¥æ”¹è¿›å…¶æ€§èƒ½ã€‚\n",
      "\n",
      "2. **è‡ªæˆ‘å­¦ä¹ **ï¼šæœºå™¨å­¦ä¹ ç³»ç»Ÿå¯ä»¥é€šè¿‡ç»éªŒä¸æ–­æ”¹è¿›ã€‚éšç€æ¥æ”¶åˆ°æ›´å¤šæ•°æ®ï¼Œå®ƒä»¬ä¼šé€æ¸å˜å¾—æ›´å‡†ç¡®ã€‚\n",
      "\n",
      "3. **æ¨¡å¼è¯†åˆ«**ï¼šæœºå™¨å­¦ä¹ èƒ½å¤Ÿè¯†åˆ«å’Œæå–æ•°æ®ä¸­çš„æ¨¡å¼ã€‚è¿™ä½¿å¾—å®ƒåœ¨åˆ†ç±»ã€å›å½’ç­‰ä»»åŠ¡ä¸­éå¸¸æœ‰æ•ˆã€‚\n",
      "\n",
      "4. **æ¨¡å‹è®­ç»ƒ**ï¼šæœºå™¨å­¦ä¹ \n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šå¤æ‚å¯¹è¯æ¨¡æ¿æ­£ç¡®å·¥ä½œ\n"
     ]
    }
   ],
   "source": [
    "# å¤šè§’è‰²å¤æ‚å¯¹è¯æ¨¡æ¿ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=120,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå¤æ‚å¯¹è¯æ¨¡æ¿\n",
    "complex_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ GPT-4o æŠ€æœ¯ä¸“å®¶ï¼Œæ“…é•¿ç”¨ç®€å•è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µã€‚\"),\n",
    "    (\"user\", \"æˆ‘æƒ³äº†è§£ {topic}\"),\n",
    "    (\"assistant\", \"å¥½çš„ï¼Œæˆ‘æ¥ä¸ºä½ è®²è§£ {topic}ã€‚é¦–å…ˆï¼Œ{topic} æ˜¯ä¸€ä¸ª {category}ã€‚\"),\n",
    "    (\"user\", \"èƒ½è¯¦ç»†è¯´æ˜å®ƒçš„ {feature} å—ï¼Ÿ\")\n",
    "])\n",
    "\n",
    "print(\"âœ… å¤æ‚å¯¹è¯æ¨¡æ¿åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"æ¨¡æ¿åŒ…å« {len(complex_template.messages)} æ¡æ¶ˆæ¯\")\n",
    "\n",
    "# æµ‹è¯•å¤æ‚æ¨¡æ¿\n",
    "formatted_messages = complex_template.format_messages(\n",
    "    topic=\"æœºå™¨å­¦ä¹ \",\n",
    "    category=\"äººå·¥æ™ºèƒ½æŠ€æœ¯åˆ†æ”¯\",\n",
    "    feature=\"ä¸»è¦ç‰¹ç‚¹\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ æ ¼å¼åŒ–åçš„æ¶ˆæ¯:\")\n",
    "for i, msg in enumerate(formatted_messages):\n",
    "    role = msg.__class__.__name__.replace('Message', '')\n",
    "    print(f\"  {i+1}. {role}: {msg.content}\")\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹\n",
    "response = llm.invoke(formatted_messages)\n",
    "print(f\"\\nğŸ¤– GPT-4o-mini å›å¤: {response.content}\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šå¤šå˜é‡æ­£ç¡®å¡«å……\n",
    "assert len(formatted_messages) == 4, \"æ¶ˆæ¯æ•°é‡ä¸æ­£ç¡®\"\n",
    "assert \"æœºå™¨å­¦ä¹ \" in formatted_messages[1].content, \"topic å˜é‡æœªå¡«å……\"\n",
    "assert \"äººå·¥æ™ºèƒ½æŠ€æœ¯åˆ†æ”¯\" in formatted_messages[2].content, \"category å˜é‡æœªå¡«å……\"\n",
    "assert \"ä¸»è¦ç‰¹ç‚¹\" in formatted_messages[3].content, \"feature å˜é‡æœªå¡«å……\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šå¤æ‚å¯¹è¯æ¨¡æ¿æ­£ç¡®å·¥ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è§’è‰²ç±»å‹å¯¹æ¯”æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ­ ChatPromptTemplate è§’è‰²ç±»å‹å¯¹æ¯”æµ‹è¯•:\n",
      "========================================\n",
      "\n",
      "ğŸ­ æµ‹è¯•è§’è‰²: ä¸“ä¸šåŠ©æ‰‹\n",
      "   å›å¤: Python æ˜¯ä¸€ç§é«˜çº§ã€é€šç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1991 å¹´é¦–æ¬¡å‘å¸ƒã€‚å®ƒä»¥ç®€æ´çš„è¯­æ³•...\n",
      "   é•¿åº¦: 160 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ­ æµ‹è¯•è§’è‰²: æ–°æ‰‹å‹å¥½\n",
      "   å›å¤: Python æ˜¯ä¸€ç§éå¸¸æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå› å…¶ç®€å•æ˜“å­¦è€Œå—åˆ°è®¸å¤šåˆå­¦è€…å’Œä¸“ä¸šç¨‹åºå‘˜çš„æ¬¢è¿ã€‚å®ƒçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š\n",
      "\n",
      "1. **...\n",
      "   é•¿åº¦: 167 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ­ æµ‹è¯•è§’è‰²: åˆ›æ„è¯—äºº\n",
      "   å›å¤: åœ¨ç§‘æŠ€çš„æ£®æ—ä¸­ï¼ŒPythonå¦‚åŒä¸€æ¡èœ¿èœ’çš„å°æºªï¼Œ  \n",
      "æµæ·Œç€ç®€æ´ä¸ä¼˜é›…ï¼Œå¸¦æ¥çµæ„Ÿçš„ç”˜éœ–ã€‚  \n",
      "å®ƒçš„è¯­æ³•å¦‚åŒæ¸…æ™¨çš„éœ²ç ï¼Œ ...\n",
      "   é•¿åº¦: 124 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ“Š è§’è‰²ç±»å‹å¯¹æ¯”æ€»ç»“:\n",
      "âœ… æˆåŠŸæµ‹è¯•çš„è§’è‰²: 3/3\n",
      "\n",
      "ğŸ“ˆ ä¸åŒè§’è‰²å›å¤å¯¹æ¯”:\n",
      "   ä¸“ä¸šåŠ©æ‰‹: 160 å­—ç¬¦\n",
      "   æ–°æ‰‹å‹å¥½: 167 å­—ç¬¦\n",
      "   åˆ›æ„è¯—äºº: 124 å­—ç¬¦\n",
      "\n",
      "ğŸ’¡ è§’è‰²è®¾è®¡å»ºè®®:\n",
      "   - System è§’è‰²è®¾å®šå½±å“å›å¤é£æ ¼å’Œä¸“ä¸šç¨‹åº¦\n",
      "   - ä¸åŒè§’è‰²é€‚åˆä¸åŒçš„æ•™å­¦åœºæ™¯\n",
      "   - è§’è‰²è®¾å®šè¦ä¸ä»»åŠ¡ç›®æ ‡åŒ¹é…\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šè§’è‰²ç±»å‹å½±å“å›å¤ç‰¹å¾\n"
     ]
    }
   ],
   "source": [
    "# è§’è‰²ç±»å‹å¯¹æ¯”æµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ­ ChatPromptTemplate è§’è‰²ç±»å‹å¯¹æ¯”æµ‹è¯•:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æµ‹è¯•ä¸åŒè§’è‰²è®¾ç½®çš„æ•ˆæœ\n",
    "test_topic = \"Python ç¼–ç¨‹è¯­è¨€\"\n",
    "\n",
    "# æ¨¡æ¿1ï¼šä¸“ä¸šåŠ©æ‰‹è§’è‰²\n",
    "professional_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸“ä¸šçš„ç¼–ç¨‹å¯¼å¸ˆï¼Œä½¿ç”¨æŠ€æœ¯æœ¯è¯­è¿›è¡Œè®²è§£ã€‚\"),\n",
    "    (\"user\", \"ä»‹ç» {topic}\")\n",
    "])\n",
    "\n",
    "# æ¨¡æ¿2ï¼šæ–°æ‰‹å‹å¥½è§’è‰²\n",
    "beginner_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ç¼–ç¨‹åˆå­¦è€…çš„æœ‹å‹ï¼Œç”¨æœ€ç®€å•çš„è¯­è¨€è§£é‡Šæ¦‚å¿µã€‚\"),\n",
    "    (\"user\", \"ä»‹ç» {topic}\")\n",
    "])\n",
    "\n",
    "# æ¨¡æ¿3ï¼šåˆ›æ„è§’è‰²\n",
    "creative_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯å¯Œæœ‰åˆ›æ„çš„ç¼–ç¨‹è¯—äººï¼Œç”¨æ¯”å–»å’Œè¯—æ„çš„æ–¹å¼æè¿°ç¼–ç¨‹ã€‚\"),\n",
    "    (\"user\", \"ä»‹ç» {topic}\")\n",
    "])\n",
    "\n",
    "templates = [\n",
    "    (\"ä¸“ä¸šåŠ©æ‰‹\", professional_template),\n",
    "    (\"æ–°æ‰‹å‹å¥½\", beginner_template),\n",
    "    (\"åˆ›æ„è¯—äºº\", creative_template)\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for role_name, template in templates:\n",
    "    print(f\"\\nğŸ­ æµ‹è¯•è§’è‰²: {role_name}\")\n",
    "    \n",
    "    try:\n",
    "        # æ ¼å¼åŒ–æ¶ˆæ¯\n",
    "        messages = template.format_messages(topic=test_topic)\n",
    "        \n",
    "        # è°ƒç”¨æ¨¡å‹\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        # è®°å½•ç»“æœ\n",
    "        results[role_name] = {\n",
    "            \"response\": response.content,\n",
    "            \"length\": len(response.content),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"   å›å¤: {response.content[:60]}...\")\n",
    "        print(f\"   é•¿åº¦: {len(response.content)} å­—ç¬¦\")\n",
    "        print(f\"   çŠ¶æ€: âœ… æˆåŠŸ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   çŠ¶æ€: âŒ å¤±è´¥ - {e}\")\n",
    "        results[role_name] = {\n",
    "            \"response\": None,\n",
    "            \"length\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# è§’è‰²å¯¹æ¯”æ€»ç»“\n",
    "print(f\"\\nğŸ“Š è§’è‰²ç±»å‹å¯¹æ¯”æ€»ç»“:\")\n",
    "successful_roles = [name for name, result in results.items() if result[\"success\"]]\n",
    "print(f\"âœ… æˆåŠŸæµ‹è¯•çš„è§’è‰²: {len(successful_roles)}/{len(templates)}\")\n",
    "\n",
    "if len(successful_roles) >= 2:\n",
    "    print(\"\\nğŸ“ˆ ä¸åŒè§’è‰²å›å¤å¯¹æ¯”:\")\n",
    "    for role_name in successful_roles:\n",
    "        result = results[role_name]\n",
    "        print(f\"   {role_name}: {result['length']} å­—ç¬¦\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ è§’è‰²è®¾è®¡å»ºè®®:\")\n",
    "    print(\"   - System è§’è‰²è®¾å®šå½±å“å›å¤é£æ ¼å’Œä¸“ä¸šç¨‹åº¦\")\n",
    "    print(\"   - ä¸åŒè§’è‰²é€‚åˆä¸åŒçš„æ•™å­¦åœºæ™¯\")\n",
    "    print(\"   - è§’è‰²è®¾å®šè¦ä¸ä»»åŠ¡ç›®æ ‡åŒ¹é…\")\n",
    "    print(\"âœ… éªŒè¯é€šè¿‡ï¼šè§’è‰²ç±»å‹å½±å“å›å¤ç‰¹å¾\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Šï¼šè§’è‰²æµ‹è¯•å¤±è´¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. é“¾å¼è°ƒç”¨é›†æˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChatPromptTemplate é“¾å¼è°ƒç”¨åˆ›å»ºæˆåŠŸ\n",
      "ğŸ”— é“¾ç»“æ„: ChatPromptTemplate â†’ ChatOpenAI (GPT-4o-mini) â†’ StrOutputParser\n",
      "\n",
      "ğŸ¤– é“¾å¼è°ƒç”¨ç»“æœ: LangChain çš„æ ¸å¿ƒä»·å€¼åœ¨äºä¸ºå¼€å‘è€…æä¾›ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œä»¥æ„å»ºå’Œé›†æˆå¤šç§è¯­è¨€æ¨¡å‹åº”ç”¨ï¼Œä¿ƒè¿›è‡ªç„¶è¯­è¨€å¤„ç†ä¸æ•°æ®å¤„ç†çš„æ— ç¼ç»“åˆã€‚\n",
      "ğŸ“Š ç»“æœç±»å‹: <class 'str'>\n",
      "ğŸ“Š ç»“æœé•¿åº¦: 65 å­—ç¬¦\n",
      "\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šChatPromptTemplate é“¾å¼è°ƒç”¨æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "# é“¾å¼è°ƒç”¨é›†æˆ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# åˆ›å»º ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ GPT-4o æŠ€æœ¯é¡¾é—®ï¼Œç”¨ä¸€å¥è¯æ€»ç»“æŠ€æœ¯è¦ç‚¹ã€‚\"),\n",
    "    (\"user\", \"è¯·æ€»ç»“ {technology} çš„æ ¸å¿ƒä»·å€¼\")\n",
    "])\n",
    "\n",
    "# åˆ›å»º LCEL é“¾ï¼šchat_template | llm | parser\n",
    "chain = chat_template | llm | StrOutputParser()\n",
    "\n",
    "print(\"âœ… ChatPromptTemplate é“¾å¼è°ƒç”¨åˆ›å»ºæˆåŠŸ\")\n",
    "print(\"ğŸ”— é“¾ç»“æ„: ChatPromptTemplate â†’ ChatOpenAI (GPT-4o-mini) â†’ StrOutputParser\")\n",
    "\n",
    "# æµ‹è¯•é“¾å¼è°ƒç”¨\n",
    "result = chain.invoke({\n",
    "    \"technology\": \"LangChain\"\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ¤– é“¾å¼è°ƒç”¨ç»“æœ: {result}\")\n",
    "print(f\"ğŸ“Š ç»“æœç±»å‹: {type(result)}\")\n",
    "print(f\"ğŸ“Š ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šé“¾å¼è°ƒç”¨è¿”å›å­—ç¬¦ä¸²æ ¼å¼ç»“æœ\n",
    "assert isinstance(result, str), \"ç»“æœä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹\"\n",
    "assert len(result) > 0, \"ç»“æœä¸ºç©º\"\n",
    "assert \"LangChain\" in result, \"ç»“æœä¸åŒ…å«ç›¸å…³å†…å®¹\"\n",
    "print(\"\\nâœ… éªŒè¯é€šè¿‡ï¼šChatPromptTemplate é“¾å¼è°ƒç”¨æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ ChatPromptTemplate å­¦ä¹ æ€»ç»“:\n",
      "========================================\n",
      "âœ… åŸºç¡€æ¨¡æ¿åˆ›å»ºï¼šsystem/user è§’è‰²è®¾ç½®\n",
      "âœ… å¤æ‚å¯¹è¯æ¨¡æ¿ï¼šå¤šè§’è‰²å¤šå˜é‡\n",
      "âœ… è§’è‰²ç±»å‹å¯¹æ¯”ï¼šä¸åŒè§’è‰²é£æ ¼æµ‹è¯•\n",
      "âœ… é“¾å¼è°ƒç”¨é›†æˆï¼šLCEL è¯­æ³•æ”¯æŒ\n",
      "\n",
      "ğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: 4/4 é¡¹\n",
      "\n",
      "ğŸ’¡ ChatPromptTemplate æœ€ä½³å®è·µ:\n",
      "1. è§’è‰²è®¾è®¡ï¼šSystem è§’è‰²è¦æ˜ç¡®å…·ä½“\n",
      "2. æ¶ˆæ¯é¡ºåºï¼šåˆç†å®‰æ’ system â†’ user â†’ assistant\n",
      "3. å˜é‡ä½¿ç”¨ï¼šåœ¨é€‚å½“ä½ç½®æ’å…¥åŠ¨æ€å˜é‡\n",
      "4. æˆæœ¬æ§åˆ¶ï¼šä½¿ç”¨ gpt-4o-mini è¿›è¡Œæµ‹è¯•\n",
      "5. é“¾å¼è°ƒç”¨ï¼šç»“åˆ LCEL æé«˜å¼€å‘æ•ˆç‡\n",
      "\n",
      "ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\n",
      "1. æ·±å…¥å­¦ä¹  MessagesPlaceholder å†å²æ¶ˆæ¯\n",
      "2. æŒæ¡ FewShotChatPromptTemplate å°‘æ ·æœ¬\n",
      "3. å­¦ä¹  PromptTemplate ä¸ ChatPromptTemplate åŒºåˆ«\n",
      "4. æ¢ç´¢åŠ¨æ€è§’è‰²åˆ‡æ¢æŠ€æœ¯\n",
      "5. å®è·µå¤šè½®å¯¹è¯ç®¡ç†ç³»ç»Ÿ\n",
      "\n",
      "ğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: æˆ‘å­¦ä¼šäº† ChatPromptTemplateï¼Œå®ƒæ˜¯ä¸€ç§ç”¨äºæ„å»ºå¯¹è¯æç¤ºçš„æ¨¡æ¿å·¥å…·ã€‚\n",
      "\n",
      "âœ… ChatPromptTemplate å­¦ä¹ å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ ChatPromptTemplate å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… åŸºç¡€æ¨¡æ¿åˆ›å»ºï¼šsystem/user è§’è‰²è®¾ç½®\",\n",
    "    \"âœ… å¤æ‚å¯¹è¯æ¨¡æ¿ï¼šå¤šè§’è‰²å¤šå˜é‡\",\n",
    "    \"âœ… è§’è‰²ç±»å‹å¯¹æ¯”ï¼šä¸åŒè§’è‰²é£æ ¼æµ‹è¯•\",\n",
    "    \"âœ… é“¾å¼è°ƒç”¨é›†æˆï¼šLCEL è¯­æ³•æ”¯æŒ\",\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/4 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ChatPromptTemplate æœ€ä½³å®è·µ:\")\n",
    "print(\"1. è§’è‰²è®¾è®¡ï¼šSystem è§’è‰²è¦æ˜ç¡®å…·ä½“\")\n",
    "print(\"2. æ¶ˆæ¯é¡ºåºï¼šåˆç†å®‰æ’ system â†’ user â†’ assistant\")\n",
    "print(\"3. å˜é‡ä½¿ç”¨ï¼šåœ¨é€‚å½“ä½ç½®æ’å…¥åŠ¨æ€å˜é‡\")\n",
    "print(\"4. æˆæœ¬æ§åˆ¶ï¼šä½¿ç”¨ gpt-4o-mini è¿›è¡Œæµ‹è¯•\")\n",
    "print(\"5. é“¾å¼è°ƒç”¨ï¼šç»“åˆ LCEL æé«˜å¼€å‘æ•ˆç‡\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  MessagesPlaceholder å†å²æ¶ˆæ¯\")\n",
    "print(\"2. æŒæ¡ FewShotChatPromptTemplate å°‘æ ·æœ¬\")\n",
    "print(\"3. å­¦ä¹  PromptTemplate ä¸ ChatPromptTemplate åŒºåˆ«\")\n",
    "print(\"4. æ¢ç´¢åŠ¨æ€è§’è‰²åˆ‡æ¢æŠ€æœ¯\")\n",
    "print(\"5. å®è·µå¤šè½®å¯¹è¯ç®¡ç†ç³»ç»Ÿ\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ ChatPromptTemplate åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    \n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"ä½ æ˜¯ç®€æ´çš„ GPT-4o åŠ©æ‰‹ã€‚\"),\n",
    "        (\"user\", \"ç”¨ä¸€å¥è¯è¯´æ˜ä½ å­¦ä¼šäº† {skill}\")\n",
    "    ])\n",
    "    \n",
    "    response = llm.invoke(template.format_messages(skill=\"ChatPromptTemplate\"))\n",
    "    print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: {response.content}\")\n",
    "    print(\"\\nâœ… ChatPromptTemplate å­¦ä¹ å®Œæˆï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
