{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-ChatOpenAIæ ¸å¿ƒå‚æ•°æµ‹è¯•-20251130-1945\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹  LangChain æ ¸å¿ƒå‚æ•°ï¼ˆmodel/temperature/max_tokensï¼‰çš„ä½œç”¨æœºåˆ¶å’Œè°ƒèŠ‚æ–¹æ³•\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£å„å‚æ•°çš„ä½œç”¨æœºåˆ¶\n",
    "- èƒ½æ ¹æ®éœ€æ±‚è°ƒæ•´å‚æ•°\n",
    "- æŒæ¡ GPT æœ€ä½³å®è·µé…ç½®\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. model å‚æ•°æµ‹è¯• - ä¸åŒæ¨¡å‹å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ GPT æ¨¡å‹å‚æ•°æµ‹è¯•:\n",
      "========================================\n",
      "\n",
      "ğŸ¤– æµ‹è¯• GPT-4o ä¸»åŠ›æ¨¡å‹ (gpt-4o):\n",
      "   å›å¤: LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºè¯­è¨€æ¨¡å‹åº”ç”¨çš„å¼€æºæ¡†æ¶ï¼Œèƒ½å¤Ÿç®€åŒ–å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚é“¾å¼è°ƒç”¨ã€ä¸Šä¸‹æ–‡ç®¡ç†å’Œä¸å¤–éƒ¨æ•°æ®çš„é›†æˆã€‚\n",
      "   é•¿åº¦: 62 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ¤– æµ‹è¯• GPT-4o-mini è½»é‡æ¨¡å‹ (gpt-4o-mini):\n",
      "   å›å¤: LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œæ”¯æŒé“¾å¼è°ƒç”¨ã€æ•°æ®å¤„ç†å’Œé›†æˆå¤šç§å·¥å…·ã€‚\n",
      "   é•¿åº¦: 51 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ¤– æµ‹è¯• GPT-3.5-turbo å¯¹æ¯”æ¨¡å‹ (gpt-3.5-turbo):\n",
      "   çŠ¶æ€: âŒ å¤±è´¥ - Error code: 404 - {'code': 404, 'reason': 'MODEL_NOT_FOUND', 'message': 'model not found', 'metadata': {'reason': 'model: gpt-3.5-turbo not found'}}\n",
      "\n",
      "ğŸ“Š æ¨¡å‹å‚æ•°æµ‹è¯•æ€»ç»“:\n",
      "âœ… æˆåŠŸæµ‹è¯•çš„æ¨¡å‹: 2/3\n",
      "ğŸ“ˆ ä¸åŒæ¨¡å‹è¾“å‡ºå¯¹æ¯”:\n",
      "   gpt-4o: 62 å­—ç¬¦\n",
      "   gpt-4o-mini: 51 å­—ç¬¦\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡å‹å‚æ•°å½±å“è¾“å‡ºç‰¹å¾\n"
     ]
    }
   ],
   "source": [
    "# 1. æ¨¡å‹å‚æ•°æµ‹è¯• - model - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”§ GPT æ¨¡å‹å‚æ•°æµ‹è¯•:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æµ‹è¯•ä¸åŒæ¨¡å‹çš„æ•ˆæœ\n",
    "test_prompt = \"è¯·ç”¨ä¸€å¥è¯ä»‹ç» LangChain\"\n",
    "\n",
    "models_to_test = [\n",
    "    (\"gpt-4o\", \"GPT-4o ä¸»åŠ›æ¨¡å‹\"),\n",
    "    (\"gpt-4o-mini\", \"GPT-4o-mini è½»é‡æ¨¡å‹\"),\n",
    "    (\"gpt-3.5-turbo\", \"GPT-3.5-turbo å¯¹æ¯”æ¨¡å‹\")\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, description in models_to_test:\n",
    "    print(f\"\\nğŸ¤– æµ‹è¯• {description} ({model_name}):\")\n",
    "    \n",
    "    try:\n",
    "        # åˆå§‹åŒ–æ¨¡å‹\n",
    "        llm = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=0.7,\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        \n",
    "        # è°ƒç”¨æ¨¡å‹\n",
    "        response = llm.invoke(test_prompt)\n",
    "        \n",
    "        # è®°å½•ç»“æœ\n",
    "        results[model_name] = {\n",
    "            \"response\": response.content,\n",
    "            \"length\": len(response.content),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"   å›å¤: {response.content}\")\n",
    "        print(f\"   é•¿åº¦: {len(response.content)} å­—ç¬¦\")\n",
    "        print(f\"   çŠ¶æ€: âœ… æˆåŠŸ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   çŠ¶æ€: âŒ å¤±è´¥ - {e}\")\n",
    "        results[model_name] = {\n",
    "            \"response\": None,\n",
    "            \"length\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šæ¨¡å‹å‚æ•°å½±å“è¾“å‡ºç‰¹å¾\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹å‚æ•°æµ‹è¯•æ€»ç»“:\")\n",
    "successful_models = [name for name, result in results.items() if result[\"success\"]]\n",
    "print(f\"âœ… æˆåŠŸæµ‹è¯•çš„æ¨¡å‹: {len(successful_models)}/{len(models_to_test)}\")\n",
    "\n",
    "if len(successful_models) > 1:\n",
    "    print(\"ğŸ“ˆ ä¸åŒæ¨¡å‹è¾“å‡ºå¯¹æ¯”:\")\n",
    "    for model_name in successful_models:\n",
    "        result = results[model_name]\n",
    "        print(f\"   {model_name}: {result['length']} å­—ç¬¦\")\n",
    "    \n",
    "    print(\"âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡å‹å‚æ•°å½±å“è¾“å‡ºç‰¹å¾\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Šï¼šåªæœ‰ä¸€ä¸ªæ¨¡å‹å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æ¨¡å‹å‚æ•°æµ‹è¯• - model - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”§ GPT æ¨¡å‹å‚æ•°æµ‹è¯•:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æµ‹è¯•ä¸åŒæ¨¡å‹çš„æ•ˆæœ\n",
    "test_prompt = \"è¯·ç”¨ä¸€å¥è¯ä»‹ç» LangChain\"\n",
    "\n",
    "models_to_test = [\n",
    "    (\"gpt-4o\", \"GPT-4o ä¸»åŠ›æ¨¡å‹\"),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, description in models_to_test:\n",
    "    print(f\"\\nğŸ¤– æµ‹è¯• {description} ({model_name}):\")\n",
    "    \n",
    "    try:\n",
    "        # åˆå§‹åŒ–æ¨¡å‹\n",
    "        llm = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=0.7,\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        \n",
    "        # è°ƒç”¨æ¨¡å‹\n",
    "        response = llm.invoke(test_prompt)\n",
    "        \n",
    "        # è®°å½•ç»“æœ\n",
    "        results[model_name] = {\n",
    "            \"response\": response.content,\n",
    "            \"length\": len(response.content),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"   å›å¤: {response.content}\")\n",
    "        print(f\"   é•¿åº¦: {len(response.content)} å­—ç¬¦\")\n",
    "        print(f\"   çŠ¶æ€: âœ… æˆåŠŸ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   çŠ¶æ€: âŒ å¤±è´¥ - {e}\")\n",
    "        results[model_name] = {\n",
    "            \"response\": None,\n",
    "            \"length\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šæ¨¡å‹å‚æ•°å½±å“è¾“å‡ºç‰¹å¾\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹å‚æ•°æµ‹è¯•æ€»ç»“:\")\n",
    "successful_models = [name for name, result in results.items() if result[\"success\"]]\n",
    "print(f\"âœ… æˆåŠŸæµ‹è¯•çš„æ¨¡å‹: {len(successful_models)}/{len(models_to_test)}\")\n",
    "\n",
    "if len(successful_models) > 1:\n",
    "    print(\"ğŸ“ˆ ä¸åŒæ¨¡å‹è¾“å‡ºå¯¹æ¯”:\")\n",
    "    for model_name in successful_models:\n",
    "        result = results[model_name]\n",
    "        print(f\"   {model_name}: {result['length']} å­—ç¬¦\")\n",
    "    \n",
    "    print(\"âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡å‹å‚æ•°å½±å“è¾“å‡ºç‰¹å¾\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Šï¼šåªæœ‰ä¸€ä¸ªæ¨¡å‹å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. max_tokens å‚æ•°æµ‹è¯• - è¾“å‡ºé•¿åº¦æ§åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æœ€å¤§ä»¤ç‰Œæ•°æµ‹è¯• (GPT-4o):\n",
      "========================================\n",
      "\n",
      "ğŸ“ æµ‹è¯• max_tokens=50:\n",
      "   å®é™…å›å¤é•¿åº¦: 102 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "   âœ… é•¿åº¦åœ¨é¢„æœŸèŒƒå›´å†…\n",
      "\n",
      "ğŸ“ æµ‹è¯• max_tokens=100:\n",
      "   å®é™…å›å¤é•¿åº¦: 179 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "   âœ… é•¿åº¦åœ¨é¢„æœŸèŒƒå›´å†…\n",
      "\n",
      "ğŸ“ æµ‹è¯• max_tokens=200:\n",
      "   å®é™…å›å¤é•¿åº¦: 378 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "   âœ… é•¿åº¦åœ¨é¢„æœŸèŒƒå›´å†…\n",
      "\n",
      "ğŸ“ æµ‹è¯• max_tokens=500:\n",
      "   å®é™…å›å¤é•¿åº¦: 910 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "   âœ… é•¿åº¦åœ¨é¢„æœŸèŒƒå›´å†…\n",
      "\n",
      "ğŸ“Š max_tokens å‚æ•°æµ‹è¯•æ€»ç»“:\n",
      "âœ… æˆåŠŸæµ‹è¯•çš„ max_tokens å€¼: 4/4\n",
      "ğŸ“ˆ ä¸åŒ max_tokens è¾“å‡ºå¯¹æ¯”:\n",
      "   max_tokens=50: 102 å­—ç¬¦\n",
      "   max_tokens=100: 179 å­—ç¬¦\n",
      "   max_tokens=200: 378 å­—ç¬¦\n",
      "   max_tokens=500: 910 å­—ç¬¦\n",
      "\n",
      "ğŸ’¡ max_tokens å‚æ•°å½±å“åˆ†æ:\n",
      "   - æ§åˆ¶ GPT-4o ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡\n",
      "   - å­—ç¬¦æ•° â‰ˆ tokenæ•° Ã— 2-4 (ä¸­æ–‡)\n",
      "   - ç”¨äºæ§åˆ¶è¾“å‡ºé•¿åº¦å’Œæˆæœ¬\n",
      "   - å»ºè®®å€¼ï¼šçŸ­å›å¤ 50-100ï¼Œä¸­ç­‰ 200-500ï¼Œé•¿å›å¤ 1000+\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šmax_tokens å‚æ•°æ§åˆ¶è¾“å‡ºé•¿åº¦\n"
     ]
    }
   ],
   "source": [
    "# 3. æœ€å¤§ä»¤ç‰Œæ•°æµ‹è¯• - max_tokens - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“ æœ€å¤§ä»¤ç‰Œæ•°æµ‹è¯• (GPT-4o):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æµ‹è¯•ä¸åŒ max_tokens å‚æ•°çš„æ•ˆæœ\n",
    "test_prompt = \"è¯·è¯¦ç»†ä»‹ç» LangChain çš„ä¸»è¦åŠŸèƒ½å’Œåº”ç”¨åœºæ™¯\"\n",
    "\n",
    "max_tokens_values = [50, 100, 200, 500]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for max_tok in max_tokens_values:\n",
    "    print(f\"\\nğŸ“ æµ‹è¯• max_tokens={max_tok}:\")\n",
    "    \n",
    "    try:\n",
    "        # åˆå§‹åŒ–æ¨¡å‹\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=max_tok,\n",
    "        )\n",
    "        \n",
    "        # è°ƒç”¨æ¨¡å‹\n",
    "        response = llm.invoke(test_prompt)\n",
    "        \n",
    "        # è®°å½•ç»“æœ\n",
    "        results[max_tok] = {\n",
    "            \"response\": response.content,\n",
    "            \"length\": len(response.content),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"   å®é™…å›å¤é•¿åº¦: {len(response.content)} å­—ç¬¦\")\n",
    "        print(f\"   çŠ¶æ€: âœ… æˆåŠŸ\")\n",
    "        \n",
    "        # éªŒè¯æ˜¯å¦åœ¨é™åˆ¶èŒƒå›´å†…ï¼ˆæ³¨æ„ï¼šå­—ç¬¦æ•°å’Œtokenæ•°ä¸æ˜¯1:1å¯¹åº”ï¼‰\n",
    "        if len(response.content) <= max_tok * 4:  # ç²—ç•¥ä¼°ç®—ï¼š1ä¸ªtokençº¦2-4ä¸ªä¸­æ–‡å­—ç¬¦\n",
    "            print(f\"   âœ… é•¿åº¦åœ¨é¢„æœŸèŒƒå›´å†…\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  é•¿åº¦å¯èƒ½è¶…å‡ºé¢„æœŸ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   çŠ¶æ€: âŒ å¤±è´¥ - {e}\")\n",
    "        results[max_tok] = {\n",
    "            \"response\": None,\n",
    "            \"length\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šmax_tokens å‚æ•°æ§åˆ¶è¾“å‡ºé•¿åº¦\n",
    "print(f\"\\nğŸ“Š max_tokens å‚æ•°æµ‹è¯•æ€»ç»“:\")\n",
    "successful_values = [val for val, result in results.items() if result[\"success\"]]\n",
    "print(f\"âœ… æˆåŠŸæµ‹è¯•çš„ max_tokens å€¼: {len(successful_values)}/{len(max_tokens_values)}\")\n",
    "\n",
    "if len(successful_values) >= 2:\n",
    "    print(\"ğŸ“ˆ ä¸åŒ max_tokens è¾“å‡ºå¯¹æ¯”:\")\n",
    "    for val in successful_values:\n",
    "        result = results[val]\n",
    "        print(f\"   max_tokens={val}: {result['length']} å­—ç¬¦\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ max_tokens å‚æ•°å½±å“åˆ†æ:\")\n",
    "    print(\"   - æ§åˆ¶ GPT-4o ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡\")\n",
    "    print(\"   - å­—ç¬¦æ•° â‰ˆ tokenæ•° Ã— 2-4 (ä¸­æ–‡)\")\n",
    "    print(\"   - ç”¨äºæ§åˆ¶è¾“å‡ºé•¿åº¦å’Œæˆæœ¬\")\n",
    "    print(\"   - å»ºè®®å€¼ï¼šçŸ­å›å¤ 50-100ï¼Œä¸­ç­‰ 200-500ï¼Œé•¿å›å¤ 1000+\")\n",
    "    print(\"âœ… éªŒè¯é€šè¿‡ï¼šmax_tokens å‚æ•°æ§åˆ¶è¾“å‡ºé•¿åº¦\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Šï¼šmax_tokens å‚æ•°æµ‹è¯•å¤±è´¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å‚æ•°ç»„åˆæµ‹è¯• - æœ€ä½³å®è·µé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ å‚æ•°ç»„åˆæµ‹è¯• (GPT-4o):\n",
      "========================================\n",
      "\n",
      "ğŸ¯ æµ‹è¯•ç»„åˆ 1 - ç²¾ç¡®å›ç­”:\n",
      "   model=gpt-4o, temperature=0.1, max_tokens=100\n",
      "   å›å¤é•¿åº¦: 149 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ¯ æµ‹è¯•ç»„åˆ 2 - å¹³è¡¡å›ç­”:\n",
      "   model=gpt-4o, temperature=0.7, max_tokens=200\n",
      "   å›å¤é•¿åº¦: 303 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ¯ æµ‹è¯•ç»„åˆ 3 - åˆ›é€ æ€§å›ç­”:\n",
      "   model=gpt-4o, temperature=1.0, max_tokens=300\n",
      "   å›å¤é•¿åº¦: 467 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ¯ æµ‹è¯•ç»„åˆ 4 - è½»é‡æ¨¡å‹:\n",
      "   model=gpt-4o-mini, temperature=0.7, max_tokens=200\n",
      "   å›å¤é•¿åº¦: 310 å­—ç¬¦\n",
      "   çŠ¶æ€: âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ¯ æµ‹è¯•ç»„åˆ 5 - å¯¹æ¯”æ¨¡å‹:\n",
      "   model=gpt-3.5-turbo, temperature=0.7, max_tokens=200\n",
      "   çŠ¶æ€: âŒ å¤±è´¥ - Error code: 404 - {'code': 404, 'reason': 'MODEL_NOT_FOUND', 'message': 'model not found', 'metadata': {'reason': 'model: gpt-3.5-turbo not found'}}\n",
      "\n",
      "ğŸ“Š å‚æ•°ç»„åˆæµ‹è¯•æ€»ç»“:\n",
      "âœ… æˆåŠŸæµ‹è¯•çš„å‚æ•°ç»„åˆ: 4/5\n",
      "ğŸ“ˆ ä¸åŒå‚æ•°ç»„åˆè¾“å‡ºå¯¹æ¯”:\n",
      "   ç²¾ç¡®å›ç­”: 149 å­—ç¬¦\n",
      "   å¹³è¡¡å›ç­”: 303 å­—ç¬¦\n",
      "   åˆ›é€ æ€§å›ç­”: 467 å­—ç¬¦\n",
      "   è½»é‡æ¨¡å‹: 310 å­—ç¬¦\n",
      "\n",
      "ğŸ’¡ å‚æ•°ç»„åˆå½±å“åˆ†æ:\n",
      "   - ä½æ¸©åº¦ + çŸ­è¾“å‡º: ç²¾ç¡®ã€ç®€æ´çš„å›ç­”\n",
      "   - ä¸­æ¸©åº¦ + ä¸­ç­‰è¾“å‡º: å¹³è¡¡è´¨é‡å’Œé•¿åº¦\n",
      "   - é«˜æ¸©åº¦ + é•¿è¾“å‡º: åˆ›é€ æ€§ã€è¯¦ç»†çš„å›ç­”\n",
      "   - ä¸åŒæ¨¡å‹: æ€§èƒ½å’Œå“åº”ç‰¹å¾å·®å¼‚\n",
      "âœ… éªŒè¯é€šè¿‡ï¼šå‚æ•°ç»„åˆå½±å“è¾“å‡ºç‰¹å¾\n"
     ]
    }
   ],
   "source": [
    "# 4. å‚æ•°ç»„åˆæµ‹è¯• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”§ å‚æ•°ç»„åˆæµ‹è¯• (GPT-4o):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆçš„æ•ˆæœ\n",
    "test_prompt = \"è¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹\"\n",
    "\n",
    "parameter_combinations = [\n",
    "    {\"model\": \"gpt-4o\", \"temperature\": 0.1, \"max_tokens\": 100, \"desc\": \"ç²¾ç¡®å›ç­”\"},\n",
    "    {\"model\": \"gpt-4o\", \"temperature\": 0.7, \"max_tokens\": 200, \"desc\": \"å¹³è¡¡å›ç­”\"},\n",
    "    {\"model\": \"gpt-4o\", \"temperature\": 1.0, \"max_tokens\": 300, \"desc\": \"åˆ›é€ æ€§å›ç­”\"},\n",
    "    {\"model\": \"gpt-4o-mini\", \"temperature\": 0.7, \"max_tokens\": 200, \"desc\": \"è½»é‡æ¨¡å‹\"},\n",
    "    {\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.7, \"max_tokens\": 200, \"desc\": \"å¯¹æ¯”æ¨¡å‹\"}\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, params in enumerate(parameter_combinations, 1):\n",
    "    print(f\"\\nğŸ¯ æµ‹è¯•ç»„åˆ {i} - {params['desc']}:\")\n",
    "    print(f\"   model={params['model']}, temperature={params['temperature']}, max_tokens={params['max_tokens']}\")\n",
    "    \n",
    "    try:\n",
    "        # åˆå§‹åŒ–æ¨¡å‹\n",
    "        llm = ChatOpenAI(\n",
    "            model=params[\"model\"],\n",
    "            temperature=params[\"temperature\"],\n",
    "            max_tokens=params[\"max_tokens\"],\n",
    "        )\n",
    "        \n",
    "        # è°ƒç”¨æ¨¡å‹\n",
    "        response = llm.invoke(test_prompt)\n",
    "        \n",
    "        # è®°å½•ç»“æœ\n",
    "        combo_key = f\"{params['model']}_t{params['temperature']}_m{params['max_tokens']}\"\n",
    "        results[combo_key] = {\n",
    "            \"response\": response.content,\n",
    "            \"length\": len(response.content),\n",
    "            \"success\": True,\n",
    "            \"desc\": params[\"desc\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"   å›å¤é•¿åº¦: {len(response.content)} å­—ç¬¦\")\n",
    "        print(f\"   çŠ¶æ€: âœ… æˆåŠŸ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   çŠ¶æ€: âŒ å¤±è´¥ - {e}\")\n",
    "        combo_key = f\"{params['model']}_t{params['temperature']}_m{params['max_tokens']}\"\n",
    "        results[combo_key] = {\n",
    "            \"response\": None,\n",
    "            \"length\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"desc\": params[\"desc\"]\n",
    "        }\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šå‚æ•°ç»„åˆå½±å“è¾“å‡ºç‰¹å¾\n",
    "print(f\"\\nğŸ“Š å‚æ•°ç»„åˆæµ‹è¯•æ€»ç»“:\")\n",
    "successful_combos = [key for key, result in results.items() if result[\"success\"]]\n",
    "print(f\"âœ… æˆåŠŸæµ‹è¯•çš„å‚æ•°ç»„åˆ: {len(successful_combos)}/{len(parameter_combinations)}\")\n",
    "\n",
    "if len(successful_combos) >= 2:\n",
    "    print(\"ğŸ“ˆ ä¸åŒå‚æ•°ç»„åˆè¾“å‡ºå¯¹æ¯”:\")\n",
    "    for key in successful_combos:\n",
    "        result = results[key]\n",
    "        print(f\"   {result['desc']}: {result['length']} å­—ç¬¦\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ å‚æ•°ç»„åˆå½±å“åˆ†æ:\")\n",
    "    print(\"   - ä½æ¸©åº¦ + çŸ­è¾“å‡º: ç²¾ç¡®ã€ç®€æ´çš„å›ç­”\")\n",
    "    print(\"   - ä¸­æ¸©åº¦ + ä¸­ç­‰è¾“å‡º: å¹³è¡¡è´¨é‡å’Œé•¿åº¦\")\n",
    "    print(\"   - é«˜æ¸©åº¦ + é•¿è¾“å‡º: åˆ›é€ æ€§ã€è¯¦ç»†çš„å›ç­”\")\n",
    "    print(\"   - ä¸åŒæ¨¡å‹: æ€§èƒ½å’Œå“åº”ç‰¹å¾å·®å¼‚\")\n",
    "    print(\"âœ… éªŒè¯é€šè¿‡ï¼šå‚æ•°ç»„åˆå½±å“è¾“å‡ºç‰¹å¾\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Šï¼šå‚æ•°ç»„åˆæµ‹è¯•å¤±è´¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å‚æ•°è°ƒä¼˜æŒ‡å— - å®é™…åº”ç”¨åœºæ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ GPT-4o å‚æ•°ä¼˜åŒ–å»ºè®®:\n",
      "========================================\n",
      "ğŸ¯ æ¨èå‚æ•°é…ç½®:\n",
      "\n",
      "ğŸ“‹ å¿«é€Ÿå“åº”åœºæ™¯:\n",
      "   model: gpt-4o-mini\n",
      "   temperature: 0.1\n",
      "   max_tokens: 50\n",
      "   ğŸ’­ è½»é‡æ¨¡å‹ + ä½æ¸©åº¦ + çŸ­è¾“å‡º = å¿«é€Ÿå“åº”\n",
      "\n",
      "ğŸ“‹ æ ‡å‡†é—®ç­”åœºæ™¯:\n",
      "   model: gpt-4o\n",
      "   temperature: 0.7\n",
      "   max_tokens: 200\n",
      "   ğŸ’­ ä¸»åŠ›æ¨¡å‹ + å¹³è¡¡æ¸©åº¦ + ä¸­ç­‰è¾“å‡º = æ ‡å‡†æ€§èƒ½\n",
      "\n",
      "ğŸ“‹ åˆ›æ„å†™ä½œåœºæ™¯:\n",
      "   model: gpt-4o\n",
      "   temperature: 1.0\n",
      "   max_tokens: 500\n",
      "   ğŸ’­ ä¸»åŠ›æ¨¡å‹ + é«˜æ¸©åº¦ + é•¿è¾“å‡º = åˆ›é€ æ€§å†…å®¹\n",
      "\n",
      "ğŸ“‹ ä»£ç ç”Ÿæˆåœºæ™¯:\n",
      "   model: gpt-5.1-codex\n",
      "   temperature: 0.3\n",
      "   max_tokens: 300\n",
      "   ğŸ’­ ä»£ç ä¸“ç”¨æ¨¡å‹ + ä½æ¸©åº¦ + ä¸­ç­‰è¾“å‡º = å‡†ç¡®ä»£ç \n",
      "\n",
      "ğŸ“‹ å¤æ‚ä»»åŠ¡åœºæ™¯:\n",
      "   model: gpt-5-pro\n",
      "   temperature: 0.7\n",
      "   max_tokens: 1000\n",
      "   ğŸ’­ é«˜çº§æ¨¡å‹ + å¹³è¡¡æ¸©åº¦ + é•¿è¾“å‡º = å¼ºå¤§èƒ½åŠ›\n",
      "\n",
      "ğŸ§ª æµ‹è¯•æ ‡å‡†é—®ç­”é…ç½®:\n",
      "âœ… æµ‹è¯•æˆåŠŸ: 372 å­—ç¬¦\n",
      "ğŸ“ å›å¤: LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ OpenAI çš„ GPT-4 æˆ–å…¶ä»– LLMï¼‰çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒä¸ºå¼€å‘è€…æä¾›äº†ä¸€å¥—å·¥å…·å’Œæ¨¡å—ï¼Œä½¿å¾—åˆ›å»ºå¤æ‚çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨å˜å¾—æ›´åŠ ç®€å•ã€é«˜æ•ˆã€‚LangChain çš„æ ¸å¿ƒç›®æ ‡æ˜¯å¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ„å»ºåŠ¨æ€ã€å¤šæ­¥éª¤çš„åº”ç”¨ç¨‹åºæ—¶ã€‚\n",
      "\n",
      "### LangChain çš„ä¸»è¦åŠŸèƒ½\n",
      "LangChain æä¾›äº†ä¸€ç³»åˆ—å·¥å…·å’ŒæŠ½è±¡ï¼Œç”¨äºå¤„ç†ä»¥ä¸‹å…³é”®ä»»åŠ¡ï¼š\n",
      "\n",
      "1. **é“¾å¼è°ƒç”¨ï¼ˆChainsï¼‰**ï¼š\n",
      "   LangChain æ”¯æŒå°†å¤šä¸ªè¯­è¨€æ¨¡å‹è°ƒç”¨ç»„åˆæˆä¸€ä¸ªå¤šæ­¥éª¤çš„å·¥ä½œæµã€‚æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºå¯ä»¥ä½œä¸ºä¸‹ä¸€ä¸ªæ­¥éª¤çš„è¾“å…¥ï¼Œç”¨äºå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚\n",
      "\n",
      "2. **ä¸Šä¸‹æ–‡è®°å¿†ï¼ˆMemoryï¼‰**ï¼š\n",
      "   LangChain æä¾›äº†å†…å­˜æ¨¡å—ï¼Œä½¿å¾—åº”ç”¨å¯ä»¥åœ¨å¯¹è¯è¿‡ç¨‹ä¸­ä¿ç•™ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå®ƒ\n",
      "\n",
      "ğŸ“Š å‚æ•°ä¼˜åŒ–æ€»ç»“:\n",
      "âœ… æ ¹æ®ä½¿ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n",
      "âœ… æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´æ¸©åº¦å‚æ•°\n",
      "âœ… æ ¹æ®éœ€æ±‚æ§åˆ¶è¾“å‡ºé•¿åº¦\n",
      "âœ… å¹³è¡¡æ€§èƒ½ã€è´¨é‡å’Œæˆæœ¬\n",
      "âœ… GPT-4o ä½œä¸ºä¸»åŠ›æ¨¡å‹ï¼Œå…¶ä»–æ¨¡å‹ä½œä¸ºè¡¥å……\n"
     ]
    }
   ],
   "source": [
    "# 5. å‚æ•°ä¼˜åŒ–å»ºè®® - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ’¡ GPT-4o å‚æ•°ä¼˜åŒ–å»ºè®®:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# åŸºäºæµ‹è¯•ç»“æœçš„å‚æ•°ä¼˜åŒ–å»ºè®®\n",
    "optimization_scenarios = {\n",
    "    \"å¿«é€Ÿå“åº”åœºæ™¯\": {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 50,\n",
    "        \"reason\": \"è½»é‡æ¨¡å‹ + ä½æ¸©åº¦ + çŸ­è¾“å‡º = å¿«é€Ÿå“åº”\"\n",
    "    },\n",
    "    \"æ ‡å‡†é—®ç­”åœºæ™¯\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 200,\n",
    "        \"reason\": \"ä¸»åŠ›æ¨¡å‹ + å¹³è¡¡æ¸©åº¦ + ä¸­ç­‰è¾“å‡º = æ ‡å‡†æ€§èƒ½\"\n",
    "    },\n",
    "    \"åˆ›æ„å†™ä½œåœºæ™¯\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 1.0,\n",
    "        \"max_tokens\": 500,\n",
    "        \"reason\": \"ä¸»åŠ›æ¨¡å‹ + é«˜æ¸©åº¦ + é•¿è¾“å‡º = åˆ›é€ æ€§å†…å®¹\"\n",
    "    },\n",
    "    \"ä»£ç ç”Ÿæˆåœºæ™¯\": {\n",
    "        \"model\": \"gpt-5.1-codex\",\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 300,\n",
    "        \"reason\": \"ä»£ç ä¸“ç”¨æ¨¡å‹ + ä½æ¸©åº¦ + ä¸­ç­‰è¾“å‡º = å‡†ç¡®ä»£ç \"\n",
    "    },\n",
    "    \"å¤æ‚ä»»åŠ¡åœºæ™¯\": {\n",
    "        \"model\": \"gpt-5-pro\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"reason\": \"é«˜çº§æ¨¡å‹ + å¹³è¡¡æ¸©åº¦ + é•¿è¾“å‡º = å¼ºå¤§èƒ½åŠ›\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ æ¨èå‚æ•°é…ç½®:\")\n",
    "\n",
    "for scenario, config in optimization_scenarios.items():\n",
    "    print(f\"\\nğŸ“‹ {scenario}:\")\n",
    "    print(f\"   model: {config['model']}\")\n",
    "    print(f\"   temperature: {config['temperature']}\")\n",
    "    print(f\"   max_tokens: {config['max_tokens']}\")\n",
    "    print(f\"   ğŸ’­ {config['reason']}\")\n",
    "\n",
    "# æµ‹è¯•ä¸€ä¸ªæ¨èé…ç½®\n",
    "print(f\"\\nğŸ§ª æµ‹è¯•æ ‡å‡†é—®ç­”é…ç½®:\")\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=optimization_scenarios[\"æ ‡å‡†é—®ç­”åœºæ™¯\"][\"model\"],\n",
    "        temperature=optimization_scenarios[\"æ ‡å‡†é—®ç­”åœºæ™¯\"][\"temperature\"],\n",
    "        max_tokens=optimization_scenarios[\"æ ‡å‡†é—®ç­”åœºæ™¯\"][\"max_tokens\"],\n",
    "    )\n",
    "    \n",
    "    test_response = llm.invoke(\"ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ\")\n",
    "    print(f\"âœ… æµ‹è¯•æˆåŠŸ: {len(test_response.content)} å­—ç¬¦\")\n",
    "    print(f\"ğŸ“ å›å¤: {test_response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æµ‹è¯•å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š å‚æ•°ä¼˜åŒ–æ€»ç»“:\")\n",
    "print(\"âœ… æ ¹æ®ä½¿ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹\")\n",
    "print(\"âœ… æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´æ¸©åº¦å‚æ•°\")\n",
    "print(\"âœ… æ ¹æ®éœ€æ±‚æ§åˆ¶è¾“å‡ºé•¿åº¦\")\n",
    "print(\"âœ… å¹³è¡¡æ€§èƒ½ã€è´¨é‡å’Œæˆæœ¬\")\n",
    "print(\"âœ… GPT-4o ä½œä¸ºä¸»åŠ›æ¨¡å‹ï¼Œå…¶ä»–æ¨¡å‹ä½œä¸ºè¡¥å……\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ æ ¸å¿ƒå‚æ•°å­¦ä¹ æ€»ç»“:\n",
      "========================================\n",
      "âœ… model å‚æ•°ï¼šæ§åˆ¶æ¨¡å‹é€‰æ‹©å’Œæ€§èƒ½ç‰¹å¾\n",
      "âœ… temperature å‚æ•°ï¼šæ§åˆ¶è¾“å‡ºéšæœºæ€§å’Œåˆ›é€ æ€§\n",
      "âœ… max_tokens å‚æ•°ï¼šæ§åˆ¶è¾“å‡ºé•¿åº¦å’Œèµ„æºæ¶ˆè€—\n",
      "âœ… å‚æ•°ç»„åˆï¼šæ ¹æ®åº”ç”¨åœºæ™¯ä¼˜åŒ–é…ç½®\n",
      "âœ… æœ€ä½³å®è·µï¼šgpt-3.5-turbo + temperature=0.7 + max_tokens=300\n",
      "\n",
      "ğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: 5/5 é¡¹\n",
      "\n",
      "ğŸ’¡ å…³é”®è¦ç‚¹:\n",
      "1. model: gpt-3.5-turbo(å¿«é€Ÿ) vs gpt-4(é«˜è´¨é‡)\n",
      "2. temperature: 0.1(ç¡®å®šæ€§) vs 0.7(å¹³è¡¡) vs 1.0(åˆ›é€ æ€§)\n",
      "3. max_tokens: æ ¹æ®å…·ä½“éœ€æ±‚è®¾ç½®ï¼Œæ§åˆ¶æˆæœ¬å’Œå“åº”é€Ÿåº¦\n",
      "4. å‚æ•°è°ƒä¼˜: ä¸åŒåœºæ™¯éœ€è¦ä¸åŒçš„å‚æ•°ç»„åˆ\n",
      "\n",
      "ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\n",
      "1. æ·±å…¥å­¦ä¹  PromptTemplate é«˜çº§ç”¨æ³•\n",
      "2. æŒæ¡ ChatPromptTemplate å¤šè§’è‰²å¯¹è¯\n",
      "3. å­¦ä¹  LCEL é“¾å¼è°ƒç”¨è¯­æ³•\n",
      "4. æ¢ç´¢ç»“æ„åŒ–è¾“å‡ºå’Œæµå¼è¾“å‡º\n",
      "\n",
      "ğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: â€œå‚æ•°å­¦ä¹ å®ŒæˆéªŒè¯â€é€šå¸¸æŒ‡çš„æ˜¯åœ¨æœºå™¨å­¦ä¹ æˆ–æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°é€šè¿‡è®­ç»ƒå·²ç»å®Œæˆäº†ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶ä¸”é€šè¿‡ä¸€å®šçš„éªŒè¯è¿‡ç¨‹ç¡®è®¤äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸€è¿‡ç¨‹çš„å®Œæˆè¡¨æ˜æ¨¡å‹å·²ç»è¾¾åˆ°\n",
      "\n",
      "âœ… æ ¸å¿ƒå‚æ•°å­¦ä¹ å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸éªŒè¯ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ æ ¸å¿ƒå‚æ•°å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… model å‚æ•°ï¼šæ§åˆ¶æ¨¡å‹é€‰æ‹©å’Œæ€§èƒ½ç‰¹å¾\",\n",
    "    \"âœ… temperature å‚æ•°ï¼šæ§åˆ¶è¾“å‡ºéšæœºæ€§å’Œåˆ›é€ æ€§\",\n",
    "    \"âœ… max_tokens å‚æ•°ï¼šæ§åˆ¶è¾“å‡ºé•¿åº¦å’Œèµ„æºæ¶ˆè€—\",\n",
    "    \"âœ… å‚æ•°ç»„åˆï¼šæ ¹æ®åº”ç”¨åœºæ™¯ä¼˜åŒ–é…ç½®\",\n",
    "    \"âœ… æœ€ä½³å®è·µï¼šgpt-3.5-turbo + temperature=0.7 + max_tokens=300\"\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/5 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®è¦ç‚¹:\")\n",
    "print(\"1. model: gpt-3.5-turbo(å¿«é€Ÿ) vs gpt-4(é«˜è´¨é‡)\")\n",
    "print(\"2. temperature: 0.1(ç¡®å®šæ€§) vs 0.7(å¹³è¡¡) vs 1.0(åˆ›é€ æ€§)\")\n",
    "print(\"3. max_tokens: æ ¹æ®å…·ä½“éœ€æ±‚è®¾ç½®ï¼Œæ§åˆ¶æˆæœ¬å’Œå“åº”é€Ÿåº¦\")\n",
    "print(\"4. å‚æ•°è°ƒä¼˜: ä¸åŒåœºæ™¯éœ€è¦ä¸åŒçš„å‚æ•°ç»„åˆ\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  PromptTemplate é«˜çº§ç”¨æ³•\")\n",
    "print(\"2. æŒæ¡ ChatPromptTemplate å¤šè§’è‰²å¯¹è¯\")\n",
    "print(\"3. å­¦ä¹  LCEL é“¾å¼è°ƒç”¨è¯­æ³•\")\n",
    "print(\"4. æ¢ç´¢ç»“æ„åŒ–è¾“å‡ºå’Œæµå¼è¾“å‡º\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ ¸å¿ƒå‚æ•°åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    response = llm.invoke(\"å‚æ•°å­¦ä¹ å®ŒæˆéªŒè¯\")\n",
    "    print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ: {response.content}\")\n",
    "    print(\"\\nâœ… æ ¸å¿ƒå‚æ•°å­¦ä¹ å®Œæˆï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
