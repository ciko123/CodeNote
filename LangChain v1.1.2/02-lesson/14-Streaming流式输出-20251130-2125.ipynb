{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 - Streaming æµå¼è¾“å‡º\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹ é€šä¹‰åƒé—®æµå¼è¾“å‡ºæŠ€æœ¯ï¼Œæ”¯æŒå®æ—¶æŸ¥çœ‹ç”Ÿæˆè¿‡ç¨‹ï¼Œæå‡ç”¨æˆ·ä½“éªŒ\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£æµå¼è¾“å‡ºåŸç†å’Œä¼˜åŠ¿\n",
    "- æŒæ¡ stream æ–¹æ³•çš„ä½¿ç”¨\n",
    "- èƒ½å®ç°é€šä¹‰åƒé—®åŸºç¡€æµå¼åŠŸèƒ½\n",
    "- æŒæ¡æµå¼æ•°æ®çš„å¤„ç†å’Œæ˜¾ç¤º\n",
    "\n",
    "## ğŸ”‘ å‰ç½®è¦æ±‚\n",
    "**æ³¨æ„**ï¼šéœ€è¦å…ˆå®Œæˆ LCEL ç®¡é“è¯­æ³•å­¦ä¹ ï¼Œç†è§£é“¾å¼ç»„åˆåŸç†\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æµå¼è¾“å‡ºåŸºç¡€æ¦‚å¿µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµå¼è¾“å‡ºåŸºç¡€æ¦‚å¿µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸŒŠ æµå¼è¾“å‡ºåŸºç¡€æ¦‚å¿µç†è§£:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥ OpenAI API é…ç½®\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ æœªæ‰¾åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡\")\n",
    "    print(\"   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=your_key_here\")\n",
    "else:\n",
    "    print(f\"âœ… OpenAI API Key å·²é…ç½® (é•¿åº¦: {len(openai_api_key)})\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ æµå¼è¾“å‡ºæ ¸å¿ƒæ¦‚å¿µ:\")\n",
    "    print(f\"   1. å®æ—¶ç”Ÿæˆï¼šé€ä¸ª token è¿”å›ç»“æœ\")\n",
    "    print(f\"   2. ç”¨æˆ·ä½“éªŒï¼šå‡å°‘ç­‰å¾…æ—¶é—´\")\n",
    "    print(f\"   3. å†…å­˜æ•ˆç‡ï¼šä¸éœ€è¦ç­‰å¾…å®Œæ•´å“åº”\")\n",
    "    print(f\"   4. è¿­ä»£å™¨ï¼šè¿”å›å¯è¿­ä»£çš„æµå¯¹è±¡\")\n",
    "    \n",
    "    # åˆ›å»ºåŸºç¡€ LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ—ï¸  åˆ›å»ºçš„ç»„ä»¶:\")\n",
    "    print(f\"   1. LLM: {type(llm).__name__}\")\n",
    "    print(f\"   2. æµå¼æ”¯æŒ: {hasattr(llm, 'stream')}\")\n",
    "    \n",
    "    # å¯¹æ¯” invoke å’Œ stream\n",
    "    print(f\"\\nğŸ” invoke vs stream å¯¹æ¯”:\")\n",
    "    \n",
    "    test_prompt = \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½\"\n",
    "    \n",
    "    # 1. ä¼ ç»Ÿ invoke æ–¹æ³•\n",
    "    print(f\"\\n   1. ä¼ ç»Ÿ invoke æ–¹æ³•:\")\n",
    "    start_time = time.time()\n",
    "    invoke_result = llm.invoke(test_prompt)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"      æ‰§è¡Œæ—¶é—´: {(end_time - start_time)*1000:.1f} ms\")\n",
    "    print(f\"      ç»“æœé•¿åº¦: {len(invoke_result.content)} å­—ç¬¦\")\n",
    "    print(f\"      å®Œæ•´å†…å®¹: {invoke_result.content}\")\n",
    "    \n",
    "    # 2. æµå¼ stream æ–¹æ³•\n",
    "    print(f\"\\n   2. æµå¼ stream æ–¹æ³•:\")\n",
    "    start_time = time.time()\n",
    "    stream_chunks = []\n",
    "    chunk_count = 0\n",
    "    \n",
    "    for chunk in llm.stream(test_prompt):\n",
    "        stream_chunks.append(chunk)\n",
    "        chunk_count += 1\n",
    "        print(f\"      Token {chunk_count}: {chunk.content}\", end=\"\")\n",
    "        time.sleep(0.1)  # æ¨¡æ‹Ÿæ˜¾ç¤ºå»¶è¿Ÿ\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n      æµå¼æ‰§è¡Œæ—¶é—´: {(end_time - start_time)*1000:.1f} ms\")\n",
    "    print(f\"      Token æ•°é‡: {chunk_count}\")\n",
    "    print(f\"      åˆå¹¶å†…å®¹: {''.join(chunk.content for chunk in stream_chunks)}\")\n",
    "    \n",
    "    # éªŒè¯ç»“æœä¸€è‡´æ€§\n",
    "    stream_content = ''.join(chunk.content for chunk in stream_chunks)\n",
    "    content_match = stream_content == invoke_result.content\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ç»“æœå¯¹æ¯”:\")\n",
    "    print(f\"   å†…å®¹ä¸€è‡´æ€§: {content_match}\")\n",
    "    print(f\"   invoke é•¿åº¦: {len(invoke_result.content)}\")\n",
    "    print(f\"   stream é•¿åº¦: {len(stream_content)}\")\n",
    "    \n",
    "    # éªŒè¯ç‚¹ï¼šæµå¼è¾“å‡ºåŸºç¡€æ¦‚å¿µæ­£ç¡®\n",
    "    assert chunk_count > 0, \"æµå¼è¾“å‡ºæ²¡æœ‰äº§ç”Ÿ token\"\n",
    "    assert len(stream_content) > 0, \"æµå¼è¾“å‡ºå†…å®¹ä¸ºç©º\"\n",
    "    assert content_match, \"æµå¼å’Œä¼ ç»Ÿè¾“å‡ºå†…å®¹ä¸ä¸€è‡´\"\n",
    "    print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæµå¼è¾“å‡ºåŸºç¡€æ¦‚å¿µç†è§£æ­£ç¡®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LCEL ç®¡é“æµå¼è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL ç®¡é“æµå¼è¾“å‡º - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”— LCEL ç®¡é“æµå¼è¾“å‡ºæµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆ›å»º LCEL ç®¡é“ç»„ä»¶\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=60\n",
    "        )\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\"],\n",
    "            template=\"è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹ {topic}ï¼Œåˆ†3ä¸ªè¦ç‚¹è¯´æ˜ã€‚\"\n",
    "        )\n",
    "        \n",
    "        parser = StrOutputParser()\n",
    "        \n",
    "        print(f\"ğŸ“ åˆ›å»ºçš„ç»„ä»¶:\")\n",
    "        print(f\"   1. Prompt: {type(prompt).__name__}\")\n",
    "        print(f\"   2. LLM: {type(llm).__name__}\")\n",
    "        print(f\"   3. Parser: {type(parser).__name__}\")\n",
    "        \n",
    "        # æ„å»º LCEL ç®¡é“\n",
    "        chain = prompt | llm | parser\n",
    "        \n",
    "        print(f\"\\nğŸ”— æ„å»º LCEL ç®¡é“:\")\n",
    "        print(f\"   ç®¡é“ç±»å‹: {type(chain)}\")\n",
    "        print(f\"   æµå¼æ”¯æŒ: {hasattr(chain, 'stream')}\")\n",
    "        \n",
    "        # 1. ç®¡é“ invoke è°ƒç”¨\n",
    "        print(f\"\\nğŸ§ª 1. ç®¡é“ invoke è°ƒç”¨:\")\n",
    "        \n",
    "        test_topic = \"æœºå™¨å­¦ä¹ \"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        invoke_result = chain.invoke({\"topic\": test_topic})\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"   ä¸»é¢˜: {test_topic}\")\n",
    "        print(f\"   æ‰§è¡Œæ—¶é—´: {(end_time - start_time)*1000:.1f} ms\")\n",
    "        print(f\"   ç»“æœé•¿åº¦: {len(invoke_result)} å­—ç¬¦\")\n",
    "        print(f\"   å®Œæ•´å†…å®¹: {invoke_result}\")\n",
    "        \n",
    "        # 2. ç®¡é“ stream è°ƒç”¨\n",
    "        print(f\"\\nğŸ§ª 2. ç®¡é“ stream è°ƒç”¨:\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        stream_chunks = []\n",
    "        chunk_count = 0\n",
    "        \n",
    "        print(f\"   æµå¼è¾“å‡ºå¼€å§‹:\")\n",
    "        for chunk in chain.stream({\"topic\": test_topic}):\n",
    "            stream_chunks.append(chunk)\n",
    "            chunk_count += 1\n",
    "            print(f\"{chunk}\", end=\"\", flush=True)\n",
    "            time.sleep(0.05)  # æ¨¡æ‹Ÿå®æ—¶æ˜¾ç¤º\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"\\n\\n   æµå¼æ‰§è¡Œæ—¶é—´: {(end_time - start_time)*1000:.1f} ms\")\n",
    "        print(f\"   Token æ•°é‡: {chunk_count}\")\n",
    "        \n",
    "        # åˆå¹¶æµå¼ç»“æœ\n",
    "        stream_content = ''.join(stream_chunks)\n",
    "        print(f\"   åˆå¹¶å†…å®¹: {stream_content}\")\n",
    "        \n",
    "        # 3. ä¸åŒä¸»é¢˜çš„æµå¼æµ‹è¯•\n",
    "        print(f\"\\nğŸ§ª 3. ä¸åŒä¸»é¢˜çš„æµå¼æµ‹è¯•:\")\n",
    "        \n",
    "        topics = [\"æ·±åº¦å­¦ä¹ \", \"è‡ªç„¶è¯­è¨€å¤„ç†\", \"è®¡ç®—æœºè§†è§‰\"]\n",
    "        \n",
    "        for i, topic in enumerate(topics, 1):\n",
    "            print(f\"\\n   ä¸»é¢˜ {i}: {topic}\")\n",
    "            print(f\"   æµå¼è¾“å‡º: \", end=\"\")\n",
    "            \n",
    "            topic_chunks = []\n",
    "            for chunk in chain.stream({\"topic\": topic}):\n",
    "                topic_chunks.append(chunk)\n",
    "                print(f\"{chunk}\", end=\"\", flush=True)\n",
    "            \n",
    "            topic_content = ''.join(topic_chunks)\n",
    "            print(f\"\\n   è¾“å‡ºé•¿åº¦: {len(topic_content)} å­—ç¬¦\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šLCEL ç®¡é“æµå¼è¾“å‡ºæ­£ç¡®\n",
    "        assert chunk_count > 0, \"ç®¡é“æµå¼è¾“å‡ºæ²¡æœ‰äº§ç”Ÿ token\"\n",
    "        assert len(stream_content) > 0, \"ç®¡é“æµå¼è¾“å‡ºå†…å®¹ä¸ºç©º\"\n",
    "        assert stream_content == invoke_result, \"ç®¡é“æµå¼å’Œä¼ ç»Ÿè¾“å‡ºä¸ä¸€è‡´\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šLCEL ç®¡é“æµå¼è¾“å‡ºæ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ LCEL ç®¡é“æµå¼è¾“å‡ºå¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æµå¼æ•°æ®å¤„ç†å’Œæ˜¾ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµå¼æ•°æ®å¤„ç†å’Œæ˜¾ç¤º - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“Š æµå¼æ•°æ®å¤„ç†å’Œæ˜¾ç¤ºæµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆ›å»ºåŸºç¡€ç®¡é“\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=80\n",
    "        )\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"item\", \"count\"],\n",
    "            template=\"è¯·åˆ—å‡º {count} ä¸ªå…³äº {item} çš„æœ‰è¶£äº‹å®ã€‚\"\n",
    "        )\n",
    "        \n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        \n",
    "        print(f\"ğŸ“ åˆ›å»ºçš„ç®¡é“:\")\n",
    "        print(f\"   æ¨¡æ¿: {prompt.template}\")\n",
    "        print(f\"   å˜é‡: {prompt.input_variables}\")\n",
    "        \n",
    "        # 1. åŸºç¡€æµå¼æ˜¾ç¤º\n",
    "        print(f\"\\nğŸ§ª 1. åŸºç¡€æµå¼æ˜¾ç¤º:\")\n",
    "        \n",
    "        def basic_stream_display(chain, inputs, description=\"\"):\n",
    "            \"\"\"åŸºç¡€æµå¼æ˜¾ç¤ºå‡½æ•°\"\"\"\n",
    "            print(f\"   {description}æµå¼è¾“å‡º: \", end=\"\")\n",
    "            \n",
    "            chunks = []\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for chunk in chain.stream(inputs):\n",
    "                chunks.append(chunk)\n",
    "                print(f\"{chunk}\", end=\"\", flush=True)\n",
    "                time.sleep(0.03)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            full_content = ''.join(chunks)\n",
    "            \n",
    "            print(f\"\\n   æ‰§è¡Œæ—¶é—´: {(end_time - start_time)*1000:.1f} ms\")\n",
    "            print(f\"   Token æ•°é‡: {len(chunks)}\")\n",
    "            print(f\"   å†…å®¹é•¿åº¦: {len(full_content)} å­—ç¬¦\")\n",
    "            \n",
    "            return chunks, full_content\n",
    "        \n",
    "        # æµ‹è¯•åŸºç¡€æ˜¾ç¤º\n",
    "        chunks1, content1 = basic_stream_display(\n",
    "            chain, \n",
    "            {\"item\": \"å¤ªç©ºæ¢ç´¢\", \"count\": 3},\n",
    "            \"å¤ªç©ºæ¢ç´¢\"\n",
    "        )\n",
    "        \n",
    "        # 2. å¸¦ç»Ÿè®¡ä¿¡æ¯çš„æµå¼æ˜¾ç¤º\n",
    "        print(f\"\\nğŸ§ª 2. å¸¦ç»Ÿè®¡ä¿¡æ¯çš„æµå¼æ˜¾ç¤º:\")\n",
    "        \n",
    "        def enhanced_stream_display(chain, inputs, description=\"\"):\n",
    "            \"\"\"å¢å¼ºæµå¼æ˜¾ç¤ºå‡½æ•°\"\"\"\n",
    "            print(f\"   {description}å¢å¼ºæµå¼è¾“å‡º:\")\n",
    "            \n",
    "            chunks = []\n",
    "            total_chars = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            print(f\"   å¼€å§‹è¾“å‡º: \", end=\"\")\n",
    "            for i, chunk in enumerate(chain.stream(inputs), 1):\n",
    "                chunks.append(chunk)\n",
    "                total_chars += len(chunk)\n",
    "                \n",
    "                # æ˜¾ç¤ºè¿›åº¦\n",
    "                print(f\"{chunk}\", end=\"\", flush=True)\n",
    "                \n",
    "                # æ¯10ä¸ªtokenæ˜¾ç¤ºç»Ÿè®¡\n",
    "                if i % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = total_chars / elapsed if elapsed > 0 else 0\n",
    "                    print(f\" [Token:{i}, å­—ç¬¦:{total_chars}, é€Ÿåº¦:{rate:.1f}å­—ç¬¦/s]\", end=\"\")\n",
    "                \n",
    "                time.sleep(0.02)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            full_content = ''.join(chunks)\n",
    "            \n",
    "            print(f\"\\n   æœ€ç»ˆç»Ÿè®¡:\")\n",
    "            print(f\"     æ€»æ—¶é—´: {(end_time - start_time)*1000:.1f} ms\")\n",
    "            print(f\"     æ€»Token: {len(chunks)}\")\n",
    "            print(f\"     æ€»å­—ç¬¦: {total_chars}\")\n",
    "            print(f\"     å¹³å‡é€Ÿåº¦: {total_chars/(end_time - start_time):.1f} å­—ç¬¦/s\")\n",
    "            \n",
    "            return chunks, full_content\n",
    "        \n",
    "        # æµ‹è¯•å¢å¼ºæ˜¾ç¤º\n",
    "        chunks2, content2 = enhanced_stream_display(\n",
    "            chain,\n",
    "            {\"item\": \"æµ·æ´‹ç”Ÿç‰©\", \"count\": 4},\n",
    "            \"æµ·æ´‹ç”Ÿç‰©\"\n",
    "        )\n",
    "        \n",
    "        # 3. ç»“æ„åŒ–æµå¼æ•°æ®å¤„ç†\n",
    "        print(f\"\\nğŸ§ª 3. ç»“æ„åŒ–æµå¼æ•°æ®å¤„ç†:\")\n",
    "        \n",
    "        def structured_stream_processor(chain, inputs, description=\"\"):\n",
    "            \"\"\"ç»“æ„åŒ–æµå¼æ•°æ®å¤„ç†\"\"\"\n",
    "            print(f\"   {description}ç»“æ„åŒ–å¤„ç†:\")\n",
    "            \n",
    "            stream_data = {\n",
    "                \"chunks\": [],\n",
    "                \"timestamps\": [],\n",
    "                \"chunk_sizes\": [],\n",
    "                \"cumulative_length\": []\n",
    "            }\n",
    "            \n",
    "            start_time = time.time()\n",
    "            cumulative_length = 0\n",
    "            \n",
    "            print(f\"   æµå¼è¾“å‡º: \", end=\"\")\n",
    "            for chunk in chain.stream(inputs):\n",
    "                current_time = time.time() - start_time\n",
    "                chunk_size = len(chunk)\n",
    "                cumulative_length += chunk_size\n",
    "                \n",
    "                stream_data[\"chunks\"].append(chunk)\n",
    "                stream_data[\"timestamps\"].append(current_time)\n",
    "                stream_data[\"chunk_sizes\"].append(chunk_size)\n",
    "                stream_data[\"cumulative_length\"].append(cumulative_length)\n",
    "                \n",
    "                print(f\"{chunk}\", end=\"\", flush=True)\n",
    "                time.sleep(0.03)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯\n",
    "            print(f\"\\n   ç»“æ„åŒ–ç»Ÿè®¡:\")\n",
    "            print(f\"     æ€»æ‰§è¡Œæ—¶é—´: {(end_time - start_time)*1000:.1f} ms\")\n",
    "            print(f\"     æ€»Chunkæ•°: {len(stream_data['chunks'])}\")\n",
    "            print(f\"     å¹³å‡Chunkå¤§å°: {sum(stream_data['chunk_sizes'])/len(stream_data['chunk_sizes']):.1f}\")\n",
    "            print(f\"     æœ€å¤§Chunk: {max(stream_data['chunk_sizes'])}\")\n",
    "            print(f\"     æœ€å°Chunk: {min(stream_data['chunk_sizes'])}\")\n",
    "            \n",
    "            return stream_data\n",
    "        \n",
    "        # æµ‹è¯•ç»“æ„åŒ–å¤„ç†\n",
    "        structured_data = structured_stream_processor(\n",
    "            chain,\n",
    "            {\"item\": \"é‡å­è®¡ç®—\", \"count\": 2},\n",
    "            \"é‡å­è®¡ç®—\"\n",
    "        )\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šæµå¼æ•°æ®å¤„ç†å’Œæ˜¾ç¤ºæ­£ç¡®\n",
    "        assert len(chunks1) > 0, \"åŸºç¡€æµå¼æ˜¾ç¤ºå¤±è´¥\"\n",
    "        assert len(chunks2) > 0, \"å¢å¼ºæµå¼æ˜¾ç¤ºå¤±è´¥\"\n",
    "        assert len(structured_data[\"chunks\"]) > 0, \"ç»“æ„åŒ–æµå¼å¤„ç†å¤±è´¥\"\n",
    "        assert len(content1) > 0, \"åŸºç¡€æµå¼å†…å®¹ä¸ºç©º\"\n",
    "        assert len(content2) > 0, \"å¢å¼ºæµå¼å†…å®¹ä¸ºç©º\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæµå¼æ•°æ®å¤„ç†å’Œæ˜¾ç¤ºæ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æµå¼æ•°æ®å¤„ç†å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æµå¼è¾“å‡ºæ€§èƒ½ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµå¼è¾“å‡ºæ€§èƒ½ä¼˜åŒ– - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âš¡ æµå¼è¾“å‡ºæ€§èƒ½ä¼˜åŒ–æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆ›å»ºä¸åŒé…ç½®çš„ LLM\n",
    "        llm_fast = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=30,\n",
    "            streaming=True\n",
    "        )\n",
    "        \n",
    "        llm_slow = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=80,\n",
    "            streaming=True\n",
    "        )\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\"],\n",
    "            template=\"è¯·ç®€å•ä»‹ç» {topic}ã€‚\"\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºä¸åŒé•¿åº¦çš„ç®¡é“\n",
    "        fast_chain = prompt | llm_fast | StrOutputParser()\n",
    "        slow_chain = prompt | llm_slow | StrOutputParser()\n",
    "        \n",
    "        print(f\"ğŸ“ åˆ›å»ºçš„æ€§èƒ½æµ‹è¯•ç»„ä»¶:\")\n",
    "        print(f\"   1. å¿«é€Ÿç®¡é“: max_tokens=30\")\n",
    "        print(f\"   2. æ…¢é€Ÿç®¡é“: max_tokens=80\")\n",
    "        \n",
    "        # 1. åŸºç¡€æ€§èƒ½å¯¹æ¯”\n",
    "        print(f\"\\nğŸ§ª 1. åŸºç¡€æ€§èƒ½å¯¹æ¯”:\")\n",
    "        \n",
    "        def measure_stream_performance(chain, inputs, description=\"\"):\n",
    "            \"\"\"æµ‹é‡æµå¼æ€§èƒ½\"\"\"\n",
    "            print(f\"   {description}æ€§èƒ½æµ‹è¯•:\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            chunks = []\n",
    "            first_token_time = None\n",
    "            \n",
    "            for chunk in chain.stream(inputs):\n",
    "                if first_token_time is None:\n",
    "                    first_token_time = time.time()\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_time = end_time - start_time\n",
    "            first_token_latency = first_token_time - start_time if first_token_time else 0\n",
    "            \n",
    "            print(f\"     æ€»æ—¶é—´: {total_time*1000:.1f} ms\")\n",
    "            print(f\"     é¦–Tokenå»¶è¿Ÿ: {first_token_latency*1000:.1f} ms\")\n",
    "            print(f\"     Tokenæ•°é‡: {len(chunks)}\")\n",
    "            print(f\"     å¹³å‡Tokené—´éš”: {(total_time/len(chunks))*1000:.1f} ms\" if chunks else \"æ— Token\")\n",
    "            print(f\"     ååé‡: {len(chunks)/total_time:.1f} Token/s\" if total_time > 0 else \"æ— æ•°æ®\")\n",
    "            \n",
    "            return {\n",
    "                \"total_time\": total_time,\n",
    "                \"first_token_latency\": first_token_latency,\n",
    "                \"chunk_count\": len(chunks),\n",
    "                \"throughput\": len(chunks)/total_time if total_time > 0 else 0\n",
    "            }\n",
    "        \n",
    "        # æ€§èƒ½æµ‹è¯•\n",
    "        test_topic = \"äººå·¥æ™ºèƒ½\"\n",
    "        \n",
    "        fast_stats = measure_stream_performance(\n",
    "            fast_chain, \n",
    "            {\"topic\": test_topic}, \n",
    "            \"å¿«é€Ÿç®¡é“\"\n",
    "        )\n",
    "        \n",
    "        slow_stats = measure_stream_performance(\n",
    "            slow_chain, \n",
    "            {\"topic\": test_topic}, \n",
    "            \"æ…¢é€Ÿç®¡é“\"\n",
    "        )\n",
    "        \n",
    "        # 2. å¹¶å‘æµå¼å¤„ç†\n",
    "        print(f\"\\nğŸ§ª 2. å¹¶å‘æµå¼å¤„ç†:\")\n",
    "        \n",
    "        def concurrent_stream_test(chain, inputs_list, max_workers=3):\n",
    "            \"\"\"å¹¶å‘æµå¼æµ‹è¯•\"\"\"\n",
    "            print(f\"   å¹¶å‘æµ‹è¯• (æœ€å¤§å¹¶å‘: {max_workers}):\")\n",
    "            \n",
    "            def single_stream_task(inputs):\n",
    "                start_time = time.time()\n",
    "                chunks = []\n",
    "                for chunk in chain.stream(inputs):\n",
    "                    chunks.append(chunk)\n",
    "                end_time = time.time()\n",
    "                return {\n",
    "                    \"chunks\": chunks,\n",
    "                    \"time\": end_time - start_time,\n",
    "                    \"count\": len(chunks)\n",
    "                }\n",
    "            \n",
    "            # ä¸²è¡Œæ‰§è¡Œ\n",
    "            start_time = time.time()\n",
    "            serial_results = []\n",
    "            for inputs in inputs_list:\n",
    "                result = single_stream_task(inputs)\n",
    "                serial_results.append(result)\n",
    "            serial_time = time.time() - start_time\n",
    "            \n",
    "            # å¹¶å‘æ‰§è¡Œ\n",
    "            start_time = time.time()\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                concurrent_results = list(executor.map(single_stream_task, inputs_list))\n",
    "            concurrent_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"     ä¸²è¡Œæ—¶é—´: {serial_time*1000:.1f} ms\")\n",
    "            print(f\"     å¹¶å‘æ—¶é—´: {concurrent_time*1000:.1f} ms\")\n",
    "            print(f\"     æ€§èƒ½æå‡: {serial_time/concurrent_time:.1f}x\")\n",
    "            \n",
    "            return serial_results, concurrent_results\n",
    "        \n",
    "        # å¹¶å‘æµ‹è¯•\n",
    "        topics = [\n",
    "            {\"topic\": \"æœºå™¨å­¦ä¹ \"},\n",
    "            {\"topic\": \"æ·±åº¦å­¦ä¹ \"},\n",
    "            {\"topic\": \"è‡ªç„¶è¯­è¨€å¤„ç†\"}\n",
    "        ]\n",
    "        \n",
    "        serial_results, concurrent_results = concurrent_stream_test(\n",
    "            fast_chain, topics, max_workers=3\n",
    "        )\n",
    "        \n",
    "        # 3. å†…å­˜ä½¿ç”¨ä¼˜åŒ–\n",
    "        print(f\"\\nğŸ§ª 3. å†…å­˜ä½¿ç”¨ä¼˜åŒ–:\")\n",
    "        \n",
    "        def memory_efficient_stream(chain, inputs, max_buffer_size=10):\n",
    "            \"\"\"å†…å­˜é«˜æ•ˆæµå¼å¤„ç†\"\"\"\n",
    "            print(f\"   å†…å­˜é«˜æ•ˆæµå¼å¤„ç† (ç¼“å†²åŒºå¤§å°: {max_buffer_size}):\")\n",
    "            \n",
    "            buffer = []\n",
    "            processed_count = 0\n",
    "            \n",
    "            print(f\"   æµå¼è¾“å‡º: \", end=\"\")\n",
    "            for chunk in chain.stream(inputs):\n",
    "                buffer.append(chunk)\n",
    "                \n",
    "                # å½“ç¼“å†²åŒºæ»¡æ—¶ï¼Œå¤„ç†å¹¶æ¸…ç©º\n",
    "                if len(buffer) >= max_buffer_size:\n",
    "                    # æ¨¡æ‹Ÿå¤„ç†ç¼“å†²åŒºå†…å®¹\n",
    "                    buffered_content = ''.join(buffer)\n",
    "                    print(f\"{buffered_content}\", end=\"\", flush=True)\n",
    "                    processed_count += len(buffer)\n",
    "                    buffer.clear()  # æ¸…ç©ºç¼“å†²åŒº\n",
    "                \n",
    "                time.sleep(0.02)\n",
    "            \n",
    "            # å¤„ç†å‰©ä½™å†…å®¹\n",
    "            if buffer:\n",
    "                remaining_content = ''.join(buffer)\n",
    "                print(f\"{remaining_content}\", end=\"\", flush=True)\n",
    "                processed_count += len(buffer)\n",
    "            \n",
    "            print(f\"\\n   å¤„ç†çš„Tokenæ•°é‡: {processed_count}\")\n",
    "            print(f\"   æœ€å¤§å†…å­˜ä½¿ç”¨: {max_buffer_size} ä¸ªToken\")\n",
    "            \n",
    "            return processed_count\n",
    "        \n",
    "        # å†…å­˜ä¼˜åŒ–æµ‹è¯•\n",
    "        memory_result = memory_efficient_stream(\n",
    "            slow_chain,\n",
    "            {\"topic\": \"æ•°æ®ç§‘å­¦\"},\n",
    "            max_buffer_size=5\n",
    "        )\n",
    "        \n",
    "        # 4. æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "        print(f\"\\nğŸ’¡ æµå¼è¾“å‡ºæ€§èƒ½ä¼˜åŒ–å»ºè®®:\")\n",
    "        print(f\"   1. æ§åˆ¶max_tokensé¿å…è¿‡é•¿è¾“å‡º\")\n",
    "        print(f\"   2. ä½¿ç”¨ç¼“å†²åŒºç®¡ç†å†…å­˜ä½¿ç”¨\")\n",
    "        print(f\"   3. å¹¶å‘å¤„ç†æé«˜ååé‡\")\n",
    "        print(f\"   4. ç›‘æ§é¦–Tokenå»¶è¿Ÿå’Œååé‡\")\n",
    "        print(f\"   5. åˆç†è®¾ç½®temperatureå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šæµå¼è¾“å‡ºæ€§èƒ½ä¼˜åŒ–æ­£ç¡®\n",
    "        assert fast_stats[\"chunk_count\"] > 0, \"å¿«é€Ÿç®¡é“æ— è¾“å‡º\"\n",
    "        assert slow_stats[\"chunk_count\"] > 0, \"æ…¢é€Ÿç®¡é“æ— è¾“å‡º\"\n",
    "        assert fast_stats[\"throughput\"] > 0, \"å¿«é€Ÿç®¡é“ååé‡ä¸º0\"\n",
    "        assert slow_stats[\"throughput\"] > 0, \"æ…¢é€Ÿç®¡é“ååé‡ä¸º0\"\n",
    "        assert memory_result > 0, \"å†…å­˜ä¼˜åŒ–å¤„ç†å¤±è´¥\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šæµå¼è¾“å‡ºæ€§èƒ½ä¼˜åŒ–æ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æµå¼è¾“å‡ºæ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸æœ€ä½³å®è·µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ Streaming æµå¼è¾“å‡ºå­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… æµå¼åŸç†ï¼šé€ä¸ªtokenå®æ—¶è¾“å‡º\",\n",
    "    \"âœ… streamæ–¹æ³•ï¼šåŸºç¡€æµå¼è°ƒç”¨\",\n",
    "    \"âœ… LCELæµå¼ï¼šç®¡é“æµå¼ç»„åˆ\",\n",
    "    \"âœ… æ•°æ®å¤„ç†ï¼šæµå¼æ•°æ®æ”¶é›†å’Œæ˜¾ç¤º\",\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: {len(verification_points)}/4 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Streaming æœ€ä½³å®è·µ:\")\n",
    "print(\"1. å®æ—¶ä½“éªŒï¼šä½¿ç”¨streamå‡å°‘ç”¨æˆ·ç­‰å¾…\")\n",
    "print(\"2. ç¼“å†²ç®¡ç†ï¼šåˆç†è®¾ç½®ç¼“å†²åŒºå¤§å°\")\n",
    "print(\"3. æ€§èƒ½ç›‘æ§ï¼šå…³æ³¨é¦–Tokenå»¶è¿Ÿå’Œååé‡\")\n",
    "print(\"4. é”™è¯¯å¤„ç†ï¼šæµå¼è¿‡ç¨‹ä¸­çš„å¼‚å¸¸æ•è·\")\n",
    "print(\"5. å†…å­˜ä¼˜åŒ–ï¼šé¿å…å¤§é‡æ•°æ®åœ¨å†…å­˜ä¸­ç´¯ç§¯\")\n",
    "\n",
    "print(\"\\nğŸ”§ æµå¼è¾“å‡ºåº”ç”¨åœºæ™¯:\")\n",
    "print(\"1. èŠå¤©åº”ç”¨ï¼šå®æ—¶æ˜¾ç¤ºå¯¹è¯å†…å®¹\")\n",
    "print(\"2. ä»£ç ç”Ÿæˆï¼šé€è¡Œæ˜¾ç¤ºç”Ÿæˆä»£ç \")\n",
    "print(\"3. æ–‡æœ¬åˆ›ä½œï¼šå®æ—¶æ˜¾ç¤ºåˆ›ä½œè¿‡ç¨‹\")\n",
    "print(\"4. æ•°æ®åˆ†æï¼šé€æ­¥æ˜¾ç¤ºåˆ†æç»“æœ\")\n",
    "print(\"5. é•¿æ–‡æœ¬å¤„ç†ï¼šé¿å…é•¿æ—¶é—´ç­‰å¾…\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  ChatMessageHistory æ¶ˆæ¯å†å²\")\n",
    "print(\"2. æŒæ¡ RunnablePassthrough æ•°æ®é€ä¼ \")\n",
    "print(\"3. å­¦ä¹  OutputParser é«˜çº§è§£ææŠ€æœ¯\")\n",
    "print(\"4. æ¢ç´¢å¼‚æ­¥æµå¼å¤„ç†æ¨¡å¼\")\n",
    "print(\"5. å®è·µç”Ÿäº§çº§æµå¼åº”ç”¨å¼€å‘\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ Streaming åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    if openai_api_key:\n",
    "        # ç®€å•æµå¼æµ‹è¯•\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=20)\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"æ€»ç»“: {text}\"\n",
    "        )\n",
    "        \n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        \n",
    "        # æµ‹è¯•æµå¼è¾“å‡º\n",
    "        chunks = []\n",
    "        for chunk in chain.stream({\"text\": \"æµå¼è¾“å‡ºæµ‹è¯•\"}):\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        full_content = ''.join(chunks)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ:\")\n",
    "        print(f\"   API çŠ¶æ€: å¯ç”¨\")\n",
    "        print(f\"   æµå¼Tokenæ•°: {len(chunks)}\")\n",
    "        print(f\"   è¾“å‡ºé•¿åº¦: {len(full_content)} å­—ç¬¦\")\n",
    "        print(f\"   è¾“å‡ºå†…å®¹: {full_content}\")\n",
    "        print(\"\\nâœ… Streaming æµå¼è¾“å‡ºå­¦ä¹ å®Œæˆï¼\")\n",
    "        \n",
    "        print(f\"\\nğŸŠ LCEL ç¼–æ’å±‚æŠ€æœ¯æŒæ¡ï¼\")\n",
    "        print(f\"   å·²æŒæ¡: Runnable + RunnableMap + LCEL + Streaming\")\n",
    "        print(f\"   ä¸‹ä¸€æ­¥: å­¦ä¹ æ¶ˆæ¯å†å²å’Œæ•°æ®é€ä¼ æŠ€æœ¯\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  æœ€ç»ˆéªŒè¯è·³è¿‡: OpenAI API Key æœªé…ç½®\")\n",
    "        print(\"   è¯·é…ç½® OPENAI_API_KEY åé‡è¯•\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
