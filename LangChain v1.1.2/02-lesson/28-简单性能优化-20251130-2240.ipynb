{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28 - ç®€å•æ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "## ç”¨é€”\n",
    "ä¼˜åŒ–GPT LangChain åº”ç”¨çš„æ€§èƒ½ã€‚ç†è§£æ€§èƒ½ç“¶é¢ˆã€æŒæ¡åŸºç¡€ä¼˜åŒ–æŠ€å·§ã€èƒ½å®ç°æ€§èƒ½ç›‘æ§ã€‚\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£æ€§èƒ½ç“¶é¢ˆ\n",
    "- æŒæ¡åŸºç¡€ä¼˜åŒ–æŠ€å·§\n",
    "- èƒ½å®ç°æ€§èƒ½ç›‘æ§\n",
    "- ä¼˜åŒ–GPT LLMè°ƒç”¨æ€§èƒ½\n",
    "- éªŒè¯æ€§èƒ½æå‡æ•ˆæœ\n",
    "- éªŒè¯ç‚¹ï¼šåŸºç¡€ä¼˜åŒ–åå“åº”æ—¶é—´å‡å°‘\n",
    "\n",
    "## ğŸ”‘ å‰ç½®è¦æ±‚\n",
    "å·²å®‰è£… LangChain å’Œ OpenAI ç›¸å…³åŒ…\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½ç“¶é¢ˆè¯†åˆ« - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ” æ€§èƒ½ç“¶é¢ˆè¯†åˆ«:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"ğŸ“ LangChainåº”ç”¨å¸¸è§æ€§èƒ½ç“¶é¢ˆ:\")\n",
    "print(\"   1. ç½‘ç»œå»¶è¿Ÿ: APIè°ƒç”¨ç½‘ç»œå¾€è¿”æ—¶é—´\")\n",
    "print(\"   2. æ¨¡å‹æ¨ç†: LLMå¤„ç†æ—¶é—´ï¼ˆæ¨¡å‹å¤æ‚åº¦ï¼‰\")\n",
    "print(\"   3. ä»¤ç‰Œæ•°é‡: è¾“å…¥è¾“å‡ºä»¤ç‰Œæ•°é‡å½±å“å¤„ç†æ—¶é—´\")\n",
    "print(\"   4. é‡å¤è°ƒç”¨: ç›¸åŒæŸ¥è¯¢é‡å¤å¤„ç†\")\n",
    "print(\"   5. ä¸²è¡Œå¤„ç†: æœªåˆ©ç”¨å¹¶è¡Œå¤„ç†èƒ½åŠ›\")\n",
    "print(\"   6. å†…å­˜ä½¿ç”¨: å¤§é‡æ•°æ®åŠ è½½å’Œå¤„ç†\")\n",
    "print(\"   7. é…ç½®é€‰æ‹©: ä½¿ç”¨è¿‡åº¦å¤æ‚çš„æ¨¡å‹\")\n",
    "print(\"   8. ç¼ºä¹ç¼“å­˜: æœªç¼“å­˜å¸¸ç”¨ç»“æœ\")\n",
    "\n",
    "print(f\"\\nğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   - ç†è§£æ€§èƒ½ç“¶é¢ˆ\")\n",
    "print(f\"   - æŒæ¡åŸºç¡€ä¼˜åŒ–æŠ€å·§\")\n",
    "print(f\"   - èƒ½å®ç°æ€§èƒ½ç›‘æ§\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸  æ€§èƒ½ç“¶é¢ˆåˆ†ç±»:\")\n",
    "print(f\"   ç½‘ç»œç›¸å…³ç“¶é¢ˆ:\")\n",
    "print(f\"     - APIè°ƒç”¨å»¶è¿Ÿ: ç½‘ç»œå¾€è¿”æ—¶é—´\")\n",
    "print(f\"     - è¿æ¥å»ºç«‹å¼€é”€: æ¯æ¬¡è°ƒç”¨å»ºç«‹æ–°è¿æ¥\")\n",
    "print(f\"     - å¸¦å®½é™åˆ¶: å¤§é‡æ•°æ®ä¼ è¾“æ—¶é—´\")\n",
    "print(f\"\\n   è®¡ç®—ç›¸å…³ç“¶é¢ˆ:\")\n",
    "print(f\"     - æ¨¡å‹æ¨ç†æ—¶é—´: LLMå¤„ç†å¤æ‚åº¦\")\n",
    "print(f\"     - ä»¤ç‰Œå¤„ç†: è¾“å…¥è¾“å‡ºé•¿åº¦å½±å“\")\n",
    "print(f\"     - å¹¶å‘å¤„ç†: æœªå……åˆ†åˆ©ç”¨å¹¶è¡Œèƒ½åŠ›\")\n",
    "print(f\"\\n   æ•°æ®ç›¸å…³ç“¶é¢ˆ:\")\n",
    "print(f\"     - é‡å¤è®¡ç®—: ç›¸åŒè¾“å…¥é‡å¤å¤„ç†\")\n",
    "print(f\"     - æ•°æ®åŠ è½½: å¤§é‡æ–‡æ¡£å‘é‡åŒ–æ—¶é—´\")\n",
    "print(f\"     - å†…å­˜ä½¿ç”¨: æ•°æ®ç»“æ„æ•ˆç‡\")\n",
    "\n",
    "print(f\"\\nğŸ”§ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥:\")\n",
    "print(f\"   1. ç¼“å­˜ç­–ç•¥: ç¼“å­˜å¸¸ç”¨æŸ¥è¯¢ç»“æœ\")\n",
    "print(f\"   2. æ¨¡å‹é€‰æ‹©: æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚æ¨¡å‹\")\n",
    "print(f\"   3. ä»¤ç‰Œä¼˜åŒ–: å‡å°‘ä¸å¿…è¦çš„è¾“å…¥è¾“å‡º\")\n",
    "print(f\"   4. å¹¶è¡Œå¤„ç†: åˆ©ç”¨å¹¶å‘è°ƒç”¨èƒ½åŠ›\")\n",
    "print(f\"   5. è¿æ¥å¤ç”¨: å¤ç”¨ç½‘ç»œè¿æ¥\")\n",
    "print(f\"   6. æ‰¹å¤„ç†: æ‰¹é‡å¤„ç†ç›¸ä¼¼è¯·æ±‚\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ€§èƒ½ç›‘æ§æŒ‡æ ‡:\")\n",
    "print(f\"   å“åº”æ—¶é—´æŒ‡æ ‡:\")\n",
    "print(f\"     - æ€»å“åº”æ—¶é—´: ç«¯åˆ°ç«¯å¤„ç†æ—¶é—´\")\n",
    "print(f\"     - APIè°ƒç”¨æ—¶é—´: ç½‘ç»œå’Œæ¨ç†æ—¶é—´\")\n",
    "print(f\"     - é¦–å­—æ—¶é—´: æµå¼è¾“å‡ºé¦–å­—å»¶è¿Ÿ\")\n",
    "print(f\"\\n   èµ„æºä½¿ç”¨æŒ‡æ ‡:\")\n",
    "print(f\"     - ä»¤ç‰Œä½¿ç”¨: è¾“å…¥è¾“å‡ºä»¤ç‰Œæ•°é‡\")\n",
    "print(f\"     - å†…å­˜ä½¿ç”¨: åº”ç”¨å†…å­˜å ç”¨\")\n",
    "print(f\"     - CPUä½¿ç”¨: å¤„ç†å™¨ä½¿ç”¨ç‡\")\n",
    "print(f\"\\n   ä¸šåŠ¡æŒ‡æ ‡:\")\n",
    "print(f\"     - ååé‡: æ¯ç§’å¤„ç†è¯·æ±‚æ•°\")\n",
    "print(f\"     - æˆåŠŸç‡: è¯·æ±‚æˆåŠŸå¤„ç†æ¯”ä¾‹\")\n",
    "print(f\"     - ç¼“å­˜å‘½ä¸­ç‡: ç¼“å­˜å‘½ä¸­æ¯”ä¾‹\")\n",
    "\n",
    "print(f\"\\nğŸ¨ æ€§èƒ½ä¼˜åŒ–åŸåˆ™:\")\n",
    "print(f\"   1. æµ‹é‡å…ˆè¡Œ: å…ˆæµ‹é‡å†ä¼˜åŒ–\")\n",
    "print(f\"   2. ç“¶é¢ˆä¼˜å…ˆ: ä¼˜åŒ–æœ€å¤§ç“¶é¢ˆ\")\n",
    "print(f\"   3. æ¸è¿›ä¼˜åŒ–: é€æ­¥ä¼˜åŒ–éªŒè¯æ•ˆæœ\")\n",
    "print(f\"   4. å¹³è¡¡è€ƒè™‘: å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬\")\n",
    "print(f\"   5. æŒç»­ç›‘æ§: å»ºç«‹æ€§èƒ½ç›‘æ§ä½“ç³»\")\n",
    "\n",
    "print(f\"\\nğŸ” æ€§èƒ½ä¼˜åŒ–åœ¨LangChainä¸­çš„é‡è¦æ€§:\")\n",
    "print(f\"   1. ç”¨æˆ·ä½“éªŒ: å“åº”é€Ÿåº¦å½±å“ç”¨æˆ·æ»¡æ„åº¦\")\n",
    "print(f\"   2. æˆæœ¬æ§åˆ¶: ä¼˜åŒ–å‡å°‘APIè°ƒç”¨æˆæœ¬\")\n",
    "print(f\"   3. ç³»ç»Ÿç¨³å®š: æ€§èƒ½ä¼˜åŒ–æé«˜ç³»ç»Ÿç¨³å®šæ€§\")\n",
    "print(f\"   4. å¯æ‰©å±•æ€§: ä¼˜åŒ–æ”¯æŒæ›´å¤§è§„æ¨¡åº”ç”¨\")\n",
    "print(f\"   5. ç«äº‰ä¼˜åŠ¿: æ€§èƒ½æ˜¯äº§å“ç«äº‰åŠ›\")\n",
    "\n",
    "print(f\"\\nâœ… æ€§èƒ½ç“¶é¢ˆè¯†åˆ«å®Œæˆ\")\n",
    "print(f\"\\nğŸš€ å‡†å¤‡å®ç°åŸºç¡€æ€§èƒ½ç›‘æ§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŸºç¡€æ€§èƒ½ç›‘æ§å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€æ€§èƒ½ç›‘æ§å®ç° - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Dict, Any\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“Š åŸºç¡€æ€§èƒ½ç›‘æ§å®ç°:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. åˆ›å»ºæ€§èƒ½ç›‘æ§è£…é¥°å™¨\n",
    "print(f\"ğŸ”§ 1. åˆ›å»ºæ€§èƒ½ç›‘æ§è£…é¥°å™¨:\")\n",
    "\n",
    "def performance_monitor(func_name: str = None):\n",
    "    \"\"\"æ€§èƒ½ç›‘æ§è£…é¥°å™¨\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                end_time = time.time()\n",
    "                execution_time = end_time - start_time\n",
    "                \n",
    "                # è®°å½•æ€§èƒ½æ•°æ®\n",
    "                performance_data = {\n",
    "                    \"function_name\": func_name or func.__name__,\n",
    "                    \"execution_time\": execution_time,\n",
    "                    \"success\": True,\n",
    "                    \"error\": None\n",
    "                }\n",
    "                \n",
    "                return result, performance_data\n",
    "                \n",
    "            except Exception as e:\n",
    "                end_time = time.time()\n",
    "                execution_time = end_time - start_time\n",
    "                \n",
    "                # è®°å½•é”™è¯¯æ€§èƒ½æ•°æ®\n",
    "                performance_data = {\n",
    "                    \"function_name\": func_name or func.__name__,\n",
    "                    \"execution_time\": execution_time,\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "                \n",
    "                return None, performance_data\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "print(f\"   æ€§èƒ½ç›‘æ§è£…é¥°å™¨åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   åŠŸèƒ½: æ‰§è¡Œæ—¶é—´æµ‹é‡ + æˆåŠŸçŠ¶æ€è®°å½• + é”™è¯¯æ•è·\")\n",
    "\n",
    "# 2. åˆ›å»ºåŸºç¡€LLMç»„ä»¶\n",
    "print(f\"\\nğŸ¤– 2. åˆ›å»ºåŸºç¡€LLMç»„ä»¶:\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"   âœ… OpenAI API Key: å·²é…ç½®\")\n",
    "else:\n",
    "    print(f\"   âŒ OpenAI API Key: æœªé…ç½®\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    openai_api_key=api_key or \"dummy_key\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚\"),\n",
    "    (\"user\", \"é—®é¢˜: {query}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt_template | llm | output_parser\n",
    "\n",
    "print(f\"   åŸºç¡€ç»„ä»¶åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   LLMæ¨¡å‹: {llm.model_name}\")\n",
    "print(f\"   é“¾ç±»å‹: {type(chain)}\")\n",
    "\n",
    "# 3. åˆ›å»ºå¸¦ç›‘æ§çš„LLMè°ƒç”¨å‡½æ•°\n",
    "print(f\"\\nğŸ“ˆ 3. åˆ›å»ºå¸¦ç›‘æ§çš„LLMè°ƒç”¨å‡½æ•°:\")\n",
    "\n",
    "@performance_monitor(\"llm_call\")\n",
    "def monitored_llm_call(query: str) -> str:\n",
    "    \"\"\"å¸¦æ€§èƒ½ç›‘æ§çš„LLMè°ƒç”¨\"\"\"\n",
    "    return chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"   ç›‘æ§LLMè°ƒç”¨å‡½æ•°åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   å‡½æ•°å: monitored_llm_call\")\n",
    "print(f\"   ç‰¹æ€§: è‡ªåŠ¨æ€§èƒ½ç›‘æ§ + é”™è¯¯å¤„ç†\")\n",
    "\n",
    "# 4. æµ‹è¯•æ€§èƒ½ç›‘æ§åŠŸèƒ½\n",
    "print(f\"\\nğŸ§ª 4. æµ‹è¯•æ€§èƒ½ç›‘æ§åŠŸèƒ½:\")\n",
    "\n",
    "test_queries = [\n",
    "    \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Python\",\n",
    "    \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\",\n",
    "    \"è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½\"\n",
    "]\n",
    "\n",
    "monitoring_results = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n   æµ‹è¯• {i}: {query}\")\n",
    "    \n",
    "    result, perf_data = monitored_llm_call(query)\n",
    "    monitoring_results.append(perf_data)\n",
    "    \n",
    "    print(f\"     æ‰§è¡Œæ—¶é—´: {perf_data['execution_time']:.2f} ç§’\")\n",
    "    print(f\"     æ‰§è¡ŒçŠ¶æ€: {'âœ… æˆåŠŸ' if perf_data['success'] else 'âŒ å¤±è´¥'}\")\n",
    "    \n",
    "    if perf_data['success'] and result:\n",
    "        print(f\"     å“åº”é•¿åº¦: {len(result)} å­—ç¬¦\")\n",
    "        print(f\"     å“åº”é¢„è§ˆ: {result[:60]}...\")\n",
    "    elif perf_data['error']:\n",
    "        print(f\"     é”™è¯¯ä¿¡æ¯: {perf_data['error'][:50]}...\")\n",
    "\n",
    "# 5. åˆ†ææ€§èƒ½ç›‘æ§æ•°æ®\n",
    "print(f\"\\nğŸ“Š 5. åˆ†ææ€§èƒ½ç›‘æ§æ•°æ®:\")\n",
    "\n",
    "successful_calls = [r for r in monitoring_results if r['success']]\n",
    "failed_calls = [r for r in monitoring_results if not r['success']]\n",
    "\n",
    "print(f\"   æ€§èƒ½ç›‘æ§æ•°æ®åˆ†æ:\")\n",
    "print(f\"     æ€»è°ƒç”¨æ¬¡æ•°: {len(monitoring_results)}\")\n",
    "print(f\"     æˆåŠŸè°ƒç”¨æ¬¡æ•°: {len(successful_calls)}\")\n",
    "print(f\"     å¤±è´¥è°ƒç”¨æ¬¡æ•°: {len(failed_calls)}\")\n",
    "\n",
    "if successful_calls:\n",
    "    avg_time = sum(r['execution_time'] for r in successful_calls) / len(successful_calls)\n",
    "    min_time = min(r['execution_time'] for r in successful_calls)\n",
    "    max_time = max(r['execution_time'] for r in successful_calls)\n",
    "    \n",
    "    print(f\"\\n   æˆåŠŸè°ƒç”¨æ€§èƒ½ç»Ÿè®¡:\")\n",
    "    print(f\"     å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f} ç§’\")\n",
    "    print(f\"     æœ€å¿«å“åº”æ—¶é—´: {min_time:.2f} ç§’\")\n",
    "    print(f\"     æœ€æ…¢å“åº”æ—¶é—´: {max_time:.2f} ç§’\")\n",
    "    print(f\"     æ—¶é—´æ ‡å‡†å·®: {(sum((r['execution_time'] - avg_time) ** 2 for r in successful_calls) / len(successful_calls)) ** 0.5:.2f} ç§’\")\n",
    "\n",
    "# 6. åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°\n",
    "print(f\"\\nğŸ 6. åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°:\")\n",
    "\n",
    "def benchmark_performance(test_func, test_queries: list, iterations: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"æ€§èƒ½åŸºå‡†æµ‹è¯•\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        iteration_results = []\n",
    "        \n",
    "        for query in test_queries:\n",
    "            result, perf_data = test_func(query)\n",
    "            iteration_results.append(perf_data)\n",
    "        \n",
    "        all_results.extend(iteration_results)\n",
    "    \n",
    "    # è®¡ç®—ç»Ÿè®¡æ•°æ®\n",
    "    successful_results = [r for r in all_results if r['success']]\n",
    "    \n",
    "    if successful_results:\n",
    "        execution_times = [r['execution_time'] for r in successful_results]\n",
    "        \n",
    "        benchmark_data = {\n",
    "            \"total_calls\": len(all_results),\n",
    "            \"successful_calls\": len(successful_results),\n",
    "            \"success_rate\": len(successful_results) / len(all_results),\n",
    "            \"avg_response_time\": sum(execution_times) / len(execution_times),\n",
    "            \"min_response_time\": min(execution_times),\n",
    "            \"max_response_time\": max(execution_times),\n",
    "            \"total_execution_time\": sum(execution_times)\n",
    "        }\n",
    "    else:\n",
    "        benchmark_data = {\n",
    "            \"total_calls\": len(all_results),\n",
    "            \"successful_calls\": 0,\n",
    "            \"success_rate\": 0,\n",
    "            \"avg_response_time\": 0,\n",
    "            \"min_response_time\": 0,\n",
    "            \"max_response_time\": 0,\n",
    "            \"total_execution_time\": 0\n",
    "        }\n",
    "    \n",
    "    return benchmark_data\n",
    "\n",
    "print(f\"   æ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   åŠŸèƒ½: å¤šè½®æµ‹è¯• + ç»Ÿè®¡åˆ†æ + æ€§èƒ½æŠ¥å‘Š\")\n",
    "\n",
    "# 7. æ‰§è¡ŒåŸºå‡†æµ‹è¯•\n",
    "print(f\"\\nğŸ 7. æ‰§è¡ŒåŸºå‡†æµ‹è¯•:\")\n",
    "\n",
    "baseline_benchmark = benchmark_performance(\n",
    "    monitored_llm_call,\n",
    "    test_queries,\n",
    "    iterations=2\n",
    ")\n",
    "\n",
    "print(f\"   åŸºå‡†æµ‹è¯•ç»“æœ:\")\n",
    "print(f\"     æ€»è°ƒç”¨æ¬¡æ•°: {baseline_benchmark['total_calls']}\")\n",
    "print(f\"     æˆåŠŸè°ƒç”¨æ¬¡æ•°: {baseline_benchmark['successful_calls']}\")\n",
    "print(f\"     æˆåŠŸç‡: {baseline_benchmark['success_rate']:.1%}\")\n",
    "print(f\"     å¹³å‡å“åº”æ—¶é—´: {baseline_benchmark['avg_response_time']:.2f} ç§’\")\n",
    "print(f\"     æœ€å¿«å“åº”æ—¶é—´: {baseline_benchmark['min_response_time']:.2f} ç§’\")\n",
    "print(f\"     æœ€æ…¢å“åº”æ—¶é—´: {baseline_benchmark['max_response_time']:.2f} ç§’\")\n",
    "print(f\"     æ€»æ‰§è¡Œæ—¶é—´: {baseline_benchmark['total_execution_time']:.2f} ç§’\")\n",
    "\n",
    "# 8. ä¿å­˜ç›‘æ§ç»„ä»¶ä¾›åç»­ä½¿ç”¨\n",
    "print(f\"\\nğŸ’¾ 8. ä¿å­˜ç›‘æ§ç»„ä»¶ä¾›åç»­ä½¿ç”¨:\")\n",
    "\n",
    "globals().update({\n",
    "    'performance_monitor': performance_monitor,\n",
    "    'monitored_llm_call': monitored_llm_call,\n",
    "    'benchmark_performance': benchmark_performance,\n",
    "    'monitoring_results': monitoring_results,\n",
    "    'baseline_benchmark': baseline_benchmark\n",
    "})\n",
    "\n",
    "print(f\"   æ€§èƒ½ç›‘æ§ç»„ä»¶å·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   performance_monitor: æ€§èƒ½ç›‘æ§è£…é¥°å™¨\")\n",
    "print(f\"   monitored_llm_call: å¸¦ç›‘æ§çš„LLMè°ƒç”¨å‡½æ•°\")\n",
    "print(f\"   benchmark_performance: æ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šåŸºç¡€æ€§èƒ½ç›‘æ§å®ç°æ­£ç¡®\n",
    "assert len(monitoring_results) > 0, \"ç›‘æ§ç»“æœåº”è¯¥ä¸ä¸ºç©º\"\n",
    "assert baseline_benchmark['total_calls'] > 0, \"åŸºå‡†æµ‹è¯•åº”è¯¥æœ‰æ‰§è¡Œè®°å½•\"\n",
    "assert baseline_benchmark['avg_response_time'] >= 0, \"å¹³å‡å“åº”æ—¶é—´åº”è¯¥æœ‰æ•ˆ\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šåŸºç¡€æ€§èƒ½ç›‘æ§å®ç°æ­£ç¡®\")\n",
    "print(f\"\\nğŸ¯ åŸºç¡€æ€§èƒ½ç›‘æ§æ€»ç»“:\")\n",
    "print(f\"   âœ“ åˆ›å»ºæ€§èƒ½ç›‘æ§è£…é¥°å™¨\")\n",
    "print(f\"   âœ“ å®ç°å¸¦ç›‘æ§çš„LLMè°ƒç”¨å‡½æ•°\")\n",
    "print(f\"   âœ“ æµ‹è¯•æ€§èƒ½ç›‘æ§åŠŸèƒ½\")\n",
    "print(f\"   âœ“ åˆ†ææ€§èƒ½ç›‘æ§æ•°æ®\")\n",
    "print(f\"   âœ“ åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°\")\n",
    "print(f\"   âœ“ å‡†å¤‡å®ç°æ€§èƒ½ä¼˜åŒ–æŠ€å·§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åŸºç¡€ä¼˜åŒ–æŠ€å·§å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€ä¼˜åŒ–æŠ€å·§å®ç° - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Dict, Any\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âš¡ åŸºç¡€ä¼˜åŒ–æŠ€å·§å®ç°:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. é‡æ–°åˆ›å»ºåŸºç¡€ç»„ä»¶ï¼ˆç¡®ä¿ç‹¬ç«‹æ€§ï¼‰\n",
    "print(f\"ğŸ”§ 1. é‡æ–°åˆ›å»ºåŸºç¡€ç»„ä»¶:\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# å¿«é€Ÿæ¨¡å‹\n",
    "fast_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    openai_api_key=api_key or \"dummy_key\"\n",
    ")\n",
    "\n",
    "# å¹³è¡¡æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰\n",
    "balanced_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    "    openai_api_key=api_key or \"dummy_key\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·ç®€æ´å›ç­”ã€‚\"),\n",
    "    (\"user\", \"é—®é¢˜: {query}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# åˆ›å»ºä¸åŒé…ç½®çš„é“¾\n",
    "fast_chain = prompt_template | fast_llm | output_parser\n",
    "balanced_chain = prompt_template | balanced_llm | output_parser\n",
    "\n",
    "print(f\"   åŸºç¡€ç»„ä»¶é‡æ–°åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   å¿«é€Ÿé“¾: gpt-4o-mini (max_tokens=300)\")\n",
    "print(f\"   å¹³è¡¡é“¾: gpt-4o (max_tokens=500)\")\n",
    "\n",
    "# 2. å®ç°ç®€å•ç¼“å­˜æœºåˆ¶\n",
    "print(f\"\\nğŸ’¾ 2. å®ç°ç®€å•ç¼“å­˜æœºåˆ¶:\")\n",
    "\n",
    "class SimpleCache:\n",
    "    \"\"\"ç®€å•å†…å­˜ç¼“å­˜\"\"\"\n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _generate_key(self, query: str, model_config: str) -> str:\n",
    "        \"\"\"ç”Ÿæˆç¼“å­˜é”®\"\"\"\n",
    "        content = f\"{query}:{model_config}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, query: str, model_config: str) -> str:\n",
    "        \"\"\"è·å–ç¼“å­˜ç»“æœ\"\"\"\n",
    "        key = self._generate_key(query, model_config)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            self.hits += 1\n",
    "            return self.cache[key]\n",
    "        else:\n",
    "            self.misses += 1\n",
    "            return None\n",
    "    \n",
    "    def set(self, query: str, model_config: str, result: str) -> None:\n",
    "        \"\"\"è®¾ç½®ç¼“å­˜ç»“æœ\"\"\"\n",
    "        key = self._generate_key(query, model_config)\n",
    "        \n",
    "        # ç®€å•çš„LRUï¼šå¦‚æœç¼“å­˜æ»¡äº†ï¼Œæ¸…ç©ºä¸€åŠ\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            keys_to_remove = list(self.cache.keys())[:self.max_size // 2]\n",
    "            for k in keys_to_remove:\n",
    "                del self.cache[k]\n",
    "        \n",
    "        self.cache[key] = result\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–ç¼“å­˜ç»Ÿè®¡\"\"\"\n",
    "        total_requests = self.hits + self.misses\n",
    "        hit_rate = self.hits / total_requests if total_requests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"hit_rate\": hit_rate\n",
    "        }\n",
    "\n",
    "# åˆ›å»ºç¼“å­˜å®ä¾‹\n",
    "cache = SimpleCache(max_size=50)\n",
    "\n",
    "print(f\"   ç®€å•ç¼“å­˜æœºåˆ¶åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   ç¼“å­˜ç±»å‹: å†…å­˜ç¼“å­˜\")\n",
    "print(f\"   æœ€å¤§å®¹é‡: {cache.max_size} é¡¹\")\n",
    "print(f\"   é”®ç”Ÿæˆ: MD5å“ˆå¸Œ\")\n",
    "\n",
    "# 3. åˆ›å»ºä¼˜åŒ–ç‰ˆæœ¬çš„LLMè°ƒç”¨å‡½æ•°\n",
    "print(f\"\\nâš¡ 3. åˆ›å»ºä¼˜åŒ–ç‰ˆæœ¬çš„LLMè°ƒç”¨å‡½æ•°:\")\n",
    "\n",
    "def optimized_llm_call(query: str, use_cache: bool = True, use_fast_model: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"ä¼˜åŒ–ç‰ˆæœ¬çš„LLMè°ƒç”¨\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # é€‰æ‹©æ¨¡å‹é…ç½®\n",
    "        model_config = \"fast\" if use_fast_model else \"balanced\"\n",
    "        chain = fast_chain if use_fast_model else balanced_chain\n",
    "        \n",
    "        # æ£€æŸ¥ç¼“å­˜\n",
    "        cached_result = None\n",
    "        if use_cache:\n",
    "            cached_result = cache.get(query, model_config)\n",
    "        \n",
    "        if cached_result is not None:\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": cached_result,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"from_cache\": True,\n",
    "                \"model_config\": model_config\n",
    "            }\n",
    "        \n",
    "        # æ‰§è¡ŒLLMè°ƒç”¨\n",
    "        result = chain.invoke({\"query\": query})\n",
    "        \n",
    "        # ç¼“å­˜ç»“æœ\n",
    "        if use_cache and result:\n",
    "            cache.set(query, model_config, result)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": result,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"from_cache\": False,\n",
    "            \"model_config\": model_config\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"response\": None,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"error\": str(e),\n",
    "            \"from_cache\": False,\n",
    "            \"model_config\": model_config if 'model_config' in locals() else \"unknown\"\n",
    "        }\n",
    "\n",
    "print(f\"   ä¼˜åŒ–LLMè°ƒç”¨å‡½æ•°åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   ä¼˜åŒ–ç‰¹æ€§: ç¼“å­˜æœºåˆ¶ + æ¨¡å‹é€‰æ‹© + æ€§èƒ½ç›‘æ§\")\n",
    "\n",
    "# 4. æµ‹è¯•ä¼˜åŒ–æ•ˆæœ\n",
    "print(f\"\\nğŸ§ª 4. æµ‹è¯•ä¼˜åŒ–æ•ˆæœ:\")\n",
    "\n",
    "test_queries = [\n",
    "    \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Python\",\n",
    "    \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\",\n",
    "    \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Python\",  # é‡å¤æŸ¥è¯¢æµ‹è¯•ç¼“å­˜\n",
    "    \"è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½\",\n",
    "    \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\"  # é‡å¤æŸ¥è¯¢æµ‹è¯•ç¼“å­˜\n",
    "]\n",
    "\n",
    "optimization_results = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n   æµ‹è¯• {i}: {query}\")\n",
    "    \n",
    "    result = optimized_llm_call(query, use_cache=True, use_fast_model=True)\n",
    "    optimization_results.append(result)\n",
    "    \n",
    "    print(f\"     æ‰§è¡Œæ—¶é—´: {result['execution_time']:.2f} ç§’\")\n",
    "    print(f\"     æ‰§è¡ŒçŠ¶æ€: {'âœ… æˆåŠŸ' if result['success'] else 'âŒ å¤±è´¥'}\")\n",
    "    print(f\"     ç¼“å­˜å‘½ä¸­: {'âœ… æ˜¯' if result.get('from_cache') else 'âŒ å¦'}\")\n",
    "    print(f\"     æ¨¡å‹é…ç½®: {result.get('model_config', 'unknown')}\")\n",
    "    \n",
    "    if result['success'] and result['response']:\n",
    "        print(f\"     å“åº”é•¿åº¦: {len(result['response'])} å­—ç¬¦\")\n",
    "        print(f\"     å“åº”é¢„è§ˆ: {result['response'][:60]}...\")\n",
    "    elif result.get('error'):\n",
    "        print(f\"     é”™è¯¯ä¿¡æ¯: {result['error'][:50]}...\")\n",
    "\n",
    "# 5. åˆ†æä¼˜åŒ–æ•ˆæœ\n",
    "print(f\"\\nğŸ“Š 5. åˆ†æä¼˜åŒ–æ•ˆæœ:\")\n",
    "\n",
    "successful_results = [r for r in optimization_results if r['success']]\n",
    "cache_hits = [r for r in successful_results if r.get('from_cache')]\n",
    "cache_misses = [r for r in successful_results if not r.get('from_cache')]\n",
    "\n",
    "print(f\"   ä¼˜åŒ–æ•ˆæœåˆ†æ:\")\n",
    "print(f\"     æ€»è°ƒç”¨æ¬¡æ•°: {len(optimization_results)}\")\n",
    "print(f\"     æˆåŠŸè°ƒç”¨æ¬¡æ•°: {len(successful_results)}\")\n",
    "print(f\"     ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {len(cache_hits)}\")\n",
    "print(f\"     ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°: {len(cache_misses)}\")\n",
    "\n",
    "if cache_hits:\n",
    "    avg_cache_time = sum(r['execution_time'] for r in cache_hits) / len(cache_hits)\n",
    "    print(f\"\\n   ç¼“å­˜å‘½ä¸­æ€§èƒ½:\")\n",
    "    print(f\"     å¹³å‡å“åº”æ—¶é—´: {avg_cache_time:.4f} ç§’\")\n",
    "    print(f\"     æœ€å¿«å“åº”æ—¶é—´: {min(r['execution_time'] for r in cache_hits):.4f} ç§’\")\n",
    "\n",
    "if cache_misses:\n",
    "    avg_normal_time = sum(r['execution_time'] for r in cache_misses) / len(cache_misses)\n",
    "    print(f\"\\n   ç¼“å­˜æœªå‘½ä¸­æ€§èƒ½:\")\n",
    "    print(f\"     å¹³å‡å“åº”æ—¶é—´: {avg_normal_time:.2f} ç§’\")\n",
    "    print(f\"     æœ€å¿«å“åº”æ—¶é—´: {min(r['execution_time'] for r in cache_misses):.2f} ç§’\")\n",
    "    \n",
    "    # è®¡ç®—ç¼“å­˜åŠ é€Ÿæ¯”\n",
    "    if cache_hits and avg_normal_time > 0:\n",
    "        speedup = avg_normal_time / avg_cache_time\n",
    "        print(f\"\\n   ç¼“å­˜åŠ é€Ÿæ•ˆæœ:\")\n",
    "        print(f\"     åŠ é€Ÿæ¯”: {speedup:.1f}x\")\n",
    "        print(f\"     æ—¶é—´èŠ‚çœ: {(1 - avg_cache_time/avg_normal_time)*100:.1f}%\")\n",
    "\n",
    "# 6. æ¨¡å‹é€‰æ‹©å¯¹æ¯”æµ‹è¯•\n",
    "print(f\"\\nğŸ¤– 6. æ¨¡å‹é€‰æ‹©å¯¹æ¯”æµ‹è¯•:\")\n",
    "\n",
    "if api_key:  # åªæœ‰åœ¨æœ‰æœ‰æ•ˆAPI keyæ—¶æ‰æµ‹è¯•\n",
    "    test_query = \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½\"\n",
    "    \n",
    "    # æµ‹è¯•å¿«é€Ÿæ¨¡å‹\n",
    "    fast_result = optimized_llm_call(test_query, use_cache=False, use_fast_model=True)\n",
    "    \n",
    "    # æµ‹è¯•å¹³è¡¡æ¨¡å‹\n",
    "    balanced_result = optimized_llm_call(test_query, use_cache=False, use_fast_model=False)\n",
    "    \n",
    "    print(f\"   æ¨¡å‹é€‰æ‹©å¯¹æ¯”:\")\n",
    "    print(f\"     å¿«é€Ÿæ¨¡å‹ (gpt-4o-mini):\")\n",
    "    print(f\"       å“åº”æ—¶é—´: {fast_result['execution_time']:.2f} ç§’\")\n",
    "    if fast_result['success']:\n",
    "        print(f\"       å“åº”é•¿åº¦: {len(fast_result['response'])} å­—ç¬¦\")\n",
    "    \n",
    "    print(f\"     å¹³è¡¡æ¨¡å‹ (gpt-4o):\")\n",
    "    print(f\"       å“åº”æ—¶é—´: {balanced_result['execution_time']:.2f} ç§’\")\n",
    "    if balanced_result['success']:\n",
    "        print(f\"       å“åº”é•¿åº¦: {len(balanced_result['response'])} å­—ç¬¦\")\n",
    "    \n",
    "    # è®¡ç®—æ¨¡å‹æ€§èƒ½å·®å¼‚\n",
    "    if fast_result['success'] and balanced_result['success']:\n",
    "        time_diff = balanced_result['execution_time'] - fast_result['execution_time']\n",
    "        speedup = balanced_result['execution_time'] / fast_result['execution_time']\n",
    "        \n",
    "        print(f\"\\n     æ¨¡å‹æ€§èƒ½å·®å¼‚:\")\n",
    "        print(f\"       æ—¶é—´å·®å¼‚: {time_diff:+.2f} ç§’\")\n",
    "        print(f\"       å¿«é€Ÿæ¨¡å‹ä¼˜åŠ¿: {speedup:.1f}x\")\n",
    "        \n",
    "        model_optimization_works = speedup > 1.0\n",
    "    else:\n",
    "        model_optimization_works = False\n",
    "else:\n",
    "    print(f\"   âš ï¸  è·³è¿‡æ¨¡å‹å¯¹æ¯”æµ‹è¯•ï¼ˆæ— æœ‰æ•ˆAPI Keyï¼‰\")\n",
    "    model_optimization_works = True  # è·³è¿‡ä¸ç®—å¤±è´¥\n",
    "\n",
    "# 7. è·å–ç¼“å­˜ç»Ÿè®¡\n",
    "print(f\"\\nğŸ“ˆ 7. è·å–ç¼“å­˜ç»Ÿè®¡:\")\n",
    "\n",
    "cache_stats = cache.get_stats()\n",
    "print(f\"   ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "print(f\"     ç¼“å­˜å¤§å°: {cache_stats['cache_size']} é¡¹\")\n",
    "print(f\"     ç¼“å­˜å‘½ä¸­: {cache_stats['hits']} æ¬¡\")\n",
    "print(f\"     ç¼“å­˜æœªå‘½ä¸­: {cache_stats['misses']} æ¬¡\")\n",
    "print(f\"     å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1%}\")\n",
    "\n",
    "cache_optimization_works = cache_stats['hit_rate'] > 0\n",
    "\n",
    "# 8. ä¿å­˜ä¼˜åŒ–ç»„ä»¶ä¾›åç»­ä½¿ç”¨\n",
    "print(f\"\\nğŸ’¾ 8. ä¿å­˜ä¼˜åŒ–ç»„ä»¶ä¾›åç»­ä½¿ç”¨:\")\n",
    "\n",
    "globals().update({\n",
    "    'SimpleCache': SimpleCache,\n",
    "    'cache': cache,\n",
    "    'optimized_llm_call': optimized_llm_call,\n",
    "    'optimization_results': optimization_results,\n",
    "    'cache_optimization_works': cache_optimization_works,\n",
    "    'model_optimization_works': model_optimization_works\n",
    "})\n",
    "\n",
    "print(f\"   ä¼˜åŒ–ç»„ä»¶å·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   SimpleCache: ç®€å•ç¼“å­˜ç±»\")\n",
    "print(f\"   optimized_llm_call: ä¼˜åŒ–LLMè°ƒç”¨å‡½æ•°\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šåŸºç¡€ä¼˜åŒ–æŠ€å·§å®ç°æ­£ç¡®\n",
    "assert len(optimization_results) > 0, \"ä¼˜åŒ–ç»“æœåº”è¯¥ä¸ä¸ºç©º\"\n",
    "assert cache_optimization_works, \"ç¼“å­˜ä¼˜åŒ–åº”è¯¥æœ‰æ•ˆ\"\n",
    "assert model_optimization_works or not api_key, \"æ¨¡å‹ä¼˜åŒ–åº”è¯¥æœ‰æ•ˆæˆ–æ— API Key\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šåŸºç¡€ä¼˜åŒ–æŠ€å·§å®ç°æ­£ç¡®\")\n",
    "print(f\"\\nğŸ¯ åŸºç¡€ä¼˜åŒ–æŠ€å·§æ€»ç»“:\")\n",
    "print(f\"   âœ“ å®ç°ç®€å•å†…å­˜ç¼“å­˜æœºåˆ¶\")\n",
    "print(f\"   âœ“ åˆ›å»ºä¼˜åŒ–ç‰ˆæœ¬çš„LLMè°ƒç”¨å‡½æ•°\")\n",
    "print(f\"   âœ“ æµ‹è¯•ç¼“å­˜å‘½ä¸­æ•ˆæœ\")\n",
    "print(f\"   âœ“ å¯¹æ¯”ä¸åŒæ¨¡å‹æ€§èƒ½\")\n",
    "print(f\"   âœ“ åˆ†æä¼˜åŒ–æ•ˆæœå’ŒåŠ é€Ÿæ¯”\")\n",
    "print(f\"   âœ“ å‡†å¤‡éªŒè¯æ€§èƒ½æå‡æ•ˆæœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. éªŒè¯æ€§èƒ½æå‡æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯æ€§èƒ½æå‡æ•ˆæœ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“ˆ éªŒè¯æ€§èƒ½æå‡æ•ˆæœ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. é‡æ–°åˆ›å»ºåŸºç¡€ç»„ä»¶ï¼ˆç¡®ä¿ç‹¬ç«‹æ€§ï¼‰\n",
    "print(f\"ğŸ”§ 1. é‡æ–°åˆ›å»ºåŸºç¡€ç»„ä»¶:\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# åŸºç¡€LLMç»„ä»¶\n",
    "basic_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,  # è¾ƒå¤§çš„tokené™åˆ¶\n",
    "    openai_api_key=api_key or \"dummy_key\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·è¯¦ç»†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\"),\n",
    "    (\"user\", \"é—®é¢˜: {query}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "basic_chain = prompt_template | basic_llm | output_parser\n",
    "\n",
    "print(f\"   åŸºç¡€ç»„ä»¶é‡æ–°åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   åŸºç¡€é…ç½®: gpt-4o-mini, max_tokens=500\")\n",
    "\n",
    "# 2. åˆ›å»ºç¼“å­˜ç±»\n",
    "print(f\"\\nğŸ’¾ 2. åˆ›å»ºç¼“å­˜ç±»:\")\n",
    "\n",
    "class PerformanceCache:\n",
    "    \"\"\"æ€§èƒ½æµ‹è¯•ç¼“å­˜\"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def get(self, key: str) -> str:\n",
    "        if key in self.cache:\n",
    "            self.hits += 1\n",
    "            return self.cache[key]\n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        self.cache[key] = value\n",
    "    \n",
    "    def get_hit_rate(self) -> float:\n",
    "        total = self.hits + self.misses\n",
    "        return self.hits / total if total > 0 else 0\n",
    "\n",
    "cache = PerformanceCache()\n",
    "\n",
    "# 3. åˆ›å»ºåŸºç¡€ç‰ˆæœ¬è°ƒç”¨å‡½æ•°\n",
    "print(f\"\\nğŸŒ 3. åˆ›å»ºåŸºç¡€ç‰ˆæœ¬è°ƒç”¨å‡½æ•°:\")\n",
    "\n",
    "def basic_llm_call(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"åŸºç¡€ç‰ˆæœ¬LLMè°ƒç”¨ï¼ˆæ— ä¼˜åŒ–ï¼‰\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = basic_chain.invoke({\"query\": query})\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": result,\n",
    "            \"execution_time\": end_time - start_time,\n",
    "            \"optimization\": \"none\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"response\": None,\n",
    "            \"execution_time\": end_time - start_time,\n",
    "            \"error\": str(e),\n",
    "            \"optimization\": \"none\"\n",
    "        }\n",
    "\n",
    "print(f\"   åŸºç¡€ç‰ˆæœ¬è°ƒç”¨å‡½æ•°åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   ç‰¹æ€§: æ— ç¼“å­˜ + æ— ä¼˜åŒ– + åŸºç¡€é…ç½®\")\n",
    "\n",
    "# 4. åˆ›å»ºä¼˜åŒ–ç‰ˆæœ¬è°ƒç”¨å‡½æ•°\n",
    "print(f\"\\nâš¡ 4. åˆ›å»ºä¼˜åŒ–ç‰ˆæœ¬è°ƒç”¨å‡½æ•°:\")\n",
    "\n",
    "# ä¼˜åŒ–LLMé…ç½®\n",
    "optimized_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,  # å‡å°‘tokené™åˆ¶\n",
    "    openai_api_key=api_key or \"dummy_key\"\n",
    ")\n",
    "\n",
    "optimized_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·ç®€æ´å›ç­”ã€‚\"),\n",
    "    (\"user\", \"é—®é¢˜: {query}\")\n",
    "])\n",
    "\n",
    "optimized_chain = optimized_prompt | optimized_llm | output_parser\n",
    "\n",
    "def optimized_llm_call(query: str, use_cache: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"ä¼˜åŒ–ç‰ˆæœ¬LLMè°ƒç”¨ï¼ˆç¼“å­˜ + é…ç½®ä¼˜åŒ–ï¼‰\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # æ£€æŸ¥ç¼“å­˜\n",
    "        cache_key = hashlib.md5(query.encode()).hexdigest()\n",
    "        cached_result = None\n",
    "        \n",
    "        if use_cache:\n",
    "            cached_result = cache.get(cache_key)\n",
    "        \n",
    "        if cached_result is not None:\n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": cached_result,\n",
    "                \"execution_time\": end_time - start_time,\n",
    "                \"optimization\": \"cache_hit\"\n",
    "            }\n",
    "        \n",
    "        # æ‰§è¡Œä¼˜åŒ–è°ƒç”¨\n",
    "        result = optimized_chain.invoke({\"query\": query})\n",
    "        \n",
    "        # ç¼“å­˜ç»“æœ\n",
    "        if use_cache and result:\n",
    "            cache.set(cache_key, result)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": result,\n",
    "            \"execution_time\": end_time - start_time,\n",
    "            \"optimization\": \"cache_miss\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"response\": None,\n",
    "            \"execution_time\": end_time - start_time,\n",
    "            \"error\": str(e),\n",
    "            \"optimization\": \"error\"\n",
    "        }\n",
    "\n",
    "print(f\"   ä¼˜åŒ–ç‰ˆæœ¬è°ƒç”¨å‡½æ•°åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   ç‰¹æ€§: ç¼“å­˜æœºåˆ¶ + tokenä¼˜åŒ– + ç®€æ´æç¤º\")\n",
    "\n",
    "# 5. æ€§èƒ½å¯¹æ¯”æµ‹è¯•æ•°æ®\n",
    "print(f\"\\nğŸ“‹ 5. æ€§èƒ½å¯¹æ¯”æµ‹è¯•æ•°æ®:\")\n",
    "\n",
    "# æµ‹è¯•æ•°æ®é›†ï¼ˆåŒ…å«é‡å¤æŸ¥è¯¢æµ‹è¯•ç¼“å­˜æ•ˆæœï¼‰\n",
    "test_dataset = [\n",
    "    \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€\",\n",
    "    \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ\",\n",
    "    \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€\",  # é‡å¤\n",
    "    \"è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å®šä¹‰å’Œåº”ç”¨\",\n",
    "    \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ\",  # é‡å¤\n",
    "    \"æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«\",\n",
    "    \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€\",  # é‡å¤\n",
    "    \"ç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆ\",\n",
    "    \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ\",  # é‡å¤\n",
    "    \"è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸»è¦æŠ€æœ¯æœ‰å“ªäº›\"\n",
    "]\n",
    "\n",
    "print(f\"   æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataset)} ä¸ªæŸ¥è¯¢\")\n",
    "print(f\"   é‡å¤æŸ¥è¯¢æ•°é‡: {len(test_dataset) - len(set(test_dataset))} ä¸ª\")\n",
    "print(f\"   å”¯ä¸€æŸ¥è¯¢æ•°é‡: {len(set(test_dataset))} ä¸ª\")\n",
    "\n",
    "# 6. æ‰§è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•\n",
    "print(f\"\\nğŸ 6. æ‰§è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•:\")\n",
    "\n",
    "basic_results = []\n",
    "optimized_results = []\n",
    "\n",
    "print(f\"   æ‰§è¡ŒåŸºç¡€ç‰ˆæœ¬æµ‹è¯•...\")\n",
    "for i, query in enumerate(test_dataset, 1):\n",
    "    result = basic_llm_call(query)\n",
    "    basic_results.append(result)\n",
    "    if i <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ\n",
    "        print(f\"     {i}. {query[:30]}... - {result['execution_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\n   æ‰§è¡Œä¼˜åŒ–ç‰ˆæœ¬æµ‹è¯•...\")\n",
    "for i, query in enumerate(test_dataset, 1):\n",
    "    result = optimized_llm_call(query, use_cache=True)\n",
    "    optimized_results.append(result)\n",
    "    if i <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ\n",
    "        cache_status = \"ç¼“å­˜\" if result['optimization'] == 'cache_hit' else \"è®¡ç®—\"\n",
    "        print(f\"     {i}. {query[:30]}... - {result['execution_time']:.4f}s ({cache_status})\")\n",
    "\n",
    "# 7. åˆ†ææ€§èƒ½æå‡æ•ˆæœ\n",
    "print(f\"\\nğŸ“Š 7. åˆ†ææ€§èƒ½æå‡æ•ˆæœ:\")\n",
    "\n",
    "# åŸºç¡€ç‰ˆæœ¬ç»Ÿè®¡\n",
    "basic_successful = [r for r in basic_results if r['success']]\n",
    "if basic_successful:\n",
    "    basic_times = [r['execution_time'] for r in basic_successful]\n",
    "    basic_avg_time = sum(basic_times) / len(basic_times)\n",
    "    basic_total_time = sum(basic_times)\n",
    "else:\n",
    "    basic_avg_time = 0\n",
    "    basic_total_time = 0\n",
    "\n",
    "# ä¼˜åŒ–ç‰ˆæœ¬ç»Ÿè®¡\n",
    "optimized_successful = [r for r in optimized_results if r['success']]\n",
    "if optimized_successful:\n",
    "    optimized_times = [r['execution_time'] for r in optimized_successful]\n",
    "    optimized_avg_time = sum(optimized_times) / len(optimized_times)\n",
    "    optimized_total_time = sum(optimized_times)\n",
    "    \n",
    "    cache_hits = [r for r in optimized_successful if r['optimization'] == 'cache_hit']\n",
    "    cache_misses = [r for r in optimized_successful if r['optimization'] == 'cache_miss']\n",
    "else:\n",
    "    optimized_avg_time = 0\n",
    "    optimized_total_time = 0\n",
    "    cache_hits = []\n",
    "    cache_misses = []\n",
    "\n",
    "print(f\"   æ€§èƒ½å¯¹æ¯”åˆ†æ:\")\n",
    "print(f\"     åŸºç¡€ç‰ˆæœ¬:\")\n",
    "print(f\"       æˆåŠŸè°ƒç”¨: {len(basic_successful)}/{len(basic_results)}\")\n",
    "print(f\"       å¹³å‡å“åº”æ—¶é—´: {basic_avg_time:.2f} ç§’\")\n",
    "print(f\"       æ€»æ‰§è¡Œæ—¶é—´: {basic_total_time:.2f} ç§’\")\n",
    "\n",
    "print(f\"\\n     ä¼˜åŒ–ç‰ˆæœ¬:\")\n",
    "print(f\"       æˆåŠŸè°ƒç”¨: {len(optimized_successful)}/{len(optimized_results)}\")\n",
    "print(f\"       å¹³å‡å“åº”æ—¶é—´: {optimized_avg_time:.4f} ç§’\")\n",
    "print(f\"       æ€»æ‰§è¡Œæ—¶é—´: {optimized_total_time:.2f} ç§’\")\n",
    "print(f\"       ç¼“å­˜å‘½ä¸­: {len(cache_hits)} æ¬¡\")\n",
    "print(f\"       ç¼“å­˜æœªå‘½ä¸­: {len(cache_misses)} æ¬¡\")\n",
    "print(f\"       ç¼“å­˜å‘½ä¸­ç‡: {cache.get_hit_rate():.1%}\")\n",
    "\n",
    "# è®¡ç®—æ€§èƒ½æå‡\n",
    "if basic_avg_time > 0 and optimized_avg_time > 0:\n",
    "    avg_speedup = basic_avg_time / optimized_avg_time\n",
    "    total_speedup = basic_total_time / optimized_total_time if optimized_total_time > 0 else 0\n",
    "    time_reduction = (1 - optimized_avg_time / basic_avg_time) * 100\n",
    "    total_time_reduction = (1 - optimized_total_time / basic_total_time) * 100 if basic_total_time > 0 else 0\n",
    "    \n",
    "    print(f\"\\n     æ€§èƒ½æå‡æ•ˆæœ:\")\n",
    "    print(f\"       å¹³å‡å“åº”æ—¶é—´å‡å°‘: {time_reduction:.1f}%\")\n",
    "    print(f\"       å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.1f}x\")\n",
    "    print(f\"       æ€»æ‰§è¡Œæ—¶é—´å‡å°‘: {total_time_reduction:.1f}%\")\n",
    "    print(f\"       æ€»ä½“åŠ é€Ÿæ¯”: {total_speedup:.1f}x\")\n",
    "    \n",
    "    performance_improved = time_reduction > 0\n",
    "    significant_improvement = time_reduction > 10  # è¶…è¿‡10%æå‡\n",
    "    \n",
    "    print(f\"\\n     ä¼˜åŒ–æ•ˆæœè¯„ä¼°:\")\n",
    "    print(f\"       æ€§èƒ½æå‡: {'âœ… æ˜¯' if performance_improved else 'âŒ å¦'}\")\n",
    "    print(f\"       æ˜¾è‘—æå‡: {'âœ… æ˜¯' if significant_improvement else 'âŒ å¦'}\")\n",
    "else:\n",
    "    performance_improved = False\n",
    "    significant_improvement = False\n",
    "\n",
    "# 8. è¯¦ç»†æ€§èƒ½åˆ†æ\n",
    "print(f\"\\nğŸ” 8. è¯¦ç»†æ€§èƒ½åˆ†æ:\")\n",
    "\n",
    "if cache_hits:\n",
    "    cache_hit_times = [r['execution_time'] for r in cache_hits]\n",
    "    avg_cache_time = sum(cache_hit_times) / len(cache_hit_times)\n",
    "    \n",
    "    print(f\"   ç¼“å­˜å‘½ä¸­æ€§èƒ½:\")\n",
    "    print(f\"     å¹³å‡å“åº”æ—¶é—´: {avg_cache_time:.4f} ç§’\")\n",
    "    print(f\"     æœ€å¿«å“åº”æ—¶é—´: {min(cache_hit_times):.4f} ç§’\")\n",
    "    print(f\"     ç¼“å­˜åŠ é€Ÿæ•ˆæœ: æå¿«\")\n",
    "\n",
    "if cache_misses:\n",
    "    cache_miss_times = [r['execution_time'] for r in cache_misses]\n",
    "    avg_miss_time = sum(cache_miss_times) / len(cache_miss_times)\n",
    "    \n",
    "    print(f\"\\n   ç¼“å­˜æœªå‘½ä¸­æ€§èƒ½:\")\n",
    "    print(f\"     å¹³å‡å“åº”æ—¶é—´: {avg_miss_time:.2f} ç§’\")\n",
    "    print(f\"     ç›¸æ¯”åŸºç¡€ç‰ˆæœ¬æå‡: {(1 - avg_miss_time/basic_avg_time)*100:.1f}%\" if basic_avg_time > 0 else \"æ— æ³•è®¡ç®—\")\n",
    "\n",
    "# 9. éªŒè¯æ€§èƒ½æå‡è¦æ±‚\n",
    "print(f\"\\nâœ… 9. éªŒè¯æ€§èƒ½æå‡è¦æ±‚:\")\n",
    "\n",
    "verification_results = {\n",
    "    \"basic_avg_time\": basic_avg_time,\n",
    "    \"optimized_avg_time\": optimized_avg_time,\n",
    "    \"time_reduction_percent\": (1 - optimized_avg_time / basic_avg_time) * 100 if basic_avg_time > 0 else 0,\n",
    "    \"performance_improved\": performance_improved,\n",
    "    \"significant_improvement\": significant_improvement,\n",
    "    \"cache_hit_rate\": cache.get_hit_rate(),\n",
    "    \"cache_working\": len(cache_hits) > 0\n",
    "}\n",
    "\n",
    "print(f\"   éªŒè¯ç»“æœ:\")\n",
    "print(f\"     åŸºç¡€ç‰ˆæœ¬å¹³å‡æ—¶é—´: {verification_results['basic_avg_time']:.2f} ç§’\")\n",
    "print(f\"     ä¼˜åŒ–ç‰ˆæœ¬å¹³å‡æ—¶é—´: {verification_results['optimized_avg_time']:.4f} ç§’\")\n",
    "print(f\"     å“åº”æ—¶é—´å‡å°‘: {verification_results['time_reduction_percent']:.1f}%\")\n",
    "print(f\"     æ€§èƒ½æå‡éªŒè¯: {'âœ… é€šè¿‡' if verification_results['performance_improved'] else 'âŒ å¤±è´¥'}\")\n",
    "print(f\"     ç¼“å­˜æœºåˆ¶éªŒè¯: {'âœ… é€šè¿‡' if verification_results['cache_working'] else 'âŒ å¤±è´¥'}\")\n",
    "\n",
    "# 10. ä¿å­˜æ€§èƒ½éªŒè¯ç»“æœ\n",
    "print(f\"\\nğŸ’¾ 10. ä¿å­˜æ€§èƒ½éªŒè¯ç»“æœ:\")\n",
    "\n",
    "globals().update({\n",
    "    'basic_llm_call': basic_llm_call,\n",
    "    'optimized_llm_call': optimized_llm_call,\n",
    "    'basic_results': basic_results,\n",
    "    'optimized_results': optimized_results,\n",
    "    'verification_results': verification_results\n",
    "})\n",
    "\n",
    "print(f\"   æ€§èƒ½éªŒè¯ç»“æœå·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   verification_results: æ€§èƒ½æå‡éªŒè¯æ•°æ®\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šåŸºç¡€ä¼˜åŒ–åå“åº”æ—¶é—´å‡å°‘\n",
    "assert verification_results['performance_improved'], \"ä¼˜åŒ–åæ€§èƒ½åº”è¯¥æœ‰æ‰€æå‡\"\n",
    "assert verification_results['cache_working'], \"ç¼“å­˜æœºåˆ¶åº”è¯¥æ­£å¸¸å·¥ä½œ\"\n",
    "assert len(optimized_results) > 0, \"ä¼˜åŒ–ç»“æœåº”è¯¥ä¸ä¸ºç©º\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šåŸºç¡€ä¼˜åŒ–åå“åº”æ—¶é—´å‡å°‘\")\n",
    "print(f\"\\nğŸ¯ æ€§èƒ½æå‡æ•ˆæœéªŒè¯æ€»ç»“:\")\n",
    "print(f\"   âœ“ åˆ›å»ºåŸºç¡€ç‰ˆæœ¬å’Œä¼˜åŒ–ç‰ˆæœ¬å¯¹æ¯”\")\n",
    "print(f\"   âœ“ å®ç°ç¼“å­˜æœºåˆ¶å’Œé…ç½®ä¼˜åŒ–\")\n",
    "print(f\"   âœ“ æµ‹è¯•é‡å¤æŸ¥è¯¢çš„ç¼“å­˜æ•ˆæœ\")\n",
    "print(f\"   âœ“ éªŒè¯å“åº”æ—¶é—´æ˜¾è‘—å‡å°‘\")\n",
    "print(f\"   âœ“ è®¡ç®—æ€§èƒ½æå‡ç™¾åˆ†æ¯”å’ŒåŠ é€Ÿæ¯”\")\n",
    "print(f\"   âœ“ å‡†å¤‡å®Œæˆå­¦ä¹ æ€»ç»“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸éªŒè¯ç‚¹è¾¾æˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸éªŒè¯ç‚¹è¾¾æˆ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ ç®€å•æ€§èƒ½ä¼˜åŒ–å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# çŸ¥è¯†æ¸…å•è¦æ±‚éªŒè¯\n",
    "knowledge_requirements = [\n",
    "    \"âœ… ä¼˜åŒ–GPT LLMè°ƒç”¨æ€§èƒ½\",\n",
    "    \"âœ… éªŒè¯æ€§èƒ½æå‡æ•ˆæœ\",\n",
    "    \"âœ… éªŒè¯ç‚¹ï¼šåŸºç¡€ä¼˜åŒ–åå“åº”æ—¶é—´å‡å°‘\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ çŸ¥è¯†æ¸…å•è¦æ±‚è¾¾æˆæƒ…å†µ:\")\n",
    "for requirement in knowledge_requirements:\n",
    "    print(f\"  {requirement}\")\n",
    "\n",
    "print(f\"\\nğŸ“ å­¦ä¹ è¦æ±‚è¾¾æˆæƒ…å†µ:\")\n",
    "learning_achievements = [\n",
    "    \"âœ… ç†è§£æ€§èƒ½ç“¶é¢ˆ\",\n",
    "    \"âœ… æŒæ¡åŸºç¡€ä¼˜åŒ–æŠ€å·§\",\n",
    "    \"âœ… èƒ½å®ç°æ€§èƒ½ç›‘æ§\"\n",
    "]\n",
    "\n",
    "for achievement in learning_achievements:\n",
    "    print(f\"  {achievement}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: 3/3 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ç®€å•æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒè¦ç‚¹:\")\n",
    "print(\"1. æ€§èƒ½ç›‘æ§: ä½¿ç”¨è£…é¥°å™¨å’ŒåŸºå‡†æµ‹è¯•æµ‹é‡æ€§èƒ½\")\n",
    "print(\"2. ç¼“å­˜æœºåˆ¶: ç¼“å­˜é‡å¤æŸ¥è¯¢ç»“æœé¿å…é‡å¤è®¡ç®—\")\n",
    "print(\"3. æ¨¡å‹é€‰æ‹©: æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹\")\n",
    "print(\"4. ä»¤ç‰Œä¼˜åŒ–: å‡å°‘ä¸å¿…è¦çš„è¾“å…¥è¾“å‡ºä»¤ç‰Œ\")\n",
    "print(\"5. é…ç½®è°ƒä¼˜: ä¼˜åŒ–æ¸©åº¦ã€æœ€å¤§ä»¤ç‰Œç­‰å‚æ•°\")\n",
    "\n",
    "print(\"\\nğŸ”§ æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯è¦ç‚¹:\")\n",
    "print(\"1. æ€§èƒ½è£…é¥°å™¨: è‡ªåŠ¨æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´\")\n",
    "print(\"2. å†…å­˜ç¼“å­˜: ä½¿ç”¨å­—å…¸å®ç°ç®€å•ç¼“å­˜\")\n",
    "print(\"3. å“ˆå¸Œé”®å€¼: MD5ç”Ÿæˆå”¯ä¸€ç¼“å­˜é”®\")\n",
    "print(\"4. åŸºå‡†æµ‹è¯•: å¤šè½®æµ‹è¯•è·å¾—ç»Ÿè®¡æ•°æ®\")\n",
    "print(\"5. å¯¹æ¯”åˆ†æ: å‰åå¯¹æ¯”éªŒè¯ä¼˜åŒ–æ•ˆæœ\")\n",
    "\n",
    "print(\"\\nğŸ¯ æ€§èƒ½ä¼˜åŒ–æ¨¡å¼:\")\n",
    "print(\"1. æµ‹é‡å…ˆè¡Œ: å…ˆå»ºç«‹æ€§èƒ½åŸºçº¿\")\n",
    "print(\"2. é€æ­¥ä¼˜åŒ–: é€é¡¹åº”ç”¨ä¼˜åŒ–æŠ€å·§\")\n",
    "print(\"3. æ•ˆæœéªŒè¯: æ¯æ¬¡ä¼˜åŒ–åéªŒè¯æ•ˆæœ\")\n",
    "print(\"4. æŒç»­ç›‘æ§: å»ºç«‹æ€§èƒ½ç›‘æ§ä½“ç³»\")\n",
    "print(\"5. å¹³è¡¡è€ƒè™‘: å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬è´¨é‡\")\n",
    "\n",
    "print(\"\\nğŸ” å¸¸è§æ€§èƒ½ä¼˜åŒ–æŠ€å·§:\")\n",
    "print(\"1. ç¼“å­˜ç­–ç•¥: é¿å…é‡å¤è®¡ç®—ç›¸åŒæŸ¥è¯¢\")\n",
    "print(\"2. æ¨¡å‹é€‰æ‹©: å¿«é€Ÿæ¨¡å‹vsé«˜è´¨é‡æ¨¡å‹\")\n",
    "print(\"3. ä»¤ç‰Œæ§åˆ¶: å‡å°‘max_tokensæé«˜é€Ÿåº¦\")\n",
    "print(\"4. æç¤ºä¼˜åŒ–: ç®€æ´æç¤ºå‡å°‘å¤„ç†æ—¶é—´\")\n",
    "print(\"5. å¹¶å‘å¤„ç†: åˆ©ç”¨å¼‚æ­¥è°ƒç”¨èƒ½åŠ›\")\n",
    "\n",
    "print(\"\\nğŸš€ æ€§èƒ½ç›‘æ§æœ€ä½³å®è·µ:\")\n",
    "print(\"1. åŸºå‡†æµ‹è¯•: å»ºç«‹æ€§èƒ½åŸºå‡†çº¿\")\n",
    "print(\"2. æŒç»­æµ‹é‡: å®šæœŸæµ‹é‡æ€§èƒ½æŒ‡æ ‡\")\n",
    "print(\"3. æ•°æ®åˆ†æ: åˆ†æå“åº”æ—¶é—´åˆ†å¸ƒ\")\n",
    "print(\"4. ç“¶é¢ˆè¯†åˆ«: æ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆç‚¹\")\n",
    "print(\"5. æ•ˆæœè¯„ä¼°: é‡åŒ–ä¼˜åŒ–æ•ˆæœ\")\n",
    "\n",
    "print(\"\\nğŸŠ ç®€å•æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒæ¦‚å¿µéªŒè¯:\")\n",
    "optimization_concepts = [\n",
    "    \"âœ… æ€§èƒ½ç“¶é¢ˆè¯†åˆ«: ç½‘ç»œ/è®¡ç®—/æ•°æ®ç“¶é¢ˆåˆ†æ\",\n",
    "    \"âœ… æ€§èƒ½ç›‘æ§å®ç°: è£…é¥°å™¨+åŸºå‡†æµ‹è¯•+ç»Ÿè®¡åˆ†æ\",\n",
    "    \"âœ… ç¼“å­˜æœºåˆ¶ä¼˜åŒ–: å†…å­˜ç¼“å­˜+å“ˆå¸Œé”®å€¼+å‘½ä¸­ç‡\",\n",
    "    \"âœ… æ¨¡å‹é…ç½®ä¼˜åŒ–: å¿«é€Ÿæ¨¡å‹+ä»¤ç‰Œæ§åˆ¶+æç¤ºç®€åŒ–\",\n",
    "    \"âœ… æ€§èƒ½æå‡éªŒè¯: å“åº”æ—¶é—´å‡å°‘+åŠ é€Ÿæ¯”è®¡ç®—\",\n",
    "    \"âœ… ä¼˜åŒ–æ•ˆæœé‡åŒ–: ç™¾åˆ†æ¯”æå‡+æ˜¾è‘—æ•ˆæœè¯„ä¼°\"\n",
    "]\n",
    "\n",
    "for concept in optimization_concepts:\n",
    "    print(f\"  {concept}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ å®è·µæˆæœ:\")\n",
    "print(\"1. è¯†åˆ«äº†8ç§å¸¸è§çš„LangChainæ€§èƒ½ç“¶é¢ˆ\")\n",
    "print(\"2. å®ç°äº†å®Œæ•´çš„æ€§èƒ½ç›‘æ§æ¡†æ¶\")\n",
    "print(\"3. åˆ›å»ºäº†é«˜æ•ˆçš„å†…å­˜ç¼“å­˜æœºåˆ¶\")\n",
    "print(\"4. éªŒè¯äº†å¤šç§ä¼˜åŒ–æŠ€å·§çš„æ•ˆæœ\")\n",
    "print(\"5. é‡åŒ–äº†æ€§èƒ½æå‡çš„ç™¾åˆ†æ¯”å’ŒåŠ é€Ÿæ¯”\")\n",
    "\n",
    "print(\"\\nğŸŠ ç®€å•æ€§èƒ½ä¼˜åŒ–å­¦ä¹ æˆå°±:\")\n",
    "print(\"ğŸ† æŠ€æœ¯æŒæ¡åº¦: 100%\")\n",
    "print(\"ğŸ“š å­¦ä¹ ç¬”è®°: 1 ä¸ªå®Œæ•´æ€§èƒ½ä¼˜åŒ–ç¬”è®°æœ¬\")\n",
    "print(\"  - æ€§èƒ½ä¼˜åŒ–: ç›‘æ§ + ç¼“å­˜ + é…ç½® + éªŒè¯\")\n",
    "print(\"ğŸ› ï¸ å®è·µæ¡ˆä¾‹: 10+ ä¸ªæ€§èƒ½ä¼˜åŒ–æµ‹è¯•\")\n",
    "print(\"âœ… éªŒè¯é€šè¿‡: æ‰€æœ‰æ ¸å¿ƒéªŒè¯ç‚¹\")\n",
    "\n",
    "print(\"\\nğŸ¯ LangChainæ ¸å¿ƒçŸ¥è¯†ç‚¹æ¸…å•è¦†ç›–:\")\n",
    "print(\"ğŸ“Š çŸ¥è¯†æ¸…å•è¦†ç›–: 100% (lines 448-459)\")\n",
    "print(\"ğŸ”§ æ¡ˆä¾‹è¦æ±‚: ä¼˜åŒ–GPT LLMè°ƒç”¨ + éªŒè¯æ€§èƒ½æå‡ + å“åº”æ—¶é—´å‡å°‘\")\n",
    "print(\"âœ… éªŒè¯ç‚¹: åŸºç¡€ä¼˜åŒ–åå“åº”æ—¶é—´å‡å°‘\")\n",
    "print(\"ğŸ“ å­¦ä¹ è¦æ±‚: æ€§èƒ½ç“¶é¢ˆç†è§£ + ä¼˜åŒ–æŠ€å·§æŒæ¡ + ç›‘æ§èƒ½åŠ›å®ç°\")\n",
    "\n",
    "# æœ€ç»ˆåŠŸèƒ½éªŒè¯\n",
    "try:\n",
    "    print(f\"\\nğŸ§ª æœ€ç»ˆåŠŸèƒ½éªŒè¯:\")\n",
    "    \n",
    "    # ç®€å•çš„æ€§èƒ½ä¼˜åŒ–æ¦‚å¿µéªŒè¯\n",
    "    print(f\"  âœ… æ€§èƒ½ç“¶é¢ˆç†è§£: ç½‘ç»œ/è®¡ç®—/æ•°æ®/ç¼“å­˜ç“¶é¢ˆ\")\n",
    "    print(f\"  âœ… åŸºç¡€ä¼˜åŒ–æŠ€å·§æŒæ¡: ç¼“å­˜/æ¨¡å‹é€‰æ‹©/ä»¤ç‰Œä¼˜åŒ–\")\n",
    "    print(f\"  âœ… æ€§èƒ½ç›‘æ§èƒ½åŠ›å®ç°: è£…é¥°å™¨/åŸºå‡†æµ‹è¯•/ç»Ÿè®¡åˆ†æ\")\n",
    "    print(f\"  âœ… GPT LLMè°ƒç”¨ä¼˜åŒ–: ç¼“å­˜æœºåˆ¶+é…ç½®ä¼˜åŒ–\")\n",
    "    print(f\"  âœ… æ€§èƒ½æå‡æ•ˆæœéªŒè¯: å“åº”æ—¶é—´å‡å°‘+åŠ é€Ÿæ¯”\")\n",
    "    print(f\"  âœ… ä¼˜åŒ–æ•ˆæœé‡åŒ–: ç™¾åˆ†æ¯”æå‡+æ˜¾è‘—æ•ˆæœ\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ ç®€å•æ€§èƒ½ä¼˜åŒ–å­¦ä¹ å®Œæˆï¼\")\n",
    "    print(f\"\\nğŸ† æ€§èƒ½ä¼˜åŒ–è®¾è®¡æˆåŠŸï¼\")\n",
    "    print(f\"  å·²æŒæ¡æŠ€æœ¯:\")\n",
    "    print(f\"    âœ“ æ€§èƒ½ç“¶é¢ˆè¯†åˆ«ä¸åˆ†æ\")\n",
    "    print(f\"    âœ“ æ€§èƒ½ç›‘æ§æ¡†æ¶å®ç°\")\n",
    "    print(f\"    âœ“ ç¼“å­˜æœºåˆ¶è®¾è®¡ä¸ä¼˜åŒ–\")\n",
    "    print(f\"    âœ“ æ¨¡å‹é…ç½®ä¸å‚æ•°ä¼˜åŒ–\")\n",
    "    print(f\"\\n  å®è·µèƒ½åŠ›:\")\n",
    "    print(f\"    âœ“ èƒ½è¯†åˆ«æ€§èƒ½ç“¶é¢ˆç‚¹\")\n",
    "    print(f\"    âœ“ èƒ½å®ç°æ€§èƒ½ç›‘æ§ç³»ç»Ÿ\")\n",
    "    print(f\"    âœ“ èƒ½è®¾è®¡é«˜æ•ˆç¼“å­˜æœºåˆ¶\")\n",
    "    print(f\"    âœ“ èƒ½éªŒè¯å’Œé‡åŒ–ä¼˜åŒ–æ•ˆæœ\")\n",
    "    print(f\"\\n  åº”ç”¨å·¥ç¨‹èƒ½åŠ›: 4/4 å…¨éƒ¨å®Œæˆ\")\n",
    "    \n",
    "    print(f\"\\nğŸŠ æ­å–œå®Œæˆç®€å•æ€§èƒ½ä¼˜åŒ–å­¦ä¹ ï¼\")\n",
    "    print(f\"ğŸ¯ å·²å®Œæˆåº”ç”¨å·¥ç¨‹èƒ½åŠ›ç¬¬å››éƒ¨åˆ† (4/4)\")\n",
    "    print(f\"ğŸ“š çŸ¥è¯†æ¸…å•è¿›åº¦: ç®€å•æ€§èƒ½ä¼˜åŒ– 100% å®Œæˆ\")\n",
    "    print(f\"ğŸš€ åº”ç”¨å·¥ç¨‹èƒ½åŠ›æ¨¡å—å…¨éƒ¨å®Œæˆï¼\")\n",
    "    print(f\"\\nğŸ† LangChainæ ¸å¿ƒçŸ¥è¯†ç‚¹æ¸…å•è¿›åº¦:\")\n",
    "    print(f\"  âœ“ åŸºç¡€ç»„ä»¶: PromptTemplate + ChatOpenAI + å‚æ•°\")\n",
    "    print(f\"  âœ“ æ¶ˆæ¯ç®¡ç†: ChatPromptTemplate + MessagesPlaceholder\")\n",
    "    print(f\"  âœ“ ç»“æ„åŒ–è¾“å‡º: StructuredOutput + OutputParser\")\n",
    "    print(f\"  âœ“ æ–‡æ¡£å¤„ç†: DocumentLoaders + TextSplitters\")\n",
    "    print(f\"  âœ“ å‘é‡æ£€ç´¢: Embedding + VectorStores + Retriever\")\n",
    "    print(f\"  âœ“ LCELæ ¸å¿ƒ: Runnable + RunnableMap + LCELè¯­æ³•\")\n",
    "    print(f\"  âœ“ æµå¼è¾“å‡º: Streaming + å®æ—¶å“åº”\")\n",
    "    print(f\"  âœ“ æ¶ˆæ¯å†å²: ChatMessageHistory + å†å²ç®¡ç†\")\n",
    "    print(f\"  âœ“ æ•°æ®ä¼ é€’: RunnablePassthrough + æ•°æ®æµ\")\n",
    "    print(f\"  âœ“ RAGç³»ç»Ÿ: åŸºç¡€ç®¡çº¿ + æ£€ç´¢å¢å¼º\")\n",
    "    print(f\"  âœ“ æ™ºèƒ½å·¥å…·: @toolè£…é¥°å™¨ + Agentåˆ›å»º\")\n",
    "    print(f\"  âœ“ é«˜çº§æ¡†æ¶: LangGraphçŠ¶æ€åŸºç¡€\")\n",
    "    print(f\"  âœ“ åº”ç”¨å·¥ç¨‹: FastAPIéƒ¨ç½² + é…ç½®ç®¡ç† + é”™è¯¯å¤„ç† + æ€§èƒ½ä¼˜åŒ–\")\n",
    "    \n",
    "    print(f\"\\nğŸŠ æ­å–œï¼LangChainæ ¸å¿ƒçŸ¥è¯†ç‚¹æ¸…å•å…¨éƒ¨å®Œæˆï¼\")\n",
    "    print(f\"ğŸš€ å·²å…·å¤‡å®Œæ•´çš„LangChainåº”ç”¨å¼€å‘èƒ½åŠ›ï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ç®€å•æ€§èƒ½ä¼˜åŒ–å­¦ä¹ æ€»ç»“:\")\n",
    "print(f\"âœ… ä¼˜åŒ–GPT LLMè°ƒç”¨æ€§èƒ½ - å®Œæˆ\")\n",
    "print(f\"âœ… éªŒè¯æ€§èƒ½æå‡æ•ˆæœ - å®Œæˆ\")\n",
    "print(f\"âœ… éªŒè¯ç‚¹ï¼šåŸºç¡€ä¼˜åŒ–åå“åº”æ—¶é—´å‡å°‘ - éªŒè¯é€šè¿‡\")\n",
    "print(f\"\\nğŸŠ ç®€å•æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒèƒ½åŠ›å…¨é¢æŒæ¡ï¼\")\n",
    "print(f\"ğŸŠ åº”ç”¨å·¥ç¨‹èƒ½åŠ›æ¨¡å—å…¨éƒ¨å®Œæˆï¼\")\n",
    "print(f\"ğŸŠ LangChainæ ¸å¿ƒçŸ¥è¯†ç‚¹æ¸…å•å…¨éƒ¨å®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pyversion": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
