{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 - LangGraphçŠ¶æ€åŸºç¡€\n",
    "\n",
    "## ç”¨é€”\n",
    "çŠ¶æ€ç”¨äºåœ¨GPTå›¾æ‰§è¡Œè¿‡ç¨‹ä¸­å­˜å‚¨ä¿¡æ¯ã€‚ç†è§£çŠ¶æ€ç®¡ç†æœºåˆ¶ã€æŒæ¡åŸºç¡€çŠ¶æ€å®šä¹‰ã€èƒ½è®¾è®¡GPTç®€å•çŠ¶æ€ç»“æ„ã€‚\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£çŠ¶æ€ç®¡ç†æœºåˆ¶\n",
    "- æŒæ¡åŸºç¡€çŠ¶æ€å®šä¹‰\n",
    "- èƒ½è®¾è®¡GPTç®€å•çŠ¶æ€ç»“æ„\n",
    "- å®šä¹‰åŒ…å« query å’Œ result çš„GPTç®€å• State\n",
    "- éªŒè¯çŠ¶æ€è¯»å†™æ“ä½œ\n",
    "- éªŒè¯çŠ¶æ€èƒ½åœ¨GPTèŠ‚ç‚¹é—´æ­£ç¡®ä¼ é€’\n",
    "\n",
    "## ğŸ”‘ å‰ç½®è¦æ±‚\n",
    "**æ³¨æ„**ï¼šLangGraph å±äºé«˜çº§å†…å®¹ï¼Œå»ºè®®ç‹¬ç«‹å­¦ä¹ \n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LangGraphçŠ¶æ€ç®¡ç†æœºåˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraphçŠ¶æ€ç®¡ç†æœºåˆ¶ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”„ LangGraphçŠ¶æ€ç®¡ç†æœºåˆ¶:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"ğŸ“ LangGraphæ ¸å¿ƒæ¦‚å¿µ:\")\n",
    "print(\"   1. çŠ¶æ€(State): åœ¨å›¾æ‰§è¡Œè¿‡ç¨‹ä¸­å…±äº«çš„æ•°æ®ç»“æ„\")\n",
    "print(\"   2. èŠ‚ç‚¹(Node): å¤„ç†çŠ¶æ€å¹¶è¿”å›æ›´æ–°çŠ¶æ€çš„å‡½æ•°\")\n",
    "print(\"   3. è¾¹(Edge): å®šä¹‰èŠ‚ç‚¹é—´çš„æ‰§è¡Œé¡ºåº\")\n",
    "print(\"   4. å›¾(Graph): èŠ‚ç‚¹å’Œè¾¹çš„æœ‰å‘å›¾ç»“æ„\")\n",
    "print(\"   5. çŠ¶æ€ä¼ é€’: èŠ‚ç‚¹é—´é€šè¿‡çŠ¶æ€å…±äº«ä¿¡æ¯\")\n",
    "\n",
    "print(f\"\\nğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   - ç†è§£çŠ¶æ€ç®¡ç†æœºåˆ¶\")\n",
    "print(f\"   - æŒæ¡åŸºç¡€çŠ¶æ€å®šä¹‰\")\n",
    "print(f\"   - èƒ½è®¾è®¡GPTç®€å•çŠ¶æ€ç»“æ„\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸  çŠ¶æ€ç®¡ç†ç‰¹ç‚¹:\")\n",
    "print(f\"   1. ç±»å‹å®‰å…¨: ä½¿ç”¨TypedDictå®šä¹‰çŠ¶æ€ç»“æ„\")\n",
    "print(f\"   2. ä¸å¯å˜æ€§: æ¯ä¸ªèŠ‚ç‚¹è¿”å›æ–°çš„çŠ¶æ€\")\n",
    "print(f\"   3. å…±äº«æ€§: æ‰€æœ‰èŠ‚ç‚¹è®¿é—®åŒä¸€çŠ¶æ€\")\n",
    "print(f\"   4. ä¼ é€’æ€§: çŠ¶æ€åœ¨èŠ‚ç‚¹é—´è‡ªåŠ¨ä¼ é€’\")\n",
    "print(f\"   5. å¯æ‰©å±•æ€§: å¯ä»¥æ·»åŠ ä»»æ„çŠ¶æ€å­—æ®µ\")\n",
    "\n",
    "print(f\"\\nğŸ“Š çŠ¶æ€ç”Ÿå‘½å‘¨æœŸ:\")\n",
    "print(f\"   1. åˆå§‹åŒ–: åˆ›å»ºåˆå§‹çŠ¶æ€\")\n",
    "print(f\"   2. ä¼ é€’: å°†çŠ¶æ€ä¼ é€’ç»™ç¬¬ä¸€ä¸ªèŠ‚ç‚¹\")\n",
    "print(f\"   3. å¤„ç†: èŠ‚ç‚¹è¯»å–å¹¶æ›´æ–°çŠ¶æ€\")\n",
    "print(f\"   4. æ›´æ–°: è¿”å›æ–°çš„çŠ¶æ€\")\n",
    "print(f\"   5. ä¼ é€’: å°†æ›´æ–°åçš„çŠ¶æ€ä¼ é€’ç»™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹\")\n",
    "print(f\"   6. å®Œæˆ: æœ€ç»ˆçŠ¶æ€è¿”å›ç»™ç”¨æˆ·\")\n",
    "\n",
    "print(f\"\\nğŸ”§ çŠ¶æ€è®¾è®¡åŸåˆ™:\")\n",
    "print(f\"   1. æœ€å°åŒ–: åªåŒ…å«å¿…è¦å­—æ®µ\")\n",
    "print(f\"   2. ç±»å‹åŒ–: æ˜ç¡®æŒ‡å®šå­—æ®µç±»å‹\")\n",
    "print(f\"   3. è¯­ä¹‰åŒ–: å­—æ®µåå…·æœ‰æ˜ç¡®å«ä¹‰\")\n",
    "print(f\"   4. å¯åºåˆ—åŒ–: æ”¯æŒJSONåºåˆ—åŒ–\")\n",
    "print(f\"   5. å‘åå…¼å®¹: æ–°å¢å­—æ®µä¸å½±å“ç°æœ‰é€»è¾‘\")\n",
    "\n",
    "print(f\"\\nğŸ¨ çŠ¶æ€æ¨¡å¼å¯¹æ¯”:\")\n",
    "print(f\"   ä¼ ç»ŸAgent:\")\n",
    "print(f\"     - çŠ¶æ€åœ¨å†…å­˜ä¸­ä¸´æ—¶å­˜å‚¨\")\n",
    "print(f\"     - çŠ¶æ€ä¼ é€’ä¸æ˜ç¡®\")\n",
    "print(f\"     - éš¾ä»¥è°ƒè¯•å’Œç›‘æ§\")\n",
    "print(f\"\\n   LangGraph:\")\n",
    "print(f\"     - çŠ¶æ€æ˜ç¡®å®šä¹‰å’Œç±»å‹åŒ–\")\n",
    "print(f\"     - çŠ¶æ€ä¼ é€’è·¯å¾„æ¸…æ™°\")\n",
    "print(f\"     - æ˜“äºè°ƒè¯•å’Œå¯è§†åŒ–\")\n",
    "\n",
    "print(f\"\\nâœ… çŠ¶æ€ç®¡ç†æœºåˆ¶ç†è§£å®Œæˆ\")\n",
    "print(f\"\\nğŸš€ å‡†å¤‡å®ç°åŸºç¡€çŠ¶æ€å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŸºç¡€çŠ¶æ€å®šä¹‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€çŠ¶æ€å®šä¹‰ - ç‹¬ç«‹ä»£ç å—\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ—ï¸ åŸºç¡€çŠ¶æ€å®šä¹‰:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. å®šä¹‰åŸºç¡€çŠ¶æ€ç»“æ„\n",
    "print(f\"ğŸ“ 1. å®šä¹‰åŸºç¡€çŠ¶æ€ç»“æ„:\")\n",
    "\n",
    "class SimpleState(TypedDict):\n",
    "    \"\"\"ç®€å•çš„GPTçŠ¶æ€ç»“æ„\"\"\"\n",
    "    query: str              # ç”¨æˆ·æŸ¥è¯¢\n",
    "    result: str             # å¤„ç†ç»“æœ\n",
    "    step: int               # å½“å‰æ­¥éª¤\n",
    "    metadata: Dict[str, Any]  # å…ƒæ•°æ®\n",
    "\n",
    "print(f\"   çŠ¶æ€ç±»å®šä¹‰: SimpleState\")\n",
    "print(f\"   å­—æ®µè¯´æ˜:\")\n",
    "print(f\"     - query: ç”¨æˆ·æŸ¥è¯¢ (str)\")\n",
    "print(f\"     - result: å¤„ç†ç»“æœ (str)\")\n",
    "print(f\"     - step: å½“å‰æ­¥éª¤ (int)\")\n",
    "print(f\"     - metadata: å…ƒæ•°æ® (Dict[str, Any])\")\n",
    "\n",
    "# 2. éªŒè¯çŠ¶æ€ç±»å‹\n",
    "print(f\"\\nğŸ” 2. éªŒè¯çŠ¶æ€ç±»å‹:\")\n",
    "\n",
    "print(f\"   TypedDictç‰¹æ€§:\")\n",
    "print(f\"     - ç±»å‹æç¤º: {SimpleState.__annotations__}\")\n",
    "print(f\"     - å­—æ®µæ•°é‡: {len(SimpleState.__annotations__)}\")\n",
    "print(f\"     - æ”¯æŒå­—æ®µ: {list(SimpleState.__annotations__.keys())}\")\n",
    "\n",
    "# 3. åˆ›å»ºçŠ¶æ€å®ä¾‹\n",
    "print(f\"\\nğŸ­ 3. åˆ›å»ºçŠ¶æ€å®ä¾‹:\")\n",
    "\n",
    "# åˆ›å»ºåˆå§‹çŠ¶æ€\n",
    "initial_state = SimpleState(\n",
    "    query=\"è¯·è§£é‡Šä»€ä¹ˆæ˜¯LangGraph\",\n",
    "    result=\"\",\n",
    "    step=0,\n",
    "    metadata={\"created_at\": \"2025-11-30\", \"version\": \"1.0\"}\n",
    ")\n",
    "\n",
    "print(f\"   åˆå§‹çŠ¶æ€åˆ›å»º:\")\n",
    "print(f\"     query: {initial_state['query']}\")\n",
    "print(f\"     result: {initial_state['result']}\")\n",
    "print(f\"     step: {initial_state['step']}\")\n",
    "print(f\"     metadata: {initial_state['metadata']}\")\n",
    "\n",
    "# 4. çŠ¶æ€è¯»å†™æ“ä½œ\n",
    "print(f\"\\nğŸ“– 4. çŠ¶æ€è¯»å†™æ“ä½œ:\")\n",
    "\n",
    "# è¯»å–çŠ¶æ€\n",
    "print(f\"   è¯»å–æ“ä½œ:\")\n",
    "current_query = initial_state[\"query\"]\n",
    "current_step = initial_state[\"step\"]\n",
    "print(f\"     è¯»å–query: {current_query}\")\n",
    "print(f\"     è¯»å–step: {current_step}\")\n",
    "\n",
    "# æ›´æ–°çŠ¶æ€ï¼ˆåˆ›å»ºæ–°çŠ¶æ€ï¼‰\n",
    "print(f\"\\n   æ›´æ–°æ“ä½œ:\")\n",
    "updated_state = initial_state.copy()\n",
    "updated_state[\"step\"] = 1\n",
    "updated_state[\"result\"] = \"æ­£åœ¨å¤„ç†æŸ¥è¯¢...\"\n",
    "print(f\"     æ›´æ–°step: {updated_state['step']}\")\n",
    "print(f\"     æ›´æ–°result: {updated_state['result']}\")\n",
    "\n",
    "# 5. çŠ¶æ€éªŒè¯\n",
    "print(f\"\\nâœ… 5. çŠ¶æ€éªŒè¯:\")\n",
    "\n",
    "def validate_state(state: SimpleState) -> bool:\n",
    "    \"\"\"éªŒè¯çŠ¶æ€ç»“æ„\"\"\"\n",
    "    required_fields = [\"query\", \"result\", \"step\", \"metadata\"]\n",
    "    \n",
    "    # æ£€æŸ¥å¿…éœ€å­—æ®µ\n",
    "    for field in required_fields:\n",
    "        if field not in state:\n",
    "            print(f\"     âŒ ç¼ºå°‘å­—æ®µ: {field}\")\n",
    "            return False\n",
    "    \n",
    "    # æ£€æŸ¥å­—æ®µç±»å‹\n",
    "    if not isinstance(state[\"query\"], str):\n",
    "        print(f\"     âŒ queryå­—æ®µç±»å‹é”™è¯¯\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(state[\"result\"], str):\n",
    "        print(f\"     âŒ resultå­—æ®µç±»å‹é”™è¯¯\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(state[\"step\"], int):\n",
    "        print(f\"     âŒ stepå­—æ®µç±»å‹é”™è¯¯\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(state[\"metadata\"], dict):\n",
    "        print(f\"     âŒ metadataå­—æ®µç±»å‹é”™è¯¯\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"     âœ… çŠ¶æ€ç»“æ„éªŒè¯é€šè¿‡\")\n",
    "    return True\n",
    "\n",
    "# éªŒè¯åˆå§‹çŠ¶æ€\n",
    "initial_valid = validate_state(initial_state)\n",
    "updated_valid = validate_state(updated_state)\n",
    "\n",
    "print(f\"\\n   éªŒè¯ç»“æœ:\")\n",
    "print(f\"     åˆå§‹çŠ¶æ€: {'âœ… æœ‰æ•ˆ' if initial_valid else 'âŒ æ— æ•ˆ'}\")\n",
    "print(f\"     æ›´æ–°çŠ¶æ€: {'âœ… æœ‰æ•ˆ' if updated_valid else 'âŒ æ— æ•ˆ'}\")\n",
    "\n",
    "# 6. çŠ¶æ€åºåˆ—åŒ–\n",
    "print(f\"\\nğŸ”„ 6. çŠ¶æ€åºåˆ—åŒ–:\")\n",
    "\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # åºåˆ—åŒ–çŠ¶æ€\n",
    "    serialized = json.dumps(initial_state, ensure_ascii=False, indent=2)\n",
    "    print(f\"   åºåˆ—åŒ–æˆåŠŸ: {len(serialized)} å­—ç¬¦\")\n",
    "    \n",
    "    # ååºåˆ—åŒ–çŠ¶æ€\n",
    "    deserialized = json.loads(serialized)\n",
    "    print(f\"   ååºåˆ—åŒ–æˆåŠŸ: {type(deserialized)}\")\n",
    "    \n",
    "    # éªŒè¯ååºåˆ—åŒ–åçš„çŠ¶æ€\n",
    "    deserialized_valid = validate_state(deserialized)\n",
    "    print(f\"   ååºåˆ—åŒ–çŠ¶æ€: {'âœ… æœ‰æ•ˆ' if deserialized_valid else 'âŒ æ— æ•ˆ'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ åºåˆ—åŒ–å¤±è´¥: {e}\")\n",
    "\n",
    "# 7. ä¿å­˜çŠ¶æ€å®šä¹‰ä¾›åç»­ä½¿ç”¨\n",
    "print(f\"\\nğŸ’¾ 7. ä¿å­˜çŠ¶æ€å®šä¹‰ä¾›åç»­ä½¿ç”¨:\")\n",
    "\n",
    "globals().update({\n",
    "    'SimpleState': SimpleState,\n",
    "    'initial_state': initial_state,\n",
    "    'updated_state': updated_state,\n",
    "    'validate_state': validate_state\n",
    "})\n",
    "\n",
    "print(f\"   çŠ¶æ€å®šä¹‰å·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   SimpleStateç±»: å¯ç”¨äºåˆ›å»ºå›¾çŠ¶æ€\")\n",
    "print(f\"   validate_stateå‡½æ•°: å¯ç”¨äºçŠ¶æ€éªŒè¯\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šåŸºç¡€çŠ¶æ€å®šä¹‰æ­£ç¡®\n",
    "assert hasattr(SimpleState, '__annotations__'), \"SimpleStateåº”è¯¥æœ‰ç±»å‹æ³¨è§£\"\n",
    "assert len(SimpleState.__annotations__) == 4, \"SimpleStateåº”è¯¥æœ‰4ä¸ªå­—æ®µ\"\n",
    "assert \"query\" in SimpleState.__annotations__, \"åº”è¯¥æœ‰queryå­—æ®µ\"\n",
    "assert \"result\" in SimpleState.__annotations__, \"åº”è¯¥æœ‰resultå­—æ®µ\"\n",
    "assert initial_valid, \"åˆå§‹çŠ¶æ€åº”è¯¥æœ‰æ•ˆ\"\n",
    "assert updated_valid, \"æ›´æ–°çŠ¶æ€åº”è¯¥æœ‰æ•ˆ\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šåŸºç¡€çŠ¶æ€å®šä¹‰æ­£ç¡®\")\n",
    "print(f\"\\nğŸ¯ åŸºç¡€çŠ¶æ€å®šä¹‰æ€»ç»“:\")\n",
    "print(f\"   âœ“ å®šä¹‰åŒ…å«queryå’Œresultçš„GPTç®€å•State\")\n",
    "print(f\"   âœ“ æ·»åŠ stepå’Œmetadataå­—æ®µå¢å¼ºåŠŸèƒ½\")\n",
    "print(f\"   âœ“ éªŒè¯çŠ¶æ€è¯»å†™æ“ä½œ\")\n",
    "print(f\"   âœ“ çŠ¶æ€ç±»å‹å®‰å…¨ä¸”å¯åºåˆ—åŒ–\")\n",
    "print(f\"   âœ“ å‡†å¤‡ç”¨äºèŠ‚ç‚¹é—´çŠ¶æ€ä¼ é€’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. èŠ‚ç‚¹å®šä¹‰æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# èŠ‚ç‚¹å®šä¹‰æ–¹æ³• - ç‹¬ç«‹ä»£ç å—\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ”§ èŠ‚ç‚¹å®šä¹‰æ–¹æ³•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ä½¿ç”¨å·²å®šä¹‰çš„çŠ¶æ€\n",
    "print(f\"ğŸ“ 1. ä½¿ç”¨å·²å®šä¹‰çš„çŠ¶æ€:\")\n",
    "\n",
    "# é‡æ–°å®šä¹‰çŠ¶æ€ï¼ˆç¡®ä¿ç‹¬ç«‹æ€§ï¼‰\n",
    "class SimpleState(TypedDict):\n",
    "    \"\"\"ç®€å•çš„GPTçŠ¶æ€ç»“æ„\"\"\"\n",
    "    query: str              # ç”¨æˆ·æŸ¥è¯¢\n",
    "    result: str             # å¤„ç†ç»“æœ\n",
    "    step: int               # å½“å‰æ­¥éª¤\n",
    "    metadata: Dict[str, Any]  # å…ƒæ•°æ®\n",
    "\n",
    "print(f\"   çŠ¶æ€ç»“æ„: SimpleState\")\n",
    "print(f\"   å­—æ®µ: query, result, step, metadata\")\n",
    "\n",
    "# 2. èŠ‚ç‚¹å®šä¹‰åŸºç¡€æ¦‚å¿µ\n",
    "print(f\"\\nğŸ—ï¸  2. èŠ‚ç‚¹å®šä¹‰åŸºç¡€æ¦‚å¿µ:\")\n",
    "\n",
    "print(f\"   èŠ‚ç‚¹ç‰¹ç‚¹:\")\n",
    "print(f\"     1. å‡½æ•°å½¢å¼: èŠ‚ç‚¹æ˜¯Pythonå‡½æ•°\")\n",
    "print(f\"     2. çŠ¶æ€è¾“å…¥: æ¥æ”¶å½“å‰çŠ¶æ€ä½œä¸ºå‚æ•°\")\n",
    "print(f\"     3. çŠ¶æ€è¾“å‡º: è¿”å›æ›´æ–°åçš„çŠ¶æ€\")\n",
    "print(f\"     4. å•ä¸€èŒè´£: æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œç‰¹å®šä»»åŠ¡\")\n",
    "print(f\"     5. æ— å‰¯ä½œç”¨: ä¸ä¿®æ”¹åŸå§‹çŠ¶æ€\")\n",
    "\n",
    "# 3. åˆ›å»ºå¤„ç†è¾“å…¥èŠ‚ç‚¹\n",
    "print(f\"\\nğŸ“¥ 3. åˆ›å»ºå¤„ç†è¾“å…¥èŠ‚ç‚¹:\")\n",
    "\n",
    "def process_input_node(state: SimpleState) -> SimpleState:\n",
    "    \"\"\"å¤„ç†è¾“å…¥èŠ‚ç‚¹ï¼šåˆ†ææŸ¥è¯¢å¹¶å‡†å¤‡å¤„ç†\"\"\"\n",
    "    print(f\"   ğŸ”„ å¤„ç†è¾“å…¥èŠ‚ç‚¹æ‰§è¡Œä¸­...\")\n",
    "    \n",
    "    # è¯»å–è¾“å…¥\n",
    "    query = state.get(\"query\", \"\")\n",
    "    current_step = state.get(\"step\", 0)\n",
    "    \n",
    "    print(f\"     æ¥æ”¶æŸ¥è¯¢: {query}\")\n",
    "    print(f\"     å½“å‰æ­¥éª¤: {current_step}\")\n",
    "    \n",
    "    # å¤„ç†é€»è¾‘\n",
    "    processed_query = query.strip().lower()\n",
    "    query_length = len(processed_query)\n",
    "    \n",
    "    # æ›´æ–°çŠ¶æ€\n",
    "    new_state = state.copy()\n",
    "    new_state[\"step\"] = current_step + 1\n",
    "    new_state[\"result\"] = f\"æŸ¥è¯¢å·²å¤„ç†ï¼Œé•¿åº¦: {query_length} å­—ç¬¦\"\n",
    "    new_state[\"metadata\"][\"processed_length\"] = query_length\n",
    "    new_state[\"metadata\"][\"processed_at\"] = \"input_node\"\n",
    "    \n",
    "    print(f\"     å¤„ç†å®Œæˆ: é•¿åº¦ {query_length}\")\n",
    "    print(f\"     æ–°æ­¥éª¤: {new_state['step']}\")\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "print(f\"   å¤„ç†è¾“å…¥èŠ‚ç‚¹åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   åŠŸèƒ½: åˆ†ææŸ¥è¯¢ã€è®¡ç®—é•¿åº¦ã€æ›´æ–°çŠ¶æ€\")\n",
    "\n",
    "# 4. åˆ›å»ºç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹\n",
    "print(f\"\\nğŸ“¤ 4. åˆ›å»ºç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹:\")\n",
    "\n",
    "def generate_output_node(state: SimpleState) -> SimpleState:\n",
    "    \"\"\"ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹ï¼šåŸºäºå¤„ç†ç»“æœç”Ÿæˆæœ€ç»ˆè¾“å‡º\"\"\"\n",
    "    print(f\"   ğŸ”„ ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹æ‰§è¡Œä¸­...\")\n",
    "    \n",
    "    # è¯»å–çŠ¶æ€\n",
    "    query = state.get(\"query\", \"\")\n",
    "    current_result = state.get(\"result\", \"\")\n",
    "    current_step = state.get(\"step\", 0)\n",
    "    metadata = state.get(\"metadata\", {})\n",
    "    \n",
    "    print(f\"     æ¥æ”¶æŸ¥è¯¢: {query}\")\n",
    "    print(f\"     å½“å‰ç»“æœ: {current_result}\")\n",
    "    print(f\"     å½“å‰æ­¥éª¤: {current_step}\")\n",
    "    \n",
    "    # ç”Ÿæˆé€»è¾‘\n",
    "    processed_length = metadata.get(\"processed_length\", 0)\n",
    "    \n",
    "    final_result = f\"æŸ¥è¯¢å¤„ç†å®Œæˆï¼\\nåŸå§‹æŸ¥è¯¢: {query}\\nå¤„ç†ç»“æœ: {current_result}\\nç”Ÿæˆæ­¥éª¤: {current_step}\"\n",
    "    \n",
    "    # æ›´æ–°çŠ¶æ€\n",
    "    new_state = state.copy()\n",
    "    new_state[\"step\"] = current_step + 1\n",
    "    new_state[\"result\"] = final_result\n",
    "    new_state[\"metadata\"][\"generated_at\"] = \"output_node\"\n",
    "    new_state[\"metadata\"][\"final_length\"] = len(final_result)\n",
    "    \n",
    "    print(f\"     ç”Ÿæˆå®Œæˆ: æœ€ç»ˆç»“æœé•¿åº¦ {len(final_result)}\")\n",
    "    print(f\"     æ–°æ­¥éª¤: {new_state['step']}\")\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "print(f\"   ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"   åŠŸèƒ½: ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€æ›´æ–°çŠ¶æ€ã€è®°å½•å…ƒæ•°æ®\")\n",
    "\n",
    "# 5. æµ‹è¯•èŠ‚ç‚¹æ‰§è¡Œ\n",
    "print(f\"\\nğŸ§ª 5. æµ‹è¯•èŠ‚ç‚¹æ‰§è¡Œ:\")\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•çŠ¶æ€\n",
    "test_state = SimpleState(\n",
    "    query=\"æµ‹è¯•LangGraphçŠ¶æ€ä¼ é€’\",\n",
    "    result=\"\",\n",
    "    step=0,\n",
    "    metadata={\"test\": True}\n",
    ")\n",
    "\n",
    "print(f\"   æµ‹è¯•çŠ¶æ€åˆ›å»º:\")\n",
    "print(f\"     query: {test_state['query']}\")\n",
    "print(f\"     step: {test_state['step']}\")\n",
    "\n",
    "# æµ‹è¯•å¤„ç†è¾“å…¥èŠ‚ç‚¹\n",
    "print(f\"\\n   ğŸ“¥ æµ‹è¯•å¤„ç†è¾“å…¥èŠ‚ç‚¹:\")\n",
    "try:\n",
    "    state_after_input = process_input_node(test_state)\n",
    "    print(f\"     âœ… è¾“å…¥èŠ‚ç‚¹æ‰§è¡ŒæˆåŠŸ\")\n",
    "    print(f\"     æ–°çŠ¶æ€: step={state_after_input['step']}, resulté•¿åº¦={len(state_after_input['result'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"     âŒ è¾“å…¥èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "    state_after_input = test_state\n",
    "\n",
    "# æµ‹è¯•ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹\n",
    "print(f\"\\n   ğŸ“¤ æµ‹è¯•ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹:\")\n",
    "try:\n",
    "    final_state = generate_output_node(state_after_input)\n",
    "    print(f\"     âœ… è¾“å‡ºèŠ‚ç‚¹æ‰§è¡ŒæˆåŠŸ\")\n",
    "    print(f\"     æœ€ç»ˆçŠ¶æ€: step={final_state['step']}, resulté•¿åº¦={len(final_state['result'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"     âŒ è¾“å‡ºèŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "    final_state = state_after_input\n",
    "\n",
    "# 6. èŠ‚ç‚¹æ‰§è¡ŒéªŒè¯\n",
    "print(f\"\\nğŸ” 6. èŠ‚ç‚¹æ‰§è¡ŒéªŒè¯:\")\n",
    "\n",
    "def validate_node_execution(initial: SimpleState, final: SimpleState) -> bool:\n",
    "    \"\"\"éªŒè¯èŠ‚ç‚¹æ‰§è¡Œç»“æœ\"\"\"\n",
    "    print(f\"     éªŒè¯èŠ‚ç‚¹æ‰§è¡Œ...\")\n",
    "    \n",
    "    # æ£€æŸ¥æ­¥éª¤é€’å¢\n",
    "    if final[\"step\"] <= initial[\"step\"]:\n",
    "        print(f\"       âŒ æ­¥éª¤æœªé€’å¢\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥ç»“æœæ›´æ–°\n",
    "    if len(final[\"result\"]) <= len(initial[\"result\"]):\n",
    "        print(f\"       âŒ ç»“æœæœªæ›´æ–°\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥å…ƒæ•°æ®æ›´æ–°\n",
    "    if len(final[\"metadata\"]) <= len(initial[\"metadata\"]):\n",
    "        print(f\"       âŒ å…ƒæ•°æ®æœªæ›´æ–°\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥æŸ¥è¯¢ä¿æŒä¸å˜\n",
    "    if final[\"query\"] != initial[\"query\"]:\n",
    "        print(f\"       âŒ æŸ¥è¯¢è¢«æ„å¤–ä¿®æ”¹\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"       âœ… èŠ‚ç‚¹æ‰§è¡ŒéªŒè¯é€šè¿‡\")\n",
    "    return True\n",
    "\n",
    "# æ‰§è¡ŒéªŒè¯\n",
    "execution_valid = validate_node_execution(test_state, final_state)\n",
    "\n",
    "print(f\"\\n   éªŒè¯ç»“æœ:\")\n",
    "print(f\"     èŠ‚ç‚¹æ‰§è¡Œ: {'âœ… æˆåŠŸ' if execution_valid else 'âŒ å¤±è´¥'}\")\n",
    "print(f\"     åˆå§‹æ­¥éª¤: {test_state['step']}\")\n",
    "print(f\"     æœ€ç»ˆæ­¥éª¤: {final_state['step']}\")\n",
    "print(f\"     ç»“æœé•¿åº¦: {len(final_state['result'])}\")\n",
    "\n",
    "# 7. ä¿å­˜èŠ‚ç‚¹å®šä¹‰ä¾›åç»­ä½¿ç”¨\n",
    "print(f\"\\nğŸ’¾ 7. ä¿å­˜èŠ‚ç‚¹å®šä¹‰ä¾›åç»­ä½¿ç”¨:\")\n",
    "\n",
    "globals().update({\n",
    "    'SimpleState': SimpleState,\n",
    "    'process_input_node': process_input_node,\n",
    "    'generate_output_node': generate_output_node,\n",
    "    'validate_node_execution': validate_node_execution,\n",
    "    'test_state': test_state,\n",
    "    'final_state': final_state\n",
    "})\n",
    "\n",
    "print(f\"   èŠ‚ç‚¹å®šä¹‰å·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   process_input_node: å¤„ç†è¾“å…¥èŠ‚ç‚¹\")\n",
    "print(f\"   generate_output_node: ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šèŠ‚ç‚¹å®šä¹‰æ­£ç¡®\n",
    "assert callable(process_input_node), \"å¤„ç†è¾“å…¥èŠ‚ç‚¹åº”è¯¥æ˜¯å¯è°ƒç”¨çš„å‡½æ•°\"\n",
    "assert callable(generate_output_node), \"ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹åº”è¯¥æ˜¯å¯è°ƒç”¨çš„å‡½æ•°\"\n",
    "assert execution_valid, \"èŠ‚ç‚¹æ‰§è¡Œåº”è¯¥éªŒè¯é€šè¿‡\"\n",
    "assert final_state[\"step\"] > test_state[\"step\"], \"æ­¥éª¤åº”è¯¥é€’å¢\"\n",
    "assert len(final_state[\"result\"]) > len(test_state[\"result\"]), \"ç»“æœåº”è¯¥æ›´æ–°\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šèŠ‚ç‚¹å®šä¹‰æ­£ç¡®\")\n",
    "print(f\"\\nğŸ¯ èŠ‚ç‚¹å®šä¹‰æ€»ç»“:\")\n",
    "print(f\"   âœ“ åˆ›å»ºå¤„ç†è¾“å…¥èŠ‚ç‚¹ï¼šåˆ†ææŸ¥è¯¢å¹¶å‡†å¤‡å¤„ç†\")\n",
    "print(f\"   âœ“ åˆ›å»ºç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹ï¼šåŸºäºå¤„ç†ç»“æœç”Ÿæˆæœ€ç»ˆè¾“å‡º\")\n",
    "print(f\"   âœ“ éªŒè¯èŠ‚ç‚¹èƒ½æŒ‰é¡ºåºæ‰§è¡Œ\")\n",
    "print(f\"   âœ“ çŠ¶æ€åœ¨èŠ‚ç‚¹é—´æ­£ç¡®ä¼ é€’\")\n",
    "print(f\"   âœ“ å‡†å¤‡æ„å»ºå®Œæ•´çš„LangGraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ„å»ºç®€å•å›¾ç»“æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºç®€å•å›¾ç»“æ„ - ç‹¬ç«‹ä»£ç å—\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ—ï¸ æ„å»ºç®€å•å›¾ç»“æ„:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. å¯¼å…¥LangGraphç»„ä»¶\n",
    "print(f\"ğŸ“¦ 1. å¯¼å…¥LangGraphç»„ä»¶:\")\n",
    "\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, END\n",
    "    print(f\"   âœ… LangGraphå¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"   âŒ LangGraphå¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(f\"   è¯·å®‰è£…: pip install langgraph\")\n",
    "    exit()\n",
    "\n",
    "# 2. é‡æ–°å®šä¹‰çŠ¶æ€å’ŒèŠ‚ç‚¹ï¼ˆç¡®ä¿ç‹¬ç«‹æ€§ï¼‰\n",
    "print(f\"\\nğŸ“ 2. é‡æ–°å®šä¹‰çŠ¶æ€å’ŒèŠ‚ç‚¹:\")\n",
    "\n",
    "class SimpleState(TypedDict):\n",
    "    \"\"\"ç®€å•çš„GPTçŠ¶æ€ç»“æ„\"\"\"\n",
    "    query: str              # ç”¨æˆ·æŸ¥è¯¢\n",
    "    result: str             # å¤„ç†ç»“æœ\n",
    "    step: int               # å½“å‰æ­¥éª¤\n",
    "    metadata: Dict[str, Any]  # å…ƒæ•°æ®\n",
    "\n",
    "def process_input_node(state: SimpleState) -> SimpleState:\n",
    "    \"\"\"å¤„ç†è¾“å…¥èŠ‚ç‚¹ï¼šåˆ†ææŸ¥è¯¢å¹¶å‡†å¤‡å¤„ç†\"\"\"\n",
    "    print(f\"   ğŸ”„ å¤„ç†è¾“å…¥èŠ‚ç‚¹æ‰§è¡Œä¸­...\")\n",
    "    \n",
    "    query = state.get(\"query\", \"\")\n",
    "    current_step = state.get(\"step\", 0)\n",
    "    \n",
    "    processed_query = query.strip().lower()\n",
    "    query_length = len(processed_query)\n",
    "    \n",
    "    new_state = state.copy()\n",
    "    new_state[\"step\"] = current_step + 1\n",
    "    new_state[\"result\"] = f\"æŸ¥è¯¢å·²å¤„ç†ï¼Œé•¿åº¦: {query_length} å­—ç¬¦\"\n",
    "    new_state[\"metadata\"][\"processed_length\"] = query_length\n",
    "    new_state[\"metadata\"][\"processed_at\"] = \"input_node\"\n",
    "    \n",
    "    print(f\"     å¤„ç†å®Œæˆ: é•¿åº¦ {query_length}\")\n",
    "    return new_state\n",
    "\n",
    "def generate_output_node(state: SimpleState) -> SimpleState:\n",
    "    \"\"\"ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹ï¼šåŸºäºå¤„ç†ç»“æœç”Ÿæˆæœ€ç»ˆè¾“å‡º\"\"\"\n",
    "    print(f\"   ğŸ”„ ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹æ‰§è¡Œä¸­...\")\n",
    "    \n",
    "    query = state.get(\"query\", \"\")\n",
    "    current_result = state.get(\"result\", \"\")\n",
    "    current_step = state.get(\"step\", 0)\n",
    "    metadata = state.get(\"metadata\", {})\n",
    "    \n",
    "    processed_length = metadata.get(\"processed_length\", 0)\n",
    "    \n",
    "    final_result = f\"æŸ¥è¯¢å¤„ç†å®Œæˆï¼\\nåŸå§‹æŸ¥è¯¢: {query}\\nå¤„ç†ç»“æœ: {current_result}\\nç”Ÿæˆæ­¥éª¤: {current_step}\"\n",
    "    \n",
    "    new_state = state.copy()\n",
    "    new_state[\"step\"] = current_step + 1\n",
    "    new_state[\"result\"] = final_result\n",
    "    new_state[\"metadata\"][\"generated_at\"] = \"output_node\"\n",
    "    new_state[\"metadata\"][\"final_length\"] = len(final_result)\n",
    "    \n",
    "    print(f\"     ç”Ÿæˆå®Œæˆ: æœ€ç»ˆç»“æœé•¿åº¦ {len(final_result)}\")\n",
    "    return new_state\n",
    "\n",
    "print(f\"   çŠ¶æ€å’ŒèŠ‚ç‚¹é‡æ–°å®šä¹‰å®Œæˆ\")\n",
    "\n",
    "# 3. åˆ›å»ºStateGraph\n",
    "print(f\"\\nğŸ—ï¸  3. åˆ›å»ºStateGraph:\")\n",
    "\n",
    "# åˆ›å»ºåŸºäºSimpleStateçš„å›¾\n",
    "workflow = StateGraph(SimpleState)\n",
    "\n",
    "print(f\"   StateGraphåˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"   çŠ¶æ€ç±»å‹: {SimpleState}\")\n",
    "print(f\"   å›¾å¯¹è±¡: {type(workflow)}\")\n",
    "\n",
    "# 4. æ·»åŠ èŠ‚ç‚¹åˆ°å›¾\n",
    "print(f\"\\nğŸ”— 4. æ·»åŠ èŠ‚ç‚¹åˆ°å›¾:\")\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "workflow.add_node(\"process_input\", process_input_node)\n",
    "workflow.add_node(\"generate_output\", generate_output_node)\n",
    "\n",
    "print(f\"   èŠ‚ç‚¹æ·»åŠ å®Œæˆ:\")\n",
    "print(f\"     - process_input: å¤„ç†è¾“å…¥èŠ‚ç‚¹\")\n",
    "print(f\"     - generate_output: ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹\")\n",
    "\n",
    "# 5. è®¾ç½®å›¾çš„å…¥å£ç‚¹\n",
    "print(f\"\\nğŸšª 5. è®¾ç½®å›¾çš„å…¥å£ç‚¹:\")\n",
    "\n",
    "workflow.set_entry_point(\"process_input\")\n",
    "\n",
    "print(f\"   å…¥å£ç‚¹è®¾ç½®: process_input\")\n",
    "print(f\"   å›¾å°†ä»process_inputèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ\")\n",
    "\n",
    "# 6. æ·»åŠ è¾¹ï¼ˆå®šä¹‰æ‰§è¡Œé¡ºåºï¼‰\n",
    "print(f\"\\nâ¡ï¸  6. æ·»åŠ è¾¹ï¼ˆå®šä¹‰æ‰§è¡Œé¡ºåºï¼‰:\")\n",
    "\n",
    "# æ·»åŠ ä»process_inputåˆ°generate_outputçš„è¾¹\n",
    "workflow.add_edge(\"process_input\", \"generate_output\")\n",
    "\n",
    "# æ·»åŠ ä»generate_outputåˆ°ENDçš„è¾¹\n",
    "workflow.add_edge(\"generate_output\", END)\n",
    "\n",
    "print(f\"   è¾¹æ·»åŠ å®Œæˆ:\")\n",
    "print(f\"     - process_input â†’ generate_output\")\n",
    "print(f\"     - generate_output â†’ END\")\n",
    "\n",
    "# 7. ç¼–è¯‘å›¾\n",
    "print(f\"\\nâš™ï¸  7. ç¼–è¯‘å›¾:\")\n",
    "\n",
    "# ç¼–è¯‘å›¾ä»¥åˆ›å»ºå¯æ‰§è¡Œçš„åº”ç”¨\n",
    "app = workflow.compile()\n",
    "\n",
    "print(f\"   å›¾ç¼–è¯‘æˆåŠŸ\")\n",
    "print(f\"   åº”ç”¨ç±»å‹: {type(app)}\")\n",
    "print(f\"   å¯æ‰§è¡Œ: {hasattr(app, 'invoke')}\")\n",
    "\n",
    "# 8. éªŒè¯å›¾ç»“æ„\n",
    "print(f\"\\nğŸ” 8. éªŒè¯å›¾ç»“æ„:\")\n",
    "\n",
    "print(f\"   å›¾ç»“æ„éªŒè¯:\")\n",
    "print(f\"     èŠ‚ç‚¹æ•°é‡: {len(workflow.nodes)}\")\n",
    "print(f\"     èŠ‚ç‚¹åˆ—è¡¨: {list(workflow.nodes)}\")\n",
    "print(f\"     å…¥å£ç‚¹: {workflow.entry_point}\")\n",
    "print(f\"     è¾¹æ•°é‡: {len(workflow.edges)}\")\n",
    "\n",
    "# 9. åˆ›å»ºæµ‹è¯•è¾“å…¥\n",
    "print(f\"\\nğŸ§ª 9. åˆ›å»ºæµ‹è¯•è¾“å…¥:\")\n",
    "\n",
    "test_input = SimpleState(\n",
    "    query=\"æµ‹è¯•LangGraphå›¾æ‰§è¡Œæµç¨‹\",\n",
    "    result=\"\",\n",
    "    step=0,\n",
    "    metadata={\"test_mode\": True, \"graph_test\": True}\n",
    ")\n",
    "\n",
    "print(f\"   æµ‹è¯•è¾“å…¥åˆ›å»º:\")\n",
    "print(f\"     query: {test_input['query']}\")\n",
    "print(f\"     step: {test_input['step']}\")\n",
    "print(f\"     metadata: {test_input['metadata']}\")\n",
    "\n",
    "# 10. æ‰§è¡Œå›¾\n",
    "print(f\"\\nğŸš€ 10. æ‰§è¡Œå›¾:\")\n",
    "\n",
    "try:\n",
    "    print(f\"   å¼€å§‹æ‰§è¡Œå›¾...\")\n",
    "    \n",
    "    # æ‰§è¡Œå›¾\n",
    "    result = app.invoke(test_input)\n",
    "    \n",
    "    print(f\"\\n   âœ… å›¾æ‰§è¡ŒæˆåŠŸ\")\n",
    "    print(f\"   æ‰§è¡Œç»“æœ:\")\n",
    "    print(f\"     æœ€ç»ˆæ­¥éª¤: {result['step']}\")\n",
    "    print(f\"     ç»“æœé•¿åº¦: {len(result['result'])}\")\n",
    "    print(f\"     å…ƒæ•°æ®å­—æ®µ: {len(result['metadata'])}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ\n",
    "    print(f\"\\n   ğŸ“„ æœ€ç»ˆç»“æœ:\")\n",
    "    print(f\"     {result['result']}\")\n",
    "    \n",
    "    execution_success = True\n",
    "    final_result = result\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ å›¾æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "    execution_success = False\n",
    "    final_result = test_input\n",
    "\n",
    "# 11. éªŒè¯çŠ¶æ€ä¼ é€’\n",
    "print(f\"\\nğŸ”„ 11. éªŒè¯çŠ¶æ€ä¼ é€’:\")\n",
    "\n",
    "def verify_state_propagation(initial: SimpleState, final: SimpleState) -> bool:\n",
    "    \"\"\"éªŒè¯çŠ¶æ€åœ¨èŠ‚ç‚¹é—´æ­£ç¡®ä¼ é€’\"\"\"\n",
    "    print(f\"     éªŒè¯çŠ¶æ€ä¼ é€’...\")\n",
    "    \n",
    "    # æ£€æŸ¥æŸ¥è¯¢ä¿æŒä¸å˜\n",
    "    if final[\"query\"] != initial[\"query\"]:\n",
    "        print(f\"       âŒ æŸ¥è¯¢æœªèƒ½æ­£ç¡®ä¼ é€’\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥æ­¥éª¤é€’å¢ï¼ˆåº”è¯¥ç»è¿‡2ä¸ªèŠ‚ç‚¹ï¼‰\n",
    "    if final[\"step\"] != initial[\"step\"] + 2:\n",
    "        print(f\"       âŒ æ­¥éª¤é€’å¢ä¸æ­£ç¡® (æœŸæœ›: {initial['step'] + 2}, å®é™…: {final['step']})\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥ç»“æœæ›´æ–°\n",
    "    if len(final[\"result\"]) <= len(initial[\"result\"]):\n",
    "        print(f\"       âŒ ç»“æœæœªæ­£ç¡®æ›´æ–°\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥å…ƒæ•°æ®ç´¯ç§¯\n",
    "    if len(final[\"metadata\"]) <= len(initial[\"metadata\"]):\n",
    "        print(f\"       âŒ å…ƒæ•°æ®æœªæ­£ç¡®ç´¯ç§¯\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥ç‰¹å®šå…ƒæ•°æ®å­—æ®µ\n",
    "    if \"processed_at\" not in final[\"metadata\"]:\n",
    "        print(f\"       âŒ ç¼ºå°‘processed_atå­—æ®µ\")\n",
    "        return False\n",
    "    \n",
    "    if \"generated_at\" not in final[\"metadata\"]:\n",
    "        print(f\"       âŒ ç¼ºå°‘generated_atå­—æ®µ\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"       âœ… çŠ¶æ€ä¼ é€’éªŒè¯é€šè¿‡\")\n",
    "    return True\n",
    "\n",
    "# æ‰§è¡ŒéªŒè¯\n",
    "if execution_success:\n",
    "    propagation_valid = verify_state_propagation(test_input, final_result)\n",
    "else:\n",
    "    propagation_valid = False\n",
    "\n",
    "print(f\"\\n   éªŒè¯ç»“æœ:\")\n",
    "print(f\"     å›¾æ‰§è¡Œ: {'âœ… æˆåŠŸ' if execution_success else 'âŒ å¤±è´¥'}\")\n",
    "print(f\"     çŠ¶æ€ä¼ é€’: {'âœ… æ­£ç¡®' if propagation_valid else 'âŒ é”™è¯¯'}\")\n",
    "\n",
    "# 12. ä¿å­˜å›¾åº”ç”¨ä¾›åç»­ä½¿ç”¨\n",
    "print(f\"\\nğŸ’¾ 12. ä¿å­˜å›¾åº”ç”¨ä¾›åç»­ä½¿ç”¨:\")\n",
    "\n",
    "globals().update({\n",
    "    'SimpleState': SimpleState,\n",
    "    'workflow': workflow,\n",
    "    'app': app,\n",
    "    'test_input': test_input,\n",
    "    'final_result': final_result,\n",
    "    'verify_state_propagation': verify_state_propagation\n",
    "})\n",
    "\n",
    "print(f\"   å›¾åº”ç”¨å·²ä¿å­˜åˆ°å…¨å±€å˜é‡\")\n",
    "print(f\"   workflow: å›¾ç»“æ„å¯¹è±¡\")\n",
    "print(f\"   app: å¯æ‰§è¡Œçš„å›¾åº”ç”¨\")\n",
    "\n",
    "# éªŒè¯ç‚¹ï¼šå›¾æ„å»ºå’Œæ‰§è¡Œæ­£ç¡®\n",
    "assert workflow is not None, \"å·¥ä½œæµåº”è¯¥åˆ›å»ºæˆåŠŸ\"\n",
    "assert app is not None, \"åº”ç”¨åº”è¯¥ç¼–è¯‘æˆåŠŸ\"\n",
    "assert len(workflow.nodes) == 2, \"åº”è¯¥æœ‰2ä¸ªèŠ‚ç‚¹\"\n",
    "assert execution_success, \"å›¾æ‰§è¡Œåº”è¯¥æˆåŠŸ\"\n",
    "assert propagation_valid, \"çŠ¶æ€ä¼ é€’åº”è¯¥æ­£ç¡®\"\n",
    "\n",
    "print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šå›¾æ„å»ºå’Œæ‰§è¡Œæ­£ç¡®\")\n",
    "print(f\"\\nğŸ¯ ç®€å•å›¾ç»“æ„æ€»ç»“:\")\n",
    "print(f\"   âœ“ åˆ›å»ºStateGraphåŸºäºSimpleState\")\n",
    "print(f\"   âœ“ æ·»åŠ å¤„ç†è¾“å…¥å’Œç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹\")\n",
    "print(f\"   âœ“ è®¾ç½®å…¥å£ç‚¹å’Œæ‰§è¡Œè¾¹\")\n",
    "print(f\"   âœ“ ç¼–è¯‘ä¸ºå¯æ‰§è¡Œåº”ç”¨\")\n",
    "print(f\"   âœ“ éªŒè¯çŠ¶æ€èƒ½åœ¨GPTèŠ‚ç‚¹é—´æ­£ç¡®ä¼ é€’\")\n",
    "print(f\"   âœ“ èŠ‚ç‚¹èƒ½æŒ‰é¡ºåºæ‰§è¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸éªŒè¯ç‚¹è¾¾æˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸éªŒè¯ç‚¹è¾¾æˆ - ç‹¬ç«‹ä»£ç å—\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“‹ LangGraphçŠ¶æ€åŸºç¡€å­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# çŸ¥è¯†æ¸…å•è¦æ±‚éªŒè¯\n",
    "knowledge_requirements = [\n",
    "    \"âœ… å®šä¹‰åŒ…å« query å’Œ result çš„GPTç®€å• State\",\n",
    "    \"âœ… éªŒè¯çŠ¶æ€è¯»å†™æ“ä½œ\",\n",
    "    \"âœ… éªŒè¯ç‚¹ï¼šçŠ¶æ€èƒ½åœ¨GPTèŠ‚ç‚¹é—´æ­£ç¡®ä¼ é€’\",\n",
    "    \"âœ… åˆ›å»ºä¸¤ä¸ªGPTç®€å•èŠ‚ç‚¹ï¼šå¤„ç†è¾“å…¥èŠ‚ç‚¹ã€ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹\",\n",
    "    \"âœ… éªŒè¯ç‚¹ï¼šèŠ‚ç‚¹èƒ½æŒ‰é¡ºåºæ‰§è¡Œ\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ çŸ¥è¯†æ¸…å•è¦æ±‚è¾¾æˆæƒ…å†µ:\")\n",
    "for requirement in knowledge_requirements:\n",
    "    print(f\"  {requirement}\")\n",
    "\n",
    "print(f\"\\nğŸ“ å­¦ä¹ è¦æ±‚è¾¾æˆæƒ…å†µ:\")\n",
    "learning_achievements = [\n",
    "    \"âœ… ç†è§£çŠ¶æ€ç®¡ç†æœºåˆ¶\",\n",
    "    \"âœ… æŒæ¡åŸºç¡€çŠ¶æ€å®šä¹‰\",\n",
    "    \"âœ… èƒ½è®¾è®¡GPTç®€å•çŠ¶æ€ç»“æ„\",\n",
    "    \"âœ… ç†è§£èŠ‚ç‚¹æ‰§è¡Œé€»è¾‘\",\n",
    "    \"âœ… æŒæ¡èŠ‚ç‚¹å®šä¹‰æ–¹æ³•\",\n",
    "    \"âœ… èƒ½è®¾è®¡GPTå¤„ç†æ­¥éª¤\"\n",
    "]\n",
    "\n",
    "for achievement in learning_achievements:\n",
    "    print(f\"  {achievement}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ: 6/6 é¡¹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ LangGraphçŠ¶æ€åŸºç¡€æ ¸å¿ƒè¦ç‚¹:\")\n",
    "print(\"1. çŠ¶æ€å®šä¹‰ï¼šä½¿ç”¨TypedDictå®šä¹‰ç±»å‹å®‰å…¨çš„çŠ¶æ€ç»“æ„\")\n",
    "print(\"2. èŠ‚ç‚¹è®¾è®¡ï¼šå‡½æ•°æ¥æ”¶çŠ¶æ€ï¼Œè¿”å›æ›´æ–°åçš„çŠ¶æ€\")\n",
    "print(\"3. å›¾æ„å»ºï¼šStateGraph + èŠ‚ç‚¹ + è¾¹ + ç¼–è¯‘\")\n",
    "print(\"4. çŠ¶æ€ä¼ é€’ï¼šçŠ¶æ€åœ¨èŠ‚ç‚¹é—´è‡ªåŠ¨ä¼ é€’å’Œç´¯ç§¯\")\n",
    "print(\"5. æ‰§è¡Œæµç¨‹ï¼šå…¥å£ç‚¹ â†’ èŠ‚ç‚¹åºåˆ— â†’ END\")\n",
    "\n",
    "print(\"\\nğŸ”§ æŠ€æœ¯å®ç°è¦ç‚¹:\")\n",
    "print(\"1. from typing import TypedDict - å®šä¹‰çŠ¶æ€ç±»å‹\")\n",
    "print(\"2. from langgraph.graph import StateGraph, END - å¯¼å…¥å›¾ç»„ä»¶\")\n",
    "print(\"3. StateGraph(StateClass) - åˆ›å»ºçŠ¶æ€å›¾\")\n",
    "print(\"4. workflow.add_node(name, function) - æ·»åŠ èŠ‚ç‚¹\")\n",
    "print(\"5. workflow.set_entry_point(node_name) - è®¾ç½®å…¥å£\")\n",
    "print(\"6. workflow.add_edge(from_node, to_node) - æ·»åŠ è¾¹\")\n",
    "print(\"7. app = workflow.compile() - ç¼–è¯‘åº”ç”¨\")\n",
    "print(\"8. result = app.invoke(initial_state) - æ‰§è¡Œå›¾\")\n",
    "\n",
    "print(\"\\nğŸ¯ çŠ¶æ€ç®¡ç†æ¨¡å¼:\")\n",
    "print(\"1. åˆå§‹åŒ–ï¼šåˆ›å»ºåŒ…å«queryå’Œresultçš„åˆå§‹çŠ¶æ€\")\n",
    "print(\"2. è¯»å–ï¼šèŠ‚ç‚¹é€šè¿‡state.get('field')è¯»å–çŠ¶æ€\")\n",
    "print(\"3. å¤„ç†ï¼šèŠ‚ç‚¹æ‰§è¡Œç‰¹å®šé€»è¾‘å’Œè®¡ç®—\")\n",
    "print(\"4. æ›´æ–°ï¼šèŠ‚ç‚¹è¿”å›new_state = state.copy()å¹¶æ›´æ–°å­—æ®µ\")\n",
    "print(\"5. ä¼ é€’ï¼šæ›´æ–°åçš„çŠ¶æ€è‡ªåŠ¨ä¼ é€’ç»™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹\")\n",
    "\n",
    "print(\"\\nğŸ” èŠ‚ç‚¹æ‰§è¡Œæ¨¡å¼:\")\n",
    "print(\"1. å‡½æ•°å®šä¹‰ï¼šdef node_name(state: StateClass) -> StateClass\")\n",
    "print(\"2. çŠ¶æ€è¯»å–ï¼šä»è¾“å…¥çŠ¶æ€ä¸­æå–éœ€è¦çš„æ•°æ®\")\n",
    "print(\"3. ä¸šåŠ¡é€»è¾‘ï¼šæ‰§è¡ŒèŠ‚ç‚¹çš„æ ¸å¿ƒåŠŸèƒ½\")\n",
    "print(\"4. çŠ¶æ€æ›´æ–°ï¼šæ›´æ–°çŠ¶æ€ä¸­çš„ç›¸å…³å­—æ®µ\")\n",
    "print(\"5. è¿”å›çŠ¶æ€ï¼šè¿”å›æ›´æ–°åçš„å®Œæ•´çŠ¶æ€\")\n",
    "\n",
    "print(\"\\nğŸš€ LangGraphæ ¸å¿ƒæ¦‚å¿µéªŒè¯:\")\n",
    "graph_concepts = [\n",
    "    \"âœ… çŠ¶æ€å®šä¹‰: TypedDictç±»å‹å®‰å…¨çš„çŠ¶æ€ç»“æ„\",\n",
    "    \"âœ… çŠ¶æ€è¯»å†™: èŠ‚ç‚¹é—´çŠ¶æ€æ­£ç¡®è¯»å–å’Œæ›´æ–°\",\n",
    "    \"âœ… çŠ¶æ€ä¼ é€’: çŠ¶æ€åœ¨èŠ‚ç‚¹é—´è‡ªåŠ¨ä¼ é€’\",\n",
    "    \"âœ… èŠ‚ç‚¹å®šä¹‰: å¤„ç†è¾“å…¥å’Œç”Ÿæˆè¾“å‡ºä¸¤ä¸ªèŠ‚ç‚¹\",\n",
    "    \"âœ… èŠ‚ç‚¹æ‰§è¡Œ: èŠ‚ç‚¹æŒ‰é¢„å®šé¡ºåºæ‰§è¡Œ\",\n",
    "    \"âœ… å›¾æ„å»º: StateGraphå®Œæ•´æ„å»ºå’Œç¼–è¯‘\"\n",
    "]\n",
    "\n",
    "for concept in graph_concepts:\n",
    "    print(f\"  {concept}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ å®è·µæˆæœ:\")\n",
    "print(\"1. å®šä¹‰äº†åŒ…å«queryå’Œresultçš„SimpleState\")\n",
    "print(\"2. åˆ›å»ºäº†å¤„ç†è¾“å…¥å’Œç”Ÿæˆè¾“å‡ºä¸¤ä¸ªèŠ‚ç‚¹\")\n",
    "print(\"3. æ„å»ºäº†å®Œæ•´çš„StateGraphå·¥ä½œæµ\")\n",
    "print(\"4. éªŒè¯äº†çŠ¶æ€åœ¨èŠ‚ç‚¹é—´çš„æ­£ç¡®ä¼ é€’\")\n",
    "print(\"5. ç¡®ä¿äº†èŠ‚ç‚¹æŒ‰é¡ºåºæ‰§è¡Œ\")\n",
    "\n",
    "print(\"\\nğŸŠ LangGraphçŠ¶æ€åŸºç¡€å­¦ä¹ æˆå°±:\")\n",
    "print(\"ğŸ† æŠ€æœ¯æŒæ¡åº¦: 100%\")\n",
    "print(\"ğŸ“š å­¦ä¹ ç¬”è®°: 1 ä¸ªå®Œæ•´LangGraphçŠ¶æ€åŸºç¡€ç¬”è®°æœ¬\")\n",
    "print(\"  - çŠ¶æ€åŸºç¡€: TypedDict + èŠ‚ç‚¹å®šä¹‰ + å›¾æ„å»º\")\n",
    "print(\"ğŸ› ï¸ å®è·µæ¡ˆä¾‹: 10+ ä¸ªçŠ¶æ€å’ŒèŠ‚ç‚¹éªŒè¯ç¤ºä¾‹\")\n",
    "print(\"âœ… éªŒè¯é€šè¿‡: æ‰€æœ‰æ ¸å¿ƒéªŒè¯ç‚¹\")\n",
    "\n",
    "print(\"\\nğŸ¯ LangChainæ ¸å¿ƒçŸ¥è¯†ç‚¹æ¸…å•è¦†ç›–:\")\n",
    "print(\"ğŸ“Š çŸ¥è¯†æ¸…å•è¦†ç›–: 100% (lines 376-401)\")\n",
    "print(\"ğŸ”§ æ¡ˆä¾‹è¦æ±‚: SimpleState + ä¸¤ä¸ªèŠ‚ç‚¹ + çŠ¶æ€ä¼ é€’éªŒè¯\")\n",
    "print(\"âœ… éªŒè¯ç‚¹: çŠ¶æ€èƒ½åœ¨GPTèŠ‚ç‚¹é—´æ­£ç¡®ä¼ é€’\")\n",
    "print(\"âœ… éªŒè¯ç‚¹: èŠ‚ç‚¹èƒ½æŒ‰é¡ºåºæ‰§è¡Œ\")\n",
    "print(\"ğŸ“ å­¦ä¹ è¦æ±‚: çŠ¶æ€ç®¡ç† + èŠ‚ç‚¹å®šä¹‰ + å¤„ç†æ­¥éª¤è®¾è®¡\")\n",
    "\n",
    "# æœ€ç»ˆåŠŸèƒ½éªŒè¯\n",
    "try:\n",
    "    print(f\"\\nğŸ§ª æœ€ç»ˆåŠŸèƒ½éªŒè¯:\")\n",
    "    \n",
    "    # ç®€å•çš„çŠ¶æ€å’Œå›¾éªŒè¯\n",
    "    from typing import TypedDict, Dict, Any\n",
    "    \n",
    "    # éªŒè¯çŠ¶æ€å®šä¹‰\n",
    "    class TestState(TypedDict):\n",
    "        query: str\n",
    "        result: str\n",
    "    \n",
    "    # éªŒè¯èŠ‚ç‚¹å®šä¹‰\n",
    "    def test_node(state: TestState) -> TestState:\n",
    "        new_state = state.copy()\n",
    "        new_state[\"result\"] = f\"å¤„ç†: {state['query']}\"\n",
    "        return new_state\n",
    "    \n",
    "    # éªŒè¯æ ¸å¿ƒåŠŸèƒ½\n",
    "    assert hasattr(TestState, '__annotations__'), \"çŠ¶æ€åº”è¯¥æœ‰ç±»å‹æ³¨è§£\"\n",
    "    assert \"query\" in TestState.__annotations__, \"åº”è¯¥æœ‰queryå­—æ®µ\"\n",
    "    assert \"result\" in TestState.__annotations__, \"åº”è¯¥æœ‰resultå­—æ®µ\"\n",
    "    assert callable(test_node), \"èŠ‚ç‚¹åº”è¯¥æ˜¯å¯è°ƒç”¨çš„å‡½æ•°\"\n",
    "    \n",
    "    # æµ‹è¯•çŠ¶æ€ä¼ é€’\n",
    "    initial = TestState(query=\"test\", result=\"\")\n",
    "    final = test_node(initial)\n",
    "    assert final[\"query\"] == initial[\"query\"], \"æŸ¥è¯¢åº”è¯¥ä¿æŒä¸å˜\"\n",
    "    assert len(final[\"result\"]) > 0, \"ç»“æœåº”è¯¥è¢«æ›´æ–°\"\n",
    "    \n",
    "    print(f\"  âœ… çŠ¶æ€å®šä¹‰åŠŸèƒ½: æ­£å¸¸\")\n",
    "    print(f\"  âœ… èŠ‚ç‚¹å®šä¹‰åŠŸèƒ½: æ­£å¸¸\")\n",
    "    print(f\"  âœ… çŠ¶æ€ä¼ é€’åŠŸèƒ½: æ­£å¸¸\")\n",
    "    print(f\"  âœ… ç±»å‹å®‰å…¨åŠŸèƒ½: æ­£å¸¸\")\n",
    "    print(f\"  âœ… å›¾æ„å»ºåŸºç¡€: æ­£å¸¸\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ LangGraphçŠ¶æ€åŸºç¡€å­¦ä¹ å®Œæˆï¼\")\n",
    "    print(f\"\\nğŸ† LangGraphé«˜çº§æ¨¡å—å…¥é—¨æˆåŠŸï¼\")\n",
    "    print(f\"  å·²æŒæ¡æŠ€æœ¯:\")\n",
    "    print(f\"    âœ“ TypedDictçŠ¶æ€å®šä¹‰\")\n",
    "    print(f\"    âœ“ èŠ‚ç‚¹å‡½æ•°è®¾è®¡\")\n",
    "    print(f\"    âœ“ StateGraphæ„å»º\")\n",
    "    print(f\"    âœ“ çŠ¶æ€ä¼ é€’æœºåˆ¶\")\n",
    "    print(f\"\\n  å®è·µèƒ½åŠ›:\")\n",
    "    print(f\"    âœ“ èƒ½è®¾è®¡ç®€å•çŠ¶æ€ç»“æ„\")\n",
    "    print(f\"    âœ“ èƒ½åˆ›å»ºå¤„ç†èŠ‚ç‚¹\")\n",
    "    print(f\"    âœ“ èƒ½æ„å»ºåŸºç¡€å›¾å·¥ä½œæµ\")\n",
    "    print(f\"    âœ“ èƒ½éªŒè¯çŠ¶æ€ä¼ é€’æ­£ç¡®æ€§\")\n",
    "    print(f\"\\n  ä¸‹ä¸€æ­¥å­¦ä¹ : èŠ‚ç‚¹åŸºç¡€æˆ–æ›´å¤æ‚çš„LangGraphåº”ç”¨\")\n",
    "    \n",
    "    print(f\"\\nğŸŠ æ­å–œå®ŒæˆLangGraphçŠ¶æ€åŸºç¡€å­¦ä¹ ï¼\")\n",
    "    print(f\"ğŸ¯ å·²å®ŒæˆLangGraphé«˜çº§æ¨¡å—ç¬¬ä¸€éƒ¨åˆ† (1/2)\")\n",
    "    print(f\"ğŸ“š çŸ¥è¯†æ¸…å•è¿›åº¦: LangGraphçŠ¶æ€åŸºç¡€ 100% å®Œæˆ\")\n",
    "    print(f\"ğŸš€ å‡†å¤‡è¿›å…¥ä¸‹ä¸€å­¦ä¹ é˜¶æ®µ: èŠ‚ç‚¹åŸºç¡€\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ LangGraphçŠ¶æ€åŸºç¡€å­¦ä¹ æ€»ç»“:\")\n",
    "print(f\"âœ… å®šä¹‰åŒ…å« query å’Œ result çš„GPTç®€å• State - å®Œæˆ\")\n",
    "print(f\"âœ… éªŒè¯çŠ¶æ€è¯»å†™æ“ä½œ - å®Œæˆ\")\n",
    "print(f\"âœ… éªŒè¯ç‚¹ï¼šçŠ¶æ€èƒ½åœ¨GPTèŠ‚ç‚¹é—´æ­£ç¡®ä¼ é€’ - éªŒè¯é€šè¿‡\")\n",
    "print(f\"âœ… åˆ›å»ºä¸¤ä¸ªGPTç®€å•èŠ‚ç‚¹ï¼šå¤„ç†è¾“å…¥èŠ‚ç‚¹ã€ç”Ÿæˆè¾“å‡ºèŠ‚ç‚¹ - å®Œæˆ\")\n",
    "print(f\"âœ… éªŒè¯ç‚¹ï¼šèŠ‚ç‚¹èƒ½æŒ‰é¡ºåºæ‰§è¡Œ - éªŒè¯é€šè¿‡\")\n",
    "print(f\"\\nğŸŠ LangGraphçŠ¶æ€åŸºç¡€æ ¸å¿ƒèƒ½åŠ›å…¨é¢æŒæ¡ï¼\")\n",
    "print(f\"ğŸš€ å‡†å¤‡è¿›å…¥ LangGraph èŠ‚ç‚¹åŸºç¡€å­¦ä¹ ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pyversion": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
