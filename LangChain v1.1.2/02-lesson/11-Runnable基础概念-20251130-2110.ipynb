{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Runnable åŸºç¡€æ¦‚å¿µ\n",
    "\n",
    "## ç”¨é€”\n",
    "å­¦ä¹  LangChain 1.0 çš„æ ¸å¿ƒæŠ½è±¡ Runnableï¼Œç†è§£é“¾å¼æ‹¼æŽ¥çš„åŸºç¡€æ¦‚å¿µå’Œå®žçŽ°æ–¹æ³•\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£ Runnable æŠ½è±¡æ¦‚å¿µå’Œè®¾è®¡åŽŸç†\n",
    "- æŽŒæ¡åŸºç¡€é“¾å¼æ‹¼æŽ¥æ–¹æ³•\n",
    "- èƒ½æž„å»ºç®€å•çš„é€šä¹‰åƒé—®å¤„ç†æµç¨‹\n",
    "- æŽŒæ¡ invoke æ–¹æ³•çš„ä½¿ç”¨\n",
    "\n",
    "## ðŸ”‘ å‰ç½®è¦æ±‚\n",
    "**æ³¨æ„**ï¼šéœ€è¦å…ˆå®Œæˆ OpenAI åŸºç¡€è°ƒç”¨å­¦ä¹ ï¼Œé…ç½®å¥½ API å¯†é’¥\n",
    "\n",
    "## ä»£ç å—ç‹¬ç«‹æ€§è¯´æ˜Ž\n",
    "**æ³¨æ„**ï¼šæ¯ä¸ªä»£ç å—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…å«å®Œæ•´çš„å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œç¡®ä¿å¯ä»¥å•ç‹¬è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Runnable æŠ½è±¡æ¦‚å¿µç†è§£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable æŠ½è±¡æ¦‚å¿µç†è§£ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½çŽ¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ðŸ”§ Runnable æŠ½è±¡æ¦‚å¿µç†è§£:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥ OpenAI API é…ç½®\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ æœªæ‰¾åˆ° OPENAI_API_KEY çŽ¯å¢ƒå˜é‡\")\n",
    "    print(\"   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=your_key_here\")\n",
    "else:\n",
    "    print(f\"âœ… OpenAI API Key å·²é…ç½® (é•¿åº¦: {len(openai_api_key)})\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ Runnable æ ¸å¿ƒæ¦‚å¿µ:\")\n",
    "    print(f\"   1. Runnable æ˜¯ LangChain 1.0 çš„æ ¸å¿ƒæŠ½è±¡\")\n",
    "    print(f\"   2. æ‰€æœ‰ç»„ä»¶éƒ½å®žçŽ° Runnable æŽ¥å£\")\n",
    "    print(f\"   3. æ”¯æŒé“¾å¼ç»„åˆå’Œç®¡é“æ“ä½œ\")\n",
    "    print(f\"   4. ç»Ÿä¸€çš„ invoke/stream/batch æ–¹æ³•\")\n",
    "    \n",
    "    # åˆ›å»ºåŸºç¡€ Runnable å¯¹è±¡\n",
    "    print(f\"\\nðŸ—ï¸  åˆ›å»ºåŸºç¡€ Runnable å¯¹è±¡:\")\n",
    "    \n",
    "    # 1. ChatOpenAI ä½œä¸º Runnable\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(f\"   LLM ç±»åž‹: {type(llm)}\")\n",
    "    print(f\"   æ˜¯å¦ä¸º Runnable: {isinstance(llm, Runnable)}\")\n",
    "    print(f\"   å¯ç”¨æ–¹æ³•: {[method for method in dir(llm) if not method.startswith('_')]}\")\n",
    "    \n",
    "    # 2. ç®€å•å‡½æ•°ä½œä¸º Runnable\n",
    "    from langchain_core.runnables import RunnableLambda\n",
    "    \n",
    "    def simple_function(text: str) -> str:\n",
    "        return f\"å¤„ç†ç»“æžœ: {text.upper()}\"\n",
    "    \n",
    "    runnable_func = RunnableLambda(simple_function)\n",
    "    print(f\"\\n   å‡½æ•° Runnable ç±»åž‹: {type(runnable_func)}\")\n",
    "    print(f\"   æ˜¯å¦ä¸º Runnable: {isinstance(runnable_func, Runnable)}\")\n",
    "    \n",
    "    # 3. æµ‹è¯•åŸºç¡€ invoke æ–¹æ³•\n",
    "    print(f\"\\nðŸ§ª æµ‹è¯•åŸºç¡€ invoke æ–¹æ³•:\")\n",
    "    \n",
    "    # æµ‹è¯•å‡½æ•° Runnable\n",
    "    test_input = \"hello runnable\"\n",
    "    func_result = runnable_func.invoke(test_input)\n",
    "    print(f\"   å‡½æ•°è¾“å…¥: {test_input}\")\n",
    "    print(f\"   å‡½æ•°è¾“å‡º: {func_result}\")\n",
    "    \n",
    "    # éªŒè¯ç‚¹ï¼šåŸºç¡€ Runnable æ¦‚å¿µç†è§£æ­£ç¡®\n",
    "    assert isinstance(llm, Runnable), \"ChatOpenAI ä¸æ˜¯ Runnable\"\n",
    "    assert isinstance(runnable_func, Runnable), \"å‡½æ•°åŒ…è£…ä¸æ˜¯ Runnable\"\n",
    "    assert func_result == \"å¤„ç†ç»“æžœ: HELLO RUNNABLE\", \"å‡½æ•°è°ƒç”¨ç»“æžœé”™è¯¯\"\n",
    "    print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šRunnable æŠ½è±¡æ¦‚å¿µç†è§£æ­£ç¡®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŸºç¡€é“¾å¼æ‹¼æŽ¥æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€é“¾å¼æ‹¼æŽ¥æ–¹æ³• - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½çŽ¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"â›“ï¸  åŸºç¡€é“¾å¼æ‹¼æŽ¥æ–¹æ³•æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆ›å»ºåŸºç¡€ç»„ä»¶\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºç®€å•çš„å¤„ç†å‡½æ•°\n",
    "        def add_prefix(text: str) -> str:\n",
    "            return f\"å‰ç¼€: {text}\"\n",
    "        \n",
    "        def add_suffix(text: str) -> str:\n",
    "            return f\"{text} :åŽç¼€\"\n",
    "        \n",
    "        # åŒ…è£…ä¸º Runnable\n",
    "        prefix_runnable = RunnableLambda(add_prefix)\n",
    "        suffix_runnable = RunnableLambda(add_suffix)\n",
    "        \n",
    "        print(f\"ðŸ“ åˆ›å»ºçš„ Runnable ç»„ä»¶:\")\n",
    "        print(f\"   1. LLM Runnable: {type(llm).__name__}\")\n",
    "        print(f\"   2. å‰ç¼€å¤„ç†: {type(prefix_runnable).__name__}\")\n",
    "        print(f\"   3. åŽç¼€å¤„ç†: {type(suffix_runnable).__name__}\")\n",
    "        \n",
    "        # æ–¹æ³•1: ä½¿ç”¨ pipe æ“ä½œç¬¦\n",
    "        print(f\"\\nðŸ”— æ–¹æ³•1: ä½¿ç”¨ pipe æ“ä½œç¬¦:\")\n",
    "        chain1 = prefix_runnable | suffix_runnable\n",
    "        \n",
    "        print(f\"   é“¾ç±»åž‹: {type(chain1)}\")\n",
    "        print(f\"   é“¾ç»„ä»¶æ•°é‡: {len(chain1.steps) if hasattr(chain1, 'steps') else 'æœªçŸ¥'}\")\n",
    "        \n",
    "        # æµ‹è¯•é“¾å¼è°ƒç”¨\n",
    "        test_input = \"ä¸­é—´å†…å®¹\"\n",
    "        result1 = chain1.invoke(test_input)\n",
    "        print(f\"   è¾“å…¥: {test_input}\")\n",
    "        print(f\"   è¾“å‡º: {result1}\")\n",
    "        \n",
    "        # æ–¹æ³•2: ä½¿ç”¨ pipe æ–¹æ³•\n",
    "        print(f\"\\nðŸ”— æ–¹æ³•2: ä½¿ç”¨ pipe æ–¹æ³•:\")\n",
    "        chain2 = prefix_runnable.pipe(suffix_runnable)\n",
    "        \n",
    "        result2 = chain2.invoke(test_input)\n",
    "        print(f\"   è¾“å…¥: {test_input}\")\n",
    "        print(f\"   è¾“å‡º: {result2}\")\n",
    "        \n",
    "        # æ–¹æ³•3: å¤šé‡é“¾å¼æ‹¼æŽ¥\n",
    "        print(f\"\\nðŸ”— æ–¹æ³•3: å¤šé‡é“¾å¼æ‹¼æŽ¥:\")\n",
    "        def process_step1(text: str) -> str:\n",
    "            return f\"æ­¥éª¤1: {text}\"\n",
    "        \n",
    "        def process_step2(text: str) -> str:\n",
    "            return f\"æ­¥éª¤2: {text}\"\n",
    "        \n",
    "        def process_step3(text: str) -> str:\n",
    "            return f\"æ­¥éª¤3: {text}\"\n",
    "        \n",
    "        step1 = RunnableLambda(process_step1)\n",
    "        step2 = RunnableLambda(process_step2)\n",
    "        step3 = RunnableLambda(process_step3)\n",
    "        \n",
    "        multi_chain = step1 | step2 | step3\n",
    "        multi_result = multi_chain.invoke(\"åŽŸå§‹æ•°æ®\")\n",
    "        \n",
    "        print(f\"   å¤šé‡é“¾è¾“å…¥: åŽŸå§‹æ•°æ®\")\n",
    "        print(f\"   å¤šé‡é“¾è¾“å‡º: {multi_result}\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šé“¾å¼æ‹¼æŽ¥æ–¹æ³•æ­£ç¡®\n",
    "        assert result1 == \"å‰ç¼€: ä¸­é—´å†…å®¹ :åŽç¼€\", \"pipe æ“ä½œç¬¦ç»“æžœé”™è¯¯\"\n",
    "        assert result2 == \"å‰ç¼€: ä¸­é—´å†…å®¹ :åŽç¼€\", \"pipe æ–¹æ³•ç»“æžœé”™è¯¯\"\n",
    "        assert \"æ­¥éª¤1:\" in multi_result and \"æ­¥éª¤2:\" in multi_result and \"æ­¥éª¤3:\" in multi_result, \"å¤šé‡é“¾ç»“æžœé”™è¯¯\"\n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šåŸºç¡€é“¾å¼æ‹¼æŽ¥æ–¹æ³•æ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ é“¾å¼æ‹¼æŽ¥æµ‹è¯•å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. prompt â†’ llm ç®€å•é“¾æž„å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt â†’ llm ç®€å•é“¾æž„å»º - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½çŽ¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ðŸ¤– prompt â†’ llm ç®€å•é“¾æž„å»ºæµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆ›å»º LLM\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=60\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»º PromptTemplate\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\"],\n",
    "            template=\"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ {topic}ï¼Œç”¨ä¸€å¥è¯æ¦‚æ‹¬ã€‚\"\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ“ åˆ›å»ºçš„ç»„ä»¶:\")\n",
    "        print(f\"   1. LLM: {type(llm).__name__}\")\n",
    "        print(f\"   2. Prompt: {type(prompt).__name__}\")\n",
    "        print(f\"   3. æ¨¡æ¿å˜é‡: {prompt.input_variables}\")\n",
    "        print(f\"   4. æ¨¡æ¿å†…å®¹: {prompt.template}\")\n",
    "        \n",
    "        # æž„å»º prompt â†’ llm é“¾\n",
    "        print(f\"\\nðŸ”— æž„å»º prompt â†’ llm é“¾:\")\n",
    "        chain = prompt | llm\n",
    "        \n",
    "        print(f\"   é“¾ç±»åž‹: {type(chain)}\")\n",
    "        print(f\"   é“¾æ˜¯å¦å¯ç”¨: {chain is not None}\")\n",
    "        \n",
    "        # æµ‹è¯•é“¾å¼è°ƒç”¨\n",
    "        print(f\"\\nðŸ§ª æµ‹è¯•é“¾å¼è°ƒç”¨:\")\n",
    "        \n",
    "        test_topics = [\n",
    "            \"äººå·¥æ™ºèƒ½\",\n",
    "            \"æœºå™¨å­¦ä¹ \", \n",
    "            \"æ·±åº¦å­¦ä¹ \"\n",
    "        ]\n",
    "        \n",
    "        for i, topic in enumerate(test_topics, 1):\n",
    "            print(f\"\\n   æµ‹è¯• {i}: ä¸»é¢˜ = {topic}\")\n",
    "            \n",
    "            # è°ƒç”¨é“¾\n",
    "            result = chain.invoke({\"topic\": topic})\n",
    "            \n",
    "            print(f\"   è¾“å…¥: {{'topic': '{topic}'}}\")\n",
    "            print(f\"   è¾“å‡ºç±»åž‹: {type(result)}\")\n",
    "            print(f\"   è¾“å‡ºå†…å®¹: {result.content}\")\n",
    "            \n",
    "            # éªŒè¯è¾“å‡ºæ ¼å¼\n",
    "            assert hasattr(result, 'content'), \"è¾“å‡ºç¼ºå°‘ content å±žæ€§\"\n",
    "            assert len(result.content) > 0, \"è¾“å‡ºå†…å®¹ä¸ºç©º\"\n",
    "            \n",
    "        # æµ‹è¯•é”™è¯¯å¤„ç†\n",
    "        print(f\"\\nðŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†:\")\n",
    "        \n",
    "        try:\n",
    "            # ç¼ºå°‘å¿…éœ€å‚æ•°\n",
    "            error_result = chain.invoke({\"wrong_param\": \"test\"})\n",
    "            print(f\"   âš ï¸  é”™è¯¯è¾“å…¥å±…ç„¶æˆåŠŸäº†: {error_result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âœ… æ­£ç¡®æ•èŽ·é”™è¯¯: {type(e).__name__}\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šprompt â†’ llm é“¾æ­£ç¡®æž„å»º\n",
    "        final_test = chain.invoke({\"topic\": \"Python\"})\n",
    "        assert hasattr(final_test, 'content'), \"æœ€ç»ˆæµ‹è¯•è¾“å‡ºæ ¼å¼é”™è¯¯\"\n",
    "        assert \"Python\" in final_test.content or \"python\" in final_test.content.lower(), \"è¾“å‡ºå†…å®¹ä¸ç›¸å…³\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šprompt â†’ llm ç®€å•é“¾æž„å»ºæ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ prompt â†’ llm é“¾æž„å»ºå¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. invoke æ–¹æ³•æ·±å…¥ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke æ–¹æ³•æ·±å…¥ä½¿ç”¨ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# åŠ è½½çŽ¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âš¡ invoke æ–¹æ³•æ·±å…¥ä½¿ç”¨æµ‹è¯•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"âŒ OpenAI API Key æœªé…ç½®\")\n",
    "else:\n",
    "    try:\n",
    "        # åˆ›å»ºæµ‹è¯•é“¾\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=40\n",
    "        )\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"item\", \"count\"],\n",
    "            template=\"è¯·åˆ—å‡º {count} ä¸ªå…³äºŽ {item} çš„å…³é”®è¯ã€‚\"\n",
    "        )\n",
    "        \n",
    "        chain = prompt | llm\n",
    "        \n",
    "        print(f\"ðŸ“ æµ‹è¯•é“¾é…ç½®:\")\n",
    "        print(f\"   è¾“å…¥å˜é‡: {prompt.input_variables}\")\n",
    "        print(f\"   æ¨¡æ¿: {prompt.template}\")\n",
    "        \n",
    "        # 1. åŸºç¡€ invoke ä½¿ç”¨\n",
    "        print(f\"\\nðŸ” 1. åŸºç¡€ invoke ä½¿ç”¨:\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result1 = chain.invoke({\"item\": \"ç¼–ç¨‹è¯­è¨€\", \"count\": 3})\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"   è¾“å…¥: {{'item': 'ç¼–ç¨‹è¯­è¨€', 'count': 3}}\")\n",
    "        print(f\"   æ‰§è¡Œæ—¶é—´: {(end_time - start_time)*1000:.1f} ms\")\n",
    "        print(f\"   è¾“å‡º: {result1.content}\")\n",
    "        \n",
    "        # 2. ä¸åŒå‚æ•°ç»„åˆæµ‹è¯•\n",
    "        print(f\"\\nðŸ” 2. ä¸åŒå‚æ•°ç»„åˆæµ‹è¯•:\")\n",
    "        \n",
    "        test_cases = [\n",
    "            {\"item\": \"æ°´æžœ\", \"count\": 2},\n",
    "            {\"item\": \"é¢œè‰²\", \"count\": 4},\n",
    "            {\"item\": \"åŠ¨ç‰©\", \"count\": 3}\n",
    "        ]\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases, 1):\n",
    "            result = chain.invoke(test_case)\n",
    "            print(f\"   æµ‹è¯• {i}: {test_case} â†’ {len(result.content)} å­—ç¬¦\")\n",
    "        \n",
    "        # 3. invoke ä¸Žå…¶ä»–æ–¹æ³•å¯¹æ¯”\n",
    "        print(f\"\\nðŸ” 3. invoke ä¸Žå…¶ä»–æ–¹æ³•å¯¹æ¯”:\")\n",
    "        \n",
    "        # åˆ›å»ºç®€å•å‡½æ•°é“¾è¿›è¡Œå¯¹æ¯”\n",
    "        def process_data(data: dict) -> str:\n",
    "            return f\"å¤„ç†: {data.get('item', 'æœªçŸ¥')} x {data.get('count', 0)}\"\n",
    "        \n",
    "        func_chain = RunnableLambda(process_data)\n",
    "        \n",
    "        # invoke æ–¹æ³•\n",
    "        invoke_result = func_chain.invoke({\"item\": \"æµ‹è¯•\", \"count\": 5})\n",
    "        print(f\"   invoke ç»“æžœ: {invoke_result}\")\n",
    "        \n",
    "        # 4. é”™è¯¯å¤„ç†å’Œå¼‚å¸¸æƒ…å†µ\n",
    "        print(f\"\\nðŸ” 4. é”™è¯¯å¤„ç†å’Œå¼‚å¸¸æƒ…å†µ:\")\n",
    "        \n",
    "        # æµ‹è¯•ç©ºè¾“å…¥\n",
    "        try:\n",
    "            empty_result = chain.invoke({})\n",
    "            print(f\"   ç©ºè¾“å…¥ç»“æžœ: {empty_result.content[:50]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ç©ºè¾“å…¥é”™è¯¯: {type(e).__name__}\")\n",
    "        \n",
    "        # æµ‹è¯•æ— æ•ˆå‚æ•°ç±»åž‹\n",
    "        try:\n",
    "            type_error_result = chain.invoke(\"string_input\")\n",
    "            print(f\"   å­—ç¬¦ä¸²è¾“å…¥ç»“æžœ: {type_error_result.content[:50]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   å­—ç¬¦ä¸²è¾“å…¥é”™è¯¯: {type(e).__name__}\")\n",
    "        \n",
    "        # 5. æ€§èƒ½æµ‹è¯•\n",
    "        print(f\"\\nðŸ” 5. æ€§èƒ½æµ‹è¯•:\")\n",
    "        \n",
    "        performance_test_cases = [\n",
    "            {\"item\": \"å¿«é€Ÿæµ‹è¯•\", \"count\": 1}\n",
    "        ] * 3\n",
    "        \n",
    "        times = []\n",
    "        for i, test_case in enumerate(performance_test_cases, 1):\n",
    "            start_time = time.time()\n",
    "            result = chain.invoke(test_case)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "            print(f\"   ç¬¬ {i} æ¬¡: {(end_time - start_time)*1000:.1f} ms\")\n",
    "        \n",
    "        avg_time = sum(times) / len(times)\n",
    "        print(f\"   å¹³å‡æ—¶é—´: {avg_time*1000:.1f} ms\")\n",
    "        \n",
    "        # éªŒè¯ç‚¹ï¼šinvoke æ–¹æ³•ä½¿ç”¨æ­£ç¡®\n",
    "        assert hasattr(result1, 'content'), \"invoke è¾“å‡ºæ ¼å¼é”™è¯¯\"\n",
    "        assert len(result1.content) > 0, \"invoke è¾“å‡ºä¸ºç©º\"\n",
    "        assert avg_time < 10, \"invoke æ‰§è¡Œæ—¶é—´è¿‡é•¿\"\n",
    "        \n",
    "        print(f\"\\nâœ… éªŒè¯é€šè¿‡ï¼šinvoke æ–¹æ³•æ·±å…¥ä½¿ç”¨æ­£ç¡®\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ invoke æ–¹æ³•æµ‹è¯•å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ä¹ æ€»ç»“ä¸Žæœ€ä½³å®žè·µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ä¹ æ€»ç»“ä¸Žæœ€ä½³å®žè·µ - ç‹¬ç«‹ä»£ç å—\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½çŽ¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ðŸ“‹ Runnable åŸºç¡€æ¦‚å¿µå­¦ä¹ æ€»ç»“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# éªŒè¯ç‚¹æ£€æŸ¥\n",
    "verification_points = [\n",
    "    \"âœ… Runnable æŠ½è±¡ï¼šæ ¸å¿ƒæŽ¥å£å’Œè®¾è®¡åŽŸç†\",\n",
    "    \"âœ… é“¾å¼æ‹¼æŽ¥ï¼špipe æ“ä½œç¬¦å’Œæ–¹æ³•\",\n",
    "    \"âœ… ç®€å•é“¾æž„å»ºï¼šprompt â†’ llm ç»„åˆ\",\n",
    "    \"âœ… invoke æ–¹æ³•ï¼šç»Ÿä¸€è°ƒç”¨æŽ¥å£\",\n",
    "]\n",
    "\n",
    "for point in verification_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ æ ¸å¿ƒæŠ€èƒ½æŽŒæ¡æƒ…å†µ: {len(verification_points)}/4 é¡¹\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Runnable æœ€ä½³å®žè·µ:\")\n",
    "print(\"1. æŠ½è±¡ç†è§£ï¼šæ‰€æœ‰ LangChain ç»„ä»¶éƒ½æ˜¯ Runnable\")\n",
    "print(\"2. é“¾å¼ç»„åˆï¼šä½¿ç”¨ | æ“ä½œç¬¦è¿›è¡Œç®¡é“è¿žæŽ¥\")\n",
    "print(\"3. ç»Ÿä¸€æŽ¥å£ï¼šinvoke/stream/batch æ–¹æ³•æ ‡å‡†åŒ–\")\n",
    "print(\"4. å‡½æ•°åŒ…è£…ï¼šRunnableLambda åŒ…è£…æ™®é€šå‡½æ•°\")\n",
    "print(\"5. é”™è¯¯å¤„ç†ï¼šé“¾ä¸­ä»»ä½•çŽ¯èŠ‚éƒ½ä¼šä¼ æ’­å¼‚å¸¸\")\n",
    "\n",
    "print(\"\\nðŸ”§ å¸¸ç”¨ Runnable ç±»åž‹:\")\n",
    "print(\"1. ChatOpenAI - LLM æ¨¡åž‹è°ƒç”¨\")\n",
    "print(\"2. PromptTemplate - æç¤ºè¯æ¨¡æ¿\")\n",
    "print(\"3. RunnableLambda - å‡½æ•°åŒ…è£…\")\n",
    "print(\"4. RunnablePassthrough - æ•°æ®é€ä¼ \")\n",
    "print(\"5. RunnableMap - å¹¶è¡Œæ‰§è¡Œ\")\n",
    "\n",
    "print(\"\\nðŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:\")\n",
    "print(\"1. æ·±å…¥å­¦ä¹  RunnableMap å¹¶è¡Œæ‰§è¡Œ\")\n",
    "print(\"2. æŽŒæ¡ RunnablePassthrough æ•°æ®é€ä¼ \")\n",
    "print(\"3. å­¦ä¹  LCEL ç®¡é“è¯­æ³• (prompt|llm|parser)\")\n",
    "print(\"4. æŽ¢ç´¢æµå¼è¾“å‡º Streaming\")\n",
    "print(\"5. å®žè·µå¤æ‚é“¾å¼ç»„åˆåº”ç”¨\")\n",
    "\n",
    "# æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ Runnable åŸºç¡€åŠŸèƒ½å¯ç”¨\n",
    "try:\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    if openai_api_key:\n",
    "        # ç®€å•æµ‹è¯•\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=30)\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"æ€»ç»“: {text}\"\n",
    "        )\n",
    "        \n",
    "        chain = prompt | llm\n",
    "        result = chain.invoke({\"text\": \"æµ‹è¯• Runnable\"})\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ:\")\n",
    "        print(f\"   API çŠ¶æ€: å¯ç”¨\")\n",
    "        print(f\"   é“¾ç±»åž‹: {type(chain).__name__}\")\n",
    "        print(f\"   è¾“å‡ºé•¿åº¦: {len(result.content)} å­—ç¬¦\")\n",
    "        print(f\"   è¾“å‡ºå†…å®¹: {result.content[:50]}...\")\n",
    "        print(\"\\nâœ… Runnable åŸºç¡€æ¦‚å¿µå­¦ä¹ å®Œæˆï¼\")\n",
    "        \n",
    "        print(f\"\\nðŸŽŠ LCEL ç¼–æŽ’å±‚å­¦ä¹ å¼€å§‹ï¼\")\n",
    "        print(f\"   å·²æŽŒæ¡: Runnable æŠ½è±¡å’ŒåŸºç¡€é“¾å¼ç»„åˆ\")\n",
    "        print(f\"   ä¸‹ä¸€æ­¥: å­¦ä¹ å¹¶è¡Œæ‰§è¡Œå’Œé«˜çº§ç»„åˆæŠ€æœ¯\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  æœ€ç»ˆéªŒè¯è·³è¿‡: OpenAI API Key æœªé…ç½®\")\n",
    "        print(\"   è¯·é…ç½® OPENAI_API_KEY åŽé‡è¯•\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
