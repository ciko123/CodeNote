{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatOpenAI GPT åŸºç¡€è°ƒç”¨ - ç»ƒä¹ é¢˜\n",
    "\n",
    "## ğŸ“‹ ç»ƒä¹ æ¦‚è§ˆ\n",
    "\n",
    "æœ¬ç»ƒä¹ å†ŒåŒ…å« 5 ä¸ªæ¸è¿›å¼ç»ƒä¹ é¢˜ï¼Œå¸®åŠ©æ‚¨æŒæ¡ ChatOpenAI GPT åŸºç¡€è°ƒç”¨çš„æ ¸å¿ƒæŠ€èƒ½ã€‚\n",
    "\n",
    "| ç»ƒä¹  | éš¾åº¦ | é¢„è®¡æ—¶é—´ | ä¸»è¦å†…å®¹ |\n",
    "|------|------|----------|----------|\n",
    "| ç»ƒä¹ 1 | â­ | 20åˆ†é’Ÿ | åŸºç¡€åˆå§‹åŒ–å’Œç®€å•å¯¹è¯ |\n",
    "| ç»ƒä¹ 2 | â­â­ | 30åˆ†é’Ÿ | å‚æ•°è°ƒä¼˜å’Œæ¨¡å‹å¯¹æ¯” |\n",
    "| ç»ƒä¹ 3 | â­â­â­ | 40åˆ†é’Ÿ | æç¤ºè¯æ¨¡æ¿å’Œå¤šè§’è‰²å¯¹è¯ |\n",
    "| ç»ƒä¹ 4 | â­â­â­â­ | 50åˆ†é’Ÿ | LCELé“¾å¼è°ƒç”¨å’Œç»“æ„åŒ–è¾“å‡º |\n",
    "| ç»ƒä¹ 5 | â­â­â­â­â­ | 60åˆ†é’Ÿ | é«˜çº§åŠŸèƒ½ç»¼åˆåº”ç”¨ |\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "- æŒæ¡ ChatOpenAI åŸºç¡€é…ç½®å’Œè°ƒç”¨\n",
    "- ç†è§£æ ¸å¿ƒå‚æ•°çš„ä½œç”¨å’Œè°ƒä¼˜æ–¹æ³•\n",
    "- ç†Ÿç»ƒä½¿ç”¨æç¤ºè¯æ¨¡æ¿ç³»ç»Ÿ\n",
    "- æŒæ¡ LCEL é“¾å¼è°ƒç”¨è¯­æ³•\n",
    "- å…·å¤‡é”™è¯¯å¤„ç†å’Œé«˜çº§åŠŸèƒ½åº”ç”¨èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç»ƒä¹ 1ï¼šChatOpenAI åŸºç¡€é…ç½®éªŒè¯ â­\n",
    "\n",
    "**é¢˜ç›®**ï¼šé…ç½® ChatOpenAI è¿æ¥åˆ° GPT æœåŠ¡ï¼Œå®ç°åŸºç¡€å¯¹è¯åŠŸèƒ½\n",
    "\n",
    "**è¦æ±‚**ï¼š\n",
    "- ä½¿ç”¨æ­£ç¡®çš„ base_url å’Œ api_key é…ç½®\n",
    "- è®¾ç½® temperature=0.7, max_tokens=500\n",
    "- å‘é€\"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±\"çš„æµ‹è¯•æ¶ˆæ¯\n",
    "- éªŒè¯å›å¤å†…å®¹ä¸ä¸ºç©ºä¸”é•¿åº¦å—æ§\n",
    "- æ‰“å°æ¨¡å‹é…ç½®ä¿¡æ¯\n",
    "\n",
    "**éªŒè¯ç‚¹**ï¼šæˆåŠŸè·å– GPT å›å¤å¹¶æ§åˆ¶è¾“å‡ºé•¿åº¦\n",
    "\n",
    "**é¢„è®¡æ—¶é—´**ï¼š20åˆ†é’Ÿ\n",
    "\n",
    "**å…ˆä¿®è¦æ±‚**ï¼šæ— "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n",
      "âœ… ç»ƒä¹ 1éªŒè¯é€šè¿‡\n"
     ]
    }
   ],
   "source": [
    "# ç»ƒä¹ 1ï¼šChatOpenAI åŸºç¡€é…ç½®éªŒè¯\n",
    "# è¯·åœ¨æ­¤å¤„å®Œæˆç»ƒä¹ 1çš„ä»£ç \n",
    "\n",
    "# TODO: å¯¼å…¥å¿…è¦çš„åº“\n",
    "# TODO: åˆå§‹åŒ– ChatOpenAI æ¨¡å‹\n",
    "# TODO: å‘é€æµ‹è¯•æ¶ˆæ¯\n",
    "# TODO: éªŒè¯å›å¤å†…å®¹\n",
    "# TODO: æ‰“å°é…ç½®ä¿¡æ¯\n",
    "\n",
    "# æç¤ºï¼šå‚è€ƒæ•™å­¦æ–‡æ¡£ä¸­çš„ç¬¬1å’Œç¬¬2ä¸ªä»£ç å—\n",
    " \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=10,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "response = model.invoke(\"ä½ å¥½?\")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "# éªŒè¯ä»£ç ï¼ˆä¸è¦ä¿®æ”¹ï¼‰\n",
    "assert 'response' in locals(), \"è¯·åˆ›å»º response å˜é‡\"\n",
    "assert hasattr(response, 'content'), \"response åº”è¯¥æœ‰ content å±æ€§\"\n",
    "assert len(response.content) > 0, \"å›å¤å†…å®¹ä¸èƒ½ä¸ºç©º\"\n",
    "print(\"âœ… ç»ƒä¹ 1éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç»ƒä¹ 2ï¼šæ ¸å¿ƒå‚æ•°è°ƒä¼˜å®éªŒ â­â­\n",
    "\n",
    "**é¢˜ç›®**ï¼šæµ‹è¯•ä¸åŒæ¸©åº¦å‚æ•°å’Œæ¨¡å‹å¯¹è¾“å‡ºé£æ ¼çš„å½±å“\n",
    "\n",
    "**è¦æ±‚**ï¼š\n",
    "- å¯¹æ¯” temperature=0.1 å’Œ temperature=1.0 çš„è¾“å‡ºå·®å¼‚\n",
    "- æµ‹è¯•è‡³å°‘3ç§ä¸åŒæ¨¡å‹ï¼ˆgpt-4o, gpt-4o-mini, gpt-4.1ï¼‰\n",
    "- ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•æç¤ºè¯è¿›è¡Œå¯¹æ¯”\n",
    "- åˆ†æå¹¶è®°å½•å„å‚æ•°é…ç½®çš„ç‰¹ç‚¹\n",
    "- ç»™å‡ºä¸åŒåœºæ™¯çš„å‚æ•°é€‰æ‹©å»ºè®®\n",
    "\n",
    "**éªŒè¯ç‚¹**ï¼šèƒ½æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„å‚æ•°é…ç½®\n",
    "\n",
    "**é¢„è®¡æ—¶é—´**ï¼š30åˆ†é’Ÿ\n",
    "\n",
    "**å…ˆä¿®è¦æ±‚**ï¼šç»ƒä¹ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatOpenAI' from 'langchain.chat_models' (c:\\ProgramData\\miniconda3\\envs\\ai\\Lib\\site-packages\\langchain\\chat_models\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ç»ƒä¹ 2ï¼šæ ¸å¿ƒå‚æ•°è°ƒä¼˜å®éªŒ\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# è¯·åœ¨æ­¤å¤„å®Œæˆç»ƒä¹ 2çš„ä»£ç \u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# æç¤ºï¼šå‚è€ƒæ•™å­¦æ–‡æ¡£ä¸­çš„ç¬¬3å’Œç¬¬4ä¸ªä»£ç å—\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# åˆ›å»ºä¸åŒæ¸©åº¦çš„æ¨¡å‹\u001b[39;00m\n\u001b[32m     15\u001b[39m low_temp_model = ChatOpenAI(\n\u001b[32m     16\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     temperature=\u001b[32m0.1\u001b[39m,\n\u001b[32m     18\u001b[39m     max_tokens=\u001b[32m100\u001b[39m,\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ChatOpenAI' from 'langchain.chat_models' (c:\\ProgramData\\miniconda3\\envs\\ai\\Lib\\site-packages\\langchain\\chat_models\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# ç»ƒä¹ 2ï¼šæ ¸å¿ƒå‚æ•°è°ƒä¼˜å®éªŒ\n",
    "# è¯·åœ¨æ­¤å¤„å®Œæˆç»ƒä¹ 2çš„ä»£ç \n",
    "\n",
    "# TODO: åˆ›å»ºä¸åŒæ¸©åº¦å‚æ•°çš„æ¨¡å‹\n",
    "# TODO: æµ‹è¯•æ¸©åº¦å¯¹è¾“å‡ºçš„å½±å“\n",
    "# TODO: æµ‹è¯•ä¸åŒæ¨¡å‹çš„å“åº”ç‰¹å¾\n",
    "# TODO: åˆ†æå‚æ•°é…ç½®æ•ˆæœ\n",
    "\n",
    "# æç¤ºï¼šå‚è€ƒæ•™å­¦æ–‡æ¡£ä¸­çš„ç¬¬3å’Œç¬¬4ä¸ªä»£ç å—\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# åˆ›å»ºä¸åŒæ¸©åº¦çš„æ¨¡å‹\n",
    "low_temp_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "medium_temp_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "high_temp_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# æµ‹è¯•æ¸©åº¦å¯¹è¾“å‡ºçš„å½±å“\n",
    "prompt = \"è¯·ç”¨ä¸€å¥è¯æè¿°äººå·¥æ™ºèƒ½çš„å‘å±•å‰æ™¯\"\n",
    "\n",
    "low_temp_response = low_temp_model.invoke(prompt)\n",
    "medium_temp_response = medium_temp_model.invoke(prompt)\n",
    "high_temp_response = high_temp_model.invoke(prompt)\n",
    "\n",
    "temp_comparison = {\n",
    "    \"low_temp\": str(low_temp_response.content),\n",
    "    \"medium_temp\": str(medium_temp_response.content),\n",
    "    \"high_temp\": str(high_temp_response.content)\n",
    "}\n",
    "\n",
    "# æµ‹è¯•ä¸åŒæ¨¡å‹çš„å“åº”ç‰¹å¾\n",
    "gpt35_model = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "gpt4_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "gpt4o_model = ChatOpenAI(\n",
    "    model_name=\"gpt-5\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "model_comparison = {\n",
    "    \"gpt-3.5\": str(gpt35_model.invoke(prompt).content),\n",
    "    \"gpt-4\": str(gpt4_model.invoke(prompt).content),\n",
    "    \"gpt-4o-mini\": str(gpt4o_model.invoke(prompt).content)\n",
    "}\n",
    "\n",
    "# éªŒè¯ä»£ç ï¼ˆä¸è¦ä¿®æ”¹ï¼‰\n",
    "# assert 'temp_comparison' in locals(), \"è¯·åˆ›å»º temp_comparison å˜é‡\"\n",
    "# assert 'model_comparison' in locals(), \"è¯·åˆ›å»º model_comparison å˜é‡\"\n",
    "# print(\"âœ… ç»ƒä¹ 2éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç»ƒä¹ 3ï¼šæç¤ºè¯æ¨¡æ¿ç³»ç»Ÿåº”ç”¨ â­â­â­\n",
    "\n",
    "**é¢˜ç›®**ï¼šä½¿ç”¨ PromptTemplate å’Œ ChatPromptTemplate å®ç°åŠ¨æ€å¯¹è¯\n",
    "\n",
    "**è¦æ±‚**ï¼š\n",
    "- åˆ›å»ºåŒ…å«å¤šä¸ªå˜é‡çš„ PromptTemplate\n",
    "- è®¾è®¡ç³»ç»Ÿè§’è‰²å’Œç”¨æˆ·è§’è‰²çš„ ChatPromptTemplate\n",
    "- å®ç°äº§å“ä»‹ç»çš„åŠ¨æ€æ¨¡æ¿\n",
    "- æµ‹è¯•ä¸åŒè¾“å…¥å˜é‡çš„æ¨¡æ¿å¡«å……\n",
    "- éªŒè¯æ¶ˆæ¯æ ¼å¼å’Œè§’è‰²è®¾ç½®æ­£ç¡®æ€§\n",
    "\n",
    "**éªŒè¯ç‚¹**ï¼šæ¨¡æ¿èƒ½æ­£ç¡®å¡«å……å˜é‡å¹¶ç”Ÿæˆè§’è‰²å¯¹è¯\n",
    "\n",
    "**é¢„è®¡æ—¶é—´**ï¼š40åˆ†é’Ÿ\n",
    "\n",
    "**å…ˆä¿®è¦æ±‚**ï¼šç»ƒä¹ 1ã€ç»ƒä¹ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 3ï¼šæç¤ºè¯æ¨¡æ¿ç³»ç»Ÿåº”ç”¨\n",
    "# è¯·åœ¨æ­¤å¤„å®Œæˆç»ƒä¹ 3çš„ä»£ç \n",
    "\n",
    "# TODO: åˆ›å»º PromptTemplate\n",
    "# TODO: åˆ›å»º ChatPromptTemplate\n",
    "# TODO: æµ‹è¯•å˜é‡å¡«å……\n",
    "# TODO: éªŒè¯æ¶ˆæ¯æ ¼å¼\n",
    "\n",
    "# æç¤ºï¼šå‚è€ƒæ•™å­¦æ–‡æ¡£ä¸­çš„ç¬¬5å’Œç¬¬6ä¸ªä»£ç å—\n",
    "\n",
    "# éªŒè¯ä»£ç ï¼ˆä¸è¦ä¿®æ”¹ï¼‰\n",
    "# assert 'prompt_template' in locals(), \"è¯·åˆ›å»º prompt_template å˜é‡\"\n",
    "# assert 'chat_template' in locals(), \"è¯·åˆ›å»º chat_template å˜é‡\"\n",
    "# print(\"âœ… ç»ƒä¹ 3éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç»ƒä¹ 4ï¼šLCELé“¾å¼è°ƒç”¨å’Œç»“æ„åŒ–è¾“å‡º â­â­â­â­\n",
    "\n",
    "**é¢˜ç›®**ï¼šæ„å»º LCEL é“¾å®ç°ç»“æ„åŒ–æ•°æ®å¤„ç†\n",
    "\n",
    "**è¦æ±‚**ï¼š\n",
    "- åˆ›å»ºå®Œæ•´çš„ LCEL é“¾ï¼šprompt | llm | parser\n",
    "- å®šä¹‰ Pydantic æ¨¡å‹çº¦æŸè¾“å‡ºæ ¼å¼\n",
    "- å®ç°ç»“æ„åŒ–æ•°æ®æå–åŠŸèƒ½\n",
    "- æµ‹è¯•é“¾å¼è°ƒç”¨çš„æ•°æ®æµä¼ é€’\n",
    "- éªŒè¯è¾“å‡ºæ•°æ®ç¬¦åˆæŒ‡å®šç»“æ„\n",
    "\n",
    "**éªŒè¯ç‚¹**ï¼šé“¾å¼è°ƒç”¨è¿”å›ç¬¦åˆ Pydantic æ¨¡å‹çš„ç»“æ„åŒ–æ•°æ®\n",
    "\n",
    "**é¢„è®¡æ—¶é—´**ï¼š50åˆ†é’Ÿ\n",
    "\n",
    "**å…ˆä¿®è¦æ±‚**ï¼šç»ƒä¹ 1ã€ç»ƒä¹ 2ã€ç»ƒä¹ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 4ï¼šLCELé“¾å¼è°ƒç”¨å’Œç»“æ„åŒ–è¾“å‡º\n",
    "# è¯·åœ¨æ­¤å¤„å®Œæˆç»ƒä¹ 4çš„ä»£ç \n",
    "\n",
    "# TODO: å®šä¹‰ Pydantic æ¨¡å‹\n",
    "# TODO: åˆ›å»º LCEL é“¾\n",
    "# TODO: æµ‹è¯•ç»“æ„åŒ–è¾“å‡º\n",
    "# TODO: éªŒè¯æ•°æ®ç»“æ„\n",
    "\n",
    "# æç¤ºï¼šå‚è€ƒæ•™å­¦æ–‡æ¡£ä¸­çš„ç¬¬7å’Œç¬¬8ä¸ªä»£ç å—\n",
    "\n",
    "# éªŒè¯ä»£ç ï¼ˆä¸è¦ä¿®æ”¹ï¼‰\n",
    "# assert 'lcel_chain' in locals(), \"è¯·åˆ›å»º lcel_chain å˜é‡\"\n",
    "# assert 'structured_result' in locals(), \"è¯·åˆ›å»º structured_result å˜é‡\"\n",
    "# print(\"âœ… ç»ƒä¹ 4éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç»ƒä¹ 5ï¼šé«˜çº§åŠŸèƒ½ç»¼åˆåº”ç”¨ â­â­â­â­â­\n",
    "\n",
    "**é¢˜ç›®**ï¼šé›†æˆæµå¼è¾“å‡ºã€æ‰¹é‡å¤„ç†ã€é”™è¯¯å¤„ç†çš„é«˜çº§åº”ç”¨\n",
    "\n",
    "**è¦æ±‚**ï¼š\n",
    "- å®ç°æµå¼è¾“å‡ºçš„å®æ—¶æ˜¾ç¤º\n",
    "- æ„å»ºæ‰¹é‡å¤„ç†çš„å¤šè¯·æ±‚ç³»ç»Ÿ\n",
    "- æ·»åŠ å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶\n",
    "- é›†æˆæ–‡æœ¬åµŒå…¥å‘é‡ç”ŸæˆåŠŸèƒ½\n",
    "- åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹\n",
    "\n",
    "**éªŒè¯ç‚¹**ï¼šç³»ç»Ÿèƒ½ç¨³å®šå¤„ç†å„ç§é«˜çº§åŠŸèƒ½åœºæ™¯\n",
    "\n",
    "**é¢„è®¡æ—¶é—´**ï¼š60åˆ†é’Ÿ\n",
    "\n",
    "**å…ˆä¿®è¦æ±‚**ï¼šç»ƒä¹ 1ã€ç»ƒä¹ 2ã€ç»ƒä¹ 3ã€ç»ƒä¹ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 5ï¼šé«˜çº§åŠŸèƒ½ç»¼åˆåº”ç”¨\n",
    "# è¯·åœ¨æ­¤å¤„å®Œæˆç»ƒä¹ 5çš„ä»£ç \n",
    "\n",
    "# TODO: å®ç°æµå¼è¾“å‡ºåŠŸèƒ½\n",
    "# TODO: æ„å»ºæ‰¹é‡å¤„ç†ç³»ç»Ÿ\n",
    "# TODO: æ·»åŠ é”™è¯¯å¤„ç†æœºåˆ¶\n",
    "# TODO: é›†æˆåµŒå…¥æ¨¡å‹åŠŸèƒ½\n",
    "# TODO: åˆ›å»ºæ™ºèƒ½é—®ç­”åŠ©æ‰‹\n",
    "\n",
    "# æç¤ºï¼šå‚è€ƒæ•™å­¦æ–‡æ¡£ä¸­çš„ç¬¬9-12ä¸ªä»£ç å—\n",
    "\n",
    "# éªŒè¯ä»£ç ï¼ˆä¸è¦ä¿®æ”¹ï¼‰\n",
    "# assert 'streaming_demo' in locals(), \"è¯·åˆ›å»º streaming_demo å˜é‡\"\n",
    "# assert 'batch_system' in locals(), \"è¯·åˆ›å»º batch_system å˜é‡\"\n",
    "# assert 'error_handler' in locals(), \"è¯·åˆ›å»º error_handler å˜é‡\"\n",
    "# assert 'qa_assistant' in locals(), \"è¯·åˆ›å»º qa_assistant å˜é‡\"\n",
    "# print(\"âœ… ç»ƒä¹ 5éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ ç»ƒä¹ ç­”æ¡ˆï¼ˆè¯·å…ˆå®Œæˆç»ƒä¹ å†æŸ¥çœ‹ï¼‰\n",
    "\n",
    "### âš ï¸ é‡è¦æé†’\n",
    "è¯·å…ˆç‹¬ç«‹å®Œæˆæ‰€æœ‰ç»ƒä¹ ï¼Œå†å‚è€ƒä»¥ä¸‹ç­”æ¡ˆã€‚ç›´æ¥æŸ¥çœ‹ç­”æ¡ˆä¼šå½±å“å­¦ä¹ æ•ˆæœï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ ç»ƒä¹ 1ç­”æ¡ˆï¼šChatOpenAI åŸºç¡€é…ç½®éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 1ï¼šChatOpenAI åŸºç¡€é…ç½®éªŒè¯ - å®Œæ•´ç­”æ¡ˆ\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ– ChatOpenAI æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",           # ä½¿ç”¨ GPT-4o æ¨¡å‹\n",
    "    temperature=0.7,          # å¹³è¡¡éšæœºæ€§å’Œä¸€è‡´æ€§\n",
    "    max_tokens=500,           # æ§åˆ¶è¾“å‡ºé•¿åº¦\n",
    ")\n",
    "\n",
    "# å‘é€æµ‹è¯•æ¶ˆæ¯\n",
    "response = llm.invoke(\"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚\")\n",
    "\n",
    "# éªŒè¯å›å¤å†…å®¹\n",
    "print(\"ğŸ¤– GPT-4o å›å¤:\")\n",
    "print(response.content)\n",
    "print(f\"\\nğŸ“Š å›å¤é•¿åº¦: {len(response.content)} å­—ç¬¦\")\n",
    "\n",
    "# æ‰“å°é…ç½®ä¿¡æ¯\n",
    "print(\"\\nğŸ“‹ æ¨¡å‹é…ç½®ä¿¡æ¯:\")\n",
    "print(f\"æ¨¡å‹: {llm.model_name}\")\n",
    "print(f\"æ¸©åº¦: {llm.temperature}\")\n",
    "print(f\"æœ€å¤§ä»¤ç‰Œæ•°: {llm.max_tokens}\")\n",
    "print(f\"API Base URL: {os.getenv('OPENAI_BASE_URL')}\")\n",
    "\n",
    "# éªŒè¯ä»£ç \n",
    "assert 'response' in locals(), \"è¯·åˆ›å»º response å˜é‡\"\n",
    "assert hasattr(response, 'content'), \"response åº”è¯¥æœ‰ content å±æ€§\"\n",
    "assert len(response.content) > 0, \"å›å¤å†…å®¹ä¸èƒ½ä¸ºç©º\"\n",
    "print(\"\\nâœ… ç»ƒä¹ 1éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å…³é”®è¦ç‚¹**ï¼š\n",
    "- `load_dotenv()` åŠ è½½ç¯å¢ƒå˜é‡ä¸­çš„ API é…ç½®\n",
    "- `temperature=0.7` æä¾›å¹³è¡¡çš„åˆ›é€ æ€§\n",
    "- `max_tokens=500` æ§åˆ¶å›å¤é•¿åº¦é¿å…è¿‡é•¿\n",
    "- `response.content` åŒ…å«å®é™…çš„æ–‡æœ¬å›å¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ç»ƒä¹ 2ç­”æ¡ˆï¼šæ ¸å¿ƒå‚æ•°è°ƒä¼˜å®éªŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 2ï¼šæ ¸å¿ƒå‚æ•°è°ƒä¼˜å®éªŒ - å®Œæ•´ç­”æ¡ˆ\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# æµ‹è¯•æç¤ºè¯\n",
    "test_prompt = \"è¯·ç”¨ä¸€ä¸ªæ¯”å–»è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ \"\n",
    "\n",
    "# åˆ›å»ºä¸åŒæ¸©åº¦å‚æ•°çš„æ¨¡å‹\n",
    "llm_low_temp = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.1,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "llm_high_temp = ChatOpenAI(\n",
    "    model=\"gpt-4o\", \n",
    "    temperature=1.0,  # é«˜æ¸©åº¦ï¼Œæ›´åˆ›é€ æ€§\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "# æµ‹è¯•æ¸©åº¦å¯¹è¾“å‡ºçš„å½±å“\n",
    "response_low = llm_low_temp.invoke(test_prompt)\n",
    "response_high = llm_high_temp.invoke(test_prompt)\n",
    "\n",
    "temp_comparison = {\n",
    "    \"ä½æ¸©åº¦(0.1)\": response_low.content,\n",
    "    \"é«˜æ¸©åº¦(1.0)\": response_high.content\n",
    "}\n",
    "\n",
    "print(\"ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°å¯¹æ¯”:\")\n",
    "print(f\"ä½æ¸©åº¦(0.1): {response_low.content}\")\n",
    "print(f\"é«˜æ¸©åº¦(1.0): {response_high.content}\")\n",
    "\n",
    "# æµ‹è¯•ä¸åŒæ¨¡å‹çš„å“åº”ç‰¹å¾\n",
    "models_to_test = [\n",
    "    (\"gpt-4o\", \"GPT-4o ä¸»åŠ›æ¨¡å‹\"),\n",
    "    (\"gpt-4o-mini\", \"GPT-4o-mini è½»é‡æ¨¡å‹\"),\n",
    "    (\"gpt-4.1\", \"GPT-4.1 æ ‡å‡†æ¨¡å‹\")\n",
    "]\n",
    "\n",
    "model_comparison = {}\n",
    "for model_name, description in models_to_test:\n",
    "    llm = ChatOpenAI(\n",
    "        model=model_name,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    response = llm.invoke(\"ç”¨ä¸€å¥è¯ä»‹ç» LangChainï¼Œä¸è¶…è¿‡50å­—\")\n",
    "    model_comparison[description] = response.content\n",
    "    print(f\"{description}: {response.content}\")\n",
    "\n",
    "# åˆ†æå‚æ•°é…ç½®æ•ˆæœ\n",
    "print(\"\\nğŸ“Š å‚æ•°é…ç½®åˆ†æ:\")\n",
    "print(\"1. æ¸©åº¦å‚æ•°ï¼šä½æ¸©åº¦è¾“å‡ºæ›´ç¨³å®šä¸€è‡´ï¼Œé«˜æ¸©åº¦è¾“å‡ºæ›´æœ‰åˆ›æ„\")\n",
    "print(\"2. æ¨¡å‹é€‰æ‹©ï¼šgpt-4o è´¨é‡æœ€ä½³ï¼Œgpt-4o-mini é€Ÿåº¦å¿«ï¼Œgpt-4.1 å¹³è¡¡æ€§å¥½\")\n",
    "\n",
    "# éªŒè¯ä»£ç \n",
    "assert 'temp_comparison' in locals(), \"è¯·åˆ›å»º temp_comparison å˜é‡\"\n",
    "assert 'model_comparison' in locals(), \"è¯·åˆ›å»º model_comparison å˜é‡\"\n",
    "print(\"\\nâœ… ç»ƒä¹ 2éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å…³é”®è¦ç‚¹**ï¼š\n",
    "- `temperature=0.1` é€‚åˆäº‹å®æ€§ã€ç¡®å®šæ€§ä»»åŠ¡\n",
    "- `temperature=1.0` é€‚åˆåˆ›æ„æ€§ã€å¤šæ ·æ€§ä»»åŠ¡\n",
    "- ä¸åŒæ¨¡å‹åœ¨é€Ÿåº¦ã€è´¨é‡ã€æˆæœ¬ä¸Šæœ‰ä¸åŒæƒè¡¡\n",
    "- é€‰æ‹©æ¨¡å‹è¦è€ƒè™‘å…·ä½“åº”ç”¨åœºæ™¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ç»ƒä¹ 3ç­”æ¡ˆï¼šæç¤ºè¯æ¨¡æ¿ç³»ç»Ÿåº”ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 3ï¼šæç¤ºè¯æ¨¡æ¿ç³»ç»Ÿåº”ç”¨ - å®Œæ•´ç­”æ¡ˆ\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# åˆ›å»º PromptTemplateï¼ˆå•è§’è‰²å¤šå˜é‡ï¼‰\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"product_name\", \"features\", \"target_audience\"],\n",
    "    template=\"\"\"è¯·ä¸º {product_name} å†™ä¸€æ®µäº§å“ä»‹ç»ã€‚\n",
    "\n",
    "äº§å“ç‰¹ç‚¹ï¼š{features}\n",
    "\n",
    "ç›®æ ‡ç”¨æˆ·ï¼š{target_audience}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "- è¯­è¨€ç®€æ´æœ‰åŠ›\n",
    "- çªå‡ºæ ¸å¿ƒå–ç‚¹\n",
    "- æ§åˆ¶åœ¨100å­—ä»¥å†…\"\"\"\n",
    ")\n",
    "\n",
    "# åˆ›å»º ChatPromptTemplateï¼ˆå¤šè§’è‰²å¯¹è¯ï¼‰\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“è¥é”€ä¸“å®¶ï¼Œæ“…é•¿æ’°å†™å¸å¼•äººçš„äº§å“ä»‹ç»ã€‚\"),\n",
    "    (\"user\", \"è¯·ä¸º {product_name} æ’°å†™è¥é”€æ–‡æ¡ˆï¼Œäº§å“ç‰¹ç‚¹ï¼š{features}ï¼Œç›®æ ‡ç”¨æˆ·ï¼š{target_audience}\")\n",
    "])\n",
    "\n",
    "# æµ‹è¯•å˜é‡å¡«å……\n",
    "test_data = {\n",
    "    \"product_name\": \"æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹\",\n",
    "    \"features\": \"AIä¸ªæ€§åŒ–æ¨èã€å®æ—¶ç­”ç–‘ã€å­¦ä¹ è¿›åº¦è·Ÿè¸ª\",\n",
    "    \"target_audience\": \"å¤§å­¦ç”Ÿå’ŒèŒåœºäººå£«\"\n",
    "}\n",
    "\n",
    "# æµ‹è¯• PromptTemplate\n",
    "formatted_prompt = prompt_template.format(**test_data)\n",
    "print(\"ğŸ“ PromptTemplate æ ¼å¼åŒ–ç»“æœ:\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "response1 = llm.invoke(formatted_prompt)\n",
    "print(f\"\\nğŸ¤– PromptTemplate å›å¤: {response1.content}\")\n",
    "\n",
    "# æµ‹è¯• ChatPromptTemplate\n",
    "formatted_messages = chat_template.format_messages(**test_data)\n",
    "print(\"\\nğŸ’¬ ChatPromptTemplate æ ¼å¼åŒ–ç»“æœ:\")\n",
    "for i, message in enumerate(formatted_messages):\n",
    "    print(f\"{i+1}. [{message.type}]: {message.content}\")\n",
    "\n",
    "response2 = llm.invoke(formatted_messages)\n",
    "print(f\"\\nğŸ¤– ChatPromptTemplate å›å¤: {response2.content}\")\n",
    "\n",
    "# éªŒè¯æ¶ˆæ¯æ ¼å¼\n",
    "print(f\"\\nğŸ“Š æ¶ˆæ¯æ ¼å¼éªŒè¯:\")\n",
    "print(f\"æ¶ˆæ¯æ•°é‡: {len(formatted_messages)}\")\n",
    "print(f\"ç³»ç»Ÿæ¶ˆæ¯ç±»å‹: {formatted_messages[0].type}\")\n",
    "print(f\"ç”¨æˆ·æ¶ˆæ¯ç±»å‹: {formatted_messages[1].type}\")\n",
    "\n",
    "# éªŒè¯ä»£ç \n",
    "assert 'prompt_template' in locals(), \"è¯·åˆ›å»º prompt_template å˜é‡\"\n",
    "assert 'chat_template' in locals(), \"è¯·åˆ›å»º chat_template å˜é‡\"\n",
    "print(\"\\nâœ… ç»ƒä¹ 3éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å…³é”®è¦ç‚¹**ï¼š\n",
    "- `PromptTemplate` é€‚åˆå•è§’è‰²ã€å¤šå˜é‡çš„ç®€å•åœºæ™¯\n",
    "- `ChatPromptTemplate` é€‚åˆå¤šè§’è‰²å¯¹è¯çš„å¤æ‚åœºæ™¯\n",
    "- å˜é‡å¡«å……ä½¿ç”¨ `{variable_name}` è¯­æ³•\n",
    "- `format()` æ–¹æ³•ç”¨äº PromptTemplateï¼Œ`format_messages()` ç”¨äº ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ç»ƒä¹ 4ç­”æ¡ˆï¼šLCELé“¾å¼è°ƒç”¨å’Œç»“æ„åŒ–è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 4ï¼šLCELé“¾å¼è°ƒç”¨å’Œç»“æ„åŒ–è¾“å‡º - å®Œæ•´ç­”æ¡ˆ\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# å®šä¹‰ Pydantic æ¨¡å‹çº¦æŸè¾“å‡ºæ ¼å¼\n",
    "class ProductAnalysis(BaseModel):\n",
    "    product_name: str = Field(description=\"äº§å“åç§°\")\n",
    "    category: str = Field(description=\"äº§å“ç±»åˆ«\")\n",
    "    features: list[str] = Field(description=\"äº§å“ç‰¹ç‚¹åˆ—è¡¨\")\n",
    "    target_audience: str = Field(description=\"ç›®æ ‡ç”¨æˆ·ç¾¤ä½“\")\n",
    "    market_potential: float = Field(description=\"å¸‚åœºæ½œåŠ›è¯„åˆ†ï¼Œ0-1ä¹‹é—´\")\n",
    "\n",
    "# åˆ›å»ºæç¤ºè¯æ¨¡æ¿\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product_description\"],\n",
    "    template=\"\"\"è¯·åˆ†æä»¥ä¸‹äº§å“æè¿°ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ä»¥ç»“æ„åŒ–æ ¼å¼è¿”å›ï¼š\n",
    "\n",
    "äº§å“æè¿°ï¼š{product_description}\n",
    "\n",
    "è¯·æå–ï¼š\n",
    "1. äº§å“åç§°\n",
    "2. äº§å“ç±»åˆ«  \n",
    "3. ä¸»è¦ç‰¹ç‚¹ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰\n",
    "4. ç›®æ ‡ç”¨æˆ·ç¾¤ä½“\n",
    "5. å¸‚åœºæ½œåŠ›è¯„åˆ†ï¼ˆ0-1ä¹‹é—´çš„æ•°å€¼ï¼‰\n",
    "\n",
    "è¯·ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ç»“æœã€‚\"\"\"\n",
    ")\n",
    "\n",
    "# åˆ›å»º LCEL é“¾ï¼šprompt | llm | parser\n",
    "lcel_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"âœ… LCEL é“¾åˆ›å»ºæˆåŠŸ\")\n",
    "print(\"ğŸ”— é“¾ç»“æ„: PromptTemplate â†’ ChatOpenAI â†’ StrOutputParser\")\n",
    "\n",
    "# æµ‹è¯•åŸºç¡€é“¾å¼è°ƒç”¨\n",
    "test_product = \"è¿™æ˜¯ä¸€æ¬¾é¢å‘å¤§å­¦ç”Ÿçš„æ™ºèƒ½å­¦ä¹ APPï¼Œæä¾›AIä¸ªæ€§åŒ–æ¨èã€å®æ—¶ç­”ç–‘ã€å­¦ä¹ è¿›åº¦è·Ÿè¸ªç­‰åŠŸèƒ½ï¼Œå¸‚åœºå‰æ™¯å¾ˆå¥½ã€‚\"\n",
    "result = lcel_chain.invoke({\"product_description\": test_product})\n",
    "\n",
    "print(f\"\\nğŸ¤– åŸºç¡€é“¾å¼è°ƒç”¨ç»“æœ: {result}\")\n",
    "print(f\"ğŸ“Š ç»“æœç±»å‹: {type(result)}\")\n",
    "\n",
    "# åˆ›å»ºç»“æ„åŒ–è¾“å‡ºé“¾\n",
    "structured_llm = llm.with_structured_output(ProductAnalysis)\n",
    "structured_chain = prompt | structured_llm\n",
    "\n",
    "# æµ‹è¯•ç»“æ„åŒ–è¾“å‡º\n",
    "structured_result = structured_chain.invoke({\"product_description\": test_product})\n",
    "\n",
    "print(f\"\\nğŸ“Š ç»“æ„åŒ–è¾“å‡ºç»“æœ:\")\n",
    "print(f\"äº§å“åç§°: {structured_result.product_name}\")\n",
    "print(f\"äº§å“ç±»åˆ«: {structured_result.category}\")\n",
    "print(f\"äº§å“ç‰¹ç‚¹: {structured_result.features}\")\n",
    "print(f\"ç›®æ ‡ç”¨æˆ·: {structured_result.target_audience}\")\n",
    "print(f\"å¸‚åœºæ½œåŠ›: {structured_result.market_potential}\")\n",
    "print(f\"ğŸ“Š ç»“æœç±»å‹: {type(structured_result)}\")\n",
    "\n",
    "# éªŒè¯æ•°æ®ç»“æ„\n",
    "print(f\"\\nğŸ” æ•°æ®ç»“æ„éªŒè¯:\")\n",
    "print(f\"æ˜¯å¦ä¸º ProductAnalysis å®ä¾‹: {isinstance(structured_result, ProductAnalysis)}\")\n",
    "print(f\"äº§å“åç§°ä¸ºå­—ç¬¦ä¸²: {isinstance(structured_result.product_name, str)}\")\n",
    "print(f\"ç‰¹ç‚¹ä¸ºåˆ—è¡¨: {isinstance(structured_result.features, list)}\")\n",
    "print(f\"å¸‚åœºæ½œåŠ›ä¸ºæ•°å€¼: {isinstance(structured_result.market_potential, float)}\")\n",
    "\n",
    "# éªŒè¯ä»£ç \n",
    "assert 'lcel_chain' in locals(), \"è¯·åˆ›å»º lcel_chain å˜é‡\"\n",
    "assert 'structured_result' in locals(), \"è¯·åˆ›å»º structured_result å˜é‡\"\n",
    "print(\"\\nâœ… ç»ƒä¹ 4éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å…³é”®è¦ç‚¹**ï¼š\n",
    "- LCEL è¯­æ³•ï¼š`prompt | llm | parser` åˆ›å»ºå¤„ç†é“¾\n",
    "- `StrOutputParser()` å°† AIMessage è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "- `with_structured_output()` é…åˆ Pydantic æ¨¡å‹å®ç°ç»“æ„åŒ–è¾“å‡º\n",
    "- Pydantic æ¨¡å‹å®šä¹‰è¾“å‡ºæ ¼å¼å’Œæ•°æ®ç±»å‹çº¦æŸ\n",
    "- é“¾å¼è°ƒç”¨æ”¯æŒå­—å…¸å‚æ•°ä¼ é€’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ç»ƒä¹ 5ç­”æ¡ˆï¼šé«˜çº§åŠŸèƒ½ç»¼åˆåº”ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ 5ï¼šé«˜çº§åŠŸèƒ½ç»¼åˆåº”ç”¨ - å®Œæ•´ç­”æ¡ˆ\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "# 1. å®ç°æµå¼è¾“å‡ºåŠŸèƒ½\n",
    "print(\"ğŸŒŠ æµå¼è¾“å‡ºåŠŸèƒ½æ¼”ç¤º:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def streaming_demo():\n",
    "    \"\"\"æµå¼è¾“å‡ºæ¼”ç¤ºå‡½æ•°\"\"\"\n",
    "    streaming_llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        streaming=True,\n",
    "    )\n",
    "    \n",
    "    prompt = \"è¯·ç”¨ä¸‰ä¸ªè¦ç‚¹ä»‹ç» LangChain çš„ä¸»è¦åŠŸèƒ½\"\n",
    "    full_response = \"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ”„ å¼€å§‹æµå¼è¾“å‡º...\")\n",
    "        for chunk in streaming_llm.stream(prompt):\n",
    "            if hasattr(chunk, 'content') and chunk.content:\n",
    "                print(chunk.content, end='', flush=True)\n",
    "                full_response += chunk.content\n",
    "        print(f\"\\nâœ… æµå¼è¾“å‡ºå®Œæˆï¼Œæ€»é•¿åº¦: {len(full_response)} å­—ç¬¦\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æµå¼è¾“å‡ºå¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "streaming_success = streaming_demo()\n",
    "\n",
    "# 2. æ„å»ºæ‰¹é‡å¤„ç†ç³»ç»Ÿ\n",
    "print(\"\\n\\nğŸ“¦ æ‰¹é‡å¤„ç†ç³»ç»Ÿæ¼”ç¤º:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def batch_system():\n",
    "    \"\"\"æ‰¹é‡å¤„ç†ç³»ç»Ÿå‡½æ•°\"\"\"\n",
    "    # å‡†å¤‡æ‰¹é‡è¾“å…¥\n",
    "    questions = [\n",
    "        \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    print(f\"ğŸ”„ æ‰¹é‡å¤„ç† {len(questions)} ä¸ªé—®é¢˜...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i, question in enumerate(questions):\n",
    "        try:\n",
    "            response = llm.invoke(question)\n",
    "            results.append(response.content)\n",
    "            print(f\"  {i+1}. {response.content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {i+1}. å¤„ç†å¤±è´¥: {e}\")\n",
    "            results.append(None)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’\")\n",
    "    return results\n",
    "\n",
    "batch_results = batch_system()\n",
    "\n",
    "# 3. æ·»åŠ é”™è¯¯å¤„ç†æœºåˆ¶\n",
    "print(\"\\n\\nğŸ›¡ï¸ é”™è¯¯å¤„ç†æœºåˆ¶æ¼”ç¤º:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def error_handler():\n",
    "    \"\"\"é”™è¯¯å¤„ç†æœºåˆ¶æ¼”ç¤º\"\"\"\n",
    "    error_cases = [\n",
    "        {\"name\": \"æ­£å¸¸é…ç½®\", \"config\": {\"model\": \"gpt-4o\"}},\n",
    "        {\"name\": \"æ— æ•ˆæ¨¡å‹\", \"config\": {\"model\": \"invalid-model\"}},\n",
    "        {\"name\": \"æ— æ•ˆAPI Key\", \"config\": {\"model\": \"gpt-4o\", \"api_key\": \"invalid-key\"}}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for case in error_cases:\n",
    "        print(f\"ğŸ” æµ‹è¯• {case['name']}...\")\n",
    "        try:\n",
    "            test_llm = ChatOpenAI(**case['config'])\n",
    "            response = test_llm.invoke(\"æµ‹è¯•æ¶ˆæ¯\")\n",
    "            results[case['name']] = {\"success\": True, \"result\": response.content}\n",
    "            print(f\"  âœ… æˆåŠŸ: {response.content[:50]}...\")\n",
    "        except Exception as e:\n",
    "            results[case['name']] = {\"success\": False, \"error\": str(e)}\n",
    "            print(f\"  âŒ å¤±è´¥: {type(e).__name__}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "error_results = error_handler()\n",
    "\n",
    "# 4. é›†æˆåµŒå…¥æ¨¡å‹åŠŸèƒ½\n",
    "print(\"\\n\\nğŸ”¤ åµŒå…¥æ¨¡å‹åŠŸèƒ½æ¼”ç¤º:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def embedding_demo():\n",
    "    \"\"\"åµŒå…¥æ¨¡å‹åŠŸèƒ½æ¼”ç¤º\"\"\"\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        \n",
    "        # æµ‹è¯•æ–‡æœ¬\n",
    "        texts = [\n",
    "            \"LangChain æ˜¯ä¸€ä¸ª LLM åº”ç”¨å¼€å‘æ¡†æ¶\",\n",
    "            \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯\",\n",
    "            \"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ \"\n",
    "        ]\n",
    "        \n",
    "        print(\"ğŸ”„ ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡...\")\n",
    "        for i, text in enumerate(texts):\n",
    "            vector = embeddings.embed_query(text)\n",
    "            print(f\"  {i+1}. {text[:20]}... â†’ {len(vector)} ç»´å‘é‡\")\n",
    "        \n",
    "        # æ‰¹é‡åµŒå…¥\n",
    "        batch_vectors = embeddings.embed_documents(texts)\n",
    "        print(f\"âœ… æ‰¹é‡åµŒå…¥å®Œæˆï¼Œç”Ÿæˆäº† {len(batch_vectors)} ä¸ªå‘é‡\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åµŒå…¥åŠŸèƒ½å¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "embedding_success = embedding_demo()\n",
    "\n",
    "# 5. åˆ›å»ºæ™ºèƒ½é—®ç­”åŠ©æ‰‹\n",
    "print(\"\\n\\nğŸ¤– æ™ºèƒ½é—®ç­”åŠ©æ‰‹æ¼”ç¤º:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "class SmartQAAssistant:\n",
    "    \"\"\"æ™ºèƒ½é—®ç­”åŠ©æ‰‹ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç§é—®é¢˜ã€‚è¯·ç”¨ç®€æ´ã€å‡†ç¡®çš„æ–¹å¼å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\"),\n",
    "            (\"user\", \"{question}\")\n",
    "        ])\n",
    "        self.chain = self.prompt | self.llm\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"å›ç­”é—®é¢˜\"\"\"\n",
    "        try:\n",
    "            response = self.chain.invoke({\"question\": question})\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"answer\": response.content,\n",
    "                \"question\": question\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"question\": question\n",
    "            }\n",
    "\n",
    "# åˆ›å»ºå¹¶æµ‹è¯•é—®ç­”åŠ©æ‰‹\n",
    "qa_assistant = SmartQAAssistant()\n",
    "\n",
    "test_questions = [\n",
    "    \"ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ\",\n",
    "    \"å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¸©åº¦å‚æ•°ï¼Ÿ\",\n",
    "    \"LCEL é“¾å¼è°ƒç”¨æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¤– æ™ºèƒ½é—®ç­”åŠ©æ‰‹æµ‹è¯•:\")\n",
    "for i, question in enumerate(test_questions):\n",
    "    result = qa_assistant.ask(question)\n",
    "    if result[\"success\"]:\n",
    "        print(f\"  Q{i+1}: {question}\")\n",
    "        print(f\"  A{i+1}: {result['answer']}\")\n",
    "    else:\n",
    "        print(f\"  Q{i+1}: {question}\")\n",
    "        print(f\"  A{i+1}: å›ç­”å¤±è´¥ - {result['error']}\")\n",
    "\n",
    "# ç»¼åˆéªŒè¯\n",
    "print(f\"\\nğŸ“Š åŠŸèƒ½éªŒè¯æ€»ç»“:\")\n",
    "print(f\"æµå¼è¾“å‡º: {'âœ…' if streaming_success else 'âŒ'}\")\n",
    "print(f\"æ‰¹é‡å¤„ç†: {'âœ…' if batch_results else 'âŒ'}\")\n",
    "print(f\"é”™è¯¯å¤„ç†: {'âœ…' if error_results else 'âŒ'}\")\n",
    "print(f\"åµŒå…¥åŠŸèƒ½: {'âœ…' if embedding_success else 'âŒ'}\")\n",
    "print(f\"é—®ç­”åŠ©æ‰‹: {'âœ…' if qa_assistant else 'âŒ'}\")\n",
    "\n",
    "# éªŒè¯ä»£ç \n",
    "assert 'streaming_demo' in locals(), \"è¯·åˆ›å»º streaming_demo å˜é‡\"\n",
    "assert 'batch_system' in locals(), \"è¯·åˆ›å»º batch_system å˜é‡\"\n",
    "assert 'error_handler' in locals(), \"è¯·åˆ›å»º error_handler å˜é‡\"\n",
    "assert 'qa_assistant' in locals(), \"è¯·åˆ›å»º qa_assistant å˜é‡\"\n",
    "print(\"\\nâœ… ç»ƒä¹ 5éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å…³é”®è¦ç‚¹**ï¼š\n",
    "- `streaming=True` å¯ç”¨æµå¼è¾“å‡ºï¼Œé€å—æ˜¾ç¤ºå“åº”\n",
    "- æ‰¹é‡å¤„ç†é€šè¿‡å¾ªç¯è°ƒç”¨å®ç°ï¼Œå¯è€ƒè™‘å¹¶å‘ä¼˜åŒ–\n",
    "- é”™è¯¯å¤„ç†ä½¿ç”¨ try-catch æ•è·å„ç§å¼‚å¸¸æƒ…å†µ\n",
    "- `OpenAIEmbeddings` ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼Œç”¨äºè¯­ä¹‰æœç´¢\n",
    "- é¢å‘å¯¹è±¡è®¾è®¡åˆ›å»ºå¯å¤ç”¨çš„æ™ºèƒ½åŠ©æ‰‹ç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š ç»ƒä¹ å®Œæˆæ£€æŸ¥\n",
    "\n",
    "### âœ… éªŒè¯æ¸…å•\n",
    "- [ ] ç»ƒä¹ 1ï¼šåŸºç¡€é…ç½®å’Œå¯¹è¯åŠŸèƒ½æ­£å¸¸\n",
    "- [ ] ç»ƒä¹ 2ï¼šå‚æ•°è°ƒä¼˜å’Œæ¨¡å‹å¯¹æ¯”å®Œæˆ\n",
    "- [ ] ç»ƒä¹ 3ï¼šæç¤ºè¯æ¨¡æ¿ç³»ç»Ÿåº”ç”¨æˆåŠŸ\n",
    "- [ ] ç»ƒä¹ 4ï¼šLCELé“¾å¼è°ƒç”¨å’Œç»“æ„åŒ–è¾“å‡ºå®ç°\n",
    "- [ ] ç»ƒä¹ 5ï¼šé«˜çº§åŠŸèƒ½ç»¼åˆåº”ç”¨å®Œæˆ\n",
    "\n",
    "### ğŸ¯ æŠ€èƒ½æŒæ¡è¯„ä¼°\n",
    "| æŠ€èƒ½ç‚¹ | æŒæ¡ç¨‹åº¦ |\n",
    "|--------|----------|\n",
    "| ChatOpenAI åˆå§‹åŒ– | â­â­â­â­â­ |\n",
    "| å‚æ•°è°ƒä¼˜ | â­â­â­â­â­ |\n",
    "| æç¤ºè¯æ¨¡æ¿ | â­â­â­â­â­ |\n",
    "| LCEL é“¾å¼è°ƒç”¨ | â­â­â­â­â­ |\n",
    "| ç»“æ„åŒ–è¾“å‡º | â­â­â­â­â­ |\n",
    "| æµå¼è¾“å‡º | â­â­â­â­â­ |\n",
    "| æ‰¹é‡å¤„ç† | â­â­â­â­â­ |\n",
    "| é”™è¯¯å¤„ç† | â­â­â­â­â­ |\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®\n",
    "1. æ·±å…¥å­¦ä¹  Agents å’Œ Tools ç³»ç»Ÿ\n",
    "2. æŒæ¡ Memory å’Œå¯¹è¯ç®¡ç†\n",
    "3. æ¢ç´¢ RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ\n",
    "4. å­¦ä¹ å‘é‡æ•°æ®åº“é›†æˆ\n",
    "5. å®è·µå¤æ‚çš„å¤šæ¨¡æ€åº”ç”¨\n",
    "\n",
    "### ğŸ’¡ å¸¸è§é—®é¢˜è§£ç­”\n",
    "\n",
    "**Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¸©åº¦å‚æ•°ï¼Ÿ**\n",
    "A: 0.1-0.3 é€‚åˆäº‹å®æ€§ä»»åŠ¡ï¼Œ0.7-0.9 é€‚åˆåˆ›æ„ä»»åŠ¡ï¼Œ1.0+ é€‚åˆé«˜åˆ›é€ æ€§åœºæ™¯\n",
    "\n",
    "**Q: LCEL é“¾å¼è°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**\n",
    "A: æ£€æŸ¥æ•°æ®ç±»å‹åŒ¹é…ï¼Œç¡®ä¿ç»„ä»¶é—´è¾“å…¥è¾“å‡ºæ ¼å¼å…¼å®¹\n",
    "\n",
    "**Q: ç»“æ„åŒ–è¾“å‡ºä¸å‡†ç¡®å¦‚ä½•è§£å†³ï¼Ÿ**\n",
    "A: åœ¨æç¤ºè¯ä¸­æ˜ç¡®æŒ‡å®š JSON æ ¼å¼ï¼Œä½¿ç”¨è¯¦ç»†çš„å­—æ®µæè¿°\n",
    "\n",
    "**Q: æµå¼è¾“å‡ºå¡ä½æ€ä¹ˆå¤„ç†ï¼Ÿ**\n",
    "A: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œé™ä½ max_tokensï¼Œä½¿ç”¨ try-catch åŒ…è£…\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ç­”æ¡ˆæ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒæŠ€èƒ½å›é¡¾\n",
    "1. **ChatOpenAI åˆå§‹åŒ–**ï¼šæŒæ¡åŸºç¡€é…ç½®å’Œå‚æ•°è®¾ç½®\n",
    "2. **å‚æ•°è°ƒä¼˜**ï¼šç†è§£æ¸©åº¦å’Œæ¨¡å‹é€‰æ‹©çš„å½±å“\n",
    "3. **æç¤ºè¯æ¨¡æ¿**ï¼šç†Ÿç»ƒä½¿ç”¨ PromptTemplate å’Œ ChatPromptTemplate\n",
    "4. **LCEL é“¾å¼è°ƒç”¨**ï¼šæ„å»ºé«˜æ•ˆçš„å¤„ç†ç®¡é“\n",
    "5. **é«˜çº§åŠŸèƒ½**ï¼šé›†æˆæµå¼è¾“å‡ºã€æ‰¹é‡å¤„ç†ã€é”™è¯¯å¤„ç†ç­‰\n",
    "\n",
    "### å­¦ä¹ å»ºè®®\n",
    "- å¤šç»ƒä¹ ä¸åŒçš„å‚æ•°ç»„åˆï¼Œæ‰¾åˆ°æœ€é€‚åˆçš„é…ç½®\n",
    "- æ·±å…¥ç†è§£ LCEL è¯­æ³•ï¼Œå®ƒæ˜¯ LangChain çš„æ ¸å¿ƒç‰¹æ€§\n",
    "- æ³¨é‡é”™è¯¯å¤„ç†ï¼Œæ„å»ºç¨³å®šå¯é çš„åº”ç”¨\n",
    "- ç»§ç»­å­¦ä¹  Agentsã€Memoryã€RAG ç­‰é«˜çº§ä¸»é¢˜\n",
    "\n",
    "**æ­å–œå®Œæˆæ‰€æœ‰ç»ƒä¹ ï¼ğŸ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
