# ğŸ¤– æ¨¡å‹ (Models)




#### ğŸ“‹ **æ ¸å¿ƒèƒ½åŠ›**

- **æ–‡æœ¬ç”Ÿæˆ** 
- **å·¥å…·è°ƒç”¨** - å¤–éƒ¨å·¥å…·é›†æˆï¼ˆæ•°æ®åº“ã€APIï¼‰
- **ç»“æ„åŒ–è¾“å‡º** - å“åº”æ ¼å¼çº¦æŸ
- **å¤šæ¨¡æ€** - å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å¤„ç†
- **æ¨ç†** - å¤šæ­¥é€»è¾‘æ¨ç†

#### ğŸ§  **ä»£ç†å¼•æ“**

- é©±åŠ¨å†³ç­–è¿‡ç¨‹
- å†³å®šå·¥å…·é€‰æ‹©ã€ç»“æœè§£é‡Šã€ç­”æ¡ˆè¾“å‡ºæ—¶æœº

#### ğŸ“Š **æ€§èƒ½å…³é”®**

- æ¨¡å‹è´¨é‡ = ä»£ç†å¯é æ€§
- ä¸åŒä¸“é•¿ï¼šå¤æ‚æŒ‡ä»¤ã€ç»“æ„åŒ–æ¨ç†ã€å¤§ä¸Šä¸‹æ–‡

#### ğŸ”§ **æŠ€æœ¯ä¼˜åŠ¿**

- ç»Ÿä¸€æ¥å£
- å¤šæä¾›å•†æ”¯æŒ
- çµæ´»åˆ‡æ¢è¯•éªŒ



## åˆå§‹åŒ–æ¨¡å‹

**æ”¯æŒçš„æä¾›å•†ï¼š**

- OpenAI
- Anthropic
- Azure
- Google Gemini
- AWS Bedrock

OpenAIä¸¾ä¾‹ï¼Œéœ€è¦å®‰è£…å¯¹åº”çš„å¤§æ¨¡å‹æä¾›å•†çš„åŒ…

```bash
pip install -U "langchain[openai]"
```

```python
import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")
response = model.invoke("Why do parrots talk?")
```

## å…³é”®æ–¹æ³•

- **Invoke (è°ƒç”¨)** - æ¨¡å‹æ¥æ”¶æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼Œåœ¨ç”Ÿæˆå®Œæ•´å“åº”åè¾“å‡ºæ¶ˆæ¯ã€‚
- **Stream (æµå¼ä¼ è¾“)** - è°ƒç”¨æ¨¡å‹ï¼Œä½†åœ¨ç”Ÿæˆæ—¶å®æ—¶æµå¼ä¼ è¾“è¾“å‡ºã€‚
- **Batch (æ‰¹å¤„ç†)** - ä»¥æ‰¹å¤„ç†æ–¹å¼å‘æ¨¡å‹å‘é€å¤šä¸ªè¯·æ±‚ä»¥å®ç°æ›´é«˜æ•ˆçš„å¤„ç†ã€‚

## å‚æ•°

- **æ ‡å‡†å‚æ•°**

- **æ¨¡å‹ä¾›åº”å•†ç‰¹æœ‰å‚æ•°**

  ChatOpenAI` æœ‰ `use_responses_api` æ¥æŒ‡å®šæ˜¯ä½¿ç”¨ OpenAI Responses è¿˜æ˜¯ Completions APIã€‚

#### æ ‡å‡†å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| **model** | `string` (å¿…éœ€) | æ¨¡å‹åç§°æˆ–æ ‡è¯†ç¬¦ï¼Œæ”¯æŒ `provider:model` æ ¼å¼ï¼ˆå¦‚ `openai:o1`ï¼‰ |
| **api_key** | `string` | æä¾›å•†èº«ä»½éªŒè¯å¯†é’¥ï¼Œé€šå¸¸é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½® |
| **temperature** | `number` | æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼šé«˜å€¼æ›´åˆ›é€ æ€§ï¼Œä½å€¼æ›´ç¡®å®šæ€§ |
| **max_tokens** | `number` | é™åˆ¶å“åº”é•¿åº¦ï¼ˆtoken æ•°é‡ï¼‰ |
| **timeout** | `number` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |
| **max_retries** | `number` | å¤±è´¥é‡è¯•æ¬¡æ•° |

```python
# ä½¿ç”¨æ¨¡å‹å‚æ•°åˆå§‹åŒ–
model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    # ä¼ é€’ç»™æ¨¡å‹çš„ kwargsï¼š
    temperature=0.7,
    timeout=30,
    max_tokens=1000,
)
```

## **Invocation **è°ƒç”¨

### Invoke (è°ƒç”¨)

è°ƒç”¨æ¨¡å‹æœ€ç›´æ¥çš„æ–¹æ³•æ˜¯ä½¿ç”¨ `invoke()` ä¼ å…¥å•ä¸ªæ¶ˆæ¯æˆ–æ¶ˆæ¯åˆ—è¡¨ã€‚

**å•ä¸ªæ¶ˆæ¯ï¼š**

```python
response = model.invoke("Why do parrots have colorful feathers?")
print(response)
```

**å­—å…¸æ ¼å¼ï¼š**
```python
conversation = [
    {"role": "system", "content": "æ‚¨æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œè´Ÿè´£å°†è‹±è¯­ç¿»è¯‘æˆæ³•è¯­ã€‚"},
    {"role": "user", "content": "ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"},
    {"role": "assistant", "content": "J'adore la programmation."},
    {"role": "user", "content": "ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åºã€‚"}
]

response = model.invoke(conversation)
print(response)  # AIMessage("J'adore crÃ©er des applications.")
```

**æ¶ˆæ¯å¯¹è±¡ï¼ˆChatæ¨¡å‹æ‰èƒ½ä½¿ç”¨ï¼‰**

```python
from langchain.messages import HumanMessage, AIMessage, SystemMessage

conversation = [
    SystemMessage("æ‚¨æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œè´Ÿè´£å°†è‹±è¯­ç¿»è¯‘æˆæ³•è¯­ã€‚"),
    HumanMessage("ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"),
    AIMessage("J'adore la programmation."),
    HumanMessage("ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åºã€‚")
]

response = model.invoke(conversation)
print(response)  # AIMessage("J'adore crÃ©er des applications.")
```

### Stream (æµå¼ä¼ è¾“)

å¤§å¤šæ•°æ¨¡å‹å¯ä»¥åœ¨ç”Ÿæˆè¾“å‡ºæ—¶æµå¼ä¼ è¾“å…¶è¾“å‡ºå†…å®¹ã€‚é€šè¿‡æ¸è¿›å¼æ˜¾ç¤ºè¾“å‡ºï¼Œæµå¼ä¼ è¾“æ˜¾è‘—æ”¹å–„äº†ç”¨æˆ·ä½“éªŒï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¾ƒé•¿çš„å“åº”ã€‚

è°ƒç”¨ `stream()` è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œåœ¨è¾“å‡ºå—äº§ç”Ÿæ—¶ç”Ÿæˆå®ƒä»¬ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å¾ªç¯å®æ—¶å¤„ç†æ¯ä¸ªå—ï¼š

**åŸºæœ¬æ–‡æœ¬æµå¼ä¼ è¾“ï¼š**
```python
for chunk in model.stream("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰å½©è‰²çš„ç¾½æ¯›ï¼Ÿ"):
    print(chunk.text, end="|", flush=True)
```

```py
# æµå¼ä¼ è¾“å·¥å…·è°ƒç”¨ã€æ¨ç†å’Œå…¶ä»–å†…å®¹

for chunk in model.stream("å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"):
    for block in chunk.content_blocks:
        if block["type"] == "reasoning" and (reasoning := block.get("reasoning")):
            print(f"æ¨ç†: {reasoning}")
        elif block["type"] == "tool_call_chunk":
            print(f"å·¥å…·è°ƒç”¨å—: {block}")
        elif block["type"] == "text":
            print(block["text"])
        else:
            ...
```

**æ„å»º AIMessageï¼š**

```python
full = None  # None | AIMessageChunk
for chunk in model.stream("å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"):
    full = chunk if full is None else full + chunk
    print(full.text)

# å¤©ç©º
# å¤©ç©ºæ˜¯
# å¤©ç©ºé€šå¸¸æ˜¯
# å¤©ç©ºé€šå¸¸æ˜¯è“è‰²
# å¤©ç©ºé€šå¸¸æ˜¯è“è‰²çš„
# ...

print(full.content_blocks)
# [{"type": "text", "text": "å¤©ç©ºé€šå¸¸æ˜¯è“è‰²çš„..."}]
```

#### Streaming events æµå¼äº‹ä»¶

```py
async for event in model.astream_events("Hello"):

    if event["event"] == "on_chat_model_start":
        print(f"Input: {event['data']['input']}")

    elif event["event"] == "on_chat_model_stream":
        print(f"Token: {event['data']['chunk'].text}")

    elif event["event"] == "on_chat_model_end":
        print(f"Full message: {event['data']['output'].text}")

    else:
        pass


# Input: Hello
# Token: Hi
# Token:  there
# Token: !
# Token:  How
# Token:  can
# Token:  I
# ...
# Full message: Hi there! How can I help today?
```

### Batch (æ‰¹å¤„ç†)

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**

- æ‰¹å¤„ç†ç‹¬ç«‹è¯·æ±‚ â†’ **æå‡æ€§èƒ½ï¼Œé™ä½æˆæœ¬**
- æ”¯æŒå¹¶è¡Œå¤„ç†

**æ–¹æ³•å¯¹æ¯”ï¼š**

- `batch()` - å®¢æˆ·ç«¯å¹¶è¡ŒåŒ–è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆç»“æœ
- `batch_as_completed()` - æµå¼ä¼ è¾“ï¼Œå®æ—¶æ¥æ”¶å„è¾“å…¥è¾“å‡º

**é‡è¦åŒºåˆ«ï¼š**

- è¿™æ˜¯å®¢æˆ·ç«¯å¹¶è¡ŒåŒ–ï¼Œéæä¾›å•†æ‰¹å¤„ç† API
- ä¸ OpenAI/Anthropic çš„æ‰¹å¤„ç† API ä¸åŒ

**ç»“æœé¡ºåºï¼š**

- `batch_as_completed()` - **æ— åºåˆ°è¾¾**
- æ¯ä¸ªç»“æœåŒ…å«è¾“å…¥ç´¢å¼•ï¼Œæ”¯æŒé‡æ„åŸå§‹é¡ºåº

**å¹¶å‘æ§åˆ¶ï¼š**

- å¤§é‡è¾“å…¥æ—¶æ§åˆ¶å¹¶è¡Œè°ƒç”¨æ•°é‡
- é€šè¿‡ `RunnableConfig` è®¾ç½® `max_concurrency` å±æ€§
- é€‚ç”¨äº `batch()` å’Œ `batch_as_completed()`



```python
responses = model.batch([
    "ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰å½©è‰²çš„ç¾½æ¯›ï¼Ÿ",
    "é£æœºæ˜¯å¦‚ä½•é£è¡Œçš„ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"
])
for response in responses:
    print(response)


# ç‹¬ç«‹è¾“å‡ºç»“æœ
for response in model.batch_as_completed([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
]):
    print(response)
```

**å¸¦æœ€å¤§å¹¶å‘æ•°çš„æ‰¹å¤„ç†ï¼š**
```python
model.batch(
    list_of_inputs,
    config={
        'max_concurrency': 5,  # é™åˆ¶ä¸º 5 ä¸ªå¹¶è¡Œè°ƒç”¨
    }
)
```


## å·¥å…·è°ƒç”¨

æ¨¡å‹å¯ä»¥è¯·æ±‚è°ƒç”¨æ‰§è¡Œä»»åŠ¡çš„å·¥å…·ï¼Œä¾‹å¦‚ä»æ•°æ®åº“è·å–æ•°æ®ã€æœç´¢ç½‘é¡µæˆ–è¿è¡Œä»£ç ã€‚å·¥å…·æ˜¯ä»¥ä¸‹å†…å®¹çš„é…å¯¹ï¼š
- ä¸€ä¸ªæ¨¡å¼ï¼ŒåŒ…æ‹¬å·¥å…·çš„åç§°ã€æè¿°å’Œ/æˆ–å‚æ•°å®šä¹‰ï¼ˆé€šå¸¸æ˜¯ JSON æ¨¡å¼ï¼‰
- è¦æ‰§è¡Œçš„å‡½æ•°æˆ–åç¨‹ã€‚

æ‚¨å¯èƒ½å¬è¯´è¿‡æœ¯è¯­â€œå‡½æ•°è°ƒç”¨â€ã€‚æˆ‘ä»¬å°†å…¶ä¸â€œå·¥å…·è°ƒç”¨â€äº’æ¢ä½¿ç”¨ã€‚



### å·¥å…·è°ƒç”¨æµç¨‹

![image-20251206152913546](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251206152913546.png)

### **ç»‘å®šç”¨æˆ·å·¥å…·**

```python
from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
    """è·å–ä½ç½®çš„å¤©æ°”ã€‚"""
    return f"{location} å¤©æ°”æ™´æœ—ã€‚"

model_with_tools = model.bind_tools([get_weather])  

response = model_with_tools.invoke("æ³¢å£«é¡¿çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
for tool_call in response.tool_calls:
    # æŸ¥çœ‹æ¨¡å‹è¿›è¡Œçš„å·¥å…·è°ƒç”¨
    print(f"å·¥å…·: {tool_call['name']}")
    print(f"å‚æ•°: {tool_call['args']}")
```



### Tool execution loop å·¥å…·æ‰§è¡Œå¾ªç¯

```py
from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
    """è·å–ä½ç½®çš„å¤©æ°”ä¿¡æ¯ã€‚"""
    return f"{location} å¤©æ°”æ™´æœ—ã€‚"

# ç»‘å®šï¼ˆå¯èƒ½æ˜¯å¤šä¸ªï¼‰å·¥å…·åˆ°æ¨¡å‹
model_with_tools = model.bind_tools([get_weather])

# æ­¥éª¤ 1ï¼šæ¨¡å‹ç”Ÿæˆå·¥å…·è°ƒç”¨
messages = [{"role": "user", "content": "æ³¢å£«é¡¿çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)

# æ­¥éª¤ 2ï¼šæ‰§è¡Œå·¥å…·å¹¶æ”¶é›†ç»“æœ
for tool_call in ai_msg.tool_calls:
    # ä½¿ç”¨ç”Ÿæˆçš„å‚æ•°æ‰§è¡Œå·¥å…·
    tool_result = get_weather.invoke(tool_call)
    messages.append(tool_result)

# æ­¥éª¤ 3ï¼šå°†ç»“æœä¼ å›æ¨¡å‹è·å–æœ€ç»ˆå“åº”
final_response = model_with_tools.invoke(messages)
print(final_response.text)
# "æ³¢å£«é¡¿å½“å‰å¤©æ°” 72Â°Fï¼Œæ™´æœ—ã€‚"
```



### Forcing tool calls å¼ºåˆ¶å·¥å…·è°ƒç”¨

```py
# å¼ºåˆ¶ä½¿ç”¨æ‰€æœ‰å·¥å…·
model_with_tools = model.bind_tools([tool_1], tool_choice="any")

# å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šå·¥å…·
model_with_tools = model.bind_tools([tool_1], tool_choice="tool_1")
```

### å¹¶è¡Œå·¥å…·è°ƒç”¨

- **é»˜è®¤å¯ç”¨**ï¼šå¤šæ•°æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹é»˜è®¤å¯ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨

- **é€‰æ‹©æ€§ç¦ç”¨**ï¼šéƒ¨åˆ†æ¨¡å‹å…è®¸ç¦ç”¨å¹¶è¡ŒåŠŸèƒ½ï¼ˆåŒ…æ‹¬OpenAIå’ŒAnthropicï¼‰

- **è®¾ç½®æ–¹æ³•**ï¼šé€šè¿‡è®¾ç½® `parallel_tool_calls=False` ç¦ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨

```py
model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke(
    "æ³¢å£«é¡¿å’Œä¸œäº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
)

# æ¨¡å‹å¯èƒ½ç”Ÿæˆå¤šä¸ªå·¥å…·è°ƒç”¨
print(response.tool_calls)
# [
#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},
#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},
# ]

# æ‰§è¡Œæ‰€æœ‰å·¥å…·ï¼ˆå¯ä»¥ä½¿ç”¨å¼‚æ­¥å¹¶è¡Œæ‰§è¡Œï¼‰
results = []
for tool_call in response.tool_calls:
    if tool_call['name'] == 'get_weather':
        result = get_weather.invoke(tool_call)
    ...
    results.append(result)
```



### Streaming tool calls æµå¼å·¥å…·è°ƒç”¨

```py
for chunk in model_with_tools.stream(
    "æ³¢å£«é¡¿å’Œä¸œäº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
):
    # å·¥å…·è°ƒç”¨å—é€æ­¥åˆ°è¾¾
    for tool_chunk in chunk.tool_call_chunks:
        if name := tool_chunk.get("name"):
            print(f"å·¥å…·: {name}")
        if id_ := tool_chunk.get("id"):
            print(f"ID: {id_}")
        if args := tool_chunk.get("args"):
            print(f"å‚æ•°: {args}")

# è¾“å‡º:
# å·¥å…·: get_weather
# ID: call_SvMlU1TVIZugrFLckFE2ceRE
# å‚æ•°: {"lo
# å‚æ•°: catio
# å‚æ•°: n": "B
# å‚æ•°: osto
# å‚æ•°: n"}
# å·¥å…·: get_weather
# ID: call_QMZdy6qInx13oWKE7KhuhOLR
# å‚æ•°: {"lo
# å‚æ•°: catio
# å‚æ•°: n": "T
# å‚æ•°: okyo
# å‚æ•°: "}
```



## Structured output ç»“æ„åŒ–è¾“å‡º

### Pydantic 

```python
from pydantic import BaseModel, Field

class Movie(BaseModel):
    """å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„ç”µå½±ã€‚"""
    title: str = Field(..., description="ç”µå½±çš„æ ‡é¢˜")
    year: int = Field(..., description="ç”µå½±å‘å¸ƒçš„å¹´ä»½")
    director: str = Field(..., description="ç”µå½±çš„å¯¼æ¼”")
    rating: float = Field(..., description="ç”µå½±è¯„åˆ†ï¼ˆæ»¡åˆ†10åˆ†ï¼‰")

model_with_structure = model.with_structured_output(Movie)
response = model_with_structure.invoke("æä¾›ç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # Movie(title="ç›—æ¢¦ç©ºé—´", year=2010, director="Christopher Nolan", rating=8.8)
```

#### æ–¹æ³•å‚æ•°

| æ–¹æ³•                 | ç‰¹ç‚¹                              |
| :------------------- | :-------------------------------- |
| **json_schema**      | ä¸“ç”¨ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½                |
| **function_calling** | é€šè¿‡å·¥å…·è°ƒç”¨å®ç°ç»“æ„åŒ–è¾“å‡º        |
| **json_mode**        | æ—©æœŸç‰ˆæœ¬ï¼ŒJSON æ¨¡å¼éœ€åœ¨æç¤ºä¸­æè¿° |

### TypedDict

- Python å†…ç½®ç±»å‹

- é€‚åˆæ— éœ€è¿è¡Œæ—¶éªŒè¯çš„åœºæ™¯

```py
from typing_extensions import TypedDict, Annotated

class MovieDict(TypedDict):
    """åŒ…å«è¯¦ç»†ä¿¡æ¯çš„ç”µå½±ã€‚"""
    title: Annotated[str, ..., "ç”µå½±æ ‡é¢˜"]
    year: Annotated[int, ..., "ç”µå½±ä¸Šæ˜ å¹´ä»½"]
    director: Annotated[str, ..., "ç”µå½±å¯¼æ¼”"]
    rating: Annotated[float, ..., "ç”µå½±è¯„åˆ†ï¼ˆæ»¡åˆ†10åˆ†ï¼‰"]

model_with_structure = model.with_structured_output(MovieDict)
response = model_with_structure.invoke("æä¾›ç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}
```

### JSON Schema

ä¸ºäº†è·å¾—æœ€å¤§çš„æ§åˆ¶æˆ–äº’æ“ä½œæ€§ï¼Œæ‚¨å¯ä»¥æä¾›åŸå§‹JSONæ¨¡å¼ã€‚

```py
import json

json_schema = {
    "title": "Movie",
    "description": "åŒ…å«è¯¦ç»†ä¿¡æ¯çš„ç”µå½±",
    "type": "object",
    "properties": {
        "title": {
            "type": "string",
            "description": "ç”µå½±æ ‡é¢˜"
        },
        "year": {
            "type": "integer",
            "description": "ç”µå½±ä¸Šæ˜ å¹´ä»½"
        },
        "director": {
            "type": "string",
            "description": "ç”µå½±å¯¼æ¼”"
        },
        "rating": {
            "type": "number",
            "description": "ç”µå½±è¯„åˆ†ï¼ˆæ»¡åˆ†10åˆ†ï¼‰"
        }
    },
    "required": ["title", "year", "director", "rating"]
}

model_with_structure = model.with_structured_output(
    json_schema,
    method="json_schema",
)
response = model_with_structure.invoke("æä¾›ç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # {'title': 'Inception', 'year': 2010, ...}
```
