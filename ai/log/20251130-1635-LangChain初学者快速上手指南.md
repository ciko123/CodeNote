# LangChain åˆå­¦è€…å¿«é€Ÿä¸Šæ‰‹æŒ‡å—

> **ç›®æ ‡**ï¼š4 å‘¨æŒæ¡ LangChain æ ¸å¿ƒæŠ€èƒ½ï¼Œèƒ½ç‹¬ç«‹å¼€å‘ AI åº”ç”¨
> 
> **é€‚åˆäººç¾¤**ï¼šPython åŸºç¡€æ‰å®ï¼Œæƒ³è¦å¿«é€Ÿå…¥é—¨ LangChain çš„åˆå­¦è€…

---

## ğŸ“‹ å­¦ä¹ è·¯çº¿å›¾

### ğŸ—“ï¸ 4 å‘¨å­¦ä¹ è®¡åˆ’

| å‘¨æ¬¡ | æ ¸å¿ƒç›®æ ‡ | ä¸»è¦å†…å®¹ | é¢„æœŸæˆæœ |
|------|----------|----------|----------|
| **Week 1** | LLM åŸºç¡€è°ƒç”¨ | ç¯å¢ƒé…ç½®ã€ChatTongyiã€Prompt æ¨¡æ¿ | ä¸ªäºº AI åŠ©æ‰‹ |
| **Week 2** | æ–‡æ¡£å¤„ç† | æ–‡æ¡£åŠ è½½ã€å‘é‡æ£€ç´¢ã€ç®€å• RAG | æ–‡æ¡£é—®ç­”ç³»ç»Ÿ |
| **Week 3** | é“¾å¼ç¼–æ’ | LCEL è¯­æ³•ã€Output Parserã€Memory | æ™ºèƒ½é—®ç­”åŠ©æ‰‹ |
| **Week 4** | å®ç”¨éƒ¨ç½² | Agent å·¥å…·ã€FastAPIã€é¡¹ç›®ä¼˜åŒ– | å¯ç”¨çš„ API æœåŠ¡ |

---

## ğŸš€ æ¨¡å— 0ï¼šç¯å¢ƒé…ç½®ä¸å¿«é€ŸéªŒè¯ï¼ˆç¬¬ 0 å¤©ï¼‰

### ğŸ¯ å­¦ä¹ ç›®æ ‡
5 åˆ†é’Ÿå†…æˆåŠŸè°ƒç”¨ç¬¬ä¸€ä¸ª LLMï¼Œé¿å…ç¯å¢ƒé…ç½®è¸©å‘

### ğŸ“š çŸ¥è¯†ç‚¹è¯´æ˜
- **ä¾èµ–å®‰è£…**ï¼šæœ€å°åŒ–ã€å¿…éœ€çš„ Python åŒ…
- **API Key é…ç½®**ï¼šé€šä¹‰åƒé—®ã€OpenAI å…¼å®¹é…ç½®
- **å¿«é€ŸéªŒè¯**ï¼šä¸€é”®æµ‹è¯•è„šæœ¬

### ğŸ› ï¸ ç¯å¢ƒé…ç½®

#### 1. å®‰è£…ä¾èµ–
```bash
pip install langchain langchain-community langchain-core
pip install python-dotenv fastapi uvicorn
pip install faiss-cpu tiktoken dashscope
```

#### 2. ç¯å¢ƒå˜é‡é…ç½®
```python
# .env æ–‡ä»¶
DASHSCOPE_API_KEY=your-api-key-here
OPENAI_API_KEY=your-api-key-here
OPENAI_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
```

#### 3. å¿«é€ŸéªŒè¯è„šæœ¬
```python
# test_env.py
import os
from dotenv import load_dotenv
from langchain_community.chat_models import ChatTongyi

load_dotenv(override=True)

print("ğŸ” ç¯å¢ƒæ£€æŸ¥:")
print(f"DASHSCOPE_API_KEY: {'âœ…' if os.getenv('DASHSCOPE_API_KEY') else 'âŒ'}")

try:
    llm = ChatTongyi(model="qwen-turbo")
    response = llm.invoke("ä½ å¥½")
    print("âœ… LLM è°ƒç”¨æˆåŠŸ!")
    print(f"ğŸ¤– å›å¤: {response.content}")
except Exception as e:
    print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
```

### âœ… éªŒè¯ç‚¹
- [ ] ä¾èµ–å®‰è£…æˆåŠŸ
- [ ] ç¯å¢ƒå˜é‡é…ç½®æ­£ç¡®
- [ ] LLM è°ƒç”¨æˆåŠŸ
- [ ] èƒ½çœ‹åˆ° AI å›å¤

---

## ğŸ’¬ æ¨¡å— 1ï¼šLLM åŸºç¡€è°ƒç”¨ï¼ˆWeek 1ï¼‰

### ğŸ¯ å­¦ä¹ ç›®æ ‡
æŒæ¡ LLM åŸºç¡€è°ƒç”¨å’Œå‚æ•°æ§åˆ¶ï¼Œå®Œæˆä¸ªäºº AI åŠ©æ‰‹

### ğŸ“š çŸ¥è¯†ç‚¹è¯´æ˜

#### 1.1 ChatTongyi åŸºç¡€è°ƒç”¨
ChatTongyi æ˜¯ä¸é€šä¹‰åƒé—®å¯¹è¯çš„æ ¸å¿ƒæ¥å£ï¼Œæ”¯æŒå‚æ•°åŒ–æ§åˆ¶è¾“å‡ºé£æ ¼ã€‚

#### 1.2 æ ¸å¿ƒå‚æ•°æ§åˆ¶
- **model**ï¼šé€‰æ‹©æ¨¡å‹ç‰ˆæœ¬ï¼ˆqwen-turbo/qwen-plus/qwen-maxï¼‰
- **temperature**ï¼šæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œæ¨è 0.7ï¼‰
- **max_tokens**ï¼šé™åˆ¶è¾“å‡ºé•¿åº¦ï¼ˆæ¨è 500-1000ï¼‰

#### 1.3 PromptTemplate æ¨¡æ¿
åˆ›å»ºå¯å¤ç”¨çš„æç¤ºè¯æ¨¡æ¿ï¼Œå®ç°å‚æ•°åŒ–å¡«å……ã€‚

### ğŸ› ï¸ æ ¸å¿ƒä»£ç æ¨¡æ¿

#### åŸºç¡€è°ƒç”¨
```python
from langchain_community.chat_models import ChatTongyi
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv

load_dotenv()

# åˆ›å»º LLM å®ä¾‹
llm = ChatTongyi(
    model="qwen-turbo",
    temperature=0.7,
    max_tokens=500
)

# å‘é€æ¶ˆæ¯
message = HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
response = llm.invoke([message])

print(f"ğŸ¤– å›å¤: {response.content}")
```

#### Prompt æ¨¡æ¿
```python
from langchain_core.prompts import PromptTemplate

# åˆ›å»ºæ¨¡æ¿
prompt = PromptTemplate.from_template(
    "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œè¯·ç”¨{style}çš„è¯­è°ƒå›ç­”ï¼š{question}"
)

# ä½¿ç”¨æ¨¡æ¿
formatted_prompt = prompt.format(
    role="Python ä¸“å®¶",
    style="ä¸“ä¸šä¸”å‹å¥½", 
    question="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
)

# è°ƒç”¨ LLM
response = llm.invoke(formatted_prompt)
print(response.content)
```

### ğŸ¯ Week 1 é¡¹ç›®ï¼šä¸ªäºº AI åŠ©æ‰‹

#### é¡¹ç›®éœ€æ±‚
åˆ›å»ºä¸€ä¸ªèƒ½å›ç­”ç¼–ç¨‹é—®é¢˜çš„ä¸ªäºº AI åŠ©æ‰‹

#### å®Œæ•´ä»£ç 
```python
class PersonalAssistant:
    def __init__(self):
        self.llm = ChatTongyi(model="qwen-turbo", temperature=0.7)
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿è§£ç­”å„ç§ç¼–ç¨‹é—®é¢˜ã€‚"
    
    def ask(self, question):
        prompt = f"{self.system_prompt}\n\nç”¨æˆ·é—®é¢˜ï¼š{question}"
        response = self.llm.invoke(prompt)
        return response.content
    
    def chat(self):
        print("ğŸ¤– ä¸ªäººç¼–ç¨‹åŠ©æ‰‹ (è¾“å…¥ 'quit' é€€å‡º)")
        while True:
            question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ")
            if question.lower() == 'quit':
                break
            
            answer = self.ask(question)
            print(f"ğŸ¤– å›ç­”: {answer}")

# ä½¿ç”¨
assistant = PersonalAssistant()
assistant.chat()
```

### âœ… Week 1 éªŒè¯ç‚¹
- [ ] èƒ½æˆåŠŸè°ƒç”¨ ChatTongyi
- [ ] èƒ½ä½¿ç”¨ PromptTemplate
- [ ] èƒ½æ§åˆ¶ LLM å‚æ•°
- [ ] å®Œæˆä¸ªäºº AI åŠ©æ‰‹é¡¹ç›®

---

## ğŸ“„ æ¨¡å— 2ï¼šæ–‡æ¡£å¤„ç†åŸºç¡€ï¼ˆWeek 2ï¼‰

### ğŸ¯ å­¦ä¹ ç›®æ ‡
æŒæ¡æ–‡æ¡£åŠ è½½ã€å‘é‡æ£€ç´¢å’Œç®€å• RAGï¼Œå®Œæˆæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

### ğŸ“š çŸ¥è¯†ç‚¹è¯´æ˜

#### 2.1 Document Loaders
ä»å¤šç§æ•°æ®æºåŠ è½½æ–‡æ¡£ï¼Œæ˜¯ RAG ç³»ç»Ÿçš„èµ·ç‚¹ã€‚

#### 2.2 æ–‡æœ¬åˆ‡åˆ†
å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆåˆé€‚å¤§å°çš„ç‰‡æ®µï¼Œæé«˜æ£€ç´¢è´¨é‡ã€‚

#### 2.3 å‘é‡æ£€ç´¢
ä½¿ç”¨ Embedding å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œå®ç°ç›¸ä¼¼åº¦æ£€ç´¢ã€‚

### ğŸ› ï¸ æ ¸å¿ƒä»£ç æ¨¡æ¿

#### æ–‡æ¡£åŠ è½½
```python
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import WebBaseLoader

# åŠ è½½æ–‡æœ¬æ–‡ä»¶
loader = TextLoader("document.txt")
docs = loader.load()

# åŠ è½½ PDF æ–‡ä»¶
loader = PyPDFLoader("document.pdf")
docs = loader.load()

# åŠ è½½ç½‘é¡µ
loader = WebBaseLoader("https://example.com")
docs = loader.load()

print(f"ğŸ“„ åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
```

#### æ–‡æœ¬åˆ‡åˆ†
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åˆ›å»ºåˆ‡åˆ†å™¨
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len
)

# åˆ‡åˆ†æ–‡æ¡£
chunks = splitter.split_documents(docs)

print(f"ğŸ“Š åˆ‡åˆ†æˆ {len(chunks)} ä¸ªç‰‡æ®µ")
```

#### å‘é‡æ£€ç´¢
```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv

load_dotenv()

# åˆ›å»º Embedding
embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002",
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# åˆ›å»ºå‘é‡åº“
vectorstore = FAISS.from_documents(chunks, embeddings)

# ç›¸ä¼¼åº¦æ£€ç´¢
query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
results = vectorstore.similarity_search(query, k=3)

for i, doc in enumerate(results, 1):
    print(f"\nğŸ“„ ç»“æœ {i}:")
    print(f"å†…å®¹: {doc.page_content[:100]}...")
```

### ğŸ¯ Week 2 é¡¹ç›®ï¼šæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

#### é¡¹ç›®éœ€æ±‚
åŸºäºæœ¬åœ°æ–‡æ¡£åˆ›å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ

#### å®Œæ•´ä»£ç 
```python
class DocumentQA:
    def __init__(self, document_path):
        self.llm = ChatTongyi(model="qwen-turbo", temperature=0.3)
        self.embeddings = OpenAIEmbeddings(
            openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.vectorstore = self._load_document(document_path)
    
    def _load_document(self, path):
        # åŠ è½½æ–‡æ¡£
        loader = TextLoader(path)
        docs = loader.load()
        
        # åˆ‡åˆ†æ–‡æ¡£
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, chunk_overlap=50
        )
        chunks = splitter.split_documents(docs)
        
        # åˆ›å»ºå‘é‡åº“
        return FAISS.from_documents(chunks, self.embeddings)
    
    def ask(self, question):
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.vectorstore.similarity_search(question, k=3)
        context = "\n".join([doc.page_content for doc in docs])
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·åŸºäºæ–‡æ¡£å†…å®¹å›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚"""
        
        # è°ƒç”¨ LLM
        response = self.llm.invoke(prompt)
        return response.content
    
    def chat(self):
        print("ğŸ“š æ–‡æ¡£é—®ç­”ç³»ç»Ÿ (è¾“å…¥ 'quit' é€€å‡º)")
        while True:
            question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ")
            if question.lower() == 'quit':
                break
            
            answer = self.ask(question)
            print(f"ğŸ¤– å›ç­”: {answer}")

# ä½¿ç”¨
qa_system = DocumentQA("knowledge.txt")
qa_system.chat()
```

### âœ… Week 2 éªŒè¯ç‚¹
- [ ] èƒ½åŠ è½½å¤šç§æ ¼å¼æ–‡æ¡£
- [ ] èƒ½æ­£ç¡®åˆ‡åˆ†æ–‡æœ¬
- [ ] èƒ½å»ºç«‹å‘é‡ç´¢å¼•
- [ ] èƒ½å®ç°æ–‡æ¡£é—®ç­”
- [ ] å®Œæˆæ–‡æ¡£é—®ç­”ç³»ç»Ÿé¡¹ç›®

---

## âš™ï¸ æ¨¡å— 3ï¼šé“¾å¼ç¼–æ’ï¼ˆWeek 3ï¼‰

### ğŸ¯ å­¦ä¹ ç›®æ ‡
æŒæ¡ LCEL è¯­æ³•å’Œé“¾å¼ç¼–æ’ï¼Œå®Œæˆæ™ºèƒ½é—®ç­”åŠ©æ‰‹

### ğŸ“š çŸ¥è¯†ç‚¹è¯´æ˜

#### 3.1 LCEL åŸºç¡€è¯­æ³•
ä½¿ç”¨ `|` æ“ä½œç¬¦åˆ›å»ºå¤„ç†é“¾ï¼Œæ˜¯ LangChain 1.0 çš„æ ¸å¿ƒå†™æ³•ã€‚

#### 3.2 Output Parser
è§£ææ¨¡å‹è¾“å‡ºï¼Œå®ç°ç»“æ„åŒ–æ•°æ®å¤„ç†ã€‚

#### 3.3 åŸºç¡€ Memory
ä¿å­˜å¯¹è¯å†å²ï¼Œå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹è¯ã€‚

### ğŸ› ï¸ æ ¸å¿ƒä»£ç æ¨¡æ¿

#### LCEL é“¾å¼è°ƒç”¨
```python
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate.from_template(
    "è§£é‡Š {concept}ï¼Œç”¨ç®€å•çš„è¯­è¨€ï¼Œé€‚åˆåˆå­¦è€…ç†è§£ã€‚"
)

# åˆ›å»ºå¤„ç†é“¾
chain = prompt | llm | StrOutputParser()

# è°ƒç”¨é“¾
result = chain.invoke({"concept": "é€’å½’"})
print(result)
```

#### JSON è¾“å‡ºè§£æ
```python
from langchain_core.output_parsers import JsonOutputParser

# åˆ›å»º JSON è§£æå™¨
parser = JsonOutputParser()

# åˆ›å»ºæç¤ºè¯ï¼ˆè¦æ±‚ JSON è¾“å‡ºï¼‰
prompt = PromptTemplate.from_template(
    """å›ç­”é—®é¢˜å¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
    
    é—®é¢˜ï¼š{question}
    
    è¯·è¾“å‡ºåŒ…å« "answer" å’Œ "confidence" å­—æ®µçš„ JSONã€‚
    {format_instructions}"""
)

# åˆ›å»ºé“¾
chain = prompt | llm | parser

# è°ƒç”¨
result = chain.invoke({
    "question": "Python æ˜¯ä»€ä¹ˆï¼Ÿ",
    "format_instructions": parser.get_format_instructions()
})

print(result)  # {'answer': '...', 'confidence': '...'}
```

#### ç®€å• Memory
```python
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import HumanMessage, AIMessage

class SimpleMemory:
    def __init__(self):
        self.messages = []
    
    def add_user_message(self, content):
        self.messages.append(HumanMessage(content=content))
    
    def add_ai_message(self, content):
        self.messages.append(AIMessage(content=content))
    
    def get_messages(self):
        return self.messages
    
    def clear(self):
        self.messages = []

# ä½¿ç”¨ Memory
memory = SimpleMemory()

def chat_with_memory(question):
    memory.add_user_message(question)
    
    # æ„å»ºåŒ…å«å†å²çš„æç¤ºè¯
    history = "\n".join([
        f"ç”¨æˆ·: {msg.content}" if isinstance(msg, HumanMessage) 
        else f"åŠ©æ‰‹: {msg.content}" 
        for msg in memory.get_messages()
    ])
    
    prompt = f"å¯¹è¯å†å²ï¼š\n{history}\n\nè¯·å›ç­”æœ€åä¸€ä¸ªé—®é¢˜ã€‚"
    response = llm.invoke(prompt)
    
    memory.add_ai_message(response.content)
    return response.content
```

### ğŸ¯ Week 3 é¡¹ç›®ï¼šæ™ºèƒ½é—®ç­”åŠ©æ‰‹

#### é¡¹ç›®éœ€æ±‚
åˆ›å»ºæœ‰è®°å¿†ã€èƒ½ç»“æ„åŒ–è¾“å‡ºçš„æ™ºèƒ½åŠ©æ‰‹

#### å®Œæ•´ä»£ç 
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import HumanMessage, AIMessage

class SmartAssistant:
    def __init__(self):
        self.llm = ChatTongyi(model="qwen-turbo", temperature=0.7)
        self.memory = BaseChatMessageHistory()
        self.parser = JsonOutputParser()
        
        # åˆ›å»ºå¯¹è¯æ¨¡æ¿
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½è®°ä½å¯¹è¯å†å²å¹¶ä»¥ JSON æ ¼å¼å›ç­”ã€‚"),
            ("placeholder", "{chat_history}"),
            ("human", "{question}")
        ])
        
        # åˆ›å»ºå¤„ç†é“¾
        self.chain = self.prompt | self.llm | self.parser
    
    def ask(self, question):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.memory.add_user_message(question)
        
        # è°ƒç”¨é“¾
        response = self.chain.invoke({
            "question": question,
            "chat_history": self.memory.messages
        })
        
        # æ·»åŠ  AI å›å¤åˆ°å†å²
        self.memory.add_ai_message(str(response))
        
        return response
    
    def chat(self):
        print("ğŸ¤– æ™ºèƒ½åŠ©æ‰‹ (è¾“å…¥ 'quit' é€€å‡º)")
        while True:
            question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ")
            if question.lower() == 'quit':
                break
            
            try:
                answer = self.ask(question)
                print(f"ğŸ¤– å›ç­”: {answer}")
            except Exception as e:
                print(f"âŒ è§£æå¤±è´¥: {e}")

# ä½¿ç”¨
assistant = SmartAssistant()
assistant.chat()
```

### âœ… Week 3 éªŒè¯ç‚¹
- [ ] èƒ½ä½¿ç”¨ LCEL è¯­æ³•
- [ ] èƒ½è§£æç»“æ„åŒ–è¾“å‡º
- [ ] èƒ½å®ç°å¯¹è¯è®°å¿†
- [ ] å®Œæˆæ™ºèƒ½åŠ©æ‰‹é¡¹ç›®

---

## ğŸ› ï¸ æ¨¡å— 4ï¼šå®ç”¨éƒ¨ç½²ï¼ˆWeek 4ï¼‰

### ğŸ¯ å­¦ä¹ ç›®æ ‡
æŒæ¡åŸºç¡€ Agent å’Œ API éƒ¨ç½²ï¼Œå®Œæˆå¯ç”¨çš„æœåŠ¡

### ğŸ“š çŸ¥è¯†ç‚¹è¯´æ˜

#### 4.1 åŸºç¡€ Tools
åˆ›å»ºå¯å¤ç”¨çš„å·¥å…·å‡½æ•°ï¼Œæ‰©å±• AI èƒ½åŠ›ã€‚

#### 4.2 ç®€å• Agent
è®© AI èƒ½å¤Ÿè°ƒç”¨å·¥å…·è§£å†³å¤æ‚é—®é¢˜ã€‚

#### 4.3 FastAPI éƒ¨ç½²
å°† LangChain åº”ç”¨å‘å¸ƒä¸º HTTP API æœåŠ¡ã€‚

### ğŸ› ï¸ æ ¸å¿ƒä»£ç æ¨¡æ¿

#### åˆ›å»ºå·¥å…·
```python
from langchain_core.tools import tool

@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {e}"

@tool
def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    return f"{city}ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦ 25Â°C"

# å·¥å…·åˆ—è¡¨
tools = [calculator, get_weather]
```

#### ç®€å• Agent
```python
from langchain_core.agents import create_tool_calling_agent
from langchain_core.agents import AgentExecutor

# åˆ›å»º Agent
agent_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å›ç­”é—®é¢˜ã€‚"),
    ("placeholder", "{chat_history}"),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}")
])

agent = create_tool_calling_agent(self.llm, tools, agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# ä½¿ç”¨ Agent
response = agent_executor.invoke({
    "input": "è®¡ç®— 123 + 456 çš„ç»“æœ"
})
```

#### FastAPI éƒ¨ç½²
```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="LangChain æ™ºèƒ½åŠ©æ‰‹ API")

class QuestionRequest(BaseModel):
    question: str
    session_id: str = "default"

class QuestionResponse(BaseModel):
    answer: str
    session_id: str

# åˆ›å»ºå…¨å±€åŠ©æ‰‹å®ä¾‹
assistant = SmartAssistant()

@app.post("/chat", response_model=QuestionResponse)
async def chat(request: QuestionRequest):
    try:
        answer = assistant.ask(request.question)
        return QuestionResponse(
            answer=str(answer),
            session_id=request.session_id
        )
    except Exception as e:
        return QuestionResponse(
            answer=f"æŠ±æ­‰ï¼Œå¤„ç†å‡ºé”™ï¼š{str(e)}",
            session_id=request.session_id
        )

@app.get("/")
async def root():
    return {"message": "LangChain æ™ºèƒ½åŠ©æ‰‹ API"}

# å¯åŠ¨å‘½ä»¤ï¼šuvicorn main:app --reload
```

### ğŸ¯ Week 4 é¡¹ç›®ï¼šå®Œæ•´çš„ API æœåŠ¡

#### é¡¹ç›®éœ€æ±‚
éƒ¨ç½²ä¸€ä¸ªåŒ…å«å·¥å…·è°ƒç”¨å’Œå¯¹è¯è®°å¿†çš„ Web API

#### å®Œæ•´ä»£ç 
```python
# main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import uvicorn

app = FastAPI(title="LangChain æ™ºèƒ½åŠ©æ‰‹ API", version="1.0.0")

# æ•°æ®æ¨¡å‹
class ChatRequest(BaseModel):
    message: str
    session_id: str = "default"

class ChatResponse(BaseModel):
    reply: str
    session_id: str
    timestamp: str

class ToolRequest(BaseModel):
    tool_name: str
    parameters: dict

class ToolResponse(BaseModel):
    result: str
    success: bool

# å…¨å±€åŠ©æ‰‹å®ä¾‹
class APIAssistant:
    def __init__(self):
        self.llm = ChatTongyi(model="qwen-turbo", temperature=0.7)
        self.sessions = {}  # ç®€å•çš„ä¼šè¯å­˜å‚¨
    
    def get_session(self, session_id):
        if session_id not in self.sessions:
            self.sessions[session_id] = []
        return self.sessions[session_id]
    
    def chat(self, message: str, session_id: str):
        session = self.get_session(session_id)
        
        # æ„å»ºå¯¹è¯å†å²
        history = "\n".join(session[-5:])  # ä¿ç•™æœ€è¿‘ 5 è½®å¯¹è¯
        
        prompt = f"""å¯¹è¯å†å²ï¼š
{history}

ç”¨æˆ·ï¼š{message}

åŠ©æ‰‹ï¼š"""
        
        response = self.llm.invoke(prompt)
        
        # ä¿å­˜å¯¹è¯
        session.append(f"ç”¨æˆ·ï¼š{message}")
        session.append(f"åŠ©æ‰‹ï¼š{response.content}")
        
        return response.content

assistant = APIAssistant()

# API ç«¯ç‚¹
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        reply = assistant.chat(request.message, request.session_id)
        
        from datetime import datetime
        return ChatResponse(
            reply=reply,
            session_id=request.session_id,
            timestamp=datetime.now().isoformat()
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "LangChain Assistant"}

@app.get("/sessions")
async def list_sessions():
    return {"sessions": list(assistant.sessions.keys())}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### âœ… Week 4 éªŒè¯ç‚¹
- [ ] èƒ½åˆ›å»ºå’Œä½¿ç”¨å·¥å…·
- [ ] èƒ½å®ç°ç®€å• Agent
- [ ] èƒ½éƒ¨ç½² FastAPI æœåŠ¡
- [ ] API èƒ½æ­£å¸¸è°ƒç”¨
- [ ] å®Œæˆå®Œæ•´é¡¹ç›®

---

## ğŸ”§ æ•…éšœæ’é™¤é™„å½•

### ğŸ› å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### 1. å¯¼å…¥é”™è¯¯
```python
# é”™è¯¯ï¼šModuleNotFoundError: No module named 'langchain.schema'
# è§£å†³ï¼šä½¿ç”¨æ–°çš„å¯¼å…¥è·¯å¾„
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
```

#### 2. API Key é—®é¢˜
```python
# é”™è¯¯ï¼šAuthenticationError: No api key provided
# è§£å†³ï¼šé‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(override=True)  # å¼ºåˆ¶é‡æ–°åŠ è½½
```

#### 3. Jupyter Kernel ç¼“å­˜
```python
# é—®é¢˜ï¼šä¿®æ”¹ .env åä¸ç”Ÿæ•ˆ
# è§£å†³ï¼šé‡å¯ kernel æˆ–é‡æ–°åŠ è½½
import os
from dotenv import load_dotenv
load_dotenv(override=True)
print(f"API Key: {os.getenv('DASHSCOPE_API_KEY')}")
```

#### 4. ç½‘ç»œè¿æ¥é—®é¢˜
```python
# é—®é¢˜ï¼šè¿æ¥è¶…æ—¶
# è§£å†³ï¼šè®¾ç½®ä»£ç†æˆ–é‡è¯•æœºåˆ¶
import requests
proxies = {
    "http": "http://your-proxy:port",
    "https": "https://your-proxy:port"
}
```

#### 5. æ¨¡å‹è°ƒç”¨å¤±è´¥
```python
# é—®é¢˜ï¼šæ¨¡å‹è¿”å›é”™è¯¯
# è§£å†³ï¼šæ£€æŸ¥å‚æ•°å’Œæ¨¡å‹åç§°
model = ChatTongyi(
    model="qwen-turbo",  # ç¡®ä¿æ¨¡å‹åç§°æ­£ç¡®
    temperature=0.7,
    max_tokens=1000
)
```

### ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### 1. å‡å°‘å»¶è¿Ÿ
```python
# ä½¿ç”¨å¼‚æ­¥è°ƒç”¨
import asyncio

async def async_chat(question):
    response = await llm.ainvoke(question)
    return response.content
```

#### 2. æ§åˆ¶æˆæœ¬
```python
# é™åˆ¶è¾“å‡ºé•¿åº¦
model = ChatTongyi(max_tokens=500)  # é¿å…è¿‡é•¿å›å¤

# ä½¿ç”¨ç¼“å­˜
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_response(question):
    return llm.invoke(question)
```

---

## ğŸ“ˆ å­¦ä¹ æˆæœè¯„ä¼°

### ğŸ† æŠ€èƒ½æ£€æŸ¥æ¸…å•

#### Week 1 åŸºç¡€æŠ€èƒ½
- [ ] ç¯å¢ƒé…ç½®å®Œæˆ
- [ ] èƒ½è°ƒç”¨ ChatTongyi
- [ ] èƒ½ä½¿ç”¨ PromptTemplate
- [ ] èƒ½æ§åˆ¶ LLM å‚æ•°
- [ ] å®Œæˆä¸ªäºº AI åŠ©æ‰‹

#### Week 2 æ•°æ®å¤„ç†æŠ€èƒ½
- [ ] èƒ½åŠ è½½å¤šç§æ–‡æ¡£æ ¼å¼
- [ ] èƒ½æ­£ç¡®åˆ‡åˆ†æ–‡æœ¬
- [ ] èƒ½å»ºç«‹å‘é‡ç´¢å¼•
- [ ] èƒ½å®ç°æ–‡æ¡£æ£€ç´¢
- [ ] å®Œæˆæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

#### Week 3 é“¾å¼ç¼–æ’æŠ€èƒ½
- [ ] èƒ½ä½¿ç”¨ LCEL è¯­æ³•
- [ ] èƒ½è§£æç»“æ„åŒ–è¾“å‡º
- [ ] èƒ½å®ç°å¯¹è¯è®°å¿†
- [ ] èƒ½æ„å»ºå¤„ç†é“¾
- [ ] å®Œæˆæ™ºèƒ½åŠ©æ‰‹

#### Week 4 éƒ¨ç½²æŠ€èƒ½
- [ ] èƒ½åˆ›å»ºå’Œä½¿ç”¨å·¥å…·
- [ ] èƒ½å®ç°ç®€å• Agent
- [ ] èƒ½éƒ¨ç½² FastAPI æœåŠ¡
- [ ] èƒ½å¤„ç† API è¯·æ±‚
- [ ] å®Œæˆå®Œæ•´é¡¹ç›®

### ğŸ¯ èƒ½åŠ›ç­‰çº§

| ç­‰çº§ | æŠ€èƒ½æè¿° | é¡¹ç›®èƒ½åŠ› |
|------|----------|----------|
| ğŸŸ¢ **åˆçº§** | åŸºç¡€è°ƒç”¨å’Œç®€å•é…ç½® | èƒ½æ­å»ºä¸ªäººåŠ©æ‰‹ |
| ğŸŸ¡ **ä¸­çº§** | æ•°æ®å¤„ç†å’Œé“¾å¼ç¼–æ’ | èƒ½å¼€å‘æ–‡æ¡£é—®ç­”ç³»ç»Ÿ |
| ğŸŸ  **é«˜çº§** | å·¥å…·è°ƒç”¨å’Œ API éƒ¨ç½² | èƒ½å‘å¸ƒå¯ç”¨æœåŠ¡ |
| ğŸ”´ **ä¸“å®¶** | ç³»ç»Ÿä¼˜åŒ–å’Œç”Ÿäº§éƒ¨ç½² | èƒ½æ„å»ºä¼ä¸šåº”ç”¨ |

---

## ğŸ® å­¦ä¹ æ¿€åŠ±

### ğŸ… æˆå°±å¾½ç« 
- ğŸ… **ç¯å¢ƒé…ç½®å¤§å¸ˆ** - 10 åˆ†é’Ÿå®Œæˆç¯å¢ƒæ­å»º
- ğŸ… **LLM è°ƒç”¨æ–°æ‰‹** - æˆåŠŸè·å–ç¬¬ä¸€ä¸ª AI å›å¤
- ğŸ… **RAG å®è·µè€…** - å®Œæˆæ–‡æ¡£é—®ç­”ç³»ç»Ÿ
- ğŸ… **é“¾å¼ç¼–æ’ä¸“å®¶** - ç†Ÿç»ƒä½¿ç”¨ LCEL è¯­æ³•
- ğŸ… **éƒ¨ç½²å·¥ç¨‹å¸ˆ** - æˆåŠŸå‘å¸ƒ API æœåŠ¡
- ğŸ… **LangChain å¼€å‘è€…** - å®Œæˆå…¨éƒ¨å­¦ä¹ å†…å®¹

### ğŸ“Š å­¦ä¹ è¿›åº¦è¿½è¸ª
```
Week 1 è¿›åº¦: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80%
Week 2 è¿›åº¦: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 70%
Week 3 è¿›åº¦: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 60%
Week 4 è¿›åº¦: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ 40%

æ€»ä½“è¿›åº¦: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 60%
```

### ğŸ¯ æ¯æ—¥ç›®æ ‡
- **Day 1-2**: ç¯å¢ƒé…ç½® + åŸºç¡€è°ƒç”¨
- **Day 3-4**: å‚æ•°æ§åˆ¶ + æ¨¡æ¿ä½¿ç”¨
- **Day 5-7**: å®Œæˆç¬¬ä¸€ä¸ªé¡¹ç›®
- **åšæŒæ¯å¤©å†™ä»£ç ï¼Œå“ªæ€•åªæœ‰ 30 åˆ†é’Ÿï¼**

---

## ğŸ“š é…å¥—èµ„æº

### ğŸ“– å¿…è¯»æ–‡æ¡£
- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [é€šä¹‰åƒé—® API æ–‡æ¡£](https://help.aliyun.com/zh/dashscope/)
- [FastAPI å®˜æ–¹æ•™ç¨‹](https://fastapi.tiangolo.com/)

### ğŸ› ï¸ å¼€å‘å·¥å…·
- **IDE**: VS Code + Python æ’ä»¶
- **ç¯å¢ƒ**: Conda æˆ– venv
- **æµ‹è¯•**: Postman æˆ– curl
- **ç‰ˆæœ¬æ§åˆ¶**: Git

### ğŸ¥ è§†é¢‘æ•™ç¨‹
- ç¯å¢ƒé…ç½® 10 åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
- LangChain æ ¸å¿ƒæ¦‚å¿µè®²è§£
- é¡¹ç›®å®æˆ˜æ¼”ç¤ºè§†é¢‘

### ğŸ’¬ ç¤¾åŒºæ”¯æŒ
- LangChain å®˜æ–¹ Discord
- GitHub Issues å’Œ Discussions
- Stack Overflow é—®ç­”

---

## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ è·¯å¾„

### ğŸ“ˆ è¿›é˜¶æ–¹å‘
1. **é«˜çº§ RAG**: é‡æ’åºã€æ··åˆæ£€ç´¢
2. **å¤æ‚ Agent**: å¤šå·¥å…·åä½œã€æ¡ä»¶æ§åˆ¶
3. **LangGraph**: çŠ¶æ€å›¾ã€å¯æ§æµç¨‹
4. **ç”Ÿäº§éƒ¨ç½²**: ç›‘æ§ã€ç¼“å­˜ã€è´Ÿè½½å‡è¡¡

### ğŸ¯ èŒä¸šå‘å±•
- **AI åº”ç”¨å¼€å‘å·¥ç¨‹å¸ˆ**
- **RAG ç³»ç»Ÿæ¶æ„å¸ˆ**
- **LLM åº”ç”¨äº§å“ç»ç†**
- **AI æŠ€æœ¯é¡¾é—®**

---

**ğŸ‰ æ­å–œï¼å®Œæˆè¿™ä¸ª 4 å‘¨å­¦ä¹ è®¡åˆ’ï¼Œä½ å°†å…·å¤‡ï¼š**

âœ… ç‹¬ç«‹å¼€å‘ LangChain åº”ç”¨çš„èƒ½åŠ›
âœ… ä»é›¶åˆ°ä¸€æ„å»º AI é¡¹ç›®çš„ç»éªŒ
âœ… éƒ¨ç½²å’Œç»´æŠ¤ AI æœåŠ¡çš„åŸºç¡€æŠ€èƒ½
âœ… ç»§ç»­æ·±å…¥å­¦ä¹ çš„é«˜çº§èµ·ç‚¹

**å¼€å§‹ä½ çš„ LangChain å­¦ä¹ ä¹‹æ—…å§ï¼** ğŸš€
