{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 36-å¸¸ç”¨å·¥å…·æ¨¡å—\n",
    "\n",
    "## ğŸ“š ç”¨é€”è¯´æ˜\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**ï¼š\n",
    "- æŒæ¡collectionsæ¨¡å—çš„é«˜çº§æ•°æ®ç»“æ„\n",
    "- ç†Ÿç»ƒä½¿ç”¨functoolsæ¨¡å—çš„å‡½æ•°å·¥å…·\n",
    "- ç†è§£itertoolsæ¨¡å—çš„è¿­ä»£å™¨æ¨¡å¼\n",
    "- èƒ½å¤Ÿä½¿ç”¨contextlibåˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨\n",
    "\n",
    "**å‰ç½®è¦æ±‚**ï¼š\n",
    "- å·²å®Œæˆ35-ç³»ç»Ÿä¿¡æ¯è·å–å­¦ä¹ \n",
    "- ç†Ÿç»ƒæŒæ¡Pythonæ•°æ®ç»“æ„å’Œå‡½æ•°\n",
    "- äº†è§£è¿­ä»£å™¨å’Œç”Ÿæˆå™¨çš„åŸºæœ¬æ¦‚å¿µ\n",
    "\n",
    "**ä¸LangChainå…³è”**ï¼š\n",
    "- collectionsæ¨¡å—æ˜¯LangChainæ•°æ®å¤„ç†çš„åŸºç¡€\n",
    "- functoolsæ¨¡å—å¯¹LangChainçš„ç¼“å­˜å’Œä¼˜åŒ–è‡³å…³é‡è¦\n",
    "- itertoolsæ¨¡å—æ”¯æŒLangChainçš„æµå¼æ•°æ®å¤„ç†\n",
    "- contextlibæ¨¡å—ç¡®ä¿LangChainèµ„æºçš„æ­£ç¡®ç®¡ç†\n",
    "- ä¸ºåç»­å­¦ä¹ LangChainçš„é«˜çº§å·¥å…·æ¨¡å¼åšå‡†å¤‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ çŸ¥è¯†ç‚¹è¦†ç›–\n",
    "\n",
    "### 4.9 å¸¸ç”¨å·¥å…·æ¨¡å— [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šå¸¸ç”¨å·¥å…·æ¨¡å—æä¾›äº†Pythonç¼–ç¨‹ä¸­çš„é«˜çº§æ•°æ®ç»“æ„å’Œå®ç”¨å·¥å…·ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ä»£ç çš„æ•ˆç‡å’Œå¯è¯»æ€§ã€‚æŒæ¡è¿™äº›æ¨¡å—å¯¹äºæ„å»ºé«˜æ•ˆã€ä¼˜é›…çš„åº”ç”¨ç¨‹åºéå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡collectionsæ¨¡å—çš„é«˜çº§æ•°æ®ç»“æ„\n",
    "- ç†è§£functoolsæ¨¡å—çš„å‡½æ•°å¼ç¼–ç¨‹å·¥å…·\n",
    "- ç†Ÿç»ƒä½¿ç”¨itertoolsæ¨¡å—çš„è¿­ä»£å™¨å·¥å…·\n",
    "- èƒ½å¤Ÿåˆ›å»ºå’Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„å·¥å…·æ¨¡å—åº”ç”¨ç¤ºä¾‹\n",
    "- è¿›è¡Œæ€§èƒ½ä¼˜åŒ–å’Œæ•ˆç‡æå‡ç»ƒä¹ \n",
    "- åº”ç”¨å·¥å…·æ¨¡å—è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„å·¥å…·åº”ç”¨ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ› ï¸ collectionsæ¨¡å—é«˜çº§æ•°æ®ç»“æ„:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from collections import Counter, defaultdict, deque, namedtuple\n",
    "from typing import Dict, List, Any, Optional, Union, Tuple, Iterator\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 1. Counter - è®¡æ•°å™¨\n",
    "print(f\"ğŸ“ 1. Counter - è®¡æ•°å™¨:\")\n",
    "\n",
    "# 1.1 åŸºç¡€è®¡æ•°æ“ä½œ\n",
    "print(f\"\\n   ğŸ”¢ 1.1 åŸºç¡€è®¡æ•°æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_counter_basic():\n",
    "    \"\"\"æ¼”ç¤ºCounteråŸºç¡€æ“ä½œ\"\"\"\n",
    "    # è¯é¢‘ç»Ÿè®¡ - LangChain tokenè®¡æ•°åº”ç”¨\n",
    "    text = \"hello world hello python world langchain python python\"\n",
    "    words = text.split()\n",
    "    \n",
    "    word_counter = Counter(words)\n",
    "    print(f\"   åŸå§‹æ–‡æœ¬: {text}\")\n",
    "    print(f\"   è¯é¢‘ç»Ÿè®¡: {dict(word_counter)}\")\n",
    "    \n",
    "    # å­—ç¬¦ç»Ÿè®¡\n",
    "    char_counter = Counter(text)\n",
    "    print(f\"   å­—ç¬¦ç»Ÿè®¡: {dict(char_counter)}\")\n",
    "    \n",
    "    # å¸¸ç”¨æ“ä½œ\n",
    "    print(f\"\\n   Counterå¸¸ç”¨æ“ä½œ:\")\n",
    "    print(f\"   æœ€å¸¸è§å•è¯: {word_counter.most_common(2)}\")\n",
    "    print(f\"   'hello'å‡ºç°æ¬¡æ•°: {word_counter['hello']}\")\n",
    "    print(f\"   'langchain'å‡ºç°æ¬¡æ•°: {word_counter['langchain']}\")\n",
    "    print(f\"   ä¸å­˜åœ¨çš„è¯'java': {word_counter['java']}\")\n",
    "    \n",
    "    # æ•°å­¦è¿ç®—\n",
    "    print(f\"\\n   Counteræ•°å­¦è¿ç®—:\")\n",
    "    counter1 = Counter('hello')\n",
    "    counter2 = Counter('world')\n",
    "    print(f\"   counter1: {dict(counter1)}\")\n",
    "    print(f\"   counter2: {dict(counter2)}\")\n",
    "    print(f\"   ç›¸åŠ : {dict(counter1 + counter2)}\")\n",
    "    print(f\"   ç›¸å‡: {dict(counter1 - counter2)}\")\n",
    "    print(f\"   äº¤é›†: {dict(counter1 & counter2)}\")\n",
    "    print(f\"   å¹¶é›†: {dict(counter1 | counter2)}\")\n",
    "\n",
    "demonstrate_counter_basic()\n",
    "\n",
    "# 1.2 LangChain Tokenè®¡æ•°åº”ç”¨\n",
    "print(f\"\\n   ğŸ¤– 1.2 LangChain Tokenè®¡æ•°åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class TokenCounter:\n",
    "    \"\"\"Tokenè®¡æ•°å™¨ - æ¨¡æ‹ŸLangChainçš„tokenç»Ÿè®¡åŠŸèƒ½\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.token_counter = Counter()\n",
    "        self.type_counter = Counter()\n",
    "    \n",
    "    def count_tokens(self, text: str, token_type: str = 'word') -> Dict[str, int]:\n",
    "        \"\"\"ç»Ÿè®¡token\"\"\"\n",
    "        if token_type == 'word':\n",
    "            tokens = text.split()\n",
    "        elif token_type == 'char':\n",
    "            tokens = list(text)\n",
    "        else:\n",
    "            tokens = text.split()  # é»˜è®¤æŒ‰å•è¯\n",
    "        \n",
    "        # æ›´æ–°è®¡æ•°å™¨\n",
    "        self.token_counter.update(tokens)\n",
    "        self.type_counter[token_type] += len(tokens)\n",
    "        \n",
    "        return dict(Counter(tokens))\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            'total_tokens': sum(self.token_counter.values()),\n",
    "            'unique_tokens': len(self.token_counter),\n",
    "            'most_common': self.token_counter.most_common(5),\n",
    "            'type_distribution': dict(self.type_counter)\n",
    "        }\n",
    "    \n",
    "    def get_token_frequency(self, token: str) -> int:\n",
    "        \"\"\"è·å–ç‰¹å®štokené¢‘ç‡\"\"\"\n",
    "        return self.token_counter[token]\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"é‡ç½®è®¡æ•°å™¨\"\"\"\n",
    "        self.token_counter.clear()\n",
    "        self.type_counter.clear()\n",
    "\n",
    "def demonstrate_token_counter():\n",
    "    \"\"\"æ¼”ç¤ºTokenè®¡æ•°å™¨\"\"\"\n",
    "    counter = TokenCounter()\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå¯¹è¯æ–‡æœ¬\n",
    "    conversations = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"I'm fine, thank you! How about you?\",\n",
    "        \"I'm doing great! Let's talk about LangChain.\",\n",
    "        \"LangChain is amazing for AI applications.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"   å¯¹è¯Tokenç»Ÿè®¡æ¼”ç¤º:\")\n",
    "    \n",
    "    for i, text in enumerate(conversations, 1):\n",
    "        print(f\"\\n   å¯¹è¯{i}: {text}\")\n",
    "        tokens = counter.count_tokens(text, 'word')\n",
    "        print(f\"   Tokenç»Ÿè®¡: {len(tokens)}ä¸ªtoken\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡\n",
    "    stats = counter.get_stats()\n",
    "    print(f\"\\n   æ€»ä½“ç»Ÿè®¡:\")\n",
    "    print(f\"   æ€»Tokenæ•°: {stats['total_tokens']}\")\n",
    "    print(f\"   å”¯ä¸€Tokenæ•°: {stats['unique_tokens']}\")\n",
    "    print(f\"   æœ€å¸¸è§Token: {stats['most_common'][:3]}\")\n",
    "    print(f\"   ç±»å‹åˆ†å¸ƒ: {stats['type_distribution']}\")\n",
    "    \n",
    "    # ç‰¹å®štokenæŸ¥è¯¢\n",
    "    print(f\"\\n   ç‰¹å®šTokené¢‘ç‡:\")\n",
    "    for token in ['LangChain', 'you', 'AI']:\n",
    "        freq = counter.get_token_frequency(token)\n",
    "        print(f\"   '{token}': {freq}æ¬¡\")\n",
    "\n",
    "demonstrate_token_counter()\n",
    "\n",
    "# 2. defaultdict - é»˜è®¤å­—å…¸\n",
    "print(f\"\\nğŸ“ 2. defaultdict - é»˜è®¤å­—å…¸:\")\n",
    "\n",
    "# 2.1 åŸºç¡€é»˜è®¤å­—å…¸æ“ä½œ\n",
    "print(f\"\\n   ğŸ—‚ï¸ 2.1 åŸºç¡€é»˜è®¤å­—å…¸æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_defaultdict_basic():\n",
    "    \"\"\"æ¼”ç¤ºdefaultdictåŸºç¡€æ“ä½œ\"\"\"\n",
    "    # æ™®é€šå­—å…¸çš„é—®é¢˜\n",
    "    normal_dict = {}\n",
    "    words = ['apple', 'banana', 'apple', 'orange', 'banana', 'apple']\n",
    "    \n",
    "    print(f\"   æ™®é€šå­—å…¸å¤„ç†åˆ†ç»„:\")\n",
    "    for word in words:\n",
    "        first_letter = word[0]\n",
    "        if first_letter not in normal_dict:\n",
    "            normal_dict[first_letter] = []\n",
    "        normal_dict[first_letter].append(word)\n",
    "    print(f\"   ç»“æœ: {normal_dict}\")\n",
    "    \n",
    "    # defaultdictçš„ä¼˜é›…è§£å†³æ–¹æ¡ˆ\n",
    "    from collections import defaultdict\n",
    "    default_dict = defaultdict(list)\n",
    "    \n",
    "    print(f\"\\n   defaultdictå¤„ç†åˆ†ç»„:\")\n",
    "    for word in words:\n",
    "        default_dict[word[0]].append(word)\n",
    "    print(f\"   ç»“æœ: {dict(default_dict)}\")\n",
    "    \n",
    "    # ä¸åŒé»˜è®¤å€¼ç±»å‹\n",
    "    print(f\"\\n   ä¸åŒé»˜è®¤å€¼ç±»å‹:\")\n",
    "    int_dict = defaultdict(int)\n",
    "    int_dict['a'] += 1\n",
    "    int_dict['b'] += 2\n",
    "    print(f\"   inté»˜è®¤å€¼: {dict(int_dict)}\")\n",
    "    \n",
    "    float_dict = defaultdict(float)\n",
    "    float_dict['x'] += 1.5\n",
    "    float_dict['y'] += 2.7\n",
    "    print(f\"   floaté»˜è®¤å€¼: {dict(float_dict)}\")\n",
    "    \n",
    "    set_dict = defaultdict(set)\n",
    "    set_dict['group1'].add('item1')\n",
    "    set_dict['group1'].add('item2')\n",
    "    set_dict['group2'].add('item3')\n",
    "    print(f\"   seté»˜è®¤å€¼: {dict(set_dict)}\")\n",
    "\n",
    "demonstrate_defaultdict_basic()\n",
    "\n",
    "# 2.2 LangChainå¯¹è¯åˆ†ç»„åº”ç”¨\n",
    "print(f\"\\n   ğŸ’¬ 2.2 LangChainå¯¹è¯åˆ†ç»„åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class ConversationGrouper:\n",
    "    \"\"\"å¯¹è¯åˆ†ç»„å™¨ - æ¨¡æ‹ŸLangChainçš„å¯¹è¯ç®¡ç†\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversations_by_user = defaultdict(list)\n",
    "        self.conversations_by_topic = defaultdict(list)\n",
    "        self.conversation_stats = defaultdict(int)\n",
    "    \n",
    "    def add_conversation(self, user_id: str, topic: str, message: str):\n",
    "        \"\"\"æ·»åŠ å¯¹è¯\"\"\"\n",
    "        timestamp = time.time()\n",
    "        \n",
    "        conversation = {\n",
    "            'user_id': user_id,\n",
    "            'topic': topic,\n",
    "            'message': message,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        # æŒ‰ç”¨æˆ·åˆ†ç»„\n",
    "        self.conversations_by_user[user_id].append(conversation)\n",
    "        \n",
    "        # æŒ‰ä¸»é¢˜åˆ†ç»„\n",
    "        self.conversations_by_topic[topic].append(conversation)\n",
    "        \n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        self.conversation_stats[f'user_{user_id}'] += 1\n",
    "        self.conversation_stats[f'topic_{topic}'] += 1\n",
    "        self.conversation_stats['total'] += 1\n",
    "    \n",
    "    def get_user_conversations(self, user_id: str) -> List[Dict]:\n",
    "        \"\"\"è·å–ç”¨æˆ·å¯¹è¯\"\"\"\n",
    "        return self.conversations_by_user[user_id]\n",
    "    \n",
    "    def get_topic_conversations(self, topic: str) -> List[Dict]:\n",
    "        \"\"\"è·å–ä¸»é¢˜å¯¹è¯\"\"\"\n",
    "        return self.conversations_by_topic[topic]\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"è·å–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        return dict(self.conversation_stats)\n",
    "    \n",
    "    def get_all_users(self) -> List[str]:\n",
    "        \"\"\"è·å–æ‰€æœ‰ç”¨æˆ·\"\"\"\n",
    "        return list(self.conversations_by_user.keys())\n",
    "    \n",
    "    def get_all_topics(self) -> List[str]:\n",
    "        \"\"\"è·å–æ‰€æœ‰ä¸»é¢˜\"\"\"\n",
    "        return list(self.conversations_by_topic.keys())\n",
    "\n",
    "def demonstrate_conversation_grouper():\n",
    "    \"\"\"æ¼”ç¤ºå¯¹è¯åˆ†ç»„å™¨\"\"\"\n",
    "    grouper = ConversationGrouper()\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå¯¹è¯æ•°æ®\n",
    "    conversations_data = [\n",
    "        ('user1', 'AI', 'What is LangChain?'),\n",
    "        ('user2', 'Python', 'How to use collections?'),\n",
    "        ('user1', 'AI', 'LangChain is a framework for LLM applications'),\n",
    "        ('user3', 'AI', 'Can you explain embeddings?'),\n",
    "        ('user2', 'Python', 'What are dataclasses?'),\n",
    "        ('user1', 'Python', 'How to optimize Python code?')\n",
    "    ]\n",
    "    \n",
    "    print(f\"   å¯¹è¯åˆ†ç»„æ¼”ç¤º:\")\n",
    "    \n",
    "    # æ·»åŠ å¯¹è¯\n",
    "    for user_id, topic, message in conversations_data:\n",
    "        grouper.add_conversation(user_id, topic, message)\n",
    "    \n",
    "    # æ˜¾ç¤ºåˆ†ç»„ç»“æœ\n",
    "    print(f\"\\n   ç”¨æˆ·åˆ†ç»„:\")\n",
    "    for user_id in grouper.get_all_users():\n",
    "        user_convs = grouper.get_user_conversations(user_id)\n",
    "        print(f\"   ç”¨æˆ·{user_id}: {len(user_convs)}æ¡å¯¹è¯\")\n",
    "        for conv in user_convs:\n",
    "            print(f\"     - {conv['topic']}: {conv['message']}\")\n",
    "    \n",
    "    print(f\"\\n   ä¸»é¢˜åˆ†ç»„:\")\n",
    "    for topic in grouper.get_all_topics():\n",
    "        topic_convs = grouper.get_topic_conversations(topic)\n",
    "        print(f\"   ä¸»é¢˜{topic}: {len(topic_convs)}æ¡å¯¹è¯\")\n",
    "    \n",
    "    # ç»Ÿè®¡ä¿¡æ¯\n",
    "    stats = grouper.get_stats()\n",
    "    print(f\"\\n   ç»Ÿè®¡ä¿¡æ¯: {stats}\")\n",
    "\n",
    "demonstrate_conversation_grouper()\n",
    "\n",
    "# 3. deque - åŒç«¯é˜Ÿåˆ—\n",
    "print(f\"\\nğŸ“ 3. deque - åŒç«¯é˜Ÿåˆ—:\")\n",
    "\n",
    "# 3.1 åŸºç¡€åŒç«¯é˜Ÿåˆ—æ“ä½œ\n",
    "print(f\"\\n   ğŸ“‹ 3.1 åŸºç¡€åŒç«¯é˜Ÿåˆ—æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_deque_basic():\n",
    "    \"\"\"æ¼”ç¤ºdequeåŸºç¡€æ“ä½œ\"\"\"\n",
    "    # åˆ›å»ºdeque\n",
    "    dq = deque([1, 2, 3])\n",
    "    print(f\"   åˆå§‹deque: {list(dq)}\")\n",
    "    \n",
    "    # ä¸¤ç«¯æ·»åŠ \n",
    "    dq.append(4)  # å³ç«¯æ·»åŠ \n",
    "    dq.appendleft(0)  # å·¦ç«¯æ·»åŠ \n",
    "    print(f\"   æ·»åŠ å: {list(dq)}\")\n",
    "    \n",
    "    # ä¸¤ç«¯ç§»é™¤\n",
    "    right_item = dq.pop()  # å³ç«¯ç§»é™¤\n",
    "    left_item = dq.popleft()  # å·¦ç«¯ç§»é™¤\n",
    "    print(f\"   ç§»é™¤å³ç«¯: {right_item}, ç§»é™¤å·¦ç«¯: {left_item}\")\n",
    "    print(f\"   ç§»é™¤å: {list(dq)}\")\n",
    "    \n",
    "    # æ‰©å±•æ“ä½œ\n",
    "    dq.extend([5, 6])  # å³ç«¯æ‰©å±•\n",
    "    dq.extendleft([-1, -2])  # å·¦ç«¯æ‰©å±•ï¼ˆæ³¨æ„é¡ºåºï¼‰\n",
    "    print(f\"   æ‰©å±•å: {list(dq)}\")\n",
    "    \n",
    "    # æ—‹è½¬æ“ä½œ\n",
    "    dq.rotate(1)  # å³æ—‹è½¬1æ­¥\n",
    "    print(f\"   å³æ—‹è½¬1æ­¥: {list(dq)}\")\n",
    "    dq.rotate(-1)  # å·¦æ—‹è½¬1æ­¥\n",
    "    print(f\"   å·¦æ—‹è½¬1æ­¥: {list(dq)}\")\n",
    "    \n",
    "    # é™åˆ¶é•¿åº¦çš„deque\n",
    "    limited_dq = deque(maxlen=3)\n",
    "    for i in range(5):\n",
    "        limited_dq.append(i)\n",
    "        print(f\"   æ·»åŠ {i}: {list(limited_dq)}\")\n",
    "\n",
    "demonstrate_deque_basic()\n",
    "\n",
    "# 3.2 LangChainå¯¹è¯ç¼“å†²åŒºåº”ç”¨\n",
    "print(f\"\\n   ğŸ’¾ 3.2 LangChainå¯¹è¯ç¼“å†²åŒºåº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class ConversationBuffer:\n",
    "    \"\"\"å¯¹è¯ç¼“å†²åŒº - æ¨¡æ‹ŸLangChainçš„å¯¹è¯å†å²ç®¡ç†\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 10):\n",
    "        self.messages = deque(maxlen=max_size)\n",
    "        self.message_count = 0\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"æ·»åŠ æ¶ˆæ¯\"\"\"\n",
    "        message = {\n",
    "            'id': self.message_count,\n",
    "            'role': role,\n",
    "            'content': content,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        self.messages.append(message)\n",
    "        self.message_count += 1\n",
    "    \n",
    "    def get_recent_messages(self, count: int = None) -> List[Dict]:\n",
    "        \"\"\"è·å–æœ€è¿‘æ¶ˆæ¯\"\"\"\n",
    "        if count is None:\n",
    "            return list(self.messages)\n",
    "        else:\n",
    "            return list(self.messages)[-count:]\n",
    "    \n",
    "    def get_conversation_context(self, window_size: int = 5) -> str:\n",
    "        \"\"\"è·å–å¯¹è¯ä¸Šä¸‹æ–‡\"\"\"\n",
    "        recent = self.get_recent_messages(window_size)\n",
    "        context = []\n",
    "        for msg in recent:\n",
    "            context.append(f\"{msg['role']}: {msg['content']}\")\n",
    "        return '\\n'.join(context)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"æ¸…ç©ºç¼“å†²åŒº\"\"\"\n",
    "        self.messages.clear()\n",
    "        self.message_count = 0\n",
    "    \n",
    "    def size(self) -> int:\n",
    "        \"\"\"è·å–å½“å‰å¤§å°\"\"\"\n",
    "        return len(self.messages)\n",
    "    \n",
    "    def is_full(self) -> bool:\n",
    "        \"\"\"æ˜¯å¦å·²æ»¡\"\"\"\n",
    "        return len(self.messages) == self.messages.maxlen\n",
    "\n",
    "def demonstrate_conversation_buffer():\n",
    "    \"\"\"æ¼”ç¤ºå¯¹è¯ç¼“å†²åŒº\"\"\"\n",
    "    buffer = ConversationBuffer(max_size=5)\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå¯¹è¯\n",
    "    conversation = [\n",
    "        ('user', 'Hello, how are you?'),\n",
    "        ('assistant', 'I\\'m fine, thank you!'),\n",
    "        ('user', 'What can you help me with?'),\n",
    "        ('assistant', 'I can help with many things!'),\n",
    "        ('user', 'Tell me about LangChain'),\n",
    "        ('assistant', 'LangChain is a framework for LLM applications'),\n",
    "        ('user', 'What are its main features?')\n",
    "    ]\n",
    "    \n",
    "    print(f\"   å¯¹è¯ç¼“å†²åŒºæ¼”ç¤º (æœ€å¤§å®¹é‡: {buffer.messages.maxlen}):\")\n",
    "    \n",
    "    for role, content in conversation:\n",
    "        buffer.add_message(role, content)\n",
    "        print(f\"\\n   æ·»åŠ æ¶ˆæ¯: {role}: {content}\")\n",
    "        print(f\"   å½“å‰ç¼“å†²åŒºå¤§å°: {buffer.size()}\")\n",
    "        print(f\"   æ˜¯å¦å·²æ»¡: {buffer.is_full()}\")\n",
    "        print(f\"   æœ€è¿‘æ¶ˆæ¯: {buffer.get_recent_messages(2)}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå¯¹è¯ä¸Šä¸‹æ–‡\n",
    "    print(f\"\\n   å¯¹è¯ä¸Šä¸‹æ–‡ (æœ€è¿‘3æ¡):\")\n",
    "    context = buffer.get_conversation_context(3)\n",
    "    print(f\"   {context}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ‰€æœ‰æ¶ˆæ¯\n",
    "    print(f\"\\n   ç¼“å†²åŒºä¸­çš„æ‰€æœ‰æ¶ˆæ¯:\")\n",
    "    for msg in buffer.get_recent_messages():\n",
    "        print(f\"   {msg['id']}: {msg['role']} - {msg['content']}\")\n",
    "\n",
    "demonstrate_conversation_buffer()\n",
    "\n",
    "print(f\"\\nâœ… collectionsæ¨¡å—é«˜çº§æ•°æ®ç»“æ„å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡Counterçš„è®¡æ•°å’Œç»Ÿè®¡åŠŸèƒ½\")\n",
    "print(f\"   âœ“ ç†è§£defaultdictçš„é»˜è®¤å€¼æœºåˆ¶\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨dequeçš„åŒç«¯æ“ä½œ\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿåº”ç”¨collectionsè§£å†³å®é™…é—®é¢˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functoolsæ¨¡å—å‡½æ•°å·¥å…· [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šfunctoolsæ¨¡å—æä¾›äº†é«˜é˜¶å‡½æ•°å’Œæ“ä½œå…¶ä»–å‡½æ•°çš„å·¥å…·ï¼Œæ”¯æŒå‡½æ•°å¼ç¼–ç¨‹æ¨¡å¼ã€‚åœ¨LangChainåº”ç”¨ä¸­ï¼Œfunctoolsæ¨¡å—å¸¸ç”¨äºç¼“å­˜ä¼˜åŒ–ã€å‡½æ•°ç»„åˆå’Œå‚æ•°ç»‘å®šã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡functoolsæ¨¡å—çš„ç¼“å­˜æœºåˆ¶\n",
    "- ç†è§£åå‡½æ•°å’Œå‡½æ•°ç»„åˆ\n",
    "- ç†Ÿç»ƒä½¿ç”¨è£…é¥°å™¨å’Œå‡½æ•°åŒ…è£…\n",
    "- èƒ½å¤Ÿè¿›è¡Œå‡½æ•°æ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„å‡½æ•°ä¼˜åŒ–åº”ç”¨\n",
    "- è¿›è¡Œç¼“å­˜å’Œæ€§èƒ½æµ‹è¯•ç»ƒä¹ \n",
    "- åº”ç”¨å‡½æ•°å·¥å…·è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„å‡½æ•°å¤„ç†ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âš™ï¸ functoolsæ¨¡å—å‡½æ•°å·¥å…·:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import functools\n",
    "import time\n",
    "import hashlib\n",
    "import pickle\n",
    "from typing import Dict, List, Any, Optional, Callable, Union\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "# 1. lru_cache - æœ€è¿‘æœ€å°‘ä½¿ç”¨ç¼“å­˜\n",
    "print(f\"ğŸ“ 1. lru_cache - æœ€è¿‘æœ€å°‘ä½¿ç”¨ç¼“å­˜:\")\n",
    "\n",
    "# 1.1 åŸºç¡€ç¼“å­˜æ“ä½œ\n",
    "print(f\"\\n   ğŸ’¾ 1.1 åŸºç¡€ç¼“å­˜æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_lru_cache_basic():\n",
    "    \"\"\"æ¼”ç¤ºlru_cacheåŸºç¡€æ“ä½œ\"\"\"\n",
    "    \n",
    "    # æ¨¡æ‹Ÿè€—æ—¶çš„è®¡ç®—å‡½æ•°\n",
    "    @functools.lru_cache(maxsize=128)\n",
    "    def fibonacci(n: int) -> int:\n",
    "        \"\"\"è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼ˆå¸¦ç¼“å­˜ï¼‰\"\"\"\n",
    "        if n < 2:\n",
    "            return n\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "    \n",
    "    # æ— ç¼“å­˜çš„ç‰ˆæœ¬\n",
    "    def fibonacci_no_cache(n: int) -> int:\n",
    "        \"\"\"è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼ˆæ— ç¼“å­˜ï¼‰\"\"\"\n",
    "        if n < 2:\n",
    "            return n\n",
    "        return fibonacci_no_cache(n-1) + fibonacci_no_cache(n-2)\n",
    "    \n",
    "    # æ€§èƒ½å¯¹æ¯”\n",
    "    test_n = 30\n",
    "    \n",
    "    print(f\"   è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ç¬¬{test_n}é¡¹:\")\n",
    "    \n",
    "    # æ— ç¼“å­˜ç‰ˆæœ¬\n",
    "    start_time = time.time()\n",
    "    result_no_cache = fibonacci_no_cache(test_n)\n",
    "    no_cache_time = time.time() - start_time\n",
    "    \n",
    "    # æœ‰ç¼“å­˜ç‰ˆæœ¬\n",
    "    start_time = time.time()\n",
    "    result_with_cache = fibonacci(test_n)\n",
    "    with_cache_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   æ— ç¼“å­˜ç»“æœ: {result_no_cache}, è€—æ—¶: {no_cache_time:.4f}ç§’\")\n",
    "    print(f\"   æœ‰ç¼“å­˜ç»“æœ: {result_with_cache}, è€—æ—¶: {with_cache_time:.4f}ç§’\")\n",
    "    print(f\"   æ€§èƒ½æå‡: {no_cache_time/with_cache_time:.1f}å€\")\n",
    "    \n",
    "    # ç¼“å­˜ä¿¡æ¯\n",
    "    print(f\"\\n   ç¼“å­˜ä¿¡æ¯:\")\n",
    "    print(f\"   ç¼“å­˜å‘½ä¸­ç‡: {fibonacci.cache_info().hits}\")\n",
    "    print(f\"   ç¼“å­˜æœªå‘½ä¸­: {fibonacci.cache_info().misses}\")\n",
    "    print(f\"   ç¼“å­˜å¤§å°: {fibonacci.cache_info().currsize}\")\n",
    "    print(f\"   æœ€å¤§ç¼“å­˜: {fibonacci.cache_info().maxsize}\")\n",
    "    \n",
    "    # æ¸…ç©ºç¼“å­˜\n",
    "    fibonacci.cache_clear()\n",
    "    print(f\"   æ¸…ç©ºç¼“å­˜åå¤§å°: {fibonacci.cache_info().currsize}\")\n",
    "\n",
    "demonstrate_lru_cache_basic()\n",
    "\n",
    "# 1.2 LangChain Embeddingç¼“å­˜åº”ç”¨\n",
    "print(f\"\\n   ğŸ¤– 1.2 LangChain Embeddingç¼“å­˜åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingCache:\n",
    "    \"\"\"Embeddingç¼“å­˜å™¨ - æ¨¡æ‹ŸLangChainçš„å‘é‡ç¼“å­˜\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 1000):\n",
    "        self.max_size = max_size\n",
    "        self.cache_stats = {'hits': 0, 'misses': 0}\n",
    "    \n",
    "    def _get_text_hash(self, text: str) -> str:\n",
    "        \"\"\"è·å–æ–‡æœ¬å“ˆå¸Œ\"\"\"\n",
    "        return hashlib.md5(text.encode()).hexdigest()\n",
    "    \n",
    "    @functools.lru_cache(maxsize=1000)\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"è·å–æ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆå¸¦ç¼“å­˜ï¼‰\"\"\"\n",
    "        # æ¨¡æ‹Ÿè€—æ—¶çš„åµŒå…¥è®¡ç®—\n",
    "        time.sleep(0.01)  # æ¨¡æ‹Ÿè®¡ç®—å»¶è¿Ÿ\n",
    "        \n",
    "        # ç®€å•çš„æ¨¡æ‹ŸåµŒå…¥ï¼ˆå®é™…åº”ç”¨ä¸­è°ƒç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹ï¼‰\n",
    "        embedding = [hash(text) % 1000 / 1000.0 for _ in range(10)]\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def batch_get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"æ‰¹é‡è·å–åµŒå…¥å‘é‡\"\"\"\n",
    "        return [self.get_embedding(text) for text in texts]\n",
    "    \n",
    "    def get_cache_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–ç¼“å­˜ä¿¡æ¯\"\"\"\n",
    "        cache_info = self.get_embedding.cache_info()\n",
    "        return {\n",
    "            'hits': cache_info.hits,\n",
    "            'misses': cache_info.misses,\n",
    "            'hit_rate': cache_info.hits / (cache_info.hits + cache_info.misses) if (cache_info.hits + cache_info.misses) > 0 else 0,\n",
    "            'current_size': cache_info.currsize,\n",
    "            'max_size': cache_info.maxsize\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"æ¸…ç©ºç¼“å­˜\"\"\"\n",
    "        self.get_embedding.cache_clear()\n",
    "\n",
    "def demonstrate_embedding_cache():\n",
    "    \"\"\"æ¼”ç¤ºåµŒå…¥ç¼“å­˜å™¨\"\"\"\n",
    "    cache = EmbeddingCache(max_size=100)\n",
    "    \n",
    "    # æµ‹è¯•æ–‡æœ¬\n",
    "    test_texts = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"What is LangChain?\",\n",
    "        \"Hello, how are you?\",  # é‡å¤æ–‡æœ¬\n",
    "        \"Python programming\",\n",
    "        \"What is LangChain?\",  # é‡å¤æ–‡æœ¬\n",
    "        \"Machine learning\",\n",
    "        \"Hello, how are you?\",  # é‡å¤æ–‡æœ¬\n",
    "    ]\n",
    "    \n",
    "    print(f\"   Embeddingç¼“å­˜æ¼”ç¤º:\")\n",
    "    \n",
    "    # é€ä¸ªè·å–åµŒå…¥\n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        start_time = time.time()\n",
    "        embedding = cache.get_embedding(text)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\n   {i}. æ–‡æœ¬: '{text}'\")\n",
    "        print(f\"      åµŒå…¥ç»´åº¦: {len(embedding)}\")\n",
    "        print(f\"      è€—æ—¶: {(end_time - start_time)*1000:.2f}ms\")\n",
    "        print(f\"      åµŒå…¥å‰5ç»´: {embedding[:5]}\")\n",
    "    \n",
    "    # æ‰¹é‡è·å–\n",
    "    print(f\"\\n   æ‰¹é‡è·å–åµŒå…¥:\")\n",
    "    batch_texts = [\"Batch test 1\", \"Batch test 2\", \"Batch test 1\"]\n",
    "    start_time = time.time()\n",
    "    batch_embeddings = cache.batch_get_embeddings(batch_texts)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"   æ‰¹é‡å¤„ç†{len(batch_texts)}ä¸ªæ–‡æœ¬ï¼Œè€—æ—¶: {(end_time - start_time)*1000:.2f}ms\")\n",
    "    \n",
    "    # ç¼“å­˜ç»Ÿè®¡\n",
    "    cache_info = cache.get_cache_info()\n",
    "    print(f\"\\n   ç¼“å­˜ç»Ÿè®¡:\")\n",
    "    print(f\"   å‘½ä¸­æ¬¡æ•°: {cache_info['hits']}\")\n",
    "    print(f\"   æœªå‘½ä¸­æ¬¡æ•°: {cache_info['misses']}\")\n",
    "    print(f\"   å‘½ä¸­ç‡: {cache_info['hit_rate']:.2%}\")\n",
    "    print(f\"   å½“å‰ç¼“å­˜å¤§å°: {cache_info['current_size']}\")\n",
    "\n",
    "demonstrate_embedding_cache()\n",
    "\n",
    "# 2. partial - åå‡½æ•°\n",
    "print(f\"\\nğŸ“ 2. partial - åå‡½æ•°:\")\n",
    "\n",
    "# 2.1 åŸºç¡€åå‡½æ•°æ“ä½œ\n",
    "print(f\"\\n   ğŸ”§ 2.1 åŸºç¡€åå‡½æ•°æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_partial_basic():\n",
    "    \"\"\"æ¼”ç¤ºpartialåŸºç¡€æ“ä½œ\"\"\"\n",
    "    \n",
    "    # åŸå§‹å‡½æ•°\n",
    "    def greet(greeting: str, name: str, punctuation: str = '!'):\n",
    "        return f\"{greeting}, {name}{punctuation}\"\n",
    "    \n",
    "    print(f\"   åŸå§‹å‡½æ•°è°ƒç”¨:\")\n",
    "    print(f\"   {greet('Hello', 'Alice')}\")\n",
    "    print(f\"   {greet('Hi', 'Bob', '.')}\")\n",
    "    \n",
    "    # åˆ›å»ºåå‡½æ•°\n",
    "    hello_greet = functools.partial(greet, 'Hello')\n",
    "    hi_greet = functools.partial(greet, 'Hi', punctuation='.')\n",
    "    excited_greet = functools.partial(greet, punctuation='!!!')\n",
    "    \n",
    "    print(f\"\\n   åå‡½æ•°è°ƒç”¨:\")\n",
    "    print(f\"   hello_greet('Alice'): {hello_greet('Alice')}\")\n",
    "    print(f\"   hi_greet('Bob'): {hi_greet('Bob')}\")\n",
    "    print(f\"   excited_greet('Hey', 'Charlie'): {excited_greet('Hey', 'Charlie')}\")\n",
    "    \n",
    "    # æŸ¥çœ‹åå‡½æ•°å±æ€§\n",
    "    print(f\"\\n   åå‡½æ•°å±æ€§:\")\n",
    "    print(f\"   hello_greet.func: {hello_greet.func.__name__}\")\n",
    "    print(f\"   hello_greet.args: {hello_greet.args}\")\n",
    "    print(f\"   hello_greet.keywords: {hello_greet.keywords}\")\n",
    "    \n",
    "    # å®é™…åº”ç”¨ï¼šæ•°å­¦å‡½æ•°\n",
    "    def power(base: float, exponent: float) -> float:\n",
    "        return base ** exponent\n",
    "    \n",
    "    square = functools.partial(power, exponent=2)\n",
    "    cube = functools.partial(power, exponent=3)\n",
    "    \n",
    "    print(f\"\\n   æ•°å­¦å‡½æ•°ååº”ç”¨:\")\n",
    "    print(f\"   square(5) = {square(5)}\")\n",
    "    print(f\"   cube(3) = {cube(3)}\")\n",
    "\n",
    "demonstrate_partial_basic()\n",
    "\n",
    "# 2.2 LangChainæç¤ºæ¨¡æ¿åº”ç”¨\n",
    "print(f\"\\n   ğŸ“ 2.2 LangChainæç¤ºæ¨¡æ¿åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class PromptTemplate:\n",
    "    \"\"\"æç¤ºæ¨¡æ¿ç±» - æ¨¡æ‹ŸLangChainçš„æç¤ºæ¨¡æ¿\"\"\"\n",
    "    \n",
    "    template: str\n",
    "    input_variables: List[str]\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        \"\"\"æ ¼å¼åŒ–æ¨¡æ¿\"\"\"\n",
    "        return self.template.format(**kwargs)\n",
    "    \n",
    "    def partial(self, **kwargs) -> 'PromptTemplate':\n",
    "        \"\"\"åˆ›å»ºéƒ¨åˆ†å¡«å……çš„æ¨¡æ¿\"\"\"\n",
    "        # é¢„å¡«å……æ¨¡æ¿\n",
    "        partial_template = self.template.format(**kwargs)\n",
    "        \n",
    "        # ç§»é™¤å·²å¡«å……çš„å˜é‡\n",
    "        remaining_vars = [var for var in self.input_variables if var not in kwargs]\n",
    "        \n",
    "        return PromptTemplate(\n",
    "            template=partial_template,\n",
    "            input_variables=remaining_vars\n",
    "        )\n",
    "\n",
    "class PromptManager:\n",
    "    \"\"\"æç¤ºç®¡ç†å™¨ - ä½¿ç”¨partialç®¡ç†æç¤ºæ¨¡æ¿\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = {}\n",
    "        self.partial_templates = {}\n",
    "    \n",
    "    def add_template(self, name: str, template: str, input_variables: List[str]):\n",
    "        \"\"\"æ·»åŠ æ¨¡æ¿\"\"\"\n",
    "        self.templates[name] = PromptTemplate(template, input_variables)\n",
    "    \n",
    "    def create_partial_template(self, template_name: str, partial_name: str, **kwargs):\n",
    "        \"\"\"åˆ›å»ºéƒ¨åˆ†å¡«å……æ¨¡æ¿\"\"\"\n",
    "        if template_name in self.templates:\n",
    "            template = self.templates[template_name]\n",
    "            self.partial_templates[partial_name] = template.partial(**kwargs)\n",
    "    \n",
    "    def format_prompt(self, template_name: str, **kwargs) -> str:\n",
    "        \"\"\"æ ¼å¼åŒ–æç¤º\"\"\"\n",
    "        # ä¼˜å…ˆä½¿ç”¨éƒ¨åˆ†æ¨¡æ¿\n",
    "        if template_name in self.partial_templates:\n",
    "            template = self.partial_templates[template_name]\n",
    "        elif template_name in self.templates:\n",
    "            template = self.templates[template_name]\n",
    "        else:\n",
    "            raise ValueError(f\"æ¨¡æ¿ '{template_name}' ä¸å­˜åœ¨\")\n",
    "        \n",
    "        return template.format(**kwargs)\n",
    "\n",
    "def demonstrate_prompt_manager():\n",
    "    \"\"\"æ¼”ç¤ºæç¤ºç®¡ç†å™¨\"\"\"\n",
    "    manager = PromptManager()\n",
    "    \n",
    "    # æ·»åŠ åŸºç¡€æ¨¡æ¿\n",
    "    manager.add_template(\n",
    "        \"qa_template\",\n",
    "        \"ä½ æ˜¯{name}ï¼Œä¸€ä¸ªä¸“ä¸šçš„{role}ã€‚è¯·å›ç­”ï¼š{question}\",\n",
    "        [\"name\", \"role\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    manager.add_template(\n",
    "        \"code_template\",\n",
    "        \"è¯·ç”¨{language}ç¼–å†™ä¸€ä¸ª{function_type}å‡½æ•°ï¼š{description}\",\n",
    "        [\"language\", \"function_type\", \"description\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"   æç¤ºæ¨¡æ¿ç®¡ç†æ¼”ç¤º:\")\n",
    "    \n",
    "    # åŸºç¡€æ ¼å¼åŒ–\n",
    "    print(f\"\\n   åŸºç¡€æ¨¡æ¿æ ¼å¼åŒ–:\")\n",
    "    qa_prompt = manager.format_prompt(\n",
    "        \"qa_template\",\n",
    "        name=\"AIåŠ©æ‰‹\",\n",
    "        role=\"Pythonä¸“å®¶\",\n",
    "        question=\"ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ\"\n",
    "    )\n",
    "    print(f\"   {qa_prompt}\")\n",
    "    \n",
    "    # åˆ›å»ºéƒ¨åˆ†æ¨¡æ¿\n",
    "    print(f\"\\n   åˆ›å»ºéƒ¨åˆ†æ¨¡æ¿:\")\n",
    "    manager.create_partial_template(\n",
    "        \"qa_template\",\n",
    "        \"python_expert_qa\",\n",
    "        name=\"Pythonä¸“å®¶\",\n",
    "        role=\"Pythonç¼–ç¨‹ä¸“å®¶\"\n",
    "    )\n",
    "    \n",
    "    manager.create_partial_template(\n",
    "        \"code_template\",\n",
    "        \"python_code\",\n",
    "        language=\"Python\"\n",
    "    )\n",
    "    \n",
    "    # ä½¿ç”¨éƒ¨åˆ†æ¨¡æ¿\n",
    "    print(f\"\\n   ä½¿ç”¨éƒ¨åˆ†æ¨¡æ¿:\")\n",
    "    python_qa = manager.format_prompt(\n",
    "        \"python_expert_qa\",\n",
    "        question=\"å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½ï¼Ÿ\"\n",
    "    )\n",
    "    print(f\"   {python_qa}\")\n",
    "    \n",
    "    python_code = manager.format_prompt(\n",
    "        \"python_code\",\n",
    "        function_type=\"æ’åº\",\n",
    "        description=\"å¯¹åˆ—è¡¨è¿›è¡Œå¿«é€Ÿæ’åº\"\n",
    "    )\n",
    "    print(f\"   {python_code}\")\n",
    "\n",
    "demonstrate_prompt_manager()\n",
    "\n",
    "# 3. wraps - è£…é¥°å™¨å·¥å…·\n",
    "print(f\"\\nğŸ“ 3. wraps - è£…é¥°å™¨å·¥å…·:\")\n",
    "\n",
    "# 3.1 åŸºç¡€è£…é¥°å™¨æ“ä½œ\n",
    "print(f\"\\n   ğŸ¨ 3.1 åŸºç¡€è£…é¥°å™¨æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_wraps_basic():\n",
    "    \"\"\"æ¼”ç¤ºwrapsåŸºç¡€æ“ä½œ\"\"\"\n",
    "    \n",
    "    # ä¸ä½¿ç”¨wrapsçš„è£…é¥°å™¨\n",
    "    def timer_no_wraps(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end = time.time()\n",
    "            print(f\"å‡½æ•° {func.__name__} è€—æ—¶: {end - start:.4f}ç§’\")\n",
    "            return result\n",
    "        return wrapper\n",
    "    \n",
    "    # ä½¿ç”¨wrapsçš„è£…é¥°å™¨\n",
    "    def timer_with_wraps(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end = time.time()\n",
    "            print(f\"å‡½æ•° {func.__name__} è€—æ—¶: {end - start:.4f}ç§’\")\n",
    "            return result\n",
    "        return wrapper\n",
    "    \n",
    "    # æµ‹è¯•å‡½æ•°\n",
    "    @timer_no_wraps\n",
    "    def test_function_no_wraps():\n",
    "        \"\"\"æµ‹è¯•å‡½æ•°ï¼ˆæ— wrapsï¼‰\"\"\"\n",
    "        time.sleep(0.1)\n",
    "        return \"æµ‹è¯•å®Œæˆ\"\n",
    "    \n",
    "    @timer_with_wraps\n",
    "    def test_function_with_wraps():\n",
    "        \"\"\"æµ‹è¯•å‡½æ•°ï¼ˆæœ‰wrapsï¼‰\"\"\"\n",
    "        time.sleep(0.1)\n",
    "        return \"æµ‹è¯•å®Œæˆ\"\n",
    "    \n",
    "    print(f\"   ä¸ä½¿ç”¨@wraps:\")\n",
    "    print(f\"   å‡½æ•°å: {test_function_no_wraps.__name__}\")\n",
    "    print(f\"   æ–‡æ¡£å­—ç¬¦ä¸²: {test_function_no_wraps.__doc__}\")\n",
    "    \n",
    "    print(f\"\\n   ä½¿ç”¨@wraps:\")\n",
    "    print(f\"   å‡½æ•°å: {test_function_with_wraps.__name__}\")\n",
    "    print(f\"   æ–‡æ¡£å­—ç¬¦ä¸²: {test_function_with_wraps.__doc__}\")\n",
    "    \n",
    "    # è°ƒç”¨å‡½æ•°\n",
    "    print(f\"\\n   å‡½æ•°è°ƒç”¨:\")\n",
    "    test_function_no_wraps()\n",
    "    test_function_with_wraps()\n",
    "\n",
    "demonstrate_wraps_basic()\n",
    "\n",
    "# 3.2 LangChain APIè°ƒç”¨è£…é¥°å™¨\n",
    "print(f\"\\n   ğŸŒ 3.2 LangChain APIè°ƒç”¨è£…é¥°å™¨:\")\n",
    "\n",
    "def api_call_decorator(max_retries: int = 3, delay: float = 1.0):\n",
    "    \"\"\"APIè°ƒç”¨è£…é¥°å™¨\"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    print(f\"   å°è¯•è°ƒç”¨ {func.__name__} (ç¬¬{attempt + 1}æ¬¡)\")\n",
    "                    result = func(*args, **kwargs)\n",
    "                    print(f\"   {func.__name__} è°ƒç”¨æˆåŠŸ\")\n",
    "                    return result\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    print(f\"   {func.__name__} è°ƒç”¨å¤±è´¥: {str(e)}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(delay)\n",
    "            \n",
    "            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥\n",
    "            print(f\"   {func.__name__} æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥\")\n",
    "            raise last_exception\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def rate_limiter(calls_per_second: float):\n",
    "    \"\"\"é€Ÿç‡é™åˆ¶è£…é¥°å™¨\"\"\"\n",
    "    min_interval = 1.0 / calls_per_second\n",
    "    last_call = [0.0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹\n",
    "    \n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - last_call[0]\n",
    "            \n",
    "            if elapsed < min_interval:\n",
    "                sleep_time = min_interval - elapsed\n",
    "                print(f\"   é€Ÿç‡é™åˆ¶ï¼šç­‰å¾… {sleep_time:.2f} ç§’\")\n",
    "                time.sleep(sleep_time)\n",
    "            \n",
    "            last_call[0] = time.time()\n",
    "            return func(*args, **kwargs)\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# æ¨¡æ‹ŸAPIå‡½æ•°\n",
    "@api_call_decorator(max_retries=3, delay=0.5)\n",
    "@rate_limiter(calls_per_second=2)\n",
    "def mock_api_call(endpoint: str, data: dict) -> dict:\n",
    "    \"\"\"æ¨¡æ‹ŸAPIè°ƒç”¨\"\"\"\n",
    "    # æ¨¡æ‹Ÿéšæœºå¤±è´¥\n",
    "    if random.random() < 0.3:  # 30%å¤±è´¥ç‡\n",
    "        raise ConnectionError(f\"æ— æ³•è¿æ¥åˆ° {endpoint}\")\n",
    "    \n",
    "    # æ¨¡æ‹ŸAPIå“åº”\n",
    "    return {\n",
    "        'endpoint': endpoint,\n",
    "        'status': 'success',\n",
    "        'data': data,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "\n",
    "def demonstrate_api_decorators():\n",
    "    \"\"\"æ¼”ç¤ºAPIè£…é¥°å™¨\"\"\"\n",
    "    print(f\"   APIè°ƒç”¨è£…é¥°å™¨æ¼”ç¤º:\")\n",
    "    \n",
    "    # æµ‹è¯•APIè°ƒç”¨\n",
    "    test_endpoints = [\n",
    "        (\"/api/chat\", {\"message\": \"Hello\"}),\n",
    "        (\"/api/embeddings\", {\"text\": \"Test text\"}),\n",
    "        (\"/api/complete\", {\"prompt\": \"Complete this\"})\n",
    "    ]\n",
    "    \n",
    "    for endpoint, data in test_endpoints:\n",
    "        try:\n",
    "            result = mock_api_call(endpoint, data)\n",
    "            print(f\"   æˆåŠŸ: {result['status']}, ç«¯ç‚¹: {result['endpoint']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   æœ€ç»ˆå¤±è´¥: {str(e)}\")\n",
    "        print()\n",
    "\n",
    "demonstrate_api_decorators()\n",
    "\n",
    "print(f\"\\nâœ… functoolsæ¨¡å—å‡½æ•°å·¥å…·å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡lru_cacheçš„ç¼“å­˜æœºåˆ¶\")\n",
    "print(f\"   âœ“ ç†è§£partialçš„åå‡½æ•°åº”ç”¨\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨wrapsè£…é¥°å™¨å·¥å…·\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿåº”ç”¨functoolsè¿›è¡Œå‡½æ•°ä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### itertoolså’Œcontextlibæ¨¡å—å·¥å…· [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šitertoolsæ¨¡å—æä¾›äº†é«˜æ•ˆçš„è¿­ä»£å™¨å·¥å…·ï¼Œæ”¯æŒå†…å­˜å‹å¥½çš„æ•°æ®å¤„ç†ï¼›contextlibæ¨¡å—ç®€åŒ–äº†ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„åˆ›å»ºã€‚åœ¨LangChainåº”ç”¨ä¸­ï¼Œè¿™äº›æ¨¡å—æ”¯æŒæµå¼æ•°æ®å¤„ç†å’Œèµ„æºç®¡ç†ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡itertoolsæ¨¡å—çš„è¿­ä»£å™¨å·¥å…·\n",
    "- ç†è§£æµå¼æ•°æ®å¤„ç†æ¨¡å¼\n",
    "- ç†Ÿç»ƒä½¿ç”¨contextlibåˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨\n",
    "- èƒ½å¤Ÿè¿›è¡Œèµ„æºç®¡ç†å’Œæ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æµå¼å¤„ç†åº”ç”¨\n",
    "- è¿›è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨è®¾è®¡ç»ƒä¹ \n",
    "- åº”ç”¨å·¥å…·æ¨¡å—è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„æµå¼å¤„ç†ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ itertoolså’Œcontextlibæ¨¡å—å·¥å…·:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import itertools\n",
    "import contextlib\n",
    "import time\n",
    "import random\n",
    "from typing import Iterator, List, Any, Optional, Generator, Dict\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "# 1. itertools - è¿­ä»£å™¨å·¥å…·\n",
    "print(f\"ğŸ“ 1. itertools - è¿­ä»£å™¨å·¥å…·:\")\n",
    "\n",
    "# 1.1 åŸºç¡€è¿­ä»£å™¨æ“ä½œ\n",
    "print(f\"\\n   ğŸ”¢ 1.1 åŸºç¡€è¿­ä»£å™¨æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_itertools_basic():\n",
    "    \"\"\"æ¼”ç¤ºitertoolsåŸºç¡€æ“ä½œ\"\"\"\n",
    "    \n",
    "    # chain - è¿æ¥å¤šä¸ªè¿­ä»£å™¨\n",
    "    print(f\"   chain - è¿æ¥è¿­ä»£å™¨:\")\n",
    "    list1 = [1, 2, 3]\n",
    "    list2 = [4, 5, 6]\n",
    "    list3 = [7, 8, 9]\n",
    "    \n",
    "    chained = itertools.chain(list1, list2, list3)\n",
    "    print(f\"   è¿æ¥ç»“æœ: {list(chained)}\")\n",
    "    \n",
    "    # islice - åˆ‡ç‰‡è¿­ä»£å™¨\n",
    "    print(f\"\\n   islice - åˆ‡ç‰‡è¿­ä»£å™¨:\")\n",
    "    numbers = range(20)\n",
    "    sliced = itertools.islice(numbers, 2, 15, 3)  # ä»2å¼€å§‹ï¼Œåˆ°15ï¼Œæ­¥é•¿3\n",
    "    print(f\"   åˆ‡ç‰‡ç»“æœ: {list(sliced)}\")\n",
    "    \n",
    "    # count - æ— é™è®¡æ•°\n",
    "    print(f\"\\n   count - æ— é™è®¡æ•°:\")\n",
    "    counter = itertools.count(10, 2)  # ä»10å¼€å§‹ï¼Œæ­¥é•¿2\n",
    "    print(f\"   å‰5ä¸ªè®¡æ•°: {list(itertools.islice(counter, 5))}\")\n",
    "    \n",
    "    # cycle - æ— é™å¾ªç¯\n",
    "    print(f\"\\n   cycle - æ— é™å¾ªç¯:\")\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    cycler = itertools.cycle(colors)\n",
    "    print(f\"   å‰8ä¸ªå¾ªç¯: {list(itertools.islice(cycler, 8))}\")\n",
    "    \n",
    "    # repeat - é‡å¤å…ƒç´ \n",
    "    print(f\"\\n   repeat - é‡å¤å…ƒç´ :\")\n",
    "    repeater = itertools.repeat('hello', 3)\n",
    "    print(f\"   é‡å¤3æ¬¡: {list(repeater)}\")\n",
    "    \n",
    "    # combinations - ç»„åˆ\n",
    "    print(f\"\\n   combinations - ç»„åˆ:\")\n",
    "    items = ['A', 'B', 'C', 'D']\n",
    "    comb = itertools.combinations(items, 2)\n",
    "    print(f\"   2ä¸ªå…ƒç´ çš„ç»„åˆ: {list(comb)}\")\n",
    "    \n",
    "    # permutations - æ’åˆ—\n",
    "    print(f\"\\n   permutations - æ’åˆ—:\")\n",
    "    perm = itertools.permutations(items, 2)\n",
    "    print(f\"   2ä¸ªå…ƒç´ çš„æ’åˆ—: {list(perm)}\")\n",
    "    \n",
    "    # product - ç¬›å¡å°”ç§¯\n",
    "    print(f\"\\n   product - ç¬›å¡å°”ç§¯:\")\n",
    "    list_a = [1, 2]\n",
    "    list_b = ['x', 'y']\n",
    "    prod = itertools.product(list_a, list_b)\n",
    "    print(f\"   ç¬›å¡å°”ç§¯: {list(prod)}\")\n",
    "\n",
    "demonstrate_itertools_basic()\n",
    "\n",
    "# 1.2 LangChainæµå¼æ•°æ®å¤„ç†åº”ç”¨\n",
    "print(f\"\\n   ğŸŒŠ 1.2 LangChainæµå¼æ•°æ®å¤„ç†åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class StreamProcessor:\n",
    "    \"\"\"æµå¼æ•°æ®å¤„ç†å™¨ - æ¨¡æ‹ŸLangChainçš„tokenæµå¤„ç†\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 3):\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def tokenize_stream(self, text: str) -> Iterator[str]:\n",
    "        \"\"\"å°†æ–‡æœ¬æµå¼tokenåŒ–\"\"\"\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            yield word\n",
    "            time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ\n",
    "    \n",
    "    def batch_tokens(self, token_stream: Iterator[str]) -> Iterator[List[str]]:\n",
    "        \"\"\"å°†tokenåˆ†æ‰¹å¤„ç†\"\"\"\n",
    "        # ä½¿ç”¨itertools.isliceè¿›è¡Œæ‰¹å¤„ç†\n",
    "        while True:\n",
    "            batch = list(itertools.islice(token_stream, self.batch_size))\n",
    "            if not batch:\n",
    "                break\n",
    "            yield batch\n",
    "    \n",
    "    def process_with_window(self, token_stream: Iterator[str], window_size: int = 2) -> Iterator[List[str]]:\n",
    "        \"\"\"æ»‘åŠ¨çª—å£å¤„ç†\"\"\"\n",
    "        # ä½¿ç”¨itertools.teeåˆ›å»ºå¤šä¸ªè¿­ä»£å™¨\n",
    "        iterators = itertools.tee(token_stream, window_size)\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªè¿­ä»£å™¨è·³è¿‡ç›¸åº”æ•°é‡çš„å…ƒç´ \n",
    "        for i, iterator in enumerate(iterators):\n",
    "            next(itertools.islice(iterator, i, i), None)\n",
    "        \n",
    "        # ä½¿ç”¨zipç»„åˆçª—å£\n",
    "        yield from zip(*iterators)\n",
    "    \n",
    "    def filter_tokens(self, token_stream: Iterator[str], min_length: int = 3) -> Iterator[str]:\n",
    "        \"\"\"è¿‡æ»¤token\"\"\"\n",
    "        return filter(lambda token: len(token) >= min_length, token_stream)\n",
    "    \n",
    "    def limit_tokens(self, token_stream: Iterator[str], max_tokens: int) -> Iterator[str]:\n",
    "        \"\"\"é™åˆ¶tokenæ•°é‡\"\"\"\n",
    "        return itertools.islice(token_stream, max_tokens)\n",
    "\n",
    "def demonstrate_stream_processor():\n",
    "    \"\"\"æ¼”ç¤ºæµå¼æ•°æ®å¤„ç†å™¨\"\"\"\n",
    "    processor = StreamProcessor(batch_size=2)\n",
    "    \n",
    "    # æµ‹è¯•æ–‡æœ¬\n",
    "    text = \"This is a sample text for streaming token processing demonstration\"\n",
    "    \n",
    "    print(f\"   æµå¼æ•°æ®å¤„ç†æ¼”ç¤º:\")\n",
    "    print(f\"   åŸå§‹æ–‡æœ¬: {text}\")\n",
    "    \n",
    "    # 1. åŸºç¡€tokenæµ\n",
    "    print(f\"\\n   1. åŸºç¡€tokenæµ:\")\n",
    "    token_stream = processor.tokenize_stream(text)\n",
    "    tokens = list(token_stream)\n",
    "    print(f\"   Tokens: {tokens}\")\n",
    "    \n",
    "    # 2. æ‰¹å¤„ç†\n",
    "    print(f\"\\n   2. æ‰¹å¤„ç† (batch_size={processor.batch_size}):\")\n",
    "    token_stream = processor.tokenize_stream(text)\n",
    "    batches = list(processor.batch_tokens(token_stream))\n",
    "    print(f\"   æ‰¹æ¬¡: {batches}\")\n",
    "    \n",
    "    # 3. æ»‘åŠ¨çª—å£\n",
    "    print(f\"\\n   3. æ»‘åŠ¨çª—å£ (window_size=2):\")\n",
    "    token_stream = processor.tokenize_stream(text)\n",
    "    windows = list(processor.process_with_window(token_stream, 2))\n",
    "    print(f\"   çª—å£: {windows}\")\n",
    "    \n",
    "    # 4. è¿‡æ»¤å’Œé™åˆ¶\n",
    "    print(f\"\\n   4. è¿‡æ»¤å’Œé™åˆ¶:\")\n",
    "    token_stream = processor.tokenize_stream(text)\n",
    "    filtered = processor.filter_tokens(token_stream, min_length=4)\n",
    "    limited = processor.limit_tokens(filtered, max_tokens=5)\n",
    "    result = list(limited)\n",
    "    print(f\"   è¿‡æ»¤åçš„å‰5ä¸ªtoken (é•¿åº¦>=4): {result}\")\n",
    "    \n",
    "    # 5. ç»„åˆå¤„ç†é“¾\n",
    "    print(f\"\\n   5. ç»„åˆå¤„ç†é“¾:\")\n",
    "    token_stream = processor.tokenize_stream(text)\n",
    "    \n",
    "    # åˆ›å»ºå¤„ç†é“¾ï¼šè¿‡æ»¤ -> é™åˆ¶ -> æ‰¹å¤„ç†\n",
    "    processing_chain = processor.batch_tokens(\n",
    "        processor.limit_tokens(\n",
    "            processor.filter_tokens(token_stream, min_length=3),\n",
    "            max_tokens=8\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    final_batches = list(processing_chain)\n",
    "    print(f\"   æœ€ç»ˆå¤„ç†ç»“æœ: {final_batches}\")\n",
    "\n",
    "demonstrate_stream_processor()\n",
    "\n",
    "# 2. contextlib - ä¸Šä¸‹æ–‡ç®¡ç†å™¨\n",
    "print(f\"\\nğŸ“ 2. contextlib - ä¸Šä¸‹æ–‡ç®¡ç†å™¨:\")\n",
    "\n",
    "# 2.1 åŸºç¡€ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ“ä½œ\n",
    "print(f\"\\n   ğŸ—ï¸ 2.1 åŸºç¡€ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_contextlib_basic():\n",
    "    \"\"\"æ¼”ç¤ºcontextlibåŸºç¡€æ“ä½œ\"\"\"\n",
    "    \n",
    "    # contextmanagerè£…é¥°å™¨\n",
    "    @contextlib.contextmanager\n",
    "    def simple_timer():\n",
    "        \"\"\"ç®€å•è®¡æ—¶å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"   è®¡æ—¶å¼€å§‹...\")\n",
    "        try:\n",
    "            yield start_time\n",
    "        finally:\n",
    "            end_time = time.time()\n",
    "            elapsed = end_time - start_time\n",
    "            print(f\"   è®¡æ—¶ç»“æŸï¼Œè€—æ—¶: {elapsed:.2f}ç§’\")\n",
    "    \n",
    "    # ä½¿ç”¨è®¡æ—¶å™¨\n",
    "    print(f\"   ä½¿ç”¨@contextmanageråˆ›å»ºè®¡æ—¶å™¨:\")\n",
    "    with simple_timer() as start:\n",
    "        time.sleep(0.5)\n",
    "        print(f\"   æ‰§è¡Œä»»åŠ¡ä¸­...\")\n",
    "    \n",
    "    # æ–‡ä»¶æ“ä½œä¸Šä¸‹æ–‡ç®¡ç†å™¨\n",
    "    @contextlib.contextmanager\n",
    "    def safe_file_write(filename: str):\n",
    "        \"\"\"å®‰å…¨æ–‡ä»¶å†™å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨\"\"\"\n",
    "        print(f\"   å‡†å¤‡å†™å…¥æ–‡ä»¶: {filename}\")\n",
    "        try:\n",
    "            file = open(filename, 'w', encoding='utf-8')\n",
    "            yield file\n",
    "        except Exception as e:\n",
    "            print(f\"   æ–‡ä»¶æ“ä½œå‡ºé”™: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            if 'file' in locals():\n",
    "                file.close()\n",
    "                print(f\"   æ–‡ä»¶å·²å…³é—­: {filename}\")\n",
    "    \n",
    "    print(f\"\\n   å®‰å…¨æ–‡ä»¶å†™å…¥æ¼”ç¤º:\")\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')\n",
    "    temp_file.close()\n",
    "    \n",
    "    try:\n",
    "        with safe_file_write(temp_file.name) as f:\n",
    "            f.write(\"Hello, contextlib!\")\n",
    "            print(f\"   æ•°æ®å†™å…¥æˆåŠŸ\")\n",
    "        \n",
    "        # éªŒè¯æ–‡ä»¶å†…å®¹\n",
    "        with open(temp_file.name, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            print(f\"   æ–‡ä»¶å†…å®¹: {content}\")\n",
    "    \n",
    "    finally:\n",
    "        if os.path.exists(temp_file.name):\n",
    "            os.unlink(temp_file.name)\n",
    "    \n",
    "    # nested - åµŒå¥—ä¸Šä¸‹æ–‡ç®¡ç†å™¨\n",
    "    print(f\"\\n   åµŒå¥—ä¸Šä¸‹æ–‡ç®¡ç†å™¨:\")\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def resource_manager(name: str):\n",
    "        print(f\"   è·å–èµ„æº: {name}\")\n",
    "        yield f\"èµ„æº_{name}\"\n",
    "        print(f\"   é‡Šæ”¾èµ„æº: {name}\")\n",
    "    \n",
    "    with contextlib.nested(\n",
    "        resource_manager(\"æ•°æ®åº“è¿æ¥\"),\n",
    "        resource_manager(\"æ–‡ä»¶å¥æŸ„\"),\n",
    "        resource_manager(\"ç½‘ç»œè¿æ¥\")\n",
    "    ) as (db, file_handle, network):\n",
    "        print(f\"   ä½¿ç”¨èµ„æº: {db}, {file_handle}, {network}\")\n",
    "\n",
    "demonstrate_contextlib_basic()\n",
    "\n",
    "# 2.2 LangChainèµ„æºç®¡ç†åº”ç”¨\n",
    "print(f\"\\n   ğŸ¤– 2.2 LangChainèµ„æºç®¡ç†åº”ç”¨:\")\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def api_session(api_key: str, timeout: float = 30.0):\n",
    "    \"\"\"APIä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨\"\"\"\n",
    "    session_info = {\n",
    "        'api_key': api_key,\n",
    "        'timeout': timeout,\n",
    "        'created_at': time.time(),\n",
    "        'active': True\n",
    "    }\n",
    "    \n",
    "    print(f\"   å»ºç«‹APIä¼šè¯: {api_key[:8]}...\")\n",
    "    \n",
    "    try:\n",
    "        yield session_info\n",
    "    except Exception as e:\n",
    "        print(f\"   APIä¼šè¯å‡ºé”™: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        session_info['active'] = False\n",
    "        session_info['closed_at'] = time.time()\n",
    "        print(f\"   å…³é—­APIä¼šè¯\")\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def memory_monitor(threshold_mb: float = 100.0):\n",
    "    \"\"\"å†…å­˜ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨\"\"\"\n",
    "    import gc\n",
    "    \n",
    "    # è·å–åˆå§‹å†…å­˜çŠ¶æ€\n",
    "    gc.collect()\n",
    "    initial_objects = len(gc.get_objects())\n",
    "    \n",
    "    print(f\"   å¼€å§‹å†…å­˜ç›‘æ§ï¼Œåˆå§‹å¯¹è±¡æ•°: {initial_objects}\")\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # æ£€æŸ¥æœ€ç»ˆå†…å­˜çŠ¶æ€\n",
    "        gc.collect()\n",
    "        final_objects = len(gc.get_objects())\n",
    "        object_increase = final_objects - initial_objects\n",
    "        \n",
    "        print(f\"   å†…å­˜ç›‘æ§ç»“æŸï¼Œæœ€ç»ˆå¯¹è±¡æ•°: {final_objects}\")\n",
    "        print(f\"   å¯¹è±¡å¢é•¿: {object_increase:+d}\")\n",
    "        \n",
    "        if object_increase > threshold_mb * 1000:  # ç®€åŒ–çš„é˜ˆå€¼æ£€æŸ¥\n",
    "            print(f\"   âš ï¸ å†…å­˜ä½¿ç”¨å¢é•¿è¾ƒå¤§ï¼Œå»ºè®®ä¼˜åŒ–\")\n",
    "\n",
    "@dataclass\n",
    "class LangChainResourceManager:\n",
    "    \"\"\"LangChainèµ„æºç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.active_resources = {}\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def model_session(self, model_name: str, max_tokens: int = 1000):\n",
    "        \"\"\"æ¨¡å‹ä¼šè¯ç®¡ç†\"\"\"\n",
    "        session_id = f\"{model_name}_{int(time.time())}\"\n",
    "        \n",
    "        session_info = {\n",
    "            'session_id': session_id,\n",
    "            'model_name': model_name,\n",
    "            'max_tokens': max_tokens,\n",
    "            'tokens_used': 0,\n",
    "            'created_at': time.time()\n",
    "        }\n",
    "        \n",
    "        self.active_resources[session_id] = session_info\n",
    "        print(f\"   å¯åŠ¨æ¨¡å‹ä¼šè¯: {model_name} (ID: {session_id})\")\n",
    "        \n",
    "        try:\n",
    "            yield session_info\n",
    "        finally:\n",
    "            session_info['closed_at'] = time.time()\n",
    "            session_info['duration'] = session_info['closed_at'] - session_info['created_at']\n",
    "            print(f\"   å…³é—­æ¨¡å‹ä¼šè¯: {model_name}, ä½¿ç”¨token: {session_info['tokens_used']}, è€—æ—¶: {session_info['duration']:.2f}ç§’\")\n",
    "            del self.active_resources[session_id]\n",
    "    \n",
    "    def simulate_token_usage(self, session_info: Dict, text: str):\n",
    "        \"\"\"æ¨¡æ‹Ÿtokenä½¿ç”¨\"\"\"\n",
    "        tokens = len(text.split())\n",
    "        session_info['tokens_used'] += tokens\n",
    "        return tokens\n",
    "    \n",
    "    def get_active_sessions(self) -> List[Dict]:\n",
    "        \"\"\"è·å–æ´»è·ƒä¼šè¯\"\"\"\n",
    "        return list(self.active_resources.values())\n",
    "\n",
    "def demonstrate_langchain_resource_manager():\n",
    "    \"\"\"æ¼”ç¤ºLangChainèµ„æºç®¡ç†å™¨\"\"\"\n",
    "    manager = LangChainResourceManager()\n",
    "    \n",
    "    print(f\"   LangChainèµ„æºç®¡ç†æ¼”ç¤º:\")\n",
    "    \n",
    "    # 1. APIä¼šè¯ç®¡ç†\n",
    "    print(f\"\\n   1. APIä¼šè¯ç®¡ç†:\")\n",
    "    try:\n",
    "        with api_session(\"sk-1234567890abcdef\", timeout=10.0) as session:\n",
    "            print(f\"   ä¼šè¯ä¿¡æ¯: {session}\")\n",
    "            # æ¨¡æ‹ŸAPIè°ƒç”¨\n",
    "            time.sleep(0.2)\n",
    "            print(f\"   æ‰§è¡ŒAPIè°ƒç”¨...\")\n",
    "    except Exception as e:\n",
    "        print(f\"   APIä¼šè¯å¼‚å¸¸: {e}\")\n",
    "    \n",
    "    # 2. å†…å­˜ç›‘æ§\n",
    "    print(f\"\\n   2. å†…å­˜ç›‘æ§:\")\n",
    "    with memory_monitor(threshold_mb=50):\n",
    "        # æ¨¡æ‹Ÿå†…å­˜å¯†é›†æ“ä½œ\n",
    "        data = [i for i in range(10000)]\n",
    "        processed = [x * 2 for x in data]\n",
    "        print(f\"   å¤„ç†äº†{len(processed)}ä¸ªæ•°æ®é¡¹\")\n",
    "    \n",
    "    # 3. æ¨¡å‹ä¼šè¯ç®¡ç†\n",
    "    print(f\"\\n   3. æ¨¡å‹ä¼šè¯ç®¡ç†:\")\n",
    "    \n",
    "    with manager.model_session(\"gpt-3.5-turbo\", max_tokens=100) as session1:\n",
    "        tokens1 = manager.simulate_token_usage(session1, \"Hello, how are you today?\")\n",
    "        print(f\"   ä¼šè¯1ä½¿ç”¨äº†{tokens1}ä¸ªtoken\")\n",
    "        \n",
    "        with manager.model_session(\"text-davinci-003\", max_tokens=200) as session2:\n",
    "            tokens2 = manager.simulate_token_usage(session2, \"What is the capital of France?\")\n",
    "            print(f\"   ä¼šè¯2ä½¿ç”¨äº†{tokens2}ä¸ªtoken\")\n",
    "            \n",
    "            tokens1_more = manager.simulate_token_usage(session1, \"Tell me more about Python.\")\n",
    "            print(f\"   ä¼šè¯1åˆä½¿ç”¨äº†{tokens1_more}ä¸ªtoken\")\n",
    "        \n",
    "        print(f\"   å½“å‰æ´»è·ƒä¼šè¯: {len(manager.get_active_sessions())}ä¸ª\")\n",
    "    \n",
    "    print(f\"   æ‰€æœ‰ä¼šè¯å·²å…³é—­ï¼Œæ´»è·ƒä¼šè¯: {len(manager.get_active_sessions())}ä¸ª\")\n",
    "\n",
    "demonstrate_langchain_resource_manager()\n",
    "\n",
    "print(f\"\\nâœ… itertoolså’Œcontextlibæ¨¡å—å·¥å…·å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡itertoolsçš„è¿­ä»£å™¨å·¥å…·\")\n",
    "print(f\"   âœ“ ç†è§£æµå¼æ•°æ®å¤„ç†æ¨¡å¼\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨contextlibåˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿè¿›è¡Œèµ„æºç®¡ç†å’Œæ€§èƒ½ä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å­¦ä¹ æ€»ç»“\n",
    "\n",
    "### âœ… çŸ¥è¯†æ¸…å•è¾¾æˆæƒ…å†µéªŒè¯\n",
    "\n",
    "**4.9 å¸¸ç”¨å·¥å…·æ¨¡å— [â­â­è¿›é˜¶]**\n",
    "- âœ… æŒæ¡collectionsæ¨¡å—çš„é«˜çº§æ•°æ®ç»“æ„\n",
    "- âœ… ç†è§£functoolsæ¨¡å—çš„å‡½æ•°å¼ç¼–ç¨‹å·¥å…·\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨itertoolsæ¨¡å—çš„è¿­ä»£å™¨å·¥å…·\n",
    "- âœ… èƒ½å¤Ÿåˆ›å»ºå’Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨\n",
    "- âœ… æŒæ¡ç¼“å­˜ä¼˜åŒ–å’Œæ€§èƒ½æå‡æŠ€å·§\n",
    "- âœ… ç†è§£æµå¼æ•°æ®å¤„ç†æ¨¡å¼\n",
    "- âœ… èƒ½å¤Ÿè¿›è¡Œèµ„æºç®¡ç†å’Œé”™è¯¯å¤„ç†\n",
    "- âœ… èƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„å·¥å…·åº”ç”¨ç³»ç»Ÿ\n",
    "\n",
    "### ğŸ¯ ä¸LangChainå­¦ä¹ çš„å…³è”\n",
    "\n",
    "**å·¥å…·æ¨¡å—é‡è¦æ€§**ï¼š\n",
    "- collectionsæ¨¡å—æ˜¯LangChainæ•°æ®å¤„ç†çš„åŸºç¡€\n",
    "- functoolsæ¨¡å—å¯¹LangChainçš„ç¼“å­˜å’Œä¼˜åŒ–è‡³å…³é‡è¦\n",
    "- itertoolsæ¨¡å—æ”¯æŒLangChainçš„æµå¼æ•°æ®å¤„ç†\n",
    "- contextlibæ¨¡å—ç¡®ä¿LangChainèµ„æºçš„æ­£ç¡®ç®¡ç†\n",
    "- å·¥å…·æ¨¡å—ç»„åˆä½¿ç”¨æå‡LangChainåº”ç”¨çš„æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§\n",
    "\n",
    "**å®é™…åº”ç”¨åœºæ™¯**ï¼š\n",
    "- LangChainçš„tokenè®¡æ•°å’Œæ–‡æœ¬å¤„ç†\n",
    "- LangChainçš„ç¼“å­˜æœºåˆ¶å’Œæ€§èƒ½ä¼˜åŒ–\n",
    "- LangChainçš„æµå¼å“åº”å’Œæ•°æ®å¤„ç†\n",
    "- LangChainçš„APIè¿æ¥å’Œèµ„æºç®¡ç†\n",
    "- LangChainçš„æç¤ºæ¨¡æ¿å’Œå‡½æ•°ç»„åˆ\n",
    "\n",
    "### ğŸ“š è¿›é˜¶å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹ å»ºè®®**ï¼š\n",
    "   - æ·±å…¥ç»ƒä¹ collectionsçš„å„ç§æ•°æ®ç»“æ„\n",
    "   - æŒæ¡functoolsçš„é«˜çº§è£…é¥°å™¨æ¨¡å¼\n",
    "   - ç†Ÿç»ƒä½¿ç”¨itertoolsè¿›è¡Œæ•°æ®å¤„ç†\n",
    "\n",
    "2. **æ‰©å±•å­¦ä¹ **ï¼š\n",
    "   - å­¦ä¹ æ›´å¤šPythonæ ‡å‡†åº“å·¥å…·æ¨¡å—\n",
    "   - äº†è§£ç¬¬ä¸‰æ–¹å·¥å…·åº“å¦‚more-itertools\n",
    "   - æ¢ç´¢å¼‚æ­¥ç¼–ç¨‹ä¸­çš„å·¥å…·æ¨¡å¼\n",
    "\n",
    "3. **å®é™…åº”ç”¨**ï¼š\n",
    "   - æ„å»ºæ™ºèƒ½çš„æ•°æ®å¤„ç†ç®¡é“\n",
    "   - å¼€å‘é«˜æ•ˆçš„ç¼“å­˜ç³»ç»Ÿ\n",
    "   - å®ç°æµå¼AIåº”ç”¨æ¡†æ¶\n",
    "\n",
    "### ğŸ”§ å¸¸è§é”™è¯¯ä¸æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **collectionsä½¿ç”¨é”™è¯¯**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šè¿‡åº¦ä½¿ç”¨Counter\n",
    "   counter = Counter()\n",
    "   for item in large_list:  # ç™¾ä¸‡çº§æ•°æ®\n",
    "       counter[item] += 1  # å¯èƒ½å†…å­˜ä¸è¶³\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨ç”Ÿæˆå™¨å’Œé™åˆ¶\n",
    "   counter = Counter()\n",
    "   for item in itertools.islice(large_list, 10000):  # é™åˆ¶å¤„ç†æ•°é‡\n",
    "       counter[item] += 1\n",
    "   ```\n",
    "\n",
    "2. **functoolsç¼“å­˜è¯¯ç”¨**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šç¼“å­˜ä¸å¯å“ˆå¸Œå¯¹è±¡\n",
    "   @lru_cache(maxsize=128)\n",
    "   def process_data(data: list):  # listä¸å¯å“ˆå¸Œ\n",
    "       return len(data)\n",
    "   \n",
    "   # æ­£ç¡®ï¼šç¼“å­˜å¯å“ˆå¸Œå¯¹è±¡æˆ–ä½¿ç”¨å­—ç¬¦ä¸²é”®\n",
    "   @lru_cache(maxsize=128)\n",
    "   def process_data(data_hash: str, data_length: int):\n",
    "       return data_length\n",
    "   ```\n",
    "\n",
    "3. **itertoolså†…å­˜æ³„æ¼**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šæ— é™è¿­ä»£å™¨ä¸é™åˆ¶\n",
    "   for item in itertools.count():  # æ— é™å¾ªç¯\n",
    "       process(item)  # å¯èƒ½æ°¸ä¸ç»“æŸ\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨isliceé™åˆ¶\n",
    "   for item in itertools.islice(itertools.count(), 1000):\n",
    "       process(item)  # åªå¤„ç†1000ä¸ª\n",
    "   ```\n",
    "\n",
    "4. **contextlibèµ„æºç®¡ç†**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå¿½ç•¥å¼‚å¸¸å¤„ç†\n",
    "   @contextmanager\n",
    "   def db_connection():\n",
    "       conn = create_connection()\n",
    "       yield conn\n",
    "       conn.close()  # å¦‚æœyieldå‡ºé”™ï¼Œè¿™é‡Œä¸ä¼šæ‰§è¡Œ\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨try-finally\n",
    "   @contextmanager\n",
    "   def db_connection():\n",
    "       conn = create_connection()\n",
    "       try:\n",
    "           yield conn\n",
    "       finally:\n",
    "           conn.close()  # ç¡®ä¿èµ„æºé‡Šæ”¾\n",
    "   ```\n",
    "\n",
    "5. **æ€§èƒ½ä¼˜åŒ–è¯¯åŒº**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šè¿‡åº¦ä¼˜åŒ–\n",
    "   @lru_cache(maxsize=1000000)  # ç¼“å­˜è¿‡å¤§\n",
    "   def simple_function(x):\n",
    "       return x * 2  # ç®€å•æ“ä½œä¸éœ€è¦ç¼“å­˜\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåˆç†ä½¿ç”¨ç¼“å­˜\n",
    "   @lru_cache(maxsize=128)\n",
    "   def expensive_computation(x):\n",
    "       # å¤æ‚è®¡ç®—æ‰éœ€è¦ç¼“å­˜\n",
    "       return sum(i**2 for i in range(x))\n",
    "   ```\n",
    "\n",
    "6. **è¿­ä»£å™¨é“¾å¼è°ƒç”¨**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šé‡å¤æ¶ˆè´¹è¿­ä»£å™¨\n",
    "   data_stream = (x for x in range(100))\n",
    "   result1 = list(data_stream)  # æ¶ˆè´¹è¿­ä»£å™¨\n",
    "   result2 = list(data_stream)  # ç©ºåˆ—è¡¨ï¼\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨teeæˆ–é‡æ–°åˆ›å»º\n",
    "   data_stream1, data_stream2 = itertools.tee(x for x in range(100))\n",
    "   result1 = list(data_stream1)\n",
    "   result2 = list(data_stream2)\n",
    "   ```\n",
    "\n",
    "### ğŸŒ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "**collectionsä¼˜åŒ–**ï¼š\n",
    "- é€‰æ‹©åˆé€‚çš„æ•°æ®ç»“æ„ï¼ˆCounter vs defaultdict vs dequeï¼‰\n",
    "- æ§åˆ¶dequeçš„maxsizeé¿å…å†…å­˜æº¢å‡º\n",
    "- ä½¿ç”¨defaultdictå‡å°‘æ¡ä»¶åˆ¤æ–­\n",
    "\n",
    "**functoolsä¼˜åŒ–**ï¼š\n",
    "- åˆç†è®¾ç½®lru_cacheçš„maxsize\n",
    "   ```python\n",
    "   # æ ¹æ®å†…å­˜å’Œè®¿é—®æ¨¡å¼è®¾ç½®\n",
    "   @lru_cache(maxsize=128)  # å°æ•°æ®é›†\n",
    "   @lru_cache(maxsize=1024)  # ä¸­ç­‰æ•°æ®é›†\n",
    "   @lru_cache(maxsize=None)  # æ— é™åˆ¶ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰\n",
    "   ```\n",
    "- ä½¿ç”¨partialç®€åŒ–å‡½æ•°è°ƒç”¨\n",
    "- åˆ©ç”¨wrapsä¿æŒè£…é¥°å™¨çš„å…ƒä¿¡æ¯\n",
    "\n",
    "**itertoolsä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨ç”Ÿæˆå™¨é¿å…å†…å­˜å ç”¨\n",
    "- åˆç†ä½¿ç”¨isliceé™åˆ¶å¤„ç†èŒƒå›´\n",
    "- ç»„åˆå¤šä¸ªè¿­ä»£å™¨å·¥å…·æ„å»ºå¤„ç†ç®¡é“\n",
    "\n",
    "**contextlibä¼˜åŒ–**ï¼š\n",
    "- ç¡®ä¿èµ„æºçš„æ­£ç¡®é‡Šæ”¾\n",
    "- ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç®€åŒ–å¼‚å¸¸å¤„ç†\n",
    "- åµŒå¥—ä½¿ç”¨ç®¡ç†å¤šä¸ªèµ„æº\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆå¸¸ç”¨å·¥å…·æ¨¡å—å­¦ä¹ ï¼**\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†Pythonå·¥å…·æ¨¡å—çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œèƒ½å¤Ÿç†Ÿç»ƒä½¿ç”¨collectionsã€functoolsã€itertoolsã€contextlibæ¨¡å—è¿›è¡Œé«˜æ•ˆç¼–ç¨‹ï¼Œç†è§£äº†ç¼“å­˜ä¼˜åŒ–ã€æµå¼å¤„ç†ã€èµ„æºç®¡ç†ç­‰é«˜çº§æ¨¡å¼ï¼Œä¸ºåç»­å­¦ä¹ LangChainçš„é«˜çº§åº”ç”¨å’Œæ€§èƒ½ä¼˜åŒ–å¥ å®šäº†åšå®åŸºç¡€ã€‚\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ é¢„å‘Š\n",
    "\n",
    "**ğŸŠ å®Œæˆç¬¬å››èŠ‚ï¼šå¸¸ç”¨æ ‡å‡†åº“**\n",
    "\n",
    "ä½ å·²ç»æˆåŠŸå®Œæˆäº†Pythonæ ‡å‡†åº“çš„å…¨éƒ¨å­¦ä¹ ï¼æ¥ä¸‹æ¥å°†è¿›å…¥æ›´é«˜çº§çš„æ•°æ®å¤„ç†å’Œå¼‚æ­¥ç¼–ç¨‹éƒ¨åˆ†ã€‚\n",
    "\n",
    "**è¿›å…¥ç¬¬äº”èŠ‚ï¼šæ•°æ®å¤„ç†**\n",
    "- 5.1-5.9 æ•°æ®å¤„ç†ï¼ˆnumpyã€pandasã€æ•°æ®å¯è§†åŒ–ç­‰ï¼‰\n",
    "\n",
    "**åç»­ç« èŠ‚é¢„å‘Š**ï¼š\n",
    "- ç¬¬å…­èŠ‚ï¼šå¼‚æ­¥ç¼–ç¨‹\n",
    "- ç¬¬ä¸ƒèŠ‚ï¼šWebå¼€å‘\n",
    "- ç¬¬å…«èŠ‚ï¼šé¡¹ç›®å·¥ç¨‹\n",
    "\n",
    "ç»§ç»­åŠ æ²¹ï¼ŒPythonç¼–ç¨‹æŠ€èƒ½å·²ç»è¾¾åˆ°è¿›é˜¶æ°´å¹³ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
