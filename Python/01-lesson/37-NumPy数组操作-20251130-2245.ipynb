{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 37-NumPyæ•°ç»„æ“ä½œ\n",
    "\n",
    "## ğŸ“š ç”¨é€”è¯´æ˜\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**ï¼š\n",
    "- æŒæ¡NumPyæ•°ç»„çš„åŸºç¡€æ“ä½œå’Œå±æ€§\n",
    "- ç†Ÿç»ƒä½¿ç”¨NumPyè¿›è¡Œæ•°å€¼è®¡ç®—å’Œæ•°ç»„æ“ä½œ\n",
    "- ç†è§£NumPyçš„å¹¿æ’­æœºåˆ¶å’Œå‘é‡åŒ–è¿ç®—\n",
    "- èƒ½å¤Ÿä½¿ç”¨NumPyå¤„ç†å¤šç»´æ•°æ®å’ŒçŸ©é˜µè¿ç®—\n",
    "\n",
    "**å‰ç½®è¦æ±‚**ï¼š\n",
    "- å·²å®Œæˆ36-å¸¸ç”¨å·¥å…·æ¨¡å—å­¦ä¹ \n",
    "- ç†Ÿç»ƒæŒæ¡Pythonæ•°æ®ç»“æ„å’Œå‡½æ•°\n",
    "- äº†è§£åŸºæœ¬çš„æ•°å­¦è¿ç®—å’ŒçŸ©é˜µæ¦‚å¿µ\n",
    "\n",
    "**ä¸LangChainå…³è”**ï¼š\n",
    "- NumPyæ˜¯LangChainå‘é‡è®¡ç®—å’ŒåµŒå…¥å¤„ç†çš„åŸºç¡€\n",
    "- NumPyæ•°ç»„æ“ä½œå¯¹LangChainçš„ç›¸ä¼¼åº¦è®¡ç®—è‡³å…³é‡è¦\n",
    "- NumPyæ”¯æŒLangChainçš„æ‰¹é‡æ•°æ®å¤„ç†å’Œä¼˜åŒ–\n",
    "- ä¸ºåç»­å­¦ä¹ æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åšå‡†å¤‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ çŸ¥è¯†ç‚¹è¦†ç›–\n",
    "\n",
    "### 5.1 NumPyæ•°ç»„æ“ä½œ [â­åŸºç¡€]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šNumPyæ˜¯Pythonç§‘å­¦è®¡ç®—çš„æ ¸å¿ƒåº“ï¼Œæä¾›äº†é«˜æ•ˆçš„å¤šç»´æ•°ç»„å¯¹è±¡å’Œä¸°å¯Œçš„æ•°å­¦å‡½æ•°ã€‚æŒæ¡NumPyå¯¹äºæ•°æ®å¤„ç†ã€æœºå™¨å­¦ä¹ å’ŒLangChainåº”ç”¨å¼€å‘è‡³å…³é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡NumPyæ•°ç»„çš„åˆ›å»ºå’ŒåŸºæœ¬æ“ä½œ\n",
    "- ç†è§£æ•°ç»„ç´¢å¼•ã€åˆ‡ç‰‡å’Œå½¢çŠ¶æ“ä½œ\n",
    "- ç†Ÿç»ƒä½¿ç”¨NumPyçš„æ•°å­¦å‡½æ•°å’Œç»Ÿè®¡æ–¹æ³•\n",
    "- èƒ½å¤Ÿè¿›è¡ŒçŸ©é˜µè¿ç®—å’Œå¹¿æ’­æ“ä½œ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ•°ç»„æ“ä½œåº”ç”¨ç¤ºä¾‹\n",
    "- è¿›è¡Œå‘é‡è®¡ç®—å’ŒçŸ©é˜µè¿ç®—ç»ƒä¹ \n",
    "- åº”ç”¨NumPyè§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„æ•°å€¼è®¡ç®—ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¢ NumPyæ•°ç»„åŸºç¡€æ“ä½œ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å®‰è£…æ£€æŸ¥å’Œå¯¼å…¥\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"âœ… NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ NumPyæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install numpy\")\n",
    "    exit()\n",
    "\n",
    "import time\n",
    "import random\n",
    "from typing import List, Tuple, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "# 1. NumPyæ•°ç»„åˆ›å»º\n",
    "print(f\"ğŸ“ 1. NumPyæ•°ç»„åˆ›å»º:\")\n",
    "\n",
    "# 1.1 åŸºç¡€æ•°ç»„åˆ›å»ºæ–¹æ³•\n",
    "print(f\"\\n   ğŸ—ï¸ 1.1 åŸºç¡€æ•°ç»„åˆ›å»ºæ–¹æ³•:\")\n",
    "\n",
    "def demonstrate_array_creation():\n",
    "    \"\"\"æ¼”ç¤ºNumPyæ•°ç»„åˆ›å»ºæ–¹æ³•\"\"\"\n",
    "    \n",
    "    # ä»Pythonåˆ—è¡¨åˆ›å»º\n",
    "    python_list = [1, 2, 3, 4, 5]\n",
    "    arr_from_list = np.array(python_list)\n",
    "    print(f\"   ä»åˆ—è¡¨åˆ›å»º: {arr_from_list}\")\n",
    "    print(f\"   æ•°ç»„ç±»å‹: {type(arr_from_list)}\")\n",
    "    print(f\"   æ•°æ®ç±»å‹: {arr_from_list.dtype}\")\n",
    "    print(f\"   æ•°ç»„å½¢çŠ¶: {arr_from_list.shape}\")\n",
    "    print(f\"   æ•°ç»„ç»´åº¦: {arr_from_list.ndim}\")\n",
    "    \n",
    "    # åˆ›å»ºä¸åŒç±»å‹çš„æ•°ç»„\n",
    "    print(f\"\\n   ä¸åŒæ•°æ®ç±»å‹çš„æ•°ç»„:\")\n",
    "    int_arr = np.array([1, 2, 3], dtype=np.int32)\n",
    "    float_arr = np.array([1.1, 2.2, 3.3], dtype=np.float64)\n",
    "    bool_arr = np.array([True, False, True], dtype=np.bool_)\n",
    "    string_arr = np.array(['hello', 'world', 'numpy'], dtype=np.str_)\n",
    "    \n",
    "    print(f\"   æ•´å‹æ•°ç»„: {int_arr}, dtype: {int_arr.dtype}\")\n",
    "    print(f\"   æµ®ç‚¹æ•°ç»„: {float_arr}, dtype: {float_arr.dtype}\")\n",
    "    print(f\"   å¸ƒå°”æ•°ç»„: {bool_arr}, dtype: {bool_arr.dtype}\")\n",
    "    print(f\"   å­—ç¬¦ä¸²æ•°ç»„: {string_arr}, dtype: {string_arr.dtype}\")\n",
    "    \n",
    "    # ä½¿ç”¨å†…ç½®å‡½æ•°åˆ›å»ºæ•°ç»„\n",
    "    print(f\"\\n   å†…ç½®å‡½æ•°åˆ›å»ºæ•°ç»„:\")\n",
    "    zeros_arr = np.zeros((3, 4))\n",
    "    ones_arr = np.ones((2, 3))\n",
    "    full_arr = np.full((2, 2), 7)\n",
    "    eye_arr = np.eye(3)\n",
    "    \n",
    "    print(f\"   zeros(3,4):\\n{zeros_arr}\")\n",
    "    print(f\"   ones(2,3):\\n{ones_arr}\")\n",
    "    print(f\"   full(2,2,7):\\n{full_arr}\")\n",
    "    print(f\"   eye(3):\\n{eye_arr}\")\n",
    "    \n",
    "    # åºåˆ—æ•°ç»„\n",
    "    print(f\"\\n   åºåˆ—æ•°ç»„:\")\n",
    "    arange_arr = np.arange(0, 10, 2)\n",
    "    linspace_arr = np.linspace(0, 1, 5)\n",
    "    logspace_arr = np.logspace(0, 2, 4)\n",
    "    \n",
    "    print(f\"   arange(0,10,2): {arange_arr}\")\n",
    "    print(f\"   linspace(0,1,5): {linspace_arr}\")\n",
    "    print(f\"   logspace(0,2,4): {logspace_arr}\")\n",
    "    \n",
    "    # éšæœºæ•°ç»„\n",
    "    print(f\"\\n   éšæœºæ•°ç»„:\")\n",
    "    random_arr = np.random.random((2, 3))\n",
    "    randint_arr = np.random.randint(0, 10, (3, 3))\n",
    "    normal_arr = np.random.normal(0, 1, (2, 2))\n",
    "    \n",
    "    print(f\"   random(2,3):\\n{random_arr}\")\n",
    "    print(f\"   randint(0,10,3,3):\\n{randint_arr}\")\n",
    "    print(f\"   normal(0,1,2,2):\\n{normal_arr}\")\n",
    "\n",
    "demonstrate_array_creation()\n",
    "\n",
    "# 1.2 LangChainå‘é‡åˆ›å»ºåº”ç”¨\n",
    "print(f\"\\n   ğŸ¤– 1.2 LangChainå‘é‡åˆ›å»ºåº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class VectorCreator:\n",
    "    \"\"\"å‘é‡åˆ›å»ºå™¨ - æ¨¡æ‹ŸLangChainçš„åµŒå…¥å‘é‡ç”Ÿæˆ\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 128):\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def create_text_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"åˆ›å»ºæ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆæ¨¡æ‹Ÿï¼‰\"\"\"\n",
    "        # ç®€å•çš„æ¨¡æ‹ŸåµŒå…¥ï¼šåŸºäºæ–‡æœ¬å“ˆå¸Œç”Ÿæˆå‘é‡\n",
    "        text_hash = hash(text)\n",
    "        np.random.seed(text_hash % (2**32))  # ç¡®ä¿ç›¸åŒæ–‡æœ¬äº§ç”Ÿç›¸åŒå‘é‡\n",
    "        embedding = np.random.normal(0, 1, self.embedding_dim)\n",
    "        # å½’ä¸€åŒ–\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        return embedding\n",
    "    \n",
    "    def create_batch_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"æ‰¹é‡åˆ›å»ºåµŒå…¥å‘é‡\"\"\"\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embedding = self.create_text_embedding(text)\n",
    "            embeddings.append(embedding)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def create_positional_encoding(self, seq_len: int) -> np.ndarray:\n",
    "        \"\"\"åˆ›å»ºä½ç½®ç¼–ç ï¼ˆæ¨¡æ‹ŸTransformerçš„ä½ç½®ç¼–ç ï¼‰\"\"\"\n",
    "        pos_encoding = np.zeros((seq_len, self.embedding_dim))\n",
    "        \n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                pos_encoding[pos, i] = np.sin(pos / (10000 ** (2 * i / self.embedding_dim)))\n",
    "                if i + 1 < self.embedding_dim:\n",
    "                    pos_encoding[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / self.embedding_dim)))\n",
    "        \n",
    "        return pos_encoding\n",
    "    \n",
    "    def create_attention_mask(self, seq_len: int, valid_tokens: int) -> np.ndarray:\n",
    "        \"\"\"åˆ›å»ºæ³¨æ„åŠ›æ©ç \"\"\"\n",
    "        mask = np.zeros((seq_len, seq_len))\n",
    "        for i in range(valid_tokens):\n",
    "            for j in range(valid_tokens):\n",
    "                mask[i, j] = 1\n",
    "        return mask\n",
    "\n",
    "def demonstrate_vector_creator():\n",
    "    \"\"\"æ¼”ç¤ºå‘é‡åˆ›å»ºå™¨\"\"\"\n",
    "    creator = VectorCreator(embedding_dim=64)\n",
    "    \n",
    "    # æµ‹è¯•æ–‡æœ¬\n",
    "    test_texts = [\n",
    "        \"Hello, world!\",\n",
    "        \"LangChain is awesome\",\n",
    "        \"Python programming\",\n",
    "        \"Machine learning\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"   å‘é‡åˆ›å»ºæ¼”ç¤º (åµŒå…¥ç»´åº¦: {creator.embedding_dim}):\")\n",
    "    \n",
    "    # å•ä¸ªæ–‡æœ¬åµŒå…¥\n",
    "    print(f\"\\n   å•ä¸ªæ–‡æœ¬åµŒå…¥:\")\n",
    "    for text in test_texts[:2]:\n",
    "        embedding = creator.create_text_embedding(text)\n",
    "        print(f\"   æ–‡æœ¬: '{text}'\")\n",
    "        print(f\"   å‘é‡å½¢çŠ¶: {embedding.shape}\")\n",
    "        print(f\"   å‘é‡èŒƒæ•°: {np.linalg.norm(embedding):.6f}\")\n",
    "        print(f\"   å‰5ç»´: {embedding[:5]}\")\n",
    "    \n",
    "    # æ‰¹é‡åµŒå…¥\n",
    "    print(f\"\\n   æ‰¹é‡åµŒå…¥:\")\n",
    "    batch_embeddings = creator.create_batch_embeddings(test_texts)\n",
    "    print(f\"   æ‰¹é‡å½¢çŠ¶: {batch_embeddings.shape}\")\n",
    "    print(f\"   æ¯ä¸ªå‘é‡çš„èŒƒæ•°: {[np.linalg.norm(vec) for vec in batch_embeddings]}\")\n",
    "    \n",
    "    # ä½ç½®ç¼–ç \n",
    "    print(f\"\\n   ä½ç½®ç¼–ç :\")\n",
    "    seq_len = 4\n",
    "    pos_encoding = creator.create_positional_encoding(seq_len)\n",
    "    print(f\"   ä½ç½®ç¼–ç å½¢çŠ¶: {pos_encoding.shape}\")\n",
    "    print(f\"   ä½ç½®ç¼–ç å‰2è¡Œå‰5åˆ—:\\n{pos_encoding[:2, :5]}\")\n",
    "    \n",
    "    # æ³¨æ„åŠ›æ©ç \n",
    "    print(f\"\\n   æ³¨æ„åŠ›æ©ç :\")\n",
    "    valid_tokens = 3\n",
    "    attention_mask = creator.create_attention_mask(seq_len, valid_tokens)\n",
    "    print(f\"   æ©ç å½¢çŠ¶: {attention_mask.shape}\")\n",
    "    print(f\"   æ³¨æ„åŠ›æ©ç :\\n{attention_mask}\")\n",
    "\n",
    "demonstrate_vector_creator()\n",
    "\n",
    "# 2. æ•°ç»„ç´¢å¼•å’Œåˆ‡ç‰‡\n",
    "print(f\"\\nğŸ“ 2. æ•°ç»„ç´¢å¼•å’Œåˆ‡ç‰‡:\")\n",
    "\n",
    "# 2.1 åŸºç¡€ç´¢å¼•å’Œåˆ‡ç‰‡æ“ä½œ\n",
    "print(f\"\\n   ğŸ” 2.1 åŸºç¡€ç´¢å¼•å’Œåˆ‡ç‰‡æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_indexing_slicing():\n",
    "    \"\"\"æ¼”ç¤ºæ•°ç»„ç´¢å¼•å’Œåˆ‡ç‰‡\"\"\"\n",
    "    \n",
    "    # ä¸€ç»´æ•°ç»„ç´¢å¼•\n",
    "    arr_1d = np.array([10, 20, 30, 40, 50])\n",
    "    print(f\"   ä¸€ç»´æ•°ç»„: {arr_1d}\")\n",
    "    print(f\"   arr_1d[0]: {arr_1d[0]}\")\n",
    "    print(f\"   arr_1d[-1]: {arr_1d[-1]}\")\n",
    "    print(f\"   arr_1d[1:4]: {arr_1d[1:4]}\")\n",
    "    print(f\"   arr_1d[::2]: {arr_1d[::2]}\")\n",
    "    \n",
    "    # äºŒç»´æ•°ç»„ç´¢å¼•\n",
    "    arr_2d = np.array([\n",
    "        [1, 2, 3, 4],\n",
    "        [5, 6, 7, 8],\n",
    "        [9, 10, 11, 12]\n",
    "    ])\n",
    "    print(f\"\\n   äºŒç»´æ•°ç»„:\\n{arr_2d}\")\n",
    "    print(f\"   arr_2d[0, 0]: {arr_2d[0, 0]}\")\n",
    "    print(f\"   arr_2d[1, :]: {arr_2d[1, :]}\")\n",
    "    print(f\"   arr_2d[:, 2]: {arr_2d[:, 2]}\")\n",
    "    print(f\"   arr_2d[0:2, 1:3]:\\n{arr_2d[0:2, 1:3]}\")\n",
    "    \n",
    "    # å¸ƒå°”ç´¢å¼•\n",
    "    print(f\"\\n   å¸ƒå°”ç´¢å¼•:\")\n",
    "    bool_arr = np.array([True, False, True, False, True])\n",
    "    filtered = arr_1d[bool_arr]\n",
    "    print(f\"   å¸ƒå°”æ•°ç»„: {bool_arr}\")\n",
    "    print(f\"   è¿‡æ»¤ç»“æœ: {filtered}\")\n",
    "    \n",
    "    # æ¡ä»¶ç´¢å¼•\n",
    "    condition = arr_1d > 25\n",
    "    print(f\"   æ¡ä»¶ (arr_1d > 25): {condition}\")\n",
    "    print(f\"   æ¡ä»¶è¿‡æ»¤ç»“æœ: {arr_1d[condition]}\")\n",
    "    \n",
    "    # èŠ±å¼ç´¢å¼•\n",
    "    print(f\"\\n   èŠ±å¼ç´¢å¼•:\")\n",
    "    indices = [0, 2, 4]\n",
    "    fancy_result = arr_1d[indices]\n",
    "    print(f\"   ç´¢å¼•åˆ—è¡¨: {indices}\")\n",
    "    print(f\"   èŠ±å¼ç´¢å¼•ç»“æœ: {fancy_result}\")\n",
    "    \n",
    "    # ä¸‰ç»´æ•°ç»„ç´¢å¼•\n",
    "    print(f\"\\n   ä¸‰ç»´æ•°ç»„ç´¢å¼•:\")\n",
    "    arr_3d = np.random.randint(0, 100, (2, 3, 4))\n",
    "    print(f\"   ä¸‰ç»´æ•°ç»„å½¢çŠ¶: {arr_3d.shape}\")\n",
    "    print(f\"   arr_3d[0, :, :]:\\n{arr_3d[0, :, :]}\")\n",
    "    print(f\"   arr_3d[:, 1, :]:\\n{arr_3d[:, 1, :]}\")\n",
    "    print(f\"   arr_3d[:, :, 2]: {arr_3d[:, :, 2]}\")\n",
    "\n",
    "demonstrate_indexing_slicing()\n",
    "\n",
    "# 2.2 LangChainå‘é‡æ£€ç´¢åº”ç”¨\n",
    "print(f\"\\n   ğŸ” 2.2 LangChainå‘é‡æ£€ç´¢åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class VectorRetriever:\n",
    "    \"\"\"å‘é‡æ£€ç´¢å™¨ - æ¨¡æ‹ŸLangChainçš„å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 64):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vectors = None\n",
    "        self.texts = []\n",
    "    \n",
    "    def add_texts(self, texts: List[str]):\n",
    "        \"\"\"æ·»åŠ æ–‡æœ¬å’Œå¯¹åº”çš„å‘é‡\"\"\"\n",
    "        creator = VectorCreator(self.embedding_dim)\n",
    "        new_vectors = creator.create_batch_embeddings(texts)\n",
    "        \n",
    "        if self.vectors is None:\n",
    "            self.vectors = new_vectors\n",
    "        else:\n",
    "            self.vectors = np.vstack([self.vectors, new_vectors])\n",
    "        \n",
    "        self.texts.extend(texts)\n",
    "    \n",
    "    def cosine_similarity(self, query_vector: np.ndarray, vectors: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "        # å½’ä¸€åŒ–å‘é‡\n",
    "        query_norm = query_vector / np.linalg.norm(query_vector)\n",
    "        vectors_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "        \n",
    "        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        similarities = np.dot(vectors_norm, query_norm)\n",
    "        return similarities\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬\"\"\"\n",
    "        if self.vectors is None:\n",
    "            return []\n",
    "        \n",
    "        creator = VectorCreator(self.embedding_dim)\n",
    "        query_vector = creator.create_text_embedding(query)\n",
    "        \n",
    "        similarities = self.cosine_similarity(query_vector, self.vectors)\n",
    "        \n",
    "        # è·å–top-kç»“æœ\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((self.texts[idx], similarities[idx]))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def filter_by_threshold(self, query: str, threshold: float = 0.5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"æ ¹æ®é˜ˆå€¼è¿‡æ»¤ç»“æœ\"\"\"\n",
    "        if self.vectors is None:\n",
    "            return []\n",
    "        \n",
    "        creator = VectorCreator(self.embedding_dim)\n",
    "        query_vector = creator.create_text_embedding(query)\n",
    "        \n",
    "        similarities = self.cosine_similarity(query_vector, self.vectors)\n",
    "        \n",
    "        # é˜ˆå€¼è¿‡æ»¤\n",
    "        mask = similarities >= threshold\n",
    "        filtered_texts = np.array(self.texts)[mask]\n",
    "        filtered_similarities = similarities[mask]\n",
    "        \n",
    "        return list(zip(filtered_texts, filtered_similarities))\n",
    "\n",
    "def demonstrate_vector_retriever():\n",
    "    \"\"\"æ¼”ç¤ºå‘é‡æ£€ç´¢å™¨\"\"\"\n",
    "    retriever = VectorRetriever(embedding_dim=64)\n",
    "    \n",
    "    # æ·»åŠ æ–‡æ¡£\n",
    "    documents = [\n",
    "        \"Python is a popular programming language\",\n",
    "        \"Machine learning uses algorithms to learn from data\",\n",
    "        \"NumPy is essential for scientific computing in Python\",\n",
    "        \"LangChain helps build applications with language models\",\n",
    "        \"Data science combines statistics, programming, and domain knowledge\"\n",
    "    ]\n",
    "    \n",
    "    retriever.add_texts(documents)\n",
    "    \n",
    "    print(f\"   å‘é‡æ£€ç´¢æ¼”ç¤º:\")\n",
    "    print(f\"   æ–‡æ¡£æ•°é‡: {len(retriever.texts)}\")\n",
    "    print(f\"   å‘é‡çŸ©é˜µå½¢çŠ¶: {retriever.vectors.shape}\")\n",
    "    \n",
    "    # æœç´¢æµ‹è¯•\n",
    "    queries = [\n",
    "        \"Python programming\",\n",
    "        \"machine learning algorithms\",\n",
    "        \"language model applications\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\n   æŸ¥è¯¢: '{query}'\")\n",
    "        results = retriever.search(query, top_k=3)\n",
    "        \n",
    "        for i, (text, similarity) in enumerate(results, 1):\n",
    "            print(f\"   {i}. {text} (ç›¸ä¼¼åº¦: {similarity:.4f})\")\n",
    "    \n",
    "    # é˜ˆå€¼è¿‡æ»¤\n",
    "    print(f\"\\n   é˜ˆå€¼è¿‡æ»¤æ¼”ç¤º (threshold=0.3):\")\n",
    "    query = \"Python\"\n",
    "    filtered_results = retriever.filter_by_threshold(query, threshold=0.3)\n",
    "    print(f\"   æŸ¥è¯¢: '{query}'\")\n",
    "    \n",
    "    for text, similarity in filtered_results:\n",
    "        print(f\"   - {text} (ç›¸ä¼¼åº¦: {similarity:.4f})\")\n",
    "\n",
    "demonstrate_vector_retriever()\n",
    "\n",
    "# 3. æ•°ç»„å½¢çŠ¶æ“ä½œ\n",
    "print(f\"\\nğŸ“ 3. æ•°ç»„å½¢çŠ¶æ“ä½œ:\")\n",
    "\n",
    "# 3.1 åŸºç¡€å½¢çŠ¶æ“ä½œ\n",
    "print(f\"\\n   ğŸ”„ 3.1 åŸºç¡€å½¢çŠ¶æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_shape_operations():\n",
    "    \"\"\"æ¼”ç¤ºæ•°ç»„å½¢çŠ¶æ“ä½œ\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºæµ‹è¯•æ•°ç»„\n",
    "    arr = np.arange(12)\n",
    "    print(f\"   åŸå§‹æ•°ç»„: {arr}\")\n",
    "    print(f\"   åŸå§‹å½¢çŠ¶: {arr.shape}\")\n",
    "    \n",
    "    # reshapeæ“ä½œ\n",
    "    arr_2d = arr.reshape(3, 4)\n",
    "    print(f\"\\n   reshape(3,4):\\n{arr_2d}\")\n",
    "    print(f\"   æ–°å½¢çŠ¶: {arr_2d.shape}\")\n",
    "    \n",
    "    arr_3d = arr.reshape(2, 2, 3)\n",
    "    print(f\"\\n   reshape(2,2,3):\\n{arr_3d}\")\n",
    "    print(f\"   æ–°å½¢çŠ¶: {arr_3d.shape}\")\n",
    "    \n",
    "    # transposeæ“ä½œ\n",
    "    print(f\"\\n   transposeæ“ä½œ:\")\n",
    "    transposed = arr_2d.transpose()\n",
    "    print(f\"   åŸæ•°ç»„(3,4):\\n{arr_2d}\")\n",
    "    print(f\"   transpose()å(4,3):\\n{transposed}\")\n",
    "    \n",
    "    # flattenæ“ä½œ\n",
    "    print(f\"\\n   flattenæ“ä½œ:\")\n",
    "    flattened = arr_3d.flatten()\n",
    "    print(f\"   å±•å¹³ç»“æœ: {flattened}\")\n",
    "    print(f\"   å±•å¹³å½¢çŠ¶: {flattened.shape}\")\n",
    "    \n",
    "    # squeezeå’Œexpand_dims\n",
    "    print(f\"\\n   squeezeå’Œexpand_dims:\")\n",
    "    arr_with_extra = np.array([[[1, 2, 3]]])  # å½¢çŠ¶(1,1,3)\n",
    "    print(f\"   åŸæ•°ç»„å½¢çŠ¶: {arr_with_extra.shape}\")\n",
    "    \n",
    "    squeezed = np.squeeze(arr_with_extra)\n",
    "    print(f\"   squeezeåå½¢çŠ¶: {squeezed.shape}\")\n",
    "    \n",
    "    expanded = np.expand_dims(squeezed, axis=0)\n",
    "    print(f\"   expand_dims(axis=0)åå½¢çŠ¶: {expanded.shape}\")\n",
    "    \n",
    "    # concatenateæ“ä½œ\n",
    "    print(f\"\\n   concatenateæ“ä½œ:\")\n",
    "    arr1 = np.array([[1, 2], [3, 4]])\n",
    "    arr2 = np.array([[5, 6], [7, 8]])\n",
    "    \n",
    "    concat_rows = np.concatenate([arr1, arr2], axis=0)\n",
    "    concat_cols = np.concatenate([arr1, arr2], axis=1)\n",
    "    \n",
    "    print(f\"   arr1:\\n{arr1}\")\n",
    "    print(f\"   arr2:\\n{arr2}\")\n",
    "    print(f\"   æŒ‰è¡Œè¿æ¥(axis=0):\\n{concat_rows}\")\n",
    "    print(f\"   æŒ‰åˆ—è¿æ¥(axis=1):\\n{concat_cols}\")\n",
    "    \n",
    "    # stackæ“ä½œ\n",
    "    print(f\"\\n   stackæ“ä½œ:\")\n",
    "    stacked = np.stack([arr1, arr2], axis=0)\n",
    "    print(f\"   stack(axis=0)å½¢çŠ¶: {stacked.shape}\")\n",
    "    print(f\"   stackç»“æœ:\\n{stacked}\")\n",
    "\n",
    "demonstrate_shape_operations()\n",
    "\n",
    "# 3.2 LangChainæ‰¹æ¬¡å¤„ç†åº”ç”¨\n",
    "print(f\"\\n   ğŸ“¦ 3.2 LangChainæ‰¹æ¬¡å¤„ç†åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class BatchProcessor:\n",
    "    \"\"\"æ‰¹æ¬¡å¤„ç†å™¨ - æ¨¡æ‹ŸLangChainçš„æ‰¹é‡æ•°æ®å¤„ç†\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 4):\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def create_batches(self, data: np.ndarray) -> List[np.ndarray]:\n",
    "        \"\"\"å°†æ•°æ®åˆ†æ‰¹\"\"\"\n",
    "        num_samples = data.shape[0]\n",
    "        batches = []\n",
    "        \n",
    "        for i in range(0, num_samples, self.batch_size):\n",
    "            batch = data[i:i + self.batch_size]\n",
    "            batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def pad_sequences(self, sequences: List[np.ndarray], max_len: Optional[int] = None) -> np.ndarray:\n",
    "        \"\"\"å¡«å……åºåˆ—åˆ°ç›¸åŒé•¿åº¦\"\"\"\n",
    "        if max_len is None:\n",
    "            max_len = max(len(seq) for seq in sequences)\n",
    "        \n",
    "        padded_sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) < max_len:\n",
    "                # ç”¨0å¡«å……\n",
    "                padded = np.pad(seq, (0, max_len - len(seq)), 'constant')\n",
    "            else:\n",
    "                padded = seq[:max_len]\n",
    "            padded_sequences.append(padded)\n",
    "        \n",
    "        return np.array(padded_sequences)\n",
    "    \n",
    "    def create_attention_masks(self, sequences: List[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"åˆ›å»ºæ³¨æ„åŠ›æ©ç \"\"\"\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "        masks = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            mask = np.ones(len(seq))\n",
    "            if len(seq) < max_len:\n",
    "                mask = np.pad(mask, (0, max_len - len(seq)), 'constant')\n",
    "            masks.append(mask)\n",
    "        \n",
    "        return np.array(masks)\n",
    "    \n",
    "    def process_batch_data(self, texts: List[str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"å¤„ç†æ‰¹æ¬¡æ•°æ®\"\"\"\n",
    "        # åˆ›å»ºåµŒå…¥å‘é‡\n",
    "        creator = VectorCreator(embedding_dim=32)\n",
    "        embeddings = creator.create_batch_embeddings(texts)\n",
    "        \n",
    "        # æ¨¡æ‹Ÿåºåˆ—æ•°æ®ï¼ˆtoken idsï¼‰\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            # ç®€å•çš„tokenåŒ–æ¨¡æ‹Ÿ\n",
    "            tokens = [hash(word) % 1000 for word in text.split()]\n",
    "            sequences.append(np.array(tokens))\n",
    "        \n",
    "        # å¡«å……åºåˆ—\n",
    "        padded_sequences = self.pad_sequences(sequences)\n",
    "        attention_masks = self.create_attention_masks(sequences)\n",
    "        \n",
    "        # åˆ†æ‰¹\n",
    "        embedding_batches = self.create_batches(embeddings)\n",
    "        sequence_batches = self.create_batches(padded_sequences)\n",
    "        mask_batches = self.create_batches(attention_masks)\n",
    "        \n",
    "        return {\n",
    "            'embeddings': embeddings,\n",
    "            'padded_sequences': padded_sequences,\n",
    "            'attention_masks': attention_masks,\n",
    "            'embedding_batches': embedding_batches,\n",
    "            'sequence_batches': sequence_batches,\n",
    "            'mask_batches': mask_batches\n",
    "        }\n",
    "\n",
    "def demonstrate_batch_processor():\n",
    "    \"\"\"æ¼”ç¤ºæ‰¹æ¬¡å¤„ç†å™¨\"\"\"\n",
    "    processor = BatchProcessor(batch_size=2)\n",
    "    \n",
    "    # æµ‹è¯•æ–‡æœ¬\n",
    "    texts = [\n",
    "        \"Hello world\",\n",
    "        \"Python programming is fun\",\n",
    "        \"Machine learning\",\n",
    "        \"Data science with NumPy\",\n",
    "        \"LangChain applications\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"   æ‰¹æ¬¡å¤„ç†æ¼”ç¤º (batch_size={processor.batch_size}):\")\n",
    "    print(f\"   è¾“å…¥æ–‡æœ¬æ•°é‡: {len(texts)}\")\n",
    "    \n",
    "    # å¤„ç†æ‰¹æ¬¡æ•°æ®\n",
    "    batch_data = processor.process_batch_data(texts)\n",
    "    \n",
    "    print(f\"\\n   å¤„ç†ç»“æœ:\")\n",
    "    print(f\"   åµŒå…¥çŸ©é˜µå½¢çŠ¶: {batch_data['embeddings'].shape}\")\n",
    "    print(f\"   å¡«å……åºåˆ—å½¢çŠ¶: {batch_data['padded_sequences'].shape}\")\n",
    "    print(f\"   æ³¨æ„åŠ›æ©ç å½¢çŠ¶: {batch_data['attention_masks'].shape}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ‰¹æ¬¡ä¿¡æ¯\n",
    "    print(f\"\\n   æ‰¹æ¬¡ä¿¡æ¯:\")\n",
    "    print(f\"   åµŒå…¥æ‰¹æ¬¡æ•°é‡: {len(batch_data['embedding_batches'])}\")\n",
    "    for i, batch in enumerate(batch_data['embedding_batches']):\n",
    "        print(f\"   æ‰¹æ¬¡{i+1}å½¢çŠ¶: {batch.shape}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºåºåˆ—å’Œæ©ç \n",
    "    print(f\"\\n   åºåˆ—å’Œæ©ç ç¤ºä¾‹:\")\n",
    "    for i, (text, seq, mask) in enumerate(zip(texts[:3], batch_data['padded_sequences'][:3], batch_data['attention_masks'][:3])):\n",
    "        print(f\"   {i+1}. æ–‡æœ¬: '{text}'\")\n",
    "        print(f\"      åºåˆ—: {seq}\")\n",
    "        print(f\"      æ©ç : {mask}\")\n",
    "\n",
    "demonstrate_batch_processor()\n",
    "\n",
    "print(f\"\\nâœ… NumPyæ•°ç»„åŸºç¡€æ“ä½œå®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡NumPyæ•°ç»„çš„åˆ›å»ºå’Œå±æ€§\")\n",
    "print(f\"   âœ“ ç†è§£æ•°ç»„ç´¢å¼•å’Œåˆ‡ç‰‡æ“ä½œ\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨æ•°ç»„å½¢çŠ¶æ“ä½œ\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿåº”ç”¨NumPyå¤„ç†å‘é‡æ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPyæ•°å­¦è¿ç®—å’Œå¹¿æ’­æœºåˆ¶ [â­åŸºç¡€]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šNumPyæä¾›äº†ä¸°å¯Œçš„æ•°å­¦å‡½æ•°å’Œå¹¿æ’­æœºåˆ¶ï¼Œæ”¯æŒé«˜æ•ˆçš„å‘é‡åŒ–è¿ç®—ã€‚æŒæ¡è¿™äº›åŠŸèƒ½å¯¹äºæ•°å€¼è®¡ç®—ã€æ•°æ®å¤„ç†å’ŒLangChainåº”ç”¨å¼€å‘éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡NumPyçš„åŸºæœ¬æ•°å­¦è¿ç®—\n",
    "- ç†è§£å¹¿æ’­æœºåˆ¶çš„å·¥ä½œåŸç†\n",
    "- ç†Ÿç»ƒä½¿ç”¨ç»Ÿè®¡å‡½æ•°å’Œèšåˆæ“ä½œ\n",
    "- èƒ½å¤Ÿè¿›è¡ŒçŸ©é˜µè¿ç®—å’Œçº¿æ€§ä»£æ•°æ“ä½œ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ•°å­¦è¿ç®—åº”ç”¨\n",
    "- è¿›è¡Œå‘é‡è®¡ç®—å’Œç›¸ä¼¼åº¦åˆ†æ\n",
    "- åº”ç”¨NumPyè§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„æ•°å€¼è®¡ç®—ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â• NumPyæ•°å­¦è¿ç®—å’Œå¹¿æ’­æœºåˆ¶:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from typing import List, Tuple, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "# 1. åŸºæœ¬æ•°å­¦è¿ç®—\n",
    "print(f\"ğŸ“ 1. åŸºæœ¬æ•°å­¦è¿ç®—:\")\n",
    "\n",
    "# 1.1 ç®—æœ¯è¿ç®—\n",
    "print(f\"\\n   ğŸ”¢ 1.1 ç®—æœ¯è¿ç®—:\")\n",
    "\n",
    "def demonstrate_arithmetic_operations():\n",
    "    \"\"\"æ¼”ç¤ºç®—æœ¯è¿ç®—\"\"\"\n",
    "    \n",
    "    # æ•°ç»„ä¸æ ‡é‡è¿ç®—\n",
    "    arr = np.array([1, 2, 3, 4, 5])\n",
    "    print(f\"   åŸæ•°ç»„: {arr}\")\n",
    "    print(f\"   arr + 2: {arr + 2}\")\n",
    "    print(f\"   arr * 3: {arr * 3}\")\n",
    "    print(f\"   arr ** 2: {arr ** 2}\")\n",
    "    print(f\"   arr // 2: {arr // 2}\")\n",
    "    print(f\"   arr % 2: {arr % 2}\")\n",
    "    \n",
    "    # æ•°ç»„é—´è¿ç®—\n",
    "    arr1 = np.array([1, 2, 3, 4])\n",
    "    arr2 = np.array([5, 6, 7, 8])\n",
    "    print(f\"\\n   æ•°ç»„é—´è¿ç®—:\")\n",
    "    print(f\"   arr1: {arr1}\")\n",
    "    print(f\"   arr2: {arr2}\")\n",
    "    print(f\"   arr1 + arr2: {arr1 + arr2}\")\n",
    "    print(f\"   arr1 * arr2: {arr1 * arr2}\")\n",
    "    print(f\"   arr1 - arr2: {arr1 - arr2}\")\n",
    "    print(f\"   arr1 / arr2: {arr1 / arr2}\")\n",
    "    \n",
    "    # æ•°å­¦å‡½æ•°\n",
    "    print(f\"\\n   æ•°å­¦å‡½æ•°:\")\n",
    "    arr_float = np.array([0, np.pi/4, np.pi/2, np.pi])\n",
    "    print(f\"   åŸæ•°ç»„: {arr_float}\")\n",
    "    print(f\"   sin(arr): {np.sin(arr_float)}\")\n",
    "    print(f\"   cos(arr): {np.cos(arr_float)}\")\n",
    "    print(f\"   exp(arr): {np.exp(arr_float[:2])}\")  # é¿å…æ•°å€¼è¿‡å¤§\n",
    "    print(f\"   log(arr): {np.log(arr_float[1:3])}\")  # é¿å…log(0)\n",
    "    print(f\"   sqrt(arr): {np.sqrt(arr_float[1:4])}\")\n",
    "    \n",
    "    # æ¯”è¾ƒè¿ç®—\n",
    "    print(f\"\\n   æ¯”è¾ƒè¿ç®—:\")\n",
    "    arr_comp = np.array([1, 3, 5, 7, 9])\n",
    "    print(f\"   åŸæ•°ç»„: {arr_comp}\")\n",
    "    print(f\"   arr > 5: {arr_comp > 5}\")\n",
    "    print(f\"   arr < 5: {arr_comp < 5}\")\n",
    "    print(f\"   arr == 5: {arr_comp == 5}\")\n",
    "    print(f\"   arr >= 3: {arr_comp >= 3}\")\n",
    "    \n",
    "    # é€»è¾‘è¿ç®—\n",
    "    print(f\"\\n   é€»è¾‘è¿ç®—:\")\n",
    "    arr_bool1 = np.array([True, False, True, False])\n",
    "    arr_bool2 = np.array([True, True, False, False])\n",
    "    print(f\"   arr1: {arr_bool1}\")\n",
    "    print(f\"   arr2: {arr_bool2}\")\n",
    "    print(f\"   arr1 & arr2: {arr_bool1 & arr_bool2}\")\n",
    "    print(f\"   arr1 | arr2: {arr_bool1 | arr_bool2}\")\n",
    "    print(f\"   ~arr1: {~arr_bool1}\")\n",
    "\n",
    "demonstrate_arithmetic_operations()\n",
    "\n",
    "# 1.2 LangChainå‘é‡è¿ç®—åº”ç”¨\n",
    "print(f\"\\n   ğŸ§® 1.2 LangChainå‘é‡è¿ç®—åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class VectorOperations:\n",
    "    \"\"\"å‘é‡è¿ç®—å™¨ - æ¨¡æ‹ŸLangChainçš„å‘é‡è®¡ç®—\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "        # å½’ä¸€åŒ–\n",
    "        vec1_norm = vec1 / np.linalg.norm(vec1)\n",
    "        vec2_norm = vec2 / np.linalg.norm(vec2)\n",
    "        \n",
    "        # è®¡ç®—ç‚¹ç§¯\n",
    "        similarity = np.dot(vec1_norm, vec2_norm)\n",
    "        return float(similarity)\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»\"\"\"\n",
    "        return float(np.linalg.norm(vec1 - vec2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def manhattan_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"è®¡ç®—æ›¼å“ˆé¡¿è·ç¦»\"\"\"\n",
    "        return float(np.sum(np.abs(vec1 - vec2)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dot_product(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"è®¡ç®—ç‚¹ç§¯\"\"\"\n",
    "        return float(np.dot(vec1, vec2))\n",
    "    \n",
    "    def vector_addition(self, vectors: List[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"å‘é‡åŠ æ³•\"\"\"\n",
    "        return np.sum(vectors, axis=0)\n",
    "    \n",
    "    def vector_average(self, vectors: List[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"å‘é‡å¹³å‡\"\"\"\n",
    "        return np.mean(vectors, axis=0)\n",
    "    \n",
    "    def weighted_average(self, vectors: List[np.ndarray], weights: List[float]) -> np.ndarray:\n",
    "        \"\"\"åŠ æƒå¹³å‡\"\"\"\n",
    "        weights_array = np.array(weights)\n",
    "        weights_array = weights_array / np.sum(weights_array)  # å½’ä¸€åŒ–æƒé‡\n",
    "        \n",
    "        weighted_vectors = []\n",
    "        for vec, weight in zip(vectors, weights_array):\n",
    "            weighted_vectors.append(vec * weight)\n",
    "        \n",
    "        return self.vector_addition(weighted_vectors)\n",
    "\n",
    "def demonstrate_vector_operations():\n",
    "    \"\"\"æ¼”ç¤ºå‘é‡è¿ç®—\"\"\"\n",
    "    vec_ops = VectorOperations()\n",
    "    \n",
    "    # åˆ›å»ºæµ‹è¯•å‘é‡\n",
    "    vec1 = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "    vec2 = np.array([2.0, 3.0, 4.0, 5.0, 6.0])\n",
    "    vec3 = np.array([0.5, 1.5, 2.5, 3.5, 4.5])\n",
    "    \n",
    "    print(f\"   å‘é‡è¿ç®—æ¼”ç¤º:\")\n",
    "    print(f\"   å‘é‡1: {vec1}\")\n",
    "    print(f\"   å‘é‡2: {vec2}\")\n",
    "    print(f\"   å‘é‡3: {vec3}\")\n",
    "    \n",
    "    # ç›¸ä¼¼åº¦è®¡ç®—\n",
    "    print(f\"\\n   ç›¸ä¼¼åº¦å’Œè·ç¦»è®¡ç®—:\")\n",
    "    cosine_sim = vec_ops.cosine_similarity(vec1, vec2)\n",
    "    euclidean_dist = vec_ops.euclidean_distance(vec1, vec2)\n",
    "    manhattan_dist = vec_ops.manhattan_distance(vec1, vec2)\n",
    "    dot_prod = vec_ops.dot_product(vec1, vec2)\n",
    "    \n",
    "    print(f\"   ä½™å¼¦ç›¸ä¼¼åº¦ (vec1, vec2): {cosine_sim:.4f}\")\n",
    "    print(f\"   æ¬§å‡ é‡Œå¾—è·ç¦» (vec1, vec2): {euclidean_dist:.4f}\")\n",
    "    print(f\"   æ›¼å“ˆé¡¿è·ç¦» (vec1, vec2): {manhattan_dist:.4f}\")\n",
    "    print(f\"   ç‚¹ç§¯ (vec1, vec2): {dot_prod:.4f}\")\n",
    "    \n",
    "    # å‘é‡è¿ç®—\n",
    "    print(f\"\\n   å‘é‡ç»„åˆè¿ç®—:\")\n",
    "    vectors = [vec1, vec2, vec3]\n",
    "    \n",
    "    vec_sum = vec_ops.vector_addition(vectors)\n",
    "    vec_avg = vec_ops.vector_average(vectors)\n",
    "    \n",
    "    weights = [0.5, 0.3, 0.2]\n",
    "    vec_weighted = vec_ops.weighted_average(vectors, weights)\n",
    "    \n",
    "    print(f\"   å‘é‡å’Œ: {vec_sum}\")\n",
    "    print(f\"   å‘é‡å¹³å‡: {vec_avg}\")\n",
    "    print(f\"   åŠ æƒå¹³å‡ (æƒé‡{weights}): {vec_weighted}\")\n",
    "    \n",
    "    # æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—\n",
    "    print(f\"\\n   æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—:\")\n",
    "    query_vector = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "    candidate_vectors = [vec1, vec2, vec3]\n",
    "    \n",
    "    similarities = []\n",
    "    for i, candidate in enumerate(candidate_vectors, 1):\n",
    "        sim = vec_ops.cosine_similarity(query_vector, candidate)\n",
    "        similarities.append((f\"å‘é‡{i}\", sim))\n",
    "    \n",
    "    print(f\"   æŸ¥è¯¢å‘é‡: {query_vector}\")\n",
    "    for name, sim in similarities:\n",
    "        print(f\"   {name}ç›¸ä¼¼åº¦: {sim:.4f}\")\n",
    "\n",
    "demonstrate_vector_operations()\n",
    "\n",
    "# 2. å¹¿æ’­æœºåˆ¶\n",
    "print(f\"\\nğŸ“ 2. å¹¿æ’­æœºåˆ¶:\")\n",
    "\n",
    "# 2.1 å¹¿æ’­è§„åˆ™å’Œåº”ç”¨\n",
    "print(f\"\\n   ğŸ“¡ 2.1 å¹¿æ’­è§„åˆ™å’Œåº”ç”¨:\")\n",
    "\n",
    "def demonstrate_broadcasting():\n",
    "    \"\"\"æ¼”ç¤ºå¹¿æ’­æœºåˆ¶\"\"\"\n",
    "    \n",
    "    # æ ‡é‡ä¸æ•°ç»„å¹¿æ’­\n",
    "    arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    scalar = 10\n",
    "    print(f\"   æ ‡é‡å¹¿æ’­:\")\n",
    "    print(f\"   æ•°ç»„:\\n{arr}\")\n",
    "    print(f\"   æ ‡é‡: {scalar}\")\n",
    "    print(f\"   arr + scalar:\\n{arr + scalar}\")\n",
    "    \n",
    "    # è¡Œå‘é‡å¹¿æ’­\n",
    "    row_vec = np.array([10, 20, 30])\n",
    "    print(f\"\\n   è¡Œå‘é‡å¹¿æ’­:\")\n",
    "    print(f\"   æ•°ç»„:\\n{arr}\")\n",
    "    print(f\"   è¡Œå‘é‡: {row_vec}\")\n",
    "    print(f\"   arr + row_vec:\\n{arr + row_vec}\")\n",
    "    \n",
    "    # åˆ—å‘é‡å¹¿æ’­\n",
    "    col_vec = np.array([[10], [20]])\n",
    "    print(f\"\\n   åˆ—å‘é‡å¹¿æ’­:\")\n",
    "    print(f\"   æ•°ç»„:\\n{arr}\")\n",
    "    print(f\"   åˆ—å‘é‡:\\n{col_vec}\")\n",
    "    print(f\"   arr + col_vec:\\n{arr + col_vec}\")\n",
    "    \n",
    "    # äºŒç»´å¹¿æ’­\n",
    "    arr1 = np.array([[1], [2], [3]])  # (3,1)\n",
    "    arr2 = np.array([[10, 20, 30]])   # (1,3)\n",
    "    print(f\"\\n   äºŒç»´å¹¿æ’­:\")\n",
    "    print(f\"   arr1 (3,1):\\n{arr1}\")\n",
    "    print(f\"   arr2 (1,3): {arr2}\")\n",
    "    print(f\"   arr1 + arr2 (3,3):\\n{arr1 + arr2}\")\n",
    "    \n",
    "    # å¹¿æ’­çš„å®é™…åº”ç”¨\n",
    "    print(f\"\\n   å¹¿æ’­å®é™…åº”ç”¨ - æ•°æ®æ ‡å‡†åŒ–:\")\n",
    "    data = np.random.normal(5, 2, (4, 3))  # 4ä¸ªæ ·æœ¬ï¼Œ3ä¸ªç‰¹å¾\n",
    "    print(f\"   åŸå§‹æ•°æ®:\\n{data}\")\n",
    "    \n",
    "    # æŒ‰åˆ—æ ‡å‡†åŒ–ï¼ˆæ¯ä¸ªç‰¹å¾ï¼‰\n",
    "    col_means = np.mean(data, axis=0)  # æ²¿è¡Œè®¡ç®—å‡å€¼ï¼Œå¾—åˆ°æ¯åˆ—çš„å‡å€¼\n",
    "    col_stds = np.std(data, axis=0)    # æ²¿è¡Œè®¡ç®—æ ‡å‡†å·®ï¼Œå¾—åˆ°æ¯åˆ—çš„æ ‡å‡†å·®\n",
    "    \n",
    "    print(f\"   åˆ—å‡å€¼: {col_means}\")\n",
    "    print(f\"   åˆ—æ ‡å‡†å·®: {col_stds}\")\n",
    "    \n",
    "    normalized_data = (data - col_means) / col_stds\n",
    "    print(f\"   æ ‡å‡†åŒ–åæ•°æ®:\\n{normalized_data}\")\n",
    "    print(f\"   æ ‡å‡†åŒ–åå‡å€¼: {np.mean(normalized_data, axis=0)}\")\n",
    "    print(f\"   æ ‡å‡†åŒ–åæ ‡å‡†å·®: {np.std(normalized_data, axis=0)}\")\n",
    "\n",
    "demonstrate_broadcasting()\n",
    "\n",
    "# 2.2 LangChainæ‰¹é‡è®¡ç®—åº”ç”¨\n",
    "print(f\"\\n   ğŸš€ 2.2 LangChainæ‰¹é‡è®¡ç®—åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class BatchCalculator:\n",
    "    \"\"\"æ‰¹é‡è®¡ç®—å™¨ - æ¨¡æ‹ŸLangChainçš„æ‰¹é‡å‘é‡è®¡ç®—\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 64):\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def compute_similarity_matrix(self, vectors: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ\"\"\"\n",
    "        # å½’ä¸€åŒ–å‘é‡\n",
    "        normalized_vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "        \n",
    "        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ\n",
    "        similarity_matrix = np.dot(normalized_vectors, normalized_vectors.T)\n",
    "        return similarity_matrix\n",
    "    \n",
    "    def compute_distance_matrix(self, vectors: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"è®¡ç®—è·ç¦»çŸ©é˜µ\"\"\"\n",
    "        num_vectors = vectors.shape[0]\n",
    "        distance_matrix = np.zeros((num_vectors, num_vectors))\n",
    "        \n",
    "        for i in range(num_vectors):\n",
    "            for j in range(i, num_vectors):\n",
    "                dist = np.linalg.norm(vectors[i] - vectors[j])\n",
    "                distance_matrix[i, j] = dist\n",
    "                distance_matrix[j, i] = dist\n",
    "        \n",
    "        return distance_matrix\n",
    "    \n",
    "    def batch_cosine_similarity(self, query_vectors: np.ndarray, \n",
    "                                candidate_vectors: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"æ‰¹é‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "        # å½’ä¸€åŒ–\n",
    "        query_norm = query_vectors / np.linalg.norm(query_vectors, axis=1, keepdims=True)\n",
    "        candidate_norm = candidate_vectors / np.linalg.norm(candidate_vectors, axis=1, keepdims=True)\n",
    "        \n",
    "        # æ‰¹é‡ç‚¹ç§¯\n",
    "        similarities = np.dot(query_norm, candidate_norm.T)\n",
    "        return similarities\n",
    "    \n",
    "    def compute_attention_weights(self, query: np.ndarray, keys: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "        scores = np.dot(query, keys.T)\n",
    "        \n",
    "        # Softmaxå½’ä¸€åŒ–\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # æ•°å€¼ç¨³å®šæ€§\n",
    "        weights = exp_scores / np.sum(exp_scores)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def weighted_sum(self, values: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"åŠ æƒæ±‚å’Œ\"\"\"\n",
    "        return np.dot(weights, values)\n",
    "\n",
    "def demonstrate_batch_calculator():\n",
    "    \"\"\"æ¼”ç¤ºæ‰¹é‡è®¡ç®—å™¨\"\"\"\n",
    "    calculator = BatchCalculator(embedding_dim=32)\n",
    "    \n",
    "    # åˆ›å»ºæµ‹è¯•å‘é‡\n",
    "    num_vectors = 5\n",
    "    vectors = np.random.normal(0, 1, (num_vectors, calculator.embedding_dim))\n",
    "    \n",
    "    print(f\"   æ‰¹é‡è®¡ç®—æ¼”ç¤º:\")\n",
    "    print(f\"   å‘é‡æ•°é‡: {num_vectors}\")\n",
    "    print(f\"   å‘é‡ç»´åº¦: {calculator.embedding_dim}\")\n",
    "    \n",
    "    # ç›¸ä¼¼åº¦çŸ©é˜µ\n",
    "    print(f\"\\n   ç›¸ä¼¼åº¦çŸ©é˜µ:\")\n",
    "    similarity_matrix = calculator.compute_similarity_matrix(vectors)\n",
    "    print(f\"   ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {similarity_matrix.shape}\")\n",
    "    print(f\"   ç›¸ä¼¼åº¦çŸ©é˜µ:\\n{similarity_matrix}\")\n",
    "    \n",
    "    # è·ç¦»çŸ©é˜µ\n",
    "    print(f\"\\n   è·ç¦»çŸ©é˜µ:\")\n",
    "    distance_matrix = calculator.compute_distance_matrix(vectors)\n",
    "    print(f\"   è·ç¦»çŸ©é˜µå½¢çŠ¶: {distance_matrix.shape}\")\n",
    "    print(f\"   è·ç¦»çŸ©é˜µ:\\n{distance_matrix}\")\n",
    "    \n",
    "    # æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—\n",
    "    print(f\"\\n   æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—:\")\n",
    "    query_vectors = vectors[:2]  # å‰2ä¸ªä½œä¸ºæŸ¥è¯¢\n",
    "    candidate_vectors = vectors[2:]  # å3ä¸ªä½œä¸ºå€™é€‰\n",
    "    \n",
    "    batch_similarities = calculator.batch_cosine_similarity(query_vectors, candidate_vectors)\n",
    "    print(f\"   æŸ¥è¯¢å‘é‡æ•°é‡: {query_vectors.shape[0]}\")\n",
    "    print(f\"   å€™é€‰å‘é‡æ•°é‡: {candidate_vectors.shape[0]}\")\n",
    "    print(f\"   æ‰¹é‡ç›¸ä¼¼åº¦çŸ©é˜µ:\\n{batch_similarities}\")\n",
    "    \n",
    "    # æ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤º\n",
    "    print(f\"\\n   æ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤º:\")\n",
    "    query = vectors[0]  # å•ä¸ªæŸ¥è¯¢\n",
    "    keys = vectors[1:4]  # 3ä¸ªé”®\n",
    "    values = vectors[1:4]  # 3ä¸ªå€¼\n",
    "    \n",
    "    attention_weights = calculator.compute_attention_weights(query, keys)\n",
    "    weighted_result = calculator.weighted_sum(values, attention_weights)\n",
    "    \n",
    "    print(f\"   æŸ¥è¯¢å‘é‡å½¢çŠ¶: {query.shape}\")\n",
    "    print(f\"   é”®å‘é‡å½¢çŠ¶: {keys.shape}\")\n",
    "    print(f\"   æ³¨æ„åŠ›æƒé‡: {attention_weights}\")\n",
    "    print(f\"   åŠ æƒç»“æœå½¢çŠ¶: {weighted_result.shape}\")\n",
    "    print(f\"   åŠ æƒç»“æœå‰5ç»´: {weighted_result[:5]}\")\n",
    "\n",
    "demonstrate_batch_calculator()\n",
    "\n",
    "# 3. ç»Ÿè®¡å‡½æ•°å’Œèšåˆæ“ä½œ\n",
    "print(f\"\\nğŸ“ 3. ç»Ÿè®¡å‡½æ•°å’Œèšåˆæ“ä½œ:\")\n",
    "\n",
    "# 3.1 åŸºç¡€ç»Ÿè®¡å‡½æ•°\n",
    "print(f\"\\n   ğŸ“Š 3.1 åŸºç¡€ç»Ÿè®¡å‡½æ•°:\")\n",
    "\n",
    "def demonstrate_statistics():\n",
    "    \"\"\"æ¼”ç¤ºç»Ÿè®¡å‡½æ•°\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºæµ‹è¯•æ•°æ®\n",
    "    data = np.random.normal(10, 3, (4, 5))  # 4è¡Œ5åˆ—çš„æ­£æ€åˆ†å¸ƒæ•°æ®\n",
    "    print(f\"   æµ‹è¯•æ•°æ®:\\n{data}\")\n",
    "    \n",
    "    # åŸºç¡€ç»Ÿè®¡é‡\n",
    "    print(f\"\\n   åŸºç¡€ç»Ÿè®¡é‡:\")\n",
    "    print(f\"   å…¨å±€å‡å€¼: {np.mean(data):.4f}\")\n",
    "    print(f\"   å…¨å±€ä¸­ä½æ•°: {np.median(data):.4f}\")\n",
    "    print(f\"   å…¨å±€æ ‡å‡†å·®: {np.std(data):.4f}\")\n",
    "    print(f\"   å…¨å±€æ–¹å·®: {np.var(data):.4f}\")\n",
    "    print(f\"   å…¨å±€æœ€å°å€¼: {np.min(data):.4f}\")\n",
    "    print(f\"   å…¨å±€æœ€å¤§å€¼: {np.max(data):.4f}\")\n",
    "    print(f\"   å…¨å±€èŒƒå›´: {np.ptp(data):.4f}\")  # peak-to-peak\n",
    "    \n",
    "    # æŒ‰è½´ç»Ÿè®¡\n",
    "    print(f\"\\n   æŒ‰è½´ç»Ÿè®¡:\")\n",
    "    print(f\"   æŒ‰è¡Œå‡å€¼ (axis=1): {np.mean(data, axis=1)}\")\n",
    "    print(f\"   æŒ‰åˆ—å‡å€¼ (axis=0): {np.mean(data, axis=0)}\")\n",
    "    print(f\"   æŒ‰è¡Œæ ‡å‡†å·® (axis=1): {np.std(data, axis=1)}\")\n",
    "    print(f\"   æŒ‰åˆ—æ ‡å‡†å·® (axis=0): {np.std(data, axis=0)}\")\n",
    "    \n",
    "    # åˆ†ä½æ•°\n",
    "    print(f\"\\n   åˆ†ä½æ•°:\")\n",
    "    flat_data = data.flatten()\n",
    "    print(f\"   25%åˆ†ä½æ•°: {np.percentile(flat_data, 25):.4f}\")\n",
    "    print(f\"   50%åˆ†ä½æ•° (ä¸­ä½æ•°): {np.percentile(flat_data, 50):.4f}\")\n",
    "    print(f\"   75%åˆ†ä½æ•°: {np.percentile(flat_data, 75):.4f}\")\n",
    "    \n",
    "    # ç´¯ç§¯æ“ä½œ\n",
    "    print(f\"\\n   ç´¯ç§¯æ“ä½œ:\")\n",
    "    arr_1d = np.array([1, 2, 3, 4, 5])\n",
    "    print(f\"   åŸæ•°ç»„: {arr_1d}\")\n",
    "    print(f\"   ç´¯ç§¯æ±‚å’Œ: {np.cumsum(arr_1d)}\")\n",
    "    print(f\"   ç´¯ç§¯ä¹˜ç§¯: {np.cumprod(arr_1d)}\")\n",
    "    \n",
    "    # ç›¸å…³æ€§\n",
    "    print(f\"\\n   ç›¸å…³æ€§:\")\n",
    "    x = np.array([1, 2, 3, 4, 5])\n",
    "    y = np.array([2, 4, 5, 4, 5])\n",
    "    correlation = np.corrcoef(x, y)[0, 1]\n",
    "    print(f\"   x: {x}\")\n",
    "    print(f\"   y: {y}\")\n",
    "    print(f\"   ç›¸å…³ç³»æ•°: {correlation:.4f}\")\n",
    "\n",
    "demonstrate_statistics()\n",
    "\n",
    "# 3.2 LangChainæ•°æ®åˆ†æåº”ç”¨\n",
    "print(f\"\\n   ğŸ“ˆ 3.2 LangChainæ•°æ®åˆ†æåº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class DataAnalyzer:\n",
    "    \"\"\"æ•°æ®åˆ†æå™¨ - æ¨¡æ‹ŸLangChainçš„æ•°æ®åˆ†æåŠŸèƒ½\"\"\"\n",
    "    \n",
    "    def analyze_embeddings(self, embeddings: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æåµŒå…¥å‘é‡\"\"\"\n",
    "        analysis = {\n",
    "            'shape': embeddings.shape,\n",
    "            'mean_norm': np.mean(np.linalg.norm(embeddings, axis=1)),\n",
    "            'std_norm': np.std(np.linalg.norm(embeddings, axis=1)),\n",
    "            'mean_values': np.mean(embeddings, axis=0),\n",
    "            'std_values': np.std(embeddings, axis=0),\n",
    "            'min_values': np.min(embeddings, axis=0),\n",
    "            'max_values': np.max(embeddings, axis=0)\n",
    "        }\n",
    "        return analysis\n",
    "    \n",
    "    def find_outliers(self, data: np.ndarray, threshold: float = 2.0) -> np.ndarray:\n",
    "        \"\"\"æŸ¥æ‰¾å¼‚å¸¸å€¼ï¼ˆåŸºäºZ-scoreï¼‰\"\"\"\n",
    "        z_scores = np.abs((data - np.mean(data, axis=0)) / np.std(data, axis=0))\n",
    "        outlier_mask = np.any(z_scores > threshold, axis=1)\n",
    "        return np.where(outlier_mask)[0]\n",
    "    \n",
    "    def compute_similarity_stats(self, similarity_matrix: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"è®¡ç®—ç›¸ä¼¼åº¦ç»Ÿè®¡\"\"\"\n",
    "        # è·å–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰\n",
    "        upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "        \n",
    "        stats = {\n",
    "            'mean_similarity': np.mean(upper_triangle),\n",
    "            'std_similarity': np.std(upper_triangle),\n",
    "            'min_similarity': np.min(upper_triangle),\n",
    "            'max_similarity': np.max(upper_triangle),\n",
    "            'median_similarity': np.median(upper_triangle)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def cluster_by_similarity(self, similarity_matrix: np.ndarray, \n",
    "                             threshold: float = 0.7) -> List[List[int]]:\n",
    "        \"\"\"åŸºäºç›¸ä¼¼åº¦è¿›è¡Œç®€å•èšç±»\"\"\"\n",
    "        n = similarity_matrix.shape[0]\n",
    "        visited = np.zeros(n, dtype=bool)\n",
    "        clusters = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            if not visited[i]:\n",
    "                cluster = [i]\n",
    "                visited[i] = True\n",
    "                \n",
    "                for j in range(i + 1, n):\n",
    "                    if not visited[j] and similarity_matrix[i, j] > threshold:\n",
    "                        cluster.append(j)\n",
    "                        visited[j] = True\n",
    "                \n",
    "                clusters.append(cluster)\n",
    "        \n",
    "        return clusters\n",
    "\n",
    "def demonstrate_data_analyzer():\n",
    "    \"\"\"æ¼”ç¤ºæ•°æ®åˆ†æå™¨\"\"\"\n",
    "    analyzer = DataAnalyzer()\n",
    "    \n",
    "    # åˆ›å»ºæµ‹è¯•åµŒå…¥\n",
    "    num_embeddings = 6\n",
    "    embedding_dim = 32\n",
    "    embeddings = np.random.normal(0, 1, (num_embeddings, embedding_dim))\n",
    "    \n",
    "    # æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼\n",
    "    embeddings[0] *= 3  # ç¬¬ä¸€ä¸ªå‘é‡ä½œä¸ºå¼‚å¸¸å€¼\n",
    "    \n",
    "    print(f\"   æ•°æ®åˆ†ææ¼”ç¤º:\")\n",
    "    print(f\"   åµŒå…¥æ•°é‡: {num_embeddings}\")\n",
    "    print(f\"   åµŒå…¥ç»´åº¦: {embedding_dim}\")\n",
    "    \n",
    "    # åµŒå…¥åˆ†æ\n",
    "    print(f\"\\n   åµŒå…¥å‘é‡åˆ†æ:\")\n",
    "    embedding_analysis = analyzer.analyze_embeddings(embeddings)\n",
    "    \n",
    "    print(f\"   å½¢çŠ¶: {embedding_analysis['shape']}\")\n",
    "    print(f\"   å¹³å‡èŒƒæ•°: {embedding_analysis['mean_norm']:.4f}\")\n",
    "    print(f\"   èŒƒæ•°æ ‡å‡†å·®: {embedding_analysis['std_norm']:.4f}\")\n",
    "    print(f\"   å¹³å‡å€¼å‰5ç»´: {embedding_analysis['mean_values'][:5]}\")\n",
    "    print(f\"   æ ‡å‡†å·®å‰5ç»´: {embedding_analysis['std_values'][:5]}\")\n",
    "    \n",
    "    # å¼‚å¸¸å€¼æ£€æµ‹\n",
    "    print(f\"\\n   å¼‚å¸¸å€¼æ£€æµ‹:\")\n",
    "    outlier_indices = analyzer.find_outliers(embeddings, threshold=2.0)\n",
    "    print(f\"   å¼‚å¸¸å€¼ç´¢å¼•: {outlier_indices}\")\n",
    "    \n",
    "    # ç›¸ä¼¼åº¦åˆ†æ\n",
    "    print(f\"\\n   ç›¸ä¼¼åº¦åˆ†æ:\")\n",
    "    calculator = BatchCalculator(embedding_dim)\n",
    "    similarity_matrix = calculator.compute_similarity_matrix(embeddings)\n",
    "    \n",
    "    similarity_stats = analyzer.compute_similarity_stats(similarity_matrix)\n",
    "    print(f\"   å¹³å‡ç›¸ä¼¼åº¦: {similarity_stats['mean_similarity']:.4f}\")\n",
    "    print(f\"   ç›¸ä¼¼åº¦æ ‡å‡†å·®: {similarity_stats['std_similarity']:.4f}\")\n",
    "    print(f\"   æœ€å°ç›¸ä¼¼åº¦: {similarity_stats['min_similarity']:.4f}\")\n",
    "    print(f\"   æœ€å¤§ç›¸ä¼¼åº¦: {similarity_stats['max_similarity']:.4f}\")\n",
    "    \n",
    "    # ç®€å•èšç±»\n",
    "    print(f\"\\n   åŸºäºç›¸ä¼¼åº¦çš„èšç±» (threshold=0.5):\")\n",
    "    clusters = analyzer.cluster_by_similarity(similarity_matrix, threshold=0.5)\n",
    "    for i, cluster in enumerate(clusters, 1):\n",
    "        print(f\"   èšç±»{i}: {cluster}\")\n",
    "\n",
    "demonstrate_data_analyzer()\n",
    "\n",
    "print(f\"\\nâœ… NumPyæ•°å­¦è¿ç®—å’Œå¹¿æ’­æœºåˆ¶å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡NumPyçš„åŸºæœ¬æ•°å­¦è¿ç®—\")\n",
    "print(f\"   âœ“ ç†è§£å¹¿æ’­æœºåˆ¶çš„å·¥ä½œåŸç†\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨ç»Ÿè®¡å‡½æ•°å’Œèšåˆæ“ä½œ\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿè¿›è¡Œå‘é‡è®¡ç®—å’Œç›¸ä¼¼åº¦åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å­¦ä¹ æ€»ç»“\n",
    "\n",
    "### âœ… çŸ¥è¯†æ¸…å•è¾¾æˆæƒ…å†µéªŒè¯\n",
    "\n",
    "**5.1 NumPyæ•°ç»„æ“ä½œ [â­åŸºç¡€]**\n",
    "- âœ… æŒæ¡NumPyæ•°ç»„çš„åˆ›å»ºå’ŒåŸºæœ¬æ“ä½œ\n",
    "- âœ… ç†è§£æ•°ç»„ç´¢å¼•ã€åˆ‡ç‰‡å’Œå½¢çŠ¶æ“ä½œ\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨NumPyçš„æ•°å­¦å‡½æ•°å’Œç»Ÿè®¡æ–¹æ³•\n",
    "- âœ… èƒ½å¤Ÿè¿›è¡ŒçŸ©é˜µè¿ç®—å’Œå¹¿æ’­æ“ä½œ\n",
    "- âœ… æŒæ¡å‘é‡è®¡ç®—å’Œç›¸ä¼¼åº¦åˆ†æ\n",
    "- âœ… ç†è§£æ‰¹æ¬¡å¤„ç†å’Œæ‰¹é‡è®¡ç®—\n",
    "- âœ… èƒ½å¤Ÿåº”ç”¨NumPyè§£å†³å®é™…é—®é¢˜\n",
    "- âœ… èƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„æ•°å€¼è®¡ç®—ç³»ç»Ÿ\n",
    "\n",
    "### ğŸ¯ ä¸LangChainå­¦ä¹ çš„å…³è”\n",
    "\n",
    "**NumPyé‡è¦æ€§**ï¼š\n",
    "- NumPyæ˜¯LangChainå‘é‡è®¡ç®—å’ŒåµŒå…¥å¤„ç†çš„åŸºç¡€\n",
    "- NumPyæ•°ç»„æ“ä½œå¯¹LangChainçš„ç›¸ä¼¼åº¦è®¡ç®—è‡³å…³é‡è¦\n",
    "- NumPyæ”¯æŒLangChainçš„æ‰¹é‡æ•°æ®å¤„ç†å’Œä¼˜åŒ–\n",
    "- NumPyä¸ºLangChainçš„æœºå™¨å­¦ä¹ åŠŸèƒ½æä¾›æ•°å­¦åŸºç¡€\n",
    "- NumPyçš„å¹¿æ’­æœºåˆ¶ç®€åŒ–äº†LangChainçš„æ‰¹é‡æ“ä½œ\n",
    "\n",
    "**å®é™…åº”ç”¨åœºæ™¯**ï¼š\n",
    "- LangChainçš„åµŒå…¥å‘é‡ç”Ÿæˆå’Œå¤„ç†\n",
    "- LangChainçš„å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢å’Œæœç´¢\n",
    "- LangChainçš„æ‰¹é‡æ–‡æœ¬å¤„ç†å’Œä¼˜åŒ–\n",
    "- LangChainçš„æ³¨æ„åŠ›æœºåˆ¶å’Œæƒé‡è®¡ç®—\n",
    "- LangChainçš„æ•°æ®åˆ†æå’Œå¼‚å¸¸æ£€æµ‹\n",
    "\n",
    "### ğŸ“š è¿›é˜¶å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹ å»ºè®®**ï¼š\n",
    "   - æ·±å…¥ç»ƒä¹ NumPyçš„é«˜çº§ç´¢å¼•å’Œå¹¿æ’­\n",
    "   - æŒæ¡æ›´å¤šçš„çº¿æ€§ä»£æ•°è¿ç®—\n",
    "   - ç†Ÿç»ƒä½¿ç”¨NumPyçš„éšæœºæ•°ç”Ÿæˆ\n",
    "\n",
    "2. **æ‰©å±•å­¦ä¹ **ï¼š\n",
    "   - å­¦ä¹ SciPyçš„ç§‘å­¦è®¡ç®—åŠŸèƒ½\n",
    "   - äº†è§£Pandasçš„æ•°æ®å¤„ç†èƒ½åŠ›\n",
    "   - æ¢ç´¢Matplotlibçš„æ•°æ®å¯è§†åŒ–\n",
    "\n",
    "3. **å®é™…åº”ç”¨**ï¼š\n",
    "   - æ„å»ºé«˜æ•ˆçš„å‘é‡æ£€ç´¢ç³»ç»Ÿ\n",
    "   - å¼€å‘æ‰¹é‡æ•°æ®å¤„ç†ç®¡é“\n",
    "   - å®ç°æœºå™¨å­¦ä¹ ç®—æ³•åŸå‹\n",
    "\n",
    "### ğŸ”§ å¸¸è§é”™è¯¯ä¸æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **æ•°ç»„å½¢çŠ¶é”™è¯¯**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå½¢çŠ¶ä¸åŒ¹é…\n",
    "   arr1 = np.array([1, 2, 3])\n",
    "   arr2 = np.array([4, 5])\n",
    "   result = arr1 + arr2  # ValueError\n",
    "   \n",
    "   # æ­£ç¡®ï¼šç¡®ä¿å½¢çŠ¶å…¼å®¹æˆ–ä½¿ç”¨å¹¿æ’­\n",
    "   arr1 = np.array([1, 2, 3])\n",
    "   arr2 = np.array([4, 5, 6])\n",
    "   result = arr1 + arr2  # æ­£å¸¸å·¥ä½œ\n",
    "   ```\n",
    "\n",
    "2. **æ•°æ®ç±»å‹ä¸åŒ¹é…**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šæ•°æ®ç±»å‹é—®é¢˜\n",
    "   int_arr = np.array([1, 2, 3])\n",
    "   result = int_arr / 2  # ç»“æœæ˜¯æµ®ç‚¹æ•°ï¼Œå¯èƒ½ä¸æ˜¯é¢„æœŸ\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹\n",
    "   float_arr = np.array([1, 2, 3], dtype=float)\n",
    "   result = float_arr / 2  # æ˜ç¡®çš„æµ®ç‚¹è¿ç®—\n",
    "   ```\n",
    "\n",
    "3. **å†…å­˜ä½¿ç”¨é—®é¢˜**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåˆ›å»ºè¿‡å¤§çš„æ•°ç»„\n",
    "   huge_array = np.zeros((100000, 100000))  # å¯èƒ½å†…å­˜ä¸è¶³\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåˆ†å—å¤„ç†æˆ–ä½¿ç”¨ç”Ÿæˆå™¨\n",
    "   chunk_size = 1000\n",
    "   for i in range(0, 100000, chunk_size):\n",
    "       chunk = np.zeros((chunk_size, 100000))\n",
    "       process_chunk(chunk)\n",
    "   ```\n",
    "\n",
    "4. **å¹¿æ’­è¯¯è§£**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå¹¿æ’­è§„åˆ™ç†è§£é”™è¯¯\n",
    "   arr1 = np.array([[1, 2], [3, 4]])  # (2,2)\n",
    "   arr2 = np.array([1, 2, 3])          # (3,) - æ— æ³•å¹¿æ’­åˆ°(2,2)\n",
    "   \n",
    "   # æ­£ç¡®ï¼šç¡®ä¿å¹¿æ’­å…¼å®¹\n",
    "   arr1 = np.array([[1, 2], [3, 4]])  # (2,2)\n",
    "   arr2 = np.array([1, 2])             # (2,) - å¯ä»¥å¹¿æ’­åˆ°(2,2)\n",
    "   result = arr1 + arr2\n",
    "   ```\n",
    "\n",
    "5. **è§†å›¾vså¤åˆ¶**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šè¯¯è®¤ä¸ºåˆ‡ç‰‡åˆ›å»ºå‰¯æœ¬\n",
    "   arr = np.array([1, 2, 3, 4, 5])\n",
    "   slice_arr = arr[1:4]\n",
    "   slice_arr[0] = 100  # ä¿®æ”¹äº†åŸå§‹æ•°ç»„ï¼\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ˜ç¡®åˆ›å»ºå‰¯æœ¬\n",
    "   arr = np.array([1, 2, 3, 4, 5])\n",
    "   slice_arr = arr[1:4].copy()  # åˆ›å»ºå‰¯æœ¬\n",
    "   slice_arr[0] = 100  # ä¸å½±å“åŸå§‹æ•°ç»„\n",
    "   ```\n",
    "\n",
    "6. **æ•°å€¼ç¨³å®šæ€§**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šæ•°å€¼æº¢å‡º\n",
    "   large_numbers = np.array([1e10, 1e10])\n",
    "   result = large_numbers ** 2  # å¯èƒ½æº¢å‡º\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨æ›´é«˜ç²¾åº¦æˆ–å½’ä¸€åŒ–\n",
    "   large_numbers = np.array([1e10, 1e10], dtype=np.float64)\n",
    "   normalized = large_numbers / 1e10\n",
    "   result = normalized ** 2\n",
    "   ```\n",
    "\n",
    "### ğŸŒ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "**æ•°ç»„æ“ä½œä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯\n",
    "- é¿å…ä¸å¿…è¦çš„æ•°ç»„å¤åˆ¶\n",
    "- åˆç†ä½¿ç”¨å¹¿æ’­æœºåˆ¶\n",
    "- é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹\n",
    "\n",
    "**å†…å­˜ä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ•°æ®\n",
    "- åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¤§æ•°ç»„\n",
    "- ä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶å¤„ç†è¶…å¤§æ•°ç»„\n",
    "- è€ƒè™‘ä½¿ç”¨ç¨€ç–çŸ©é˜µ\n",
    "\n",
    "**è®¡ç®—ä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨NumPyçš„å†…ç½®å‡½æ•°\n",
    "- åˆ©ç”¨BLAS/LAPACKä¼˜åŒ–\n",
    "- è€ƒè™‘ä½¿ç”¨GPUåŠ é€Ÿï¼ˆCuPyï¼‰\n",
    "- åˆç†ä½¿ç”¨ç¼“å­˜æœºåˆ¶\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®ŒæˆNumPyæ•°ç»„æ“ä½œå­¦ä¹ ï¼**\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†NumPyæ•°ç»„æ“ä½œçš„æ ¸å¿ƒæŠ€èƒ½ï¼Œèƒ½å¤Ÿç†Ÿç»ƒè¿›è¡Œæ•°ç»„åˆ›å»ºã€ç´¢å¼•åˆ‡ç‰‡ã€å½¢çŠ¶æ“ä½œã€æ•°å­¦è¿ç®—ã€å‘é‡è®¡ç®—ç­‰ï¼Œä¸ºåç»­å­¦ä¹ Pandasæ•°æ®å¤„ç†å’Œæœºå™¨å­¦ä¹ å¥ å®šäº†åšå®çš„æ•°å€¼è®¡ç®—åŸºç¡€ã€‚\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ é¢„å‘Š\n",
    "\n",
    "**ç»§ç»­ç¬¬äº”èŠ‚ï¼šæ•°æ®å¤„ç†**\n",
    "- 5.2 Pandasæ•°æ®å¤„ç†\n",
    "- 5.3 æ•°æ®å¯è§†åŒ–åŸºç¡€\n",
    "- 5.4 æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†\n",
    "\n",
    "**åç»­ç« èŠ‚é¢„å‘Š**ï¼š\n",
    "- ç»Ÿè®¡åˆ†æåº”ç”¨\n",
    "- æœºå™¨å­¦ä¹ åŸºç¡€\n",
    "- æ—¶é—´åºåˆ—åˆ†æ\n",
    "\n",
    "ç»§ç»­åŠ æ²¹ï¼Œæ•°æ®å¤„ç†æŠ€èƒ½æ­£åœ¨å¿«é€Ÿæå‡ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
