{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42-æœºå™¨å­¦ä¹ åŸºç¡€\n",
    "\n",
    "## ğŸ“š ç”¨é€”è¯´æ˜\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**ï¼š\n",
    "- æŒæ¡æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œåˆ†ç±»\n",
    "- ç†Ÿç»ƒä½¿ç”¨scikit-learnè¿›è¡Œç›‘ç£å­¦ä¹ \n",
    "- ç†è§£æ— ç›‘ç£å­¦ä¹ ç®—æ³•å’Œåº”ç”¨\n",
    "- èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥ä½œæµ\n",
    "\n",
    "**å‰ç½®è¦æ±‚**ï¼š\n",
    "- å·²å®Œæˆ41-ç»Ÿè®¡åˆ†æåº”ç”¨å­¦ä¹ \n",
    "- ç†Ÿç»ƒæŒæ¡NumPyã€Pandasæ•°æ®å¤„ç†\n",
    "- äº†è§£åŸºæœ¬çš„ç»Ÿè®¡å­¦æ¦‚å¿µ\n",
    "\n",
    "**ä¸LangChainå…³è”**ï¼š\n",
    "- æœºå™¨å­¦ä¹ æ˜¯LangChainæ™ºèƒ½åº”ç”¨çš„æ ¸å¿ƒ\n",
    "- æ”¯æŒLangChainçš„æ–‡æœ¬åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æ\n",
    "- ä¸ºLangChainçš„æ¨èç³»ç»Ÿæä¾›ç®—æ³•åŸºç¡€\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„é¢„æµ‹å’Œå†³ç­–èƒ½åŠ›\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ çŸ¥è¯†ç‚¹è¦†ç›–\n",
    "\n",
    "### 5.6 æœºå™¨å­¦ä¹ åŸºç¡€ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šæœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚æŒæ¡è¿™äº›æŠ€èƒ½å¯¹äºæ„å»ºæ™ºèƒ½LangChainåº”ç”¨éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ\n",
    "- ç†è§£ç›‘ç£å­¦ä¹ ç®—æ³•åŸç†\n",
    "- ç†Ÿç»ƒä½¿ç”¨æ— ç›‘ç£å­¦ä¹ æ–¹æ³•\n",
    "- èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„MLå·¥ä½œæµ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æœºå™¨å­¦ä¹ åº”ç”¨ç¤ºä¾‹\n",
    "- è¿›è¡Œå¤šç§ç®—æ³•çš„æ€§èƒ½æ¯”è¾ƒ\n",
    "- åº”ç”¨æœºå™¨å­¦ä¹ è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æœºå™¨å­¦ä¹ ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¤– æœºå™¨å­¦ä¹ åŸºç¡€:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Any, Optional, Union, Dict\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æœºå™¨å­¦ä¹ åº“\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, RFE\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                           confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "                           mean_squared_error, mean_absolute_error, r2_score,\n",
    "                           silhouette_score, calinski_harabasz_score, davies_bouldin_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# ç›‘ç£å­¦ä¹ ç®—æ³•\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, \n",
    "                          GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "                          AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# æ— ç›‘ç£å­¦ä¹ ç®—æ³•\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.decomposition import PCA, FactorAnalysis, NMF\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"âœ… Pandasç‰ˆæœ¬: {pd.__version__}\")\n",
    "print(f\"âœ… NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"âœ… Scikit-learnç‰ˆæœ¬: {sklearn.__version__}\")\n",
    "\n",
    "# 1. æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ\n",
    "print(f\"\\nğŸ“ 1. æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ:\")\n",
    "\n",
    "# 1.1 åˆ›å»ºæœºå™¨å­¦ä¹ æ•°æ®é›†\n",
    "print(f\"\\n   ğŸ“Š 1.1 åˆ›å»ºæœºå™¨å­¦ä¹ æ•°æ®é›†:\")\n",
    "\n",
    "def create_ml_dataset(num_records: int = 2000):\n",
    "    \"\"\"åˆ›å»ºç”¨äºæœºå™¨å­¦ä¹ çš„ç¤ºä¾‹æ•°æ®é›†\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # åŸºç¡€ç‰¹å¾\n",
    "    data = {\n",
    "        'customer_id': range(1, num_records + 1),\n",
    "        'age': np.random.normal(35, 12, num_records),\n",
    "        'income': np.random.lognormal(10.5, 0.6, num_records),\n",
    "        'education_years': np.random.normal(15, 3, num_records),\n",
    "        'work_experience': np.random.exponential(6, num_records),\n",
    "        'credit_score': np.random.normal(650, 100, num_records),\n",
    "        'debt_to_income': np.random.beta(2, 5, num_records),\n",
    "        'savings_rate': np.random.beta(1, 3, num_records),\n",
    "        'transaction_count': np.random.poisson(25, num_records),\n",
    "        'avg_transaction_amount': np.random.exponential(100, num_records),\n",
    "        'account_balance': np.random.lognormal(8, 2, num_records),\n",
    "        'loan_amount': np.random.exponential(50000, num_records),\n",
    "        'loan_term_months': np.random.choice([12, 24, 36, 48, 60], num_records),\n",
    "        'employment_type': np.random.choice(['å…¨èŒ', 'å…¼èŒ', 'è‡ªç”±èŒä¸š', 'é€€ä¼‘'], num_records, p=[0.6, 0.2, 0.15, 0.05]),\n",
    "        'marital_status': np.random.choice(['å•èº«', 'å·²å©š', 'ç¦»å¼‚'], num_records, p=[0.35, 0.55, 0.1]),\n",
    "        'home_ownership': np.random.choice(['ç§Ÿæˆ¿', 'è‡ªæœ‰ä½æˆ¿', 'æŒ‰æ­'], num_records, p=[0.3, 0.4, 0.3]),\n",
    "        'region': np.random.choice(['ååŒ—', 'åä¸œ', 'åå—', 'è¥¿éƒ¨'], num_records),\n",
    "        'has_credit_card': np.random.choice([True, False], num_records, p=[0.7, 0.3]),\n",
    "        'has_loan_history': np.random.choice([True, False], num_records, p=[0.4, 0.6]),\n",
    "        'customer_segment': np.random.choice(['A', 'B', 'C', 'D'], num_records, p=[0.2, 0.3, 0.35, 0.15])\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ç¡®ä¿æ•°æ®åˆç†æ€§\n",
    "    df['age'] = np.clip(df['age'], 18, 80)\n",
    "    df['education_years'] = np.clip(df['education_years'], 8, 25)\n",
    "    df['work_experience'] = np.clip(df['work_experience'], 0, 50)\n",
    "    df['credit_score'] = np.clip(df['credit_score'], 300, 850)\n",
    "    df['income'] = np.clip(df['income'], 15000, 800000)\n",
    "    df['account_balance'] = np.clip(df['account_balance'], 0, 1000000)\n",
    "    df['loan_amount'] = np.clip(df['loan_amount'], 1000, 500000)\n",
    "    \n",
    "    # åˆ›å»ºç›®æ ‡å˜é‡ - åŸºäºç‰¹å¾çš„é€»è¾‘å…³ç³»\n",
    "    # ä¿¡ç”¨è¯„åˆ†ç›®æ ‡ (åˆ†ç±»)\n",
    "    credit_score_threshold = 650\n",
    "    df['credit_risk'] = ((df['credit_score'] < credit_score_threshold) | \n",
    "                        (df['debt_to_income'] > 0.4) | \n",
    "                        (df['has_loan_history'] == True) |\n",
    "                        (df['savings_rate'] < 0.1)).astype(int)\n",
    "    \n",
    "    # è´·æ¬¾é‡‘é¢é¢„æµ‹ (å›å½’)\n",
    "    df['predicted_loan_amount'] = (\n",
    "        df['income'] * 0.3 + \n",
    "        df['credit_score'] * 50 + \n",
    "        df['account_balance'] * 0.1 + \n",
    "        np.random.normal(0, 5000, num_records)\n",
    "    )\n",
    "    df['predicted_loan_amount'] = np.clip(df['predicted_loan_amount'], 1000, 500000)\n",
    "    \n",
    "    # å®¢æˆ·ç»†åˆ† (èšç±»æ ‡ç­¾)\n",
    "    # åŸºäºæ”¶å…¥å’Œå¹´é¾„çš„è‡ªç„¶åˆ†ç»„\n",
    "    conditions = [\n",
    "        (df['income'] > 100000) & (df['age'] < 40),\n",
    "        (df['income'] > 80000) & (df['age'] >= 40),\n",
    "        (df['income'] <= 80000) & (df['age'] < 35),\n",
    "        (df['income'] <= 80000) & (df['age'] >= 35)\n",
    "    ]\n",
    "    choices = ['é«˜æ”¶å…¥å¹´è½»ç¾¤ä½“', 'é«˜æ”¶å…¥æˆç†Ÿç¾¤ä½“', 'ä½æ”¶å…¥å¹´è½»ç¾¤ä½“', 'ä½æ”¶å…¥æˆç†Ÿç¾¤ä½“']\n",
    "    df['customer_cluster'] = np.select(conditions, choices, default='å…¶ä»–')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# åˆ›å»ºæœºå™¨å­¦ä¹ æ•°æ®é›†\n",
    "ml_df = create_ml_dataset(2000)\n",
    "print(f\"   åˆ›å»ºäº†åŒ…å« {len(ml_df)} è¡Œçš„æœºå™¨å­¦ä¹ æ•°æ®é›†\")\n",
    "print(f\"   ç‰¹å¾æ•°é‡: {len([col for col in ml_df.columns if col not in ['customer_id', 'credit_risk', 'predicted_loan_amount', 'customer_cluster']])}\")\n",
    "print(f\"   åˆ†ç±»ä»»åŠ¡: ä¿¡ç”¨é£é™©è¯„ä¼°\")\n",
    "print(f\"   å›å½’ä»»åŠ¡: è´·æ¬¾é‡‘é¢é¢„æµ‹\")\n",
    "print(f\"   èšç±»ä»»åŠ¡: å®¢æˆ·ç»†åˆ†\")\n",
    "\n",
    "# 1.2 æœºå™¨å­¦ä¹ åŸºç¡€æ¡†æ¶\n",
    "print(f\"\\n   ğŸ”§ 1.2 æœºå™¨å­¦ä¹ åŸºç¡€æ¡†æ¶:\")\n",
    "\n",
    "@dataclass\n",
    "class MLFramework:\n",
    "    \"\"\"æœºå™¨å­¦ä¹ åŸºç¡€æ¡†æ¶\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_data(self, target_col: str, feature_cols: List[str] = None, \n",
    "                     test_size: float = 0.2, random_state: int = 42):\n",
    "        \"\"\"å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®\"\"\"\n",
    "        print(f\"   å‡†å¤‡æ•°æ®: ç›®æ ‡å˜é‡ {target_col}\")\n",
    "        \n",
    "        if feature_cols is None:\n",
    "            # è‡ªåŠ¨é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆæ’é™¤IDå’Œç›®æ ‡å˜é‡ï¼‰\n",
    "            exclude_cols = ['customer_id', target_col, 'predicted_loan_amount', 'customer_cluster']\n",
    "            feature_cols = [col for col in self.df.columns if col not in exclude_cols]\n",
    "        \n",
    "        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡\n",
    "        X = self.df[feature_cols].copy()\n",
    "        y = self.df[target_col].copy()\n",
    "        \n",
    "        # å¤„ç†ç¼ºå¤±å€¼\n",
    "        X = X.fillna(X.mean(numeric_only=True))\n",
    "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in categorical_cols:\n",
    "            X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 'Unknown')\n",
    "        \n",
    "        # ç¼–ç åˆ†ç±»å˜é‡\n",
    "        label_encoders = {}\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "        \n",
    "        # å¦‚æœç›®æ ‡å˜é‡æ˜¯åˆ†ç±»çš„ï¼Œä¹Ÿéœ€è¦ç¼–ç \n",
    "        if y.dtype == 'object':\n",
    "            target_encoder = LabelEncoder()\n",
    "            y = target_encoder.fit_transform(y.astype(str))\n",
    "            self.target_encoder = target_encoder\n",
    "        \n",
    "        # åˆ†å‰²æ•°æ®\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y if len(np.unique(y)) > 1 else None\n",
    "        )\n",
    "        \n",
    "        self.feature_cols = feature_cols\n",
    "        self.label_encoders = label_encoders\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        print(f\"      è®­ç»ƒé›†å¤§å°: {self.X_train.shape}\")\n",
    "        print(f\"      æµ‹è¯•é›†å¤§å°: {self.X_test.shape}\")\n",
    "        print(f\"      ç‰¹å¾æ•°é‡: {len(feature_cols)}\")\n",
    "        \n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def scale_features(self, method: str = 'standard'):\n",
    "        \"\"\"ç‰¹å¾ç¼©æ”¾\"\"\"\n",
    "        print(f\"   ç‰¹å¾ç¼©æ”¾: {method}\")\n",
    "        \n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"è¯·å…ˆè°ƒç”¨ prepare_data æ–¹æ³•\")\n",
    "        \n",
    "        if method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(\"ä¸æ”¯æŒçš„ç¼©æ”¾æ–¹æ³•\")\n",
    "        \n",
    "        # æ‹Ÿåˆè®­ç»ƒæ•°æ®å¹¶è½¬æ¢\n",
    "        self.X_train_scaled = scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = scaler.transform(self.X_test)\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        print(f\"      ç¼©æ”¾å®Œæˆï¼Œç‰¹å¾å‡å€¼: {np.mean(self.X_train_scaled, axis=0)[:5]}\")\n",
    "        \n",
    "        return self.X_train_scaled, self.X_test_scaled\n",
    "    \n",
    "    def evaluate_classification(self, y_true, y_pred, y_prob=None, model_name=\"æ¨¡å‹\"):\n",
    "        \"\"\"è¯„ä¼°åˆ†ç±»æ¨¡å‹\"\"\"\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_true, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        if y_prob is not None and len(np.unique(y_true)) == 2:\n",
    "            metrics['auc_roc'] = roc_auc_score(y_true, y_prob)\n",
    "        \n",
    "        metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred).tolist()\n",
    "        \n",
    "        print(f\"   {model_name} åˆ†ç±»è¯„ä¼°:\")\n",
    "        print(f\"      å‡†ç¡®ç‡: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"      ç²¾ç¡®ç‡: {metrics['precision']:.3f}\")\n",
    "        print(f\"      å¬å›ç‡: {metrics['recall']:.3f}\")\n",
    "        print(f\"      F1åˆ†æ•°: {metrics['f1_score']:.3f}\")\n",
    "        if 'auc_roc' in metrics:\n",
    "            print(f\"      AUC-ROC: {metrics['auc_roc']:.3f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def evaluate_regression(self, y_true, y_pred, model_name=\"æ¨¡å‹\"):\n",
    "        \"\"\"è¯„ä¼°å›å½’æ¨¡å‹\"\"\"\n",
    "        metrics = {\n",
    "            'mse': mean_squared_error(y_true, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'r2_score': r2_score(y_true, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"   {model_name} å›å½’è¯„ä¼°:\")\n",
    "        print(f\"      MSE: {metrics['mse']:.2f}\")\n",
    "        print(f\"      RMSE: {metrics['rmse']:.2f}\")\n",
    "        print(f\"      MAE: {metrics['mae']:.2f}\")\n",
    "        print(f\"      RÂ²: {metrics['r2_score']:.3f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def cross_validate_model(self, model, X, y, cv=5, scoring='accuracy'):\n",
    "        \"\"\"äº¤å‰éªŒè¯\"\"\"\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "        \n",
    "        print(f\"   äº¤å‰éªŒè¯ç»“æœ ({scoring}):\")\n",
    "        print(f\"      å¹³å‡åˆ†æ•°: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "        print(f\"      å„æŠ˜åˆ†æ•°: {[f'{score:.3f}' for score in scores]}\")\n",
    "        \n",
    "        return {\n",
    "            'mean_score': scores.mean(),\n",
    "            'std_score': scores.std(),\n",
    "            'scores': scores.tolist()\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–æœºå™¨å­¦ä¹ æ¡†æ¶\n",
    "ml_framework = MLFramework(ml_df)\n",
    "print(f\"   âœ… æœºå™¨å­¦ä¹ æ¡†æ¶åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "print(f\"\\nâœ… æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µå®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ\")\n",
    "print(f\"   âœ“ ç†è§£ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ \")\n",
    "print(f\"   âœ“ ç†Ÿæ‚‰æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿå‡†å¤‡å’Œé¢„å¤„ç†MLæ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç›‘ç£å­¦ä¹ ç®—æ³• [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸»è¦åˆ†æ”¯ï¼ŒåŒ…æ‹¬åˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚æŒæ¡è¿™äº›ç®—æ³•å¯¹äºæ„å»ºé¢„æµ‹æ¨¡å‹éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡çº¿æ€§æ¨¡å‹çš„åŸºæœ¬åŸç†\n",
    "- ç†è§£æ ‘æ¨¡å‹å’Œé›†æˆæ–¹æ³•\n",
    "- ç†Ÿç»ƒä½¿ç”¨æ”¯æŒå‘é‡æœºå’ŒKè¿‘é‚»\n",
    "- èƒ½å¤Ÿæ¯”è¾ƒå’Œé€‰æ‹©åˆé€‚çš„ç®—æ³•\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„ç›‘ç£å­¦ä¹ æµç¨‹\n",
    "- è¿›è¡Œå¤šç§ç®—æ³•çš„æ€§èƒ½æ¯”è¾ƒ\n",
    "- åº”ç”¨ç›‘ç£å­¦ä¹ è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„ç›‘ç£å­¦ä¹ ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ¯ ç›‘ç£å­¦ä¹ ç®—æ³•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. åˆ†ç±»ç®—æ³•\n",
    "print(f\"ğŸ“ 1. åˆ†ç±»ç®—æ³•:\")\n",
    "\n",
    "@dataclass\n",
    "class SupervisedLearning:\n",
    "    \"\"\"ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, ml_framework: MLFramework):\n",
    "        self.ml_framework = ml_framework\n",
    "        self.classification_models = {}\n",
    "        self.regression_models = {}\n",
    "        self.classification_results = {}\n",
    "        self.regression_results = {}\n",
    "    \n",
    "    def train_classification_models(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"è®­ç»ƒåˆ†ç±»æ¨¡å‹\"\"\"\n",
    "        print(f\"   è®­ç»ƒåˆ†ç±»æ¨¡å‹...\")\n",
    "        \n",
    "        # å®šä¹‰åˆ†ç±»æ¨¡å‹\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "            'SVM': SVC(random_state=42, probability=True),\n",
    "            'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "            'Naive Bayes': GaussianNB()\n",
    "        }\n",
    "        \n",
    "        # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\n   ğŸ“Š è®­ç»ƒ {name}:\")\n",
    "            \n",
    "            # è®­ç»ƒæ¨¡å‹\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # é¢„æµ‹\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            # è¯„ä¼°\n",
    "            metrics = self.ml_framework.evaluate_classification(y_test, y_pred, y_prob, name)\n",
    "            \n",
    "            # ä¿å­˜æ¨¡å‹å’Œç»“æœ\n",
    "            self.classification_models[name] = model\n",
    "            self.classification_results[name] = {\n",
    "                'model': model,\n",
    "                'predictions': y_pred,\n",
    "                'probabilities': y_prob,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "        \n",
    "        return self.classification_results\n",
    "    \n",
    "    def train_regression_models(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"è®­ç»ƒå›å½’æ¨¡å‹\"\"\"\n",
    "        print(f\"   è®­ç»ƒå›å½’æ¨¡å‹...\")\n",
    "        \n",
    "        # å®šä¹‰å›å½’æ¨¡å‹\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "            'Lasso Regression': Lasso(alpha=1.0, random_state=42),\n",
    "            'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "            'Random Forest': RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "            'SVR': SVR(kernel='rbf'),\n",
    "            'KNN': KNeighborsRegressor(n_neighbors=5)\n",
    "        }\n",
    "        \n",
    "        # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\n   ğŸ“Š è®­ç»ƒ {name}:\")\n",
    "            \n",
    "            # è®­ç»ƒæ¨¡å‹\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # é¢„æµ‹\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # è¯„ä¼°\n",
    "            metrics = self.ml_framework.evaluate_regression(y_test, y_pred, name)\n",
    "            \n",
    "            # ä¿å­˜æ¨¡å‹å’Œç»“æœ\n",
    "            self.regression_models[name] = model\n",
    "            self.regression_results[name] = {\n",
    "                'model': model,\n",
    "                'predictions': y_pred,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "        \n",
    "        return self.regression_results\n",
    "    \n",
    "    def hyperparameter_tuning(self, model, param_grid, X_train, y_train, cv=5):\n",
    "        \"\"\"è¶…å‚æ•°è°ƒä¼˜\"\"\"\n",
    "        print(f\"   æ‰§è¡Œè¶…å‚æ•°è°ƒä¼˜...\")\n",
    "        \n",
    "        # ç½‘æ ¼æœç´¢\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"      æœ€ä½³å‚æ•°: {grid_search.best_params_}\")\n",
    "        print(f\"      æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.3f}\")\n",
    "        \n",
    "        return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "    \n",
    "    def feature_importance_analysis(self, model, feature_names, model_name=\"æ¨¡å‹\"):\n",
    "        \"\"\"ç‰¹å¾é‡è¦æ€§åˆ†æ\"\"\"\n",
    "        print(f\"   {model_name} ç‰¹å¾é‡è¦æ€§åˆ†æ:\")\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"      å‰5ä¸ªé‡è¦ç‰¹å¾:\")\n",
    "            for i, row in importance_df.head().iterrows():\n",
    "                print(f\"        {row['feature']}: {row['importance']:.3f}\")\n",
    "            \n",
    "            # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            top_features = importance_df.head(10)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "            plt.xlabel('é‡è¦æ€§')\n",
    "            plt.title(f'{model_name} - ç‰¹å¾é‡è¦æ€§', fontweight='bold')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return importance_df\n",
    "        else:\n",
    "            print(f\"      è¯¥æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ\")\n",
    "            return None\n",
    "    \n",
    "    def learning_curve_analysis(self, model, X, y, cv=5):\n",
    "        \"\"\"å­¦ä¹ æ›²çº¿åˆ†æ\"\"\"\n",
    "        print(f\"   å­¦ä¹ æ›²çº¿åˆ†æ...\")\n",
    "        \n",
    "        from sklearn.model_selection import learning_curve\n",
    "        \n",
    "        train_sizes, train_scores, val_scores = learning_curve(\n",
    "            model, X, y, cv=cv, n_jobs=-1, \n",
    "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "            scoring='accuracy'\n",
    "        )\n",
    "        \n",
    "        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        # ç»˜åˆ¶å­¦ä¹ æ›²çº¿\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes, train_mean, 'o-', color='blue', label='è®­ç»ƒåˆ†æ•°')\n",
    "        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "        \n",
    "        plt.plot(train_sizes, val_mean, 'o-', color='red', label='éªŒè¯åˆ†æ•°')\n",
    "        plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "        \n",
    "        plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')\n",
    "        plt.ylabel('å‡†ç¡®ç‡')\n",
    "        plt.title('å­¦ä¹ æ›²çº¿', fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'train_sizes': train_sizes,\n",
    "            'train_scores': train_scores,\n",
    "            'val_scores': val_scores\n",
    "        }\n",
    "    \n",
    "    def compare_models_performance(self, results_dict, task_type='classification'):\n",
    "        \"\"\"æ¯”è¾ƒæ¨¡å‹æ€§èƒ½\"\"\"\n",
    "        print(f\"\\n   ğŸ“ˆ æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:\")\n",
    "        \n",
    "        # å‡†å¤‡æ¯”è¾ƒæ•°æ®\n",
    "        comparison_data = []\n",
    "        \n",
    "        for model_name, result in results_dict.items():\n",
    "            metrics = result['metrics']\n",
    "            \n",
    "            if task_type == 'classification':\n",
    "                row = {\n",
    "                    'æ¨¡å‹': model_name,\n",
    "                    'å‡†ç¡®ç‡': metrics['accuracy'],\n",
    "                    'ç²¾ç¡®ç‡': metrics['precision'],\n",
    "                    'å¬å›ç‡': metrics['recall'],\n",
    "                    'F1åˆ†æ•°': metrics['f1_score']\n",
    "                }\n",
    "                if 'auc_roc' in metrics:\n",
    "                    row['AUC-ROC'] = metrics['auc_roc']\n",
    "            else:  # regression\n",
    "                row = {\n",
    "                    'æ¨¡å‹': model_name,\n",
    "                    'RMSE': metrics['rmse'],\n",
    "                    'MAE': metrics['mae'],\n",
    "                    'RÂ²': metrics['r2_score']\n",
    "                }\n",
    "            \n",
    "            comparison_data.append(row)\n",
    "        \n",
    "        # åˆ›å»ºæ¯”è¾ƒè¡¨\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ\n",
    "        print(comparison_df.round(4).to_string(index=False))\n",
    "        \n",
    "        return comparison_df\n",
    "\n",
    "# æ¼”ç¤ºç›‘ç£å­¦ä¹ \n",
    "print(f\"\\n   ğŸ”§ ç›‘ç£å­¦ä¹ æ¼”ç¤º:\")\n",
    "supervised_learning = SupervisedLearning(ml_framework)\n",
    "\n",
    "# 1. åˆ†ç±»ä»»åŠ¡ - ä¿¡ç”¨é£é™©è¯„ä¼°\n",
    "print(f\"\\n   ğŸ¯ åˆ†ç±»ä»»åŠ¡: ä¿¡ç”¨é£é™©è¯„ä¼°\")\n",
    "\n",
    "# å‡†å¤‡åˆ†ç±»æ•°æ®\n",
    "classification_features = ['age', 'income', 'education_years', 'work_experience', \n",
    "                          'credit_score', 'debt_to_income', 'savings_rate', \n",
    "                          'transaction_count', 'account_balance', 'loan_amount',\n",
    "                          'employment_type', 'marital_status', 'home_ownership', \n",
    "                          'region', 'has_credit_card', 'has_loan_history']\n",
    "\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = ml_framework.prepare_data(\n",
    "    'credit_risk', classification_features\n",
    ")\n",
    "X_train_scaled_clf, X_test_scaled_clf = ml_framework.scale_features('standard')\n",
    "\n",
    "# è®­ç»ƒåˆ†ç±»æ¨¡å‹\n",
    "classification_results = supervised_learning.train_classification_models(\n",
    "    X_train_scaled_clf, X_test_scaled_clf, y_train_clf, y_test_clf\n",
    ")\n",
    "\n",
    "# 2. å›å½’ä»»åŠ¡ - è´·æ¬¾é‡‘é¢é¢„æµ‹\n",
    "print(f\"\\n   ğŸ¯ å›å½’ä»»åŠ¡: è´·æ¬¾é‡‘é¢é¢„æµ‹\")\n",
    "\n",
    "# å‡†å¤‡å›å½’æ•°æ®\n",
    "regression_features = ['age', 'income', 'education_years', 'work_experience', \n",
    "                       'credit_score', 'debt_to_income', 'savings_rate', \n",
    "                       'transaction_count', 'account_balance', \n",
    "                       'employment_type', 'marital_status', 'home_ownership', \n",
    "                       'region', 'has_credit_card', 'has_loan_history']\n",
    "\n",
    "# åˆ›å»ºå›å½’æ¡†æ¶\n",
    "ml_framework_regression = MLFramework(ml_df)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = ml_framework_regression.prepare_data(\n",
    "    'predicted_loan_amount', regression_features\n",
    ")\n",
    "X_train_scaled_reg, X_test_scaled_reg = ml_framework_regression.scale_features('standard')\n",
    "\n",
    "# è®­ç»ƒå›å½’æ¨¡å‹\n",
    "supervised_learning_reg = SupervisedLearning(ml_framework_regression)\n",
    "regression_results = supervised_learning_reg.train_regression_models(\n",
    "    X_train_scaled_reg, X_test_scaled_reg, y_train_reg, y_test_reg\n",
    ")\n",
    "\n",
    "# 3. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ\n",
    "print(f\"\\n   ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:\")\n",
    "\n",
    "# åˆ†ç±»æ¨¡å‹æ¯”è¾ƒ\n",
    "print(f\"\\n   ğŸ¯ åˆ†ç±»æ¨¡å‹æ€§èƒ½æ’å:\")\n",
    "clf_comparison = supervised_learning.compare_models_performance(\n",
    "    classification_results, 'classification'\n",
    ")\n",
    "\n",
    "# å›å½’æ¨¡å‹æ¯”è¾ƒ\n",
    "print(f\"\\n   ğŸ¯ å›å½’æ¨¡å‹æ€§èƒ½æ’å:\")\n",
    "reg_comparison = supervised_learning_reg.compare_models_performance(\n",
    "    regression_results, 'regression'\n",
    ")\n",
    "\n",
    "# 4. ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
    "print(f\"\\n   ğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ:\")\n",
    "\n",
    "# ä½¿ç”¨æœ€ä½³åˆ†ç±»æ¨¡å‹è¿›è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ\n",
    "best_clf_model = classification_results['Random Forest']['model']\n",
    "feature_importance = supervised_learning.feature_importance_analysis(\n",
    "    best_clf_model, classification_features, 'Random Forest'\n",
    ")\n",
    "\n",
    "# 5. å­¦ä¹ æ›²çº¿åˆ†æ\n",
    "print(f\"\\n   ğŸ“ˆ å­¦ä¹ æ›²çº¿åˆ†æ:\")\n",
    "\n",
    "# å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œå­¦ä¹ æ›²çº¿åˆ†æ\n",
    "learning_curve_data = supervised_learning.learning_curve_analysis(\n",
    "    best_clf_model, X_train_scaled_clf, y_train_clf\n",
    ")\n",
    "\n",
    "# 6. è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹\n",
    "print(f\"\\n   âš™ï¸ è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹:\")\n",
    "\n",
    "# éšæœºæ£®æ—è¶…å‚æ•°è°ƒä¼˜\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "best_rf_model, best_params, best_score = supervised_learning.hyperparameter_tuning(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_param_grid,\n",
    "    X_train_scaled_clf,\n",
    "    y_train_clf,\n",
    "    cv=3  # å‡å°‘æŠ˜æ•°ä»¥åŠ å¿«é€Ÿåº¦\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… ç›‘ç£å­¦ä¹ ç®—æ³•å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡çº¿æ€§æ¨¡å‹çš„åŸºæœ¬åŸç†\")\n",
    "print(f\"   âœ“ ç†è§£æ ‘æ¨¡å‹å’Œé›†æˆæ–¹æ³•\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨æ”¯æŒå‘é‡æœºå’ŒKè¿‘é‚»\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿæ¯”è¾ƒå’Œé€‰æ‹©åˆé€‚çš„ç®—æ³•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ— ç›‘ç£å­¦ä¹ ç®—æ³• [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šæ— ç›‘ç£å­¦ä¹ ç”¨äºå‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ï¼ŒåŒ…æ‹¬èšç±»ã€é™ç»´å’Œå¼‚å¸¸æ£€æµ‹ã€‚æŒæ¡è¿™äº›æŠ€èƒ½å¯¹äºæ•°æ®æ¢ç´¢å’Œç‰¹å¾å·¥ç¨‹éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡èšç±»ç®—æ³•çš„åŸºæœ¬åŸç†\n",
    "- ç†è§£é™ç»´æŠ€æœ¯çš„ä½œç”¨\n",
    "- ç†Ÿç»ƒä½¿ç”¨å¼‚å¸¸æ£€æµ‹æ–¹æ³•\n",
    "- èƒ½å¤Ÿè¯„ä¼°æ— ç›‘ç£å­¦ä¹ çš„æ•ˆæœ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ— ç›‘ç£å­¦ä¹ æµç¨‹\n",
    "- è¿›è¡Œå¤šç§èšç±»ç®—æ³•çš„æ¯”è¾ƒ\n",
    "- åº”ç”¨æ— ç›‘ç£å­¦ä¹ è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æ— ç›‘ç£å­¦ä¹ ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ” æ— ç›‘ç£å­¦ä¹ ç®—æ³•:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. èšç±»ç®—æ³•\n",
    "print(f\"ğŸ“ 1. èšç±»ç®—æ³•:\")\n",
    "\n",
    "@dataclass\n",
    "class UnsupervisedLearning:\n",
    "    \"\"\"æ— ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.clustering_models = {}\n",
    "        self.clustering_results = {}\n",
    "        self.dimensionality_reduction_results = {}\n",
    "    \n",
    "    def prepare_clustering_data(self, feature_cols: List[str] = None, scale_method: str = 'standard'):\n",
    "        \"\"\"å‡†å¤‡èšç±»æ•°æ®\"\"\"\n",
    "        print(f\"   å‡†å¤‡èšç±»æ•°æ®...\")\n",
    "        \n",
    "        if feature_cols is None:\n",
    "            # é€‰æ‹©æ•°å€¼ç‰¹å¾\n",
    "            numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            exclude_cols = ['customer_id', 'credit_risk', 'predicted_loan_amount']\n",
    "            feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        \n",
    "        # æå–ç‰¹å¾\n",
    "        X = self.df[feature_cols].copy()\n",
    "        \n",
    "        # å¤„ç†ç¼ºå¤±å€¼\n",
    "        X = X.fillna(X.mean())\n",
    "        \n",
    "        # ç‰¹å¾ç¼©æ”¾\n",
    "        if scale_method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scale_method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "        \n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        self.feature_cols = feature_cols\n",
    "        self.X_scaled = X_scaled\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        print(f\"      ç‰¹å¾æ•°é‡: {len(feature_cols)}\")\n",
    "        print(f\"      æ ·æœ¬æ•°é‡: {X_scaled.shape[0]}\")\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def kmeans_clustering(self, X, n_clusters: int = 4, random_state: int = 42):\n",
    "        \"\"\"K-meansèšç±»\"\"\"\n",
    "        print(f\"   æ‰§è¡ŒK-meansèšç±» (k={n_clusters})...\")\n",
    "        \n",
    "        # è®­ç»ƒæ¨¡å‹\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        calinski_harabasz_idx = calinski_harabasz_score(X, cluster_labels)\n",
    "        davies_bouldin_idx = davies_bouldin_score(X, cluster_labels)\n",
    "        \n",
    "        result = {\n",
    "            'model': kmeans,\n",
    "            'labels': cluster_labels,\n",
    "            'centers': kmeans.cluster_centers_,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'calinski_harabasz_score': calinski_harabasz_idx,\n",
    "            'davies_bouldin_score': davies_bouldin_idx,\n",
    "            'inertia': kmeans.inertia_\n",
    "        }\n",
    "        \n",
    "        print(f\"      è½®å»“ç³»æ•°: {silhouette_avg:.3f}\")\n",
    "        print(f\"      Calinski-HarabaszæŒ‡æ•°: {calinski_harabasz_idx:.1f}\")\n",
    "        print(f\"      Davies-BouldinæŒ‡æ•°: {davies_bouldin_idx:.3f}\")\n",
    "        print(f\"      æƒ¯æ€§: {kmeans.inertia_:.2f}\")\n",
    "        \n",
    "        self.clustering_models['KMeans'] = kmeans\n",
    "        self.clustering_results['KMeans'] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def hierarchical_clustering(self, X, n_clusters: int = 4, linkage: str = 'ward'):\n",
    "        \"\"\"å±‚æ¬¡èšç±»\"\"\"\n",
    "        print(f\"   æ‰§è¡Œå±‚æ¬¡èšç±» (k={n_clusters}, linkage={linkage})...\")\n",
    "        \n",
    "        # è®­ç»ƒæ¨¡å‹\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "        cluster_labels = hierarchical.fit_predict(X)\n",
    "        \n",
    "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        calinski_harabasz_idx = calinski_harabasz_score(X, cluster_labels)\n",
    "        davies_bouldin_idx = davies_bouldin_score(X, cluster_labels)\n",
    "        \n",
    "        result = {\n",
    "            'model': hierarchical,\n",
    "            'labels': cluster_labels,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'calinski_harabasz_score': calinski_harabasz_idx,\n",
    "            'davies_bouldin_score': davies_bouldin_score\n",
    "        }\n",
    "        \n",
    "        print(f\"      è½®å»“ç³»æ•°: {silhouette_avg:.3f}\")\n",
    "        print(f\"      Calinski-HarabaszæŒ‡æ•°: {calinski_harabasz_idx:.1f}\")\n",
    "        print(f\"      Davies-BouldinæŒ‡æ•°: {davies_bouldin_idx:.3f}\")\n",
    "        \n",
    "        self.clustering_models['Hierarchical'] = hierarchical\n",
    "        self.clustering_results['Hierarchical'] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def dbscan_clustering(self, X, eps: float = 0.5, min_samples: int = 5):\n",
    "        \"\"\"DBSCANèšç±»\"\"\"\n",
    "        print(f\"   æ‰§è¡ŒDBSCANèšç±» (eps={eps}, min_samples={min_samples})...\")\n",
    "        \n",
    "        # è®­ç»ƒæ¨¡å‹\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        cluster_labels = dbscan.fit_predict(X)\n",
    "        \n",
    "        # è®¡ç®—èšç±»æ•°é‡ï¼ˆæ’é™¤å™ªå£°ç‚¹ï¼‰\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        n_noise = list(cluster_labels).count(-1)\n",
    "        \n",
    "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰å¤šä¸ªèšç±»ï¼‰\n",
    "        if n_clusters > 1:\n",
    "            silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "            calinski_harabasz_idx = calinski_harabasz_score(X, cluster_labels)\n",
    "            davies_bouldin_idx = davies_bouldin_score(X, cluster_labels)\n",
    "        else:\n",
    "            silhouette_avg = calinski_harabasz_idx = davies_bouldin_idx = None\n",
    "        \n",
    "        result = {\n",
    "            'model': dbscan,\n",
    "            'labels': cluster_labels,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'calinski_harabasz_score': calinski_harabasz_idx,\n",
    "            'davies_bouldin_score': davies_bouldin_idx\n",
    "        }\n",
    "        \n",
    "        print(f\"      å‘ç°èšç±»æ•°: {n_clusters}\")\n",
    "        print(f\"      å™ªå£°ç‚¹æ•°: {n_noise}\")\n",
    "        if silhouette_avg is not None:\n",
    "            print(f\"      è½®å»“ç³»æ•°: {silhouette_avg:.3f}\")\n",
    "        \n",
    "        self.clustering_models['DBSCAN'] = dbscan\n",
    "        self.clustering_results['DBSCAN'] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def gaussian_mixture_clustering(self, X, n_components: int = 4, random_state: int = 42):\n",
    "        \"\"\"é«˜æ–¯æ··åˆæ¨¡å‹èšç±»\"\"\"\n",
    "        print(f\"   æ‰§è¡Œé«˜æ–¯æ··åˆæ¨¡å‹èšç±» (components={n_components})...\")\n",
    "        \n",
    "        # è®­ç»ƒæ¨¡å‹\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "        cluster_labels = gmm.fit_predict(X)\n",
    "        \n",
    "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        calinski_harabasz_idx = calinski_harabasz_score(X, cluster_labels)\n",
    "        davies_bouldin_idx = davies_bouldin_score(X, cluster_labels)\n",
    "        \n",
    "        result = {\n",
    "            'model': gmm,\n",
    "            'labels': cluster_labels,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'calinski_harabasz_score': calinski_harabasz_idx,\n",
    "            'davies_bouldin_score': davies_bouldin_idx,\n",
    "            'aic': gmm.aic(X),\n",
    "            'bic': gmm.bic(X),\n",
    "            'log_likelihood': gmm.score(X)\n",
    "        }\n",
    "        \n",
    "        print(f\"      è½®å»“ç³»æ•°: {silhouette_avg:.3f}\")\n",
    "        print(f\"      Calinski-HarabaszæŒ‡æ•°: {calinski_harabasz_idx:.1f}\")\n",
    "        print(f\"      Davies-BouldinæŒ‡æ•°: {davies_bouldin_idx:.3f}\")\n",
    "        print(f\"      AIC: {gmm.aic(X):.1f}\")\n",
    "        print(f\"      BIC: {gmm.bic(X):.1f}\")\n",
    "        \n",
    "        self.clustering_models['GaussianMixture'] = gmm\n",
    "        self.clustering_results['GaussianMixture'] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def optimal_clusters_elbow(self, X, max_clusters: int = 10):\n",
    "        \"\"\"ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜èšç±»æ•°\"\"\"\n",
    "        print(f\"   ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜èšç±»æ•°...\")\n",
    "        \n",
    "        inertias = []\n",
    "        silhouette_scores = []\n",
    "        k_range = range(2, max_clusters + 1)\n",
    "        \n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            kmeans.fit(X)\n",
    "            inertias.append(kmeans.inertia_)\n",
    "            \n",
    "            labels = kmeans.labels_\n",
    "            silhouette_avg = silhouette_score(X, labels)\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "        \n",
    "        # ç»˜åˆ¶è‚˜éƒ¨å›¾\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # è‚˜éƒ¨å›¾\n",
    "        ax1.plot(k_range, inertias, 'bo-')\n",
    "        ax1.set_xlabel('èšç±»æ•° k')\n",
    "        ax1.set_ylabel('æƒ¯æ€§')\n",
    "        ax1.set_title('è‚˜éƒ¨æ³•åˆ™', fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # è½®å»“ç³»æ•°å›¾\n",
    "        ax2.plot(k_range, silhouette_scores, 'ro-')\n",
    "        ax2.set_xlabel('èšç±»æ•° k')\n",
    "        ax2.set_ylabel('è½®å»“ç³»æ•°')\n",
    "        ax2.set_title('è½®å»“ç³»æ•°', fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ä¼˜kå€¼\n",
    "        optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "        \n",
    "        print(f\"      åŸºäºè½®å»“ç³»æ•°çš„æœ€ä¼˜èšç±»æ•°: {optimal_k_silhouette}\")\n",
    "        \n",
    "        return {\n",
    "            'k_range': list(k_range),\n",
    "            'inertias': inertias,\n",
    "            'silhouette_scores': silhouette_scores,\n",
    "            'optimal_k_silhouette': optimal_k_silhouette\n",
    "        }\n",
    "    \n",
    "    def pca_dimensionality_reduction(self, X, n_components: int = 2, random_state: int = 42):\n",
    "        \"\"\"PCAé™ç»´\"\"\"\n",
    "        print(f\"   æ‰§è¡ŒPCAé™ç»´ (components={n_components})...\")\n",
    "        \n",
    "        # è®­ç»ƒPCA\n",
    "        pca = PCA(n_components=n_components, random_state=random_state)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        # è®¡ç®—è§£é‡Šæ–¹å·®æ¯”\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "        \n",
    "        result = {\n",
    "            'model': pca,\n",
    "            'transformed_data': X_pca,\n",
    "            'explained_variance_ratio': explained_variance_ratio,\n",
    "            'cumulative_variance_ratio': cumulative_variance_ratio,\n",
    "            'components': pca.components_,\n",
    "            'singular_values': pca.singular_values_\n",
    "        }\n",
    "        \n",
    "        print(f\"      è§£é‡Šæ–¹å·®æ¯”: {explained_variance_ratio}\")\n",
    "        print(f\"      ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”: {cumulative_variance_ratio}\")\n",
    "        \n",
    "        # å¯è§†åŒ–é™ç»´ç»“æœ\n",
    "        if n_components == 2:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6, s=50)\n",
    "            plt.xlabel(f'PC1 ({explained_variance_ratio[0]:.1%} variance)')\n",
    "            plt.ylabel(f'PC2 ({explained_variance_ratio[1]:.1%} variance)')\n",
    "            plt.title('PCAé™ç»´ç»“æœ', fontweight='bold')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        self.dimensionality_reduction_results['PCA'] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def tsne_visualization(self, X, n_components: int = 2, perplexity: int = 30, random_state: int = 42):\n",
    "        \"\"\"t-SNEå¯è§†åŒ–\"\"\"\n",
    "        print(f\"   æ‰§è¡Œt-SNEå¯è§†åŒ– (components={n_components}, perplexity={perplexity})...\")\n",
    "        \n",
    "        # è®­ç»ƒt-SNE\n",
    "        tsne = TSNE(n_components=n_components, perplexity=perplexity, \n",
    "                   random_state=random_state, n_iter=1000)\n",
    "        X_tsne = tsne.fit_transform(X)\n",
    "        \n",
    "        result = {\n",
    "            'model': tsne,\n",
    "            'transformed_data': X_tsne,\n",
    "            'kl_divergence': tsne.kl_divergence_,\n",
    "            'n_iter': tsne.n_iter_\n",
    "        }\n",
    "        \n",
    "        print(f\"      KLæ•£åº¦: {tsne.kl_divergence_:.4f}\")\n",
    "        print(f\"      è¿­ä»£æ¬¡æ•°: {tsne.n_iter_}\")\n",
    "        \n",
    "        # å¯è§†åŒ–t-SNEç»“æœ\n",
    "        if n_components == 2:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.6, s=50, c='viridis')\n",
    "            plt.xlabel('t-SNE 1')\n",
    "            plt.ylabel('t-SNE 2')\n",
    "            plt.title('t-SNEå¯è§†åŒ–ç»“æœ', fontweight='bold')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        self.dimensionality_reduction_results['tSNE'] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def visualize_clustering_results(self, X, cluster_labels, algorithm_name=\"èšç±»\"):\n",
    "        \"\"\"å¯è§†åŒ–èšç±»ç»“æœ\"\"\"\n",
    "        print(f\"   å¯è§†åŒ–{algorithm_name}ç»“æœ...\")\n",
    "        \n",
    "        # å¦‚æœç‰¹å¾ç»´åº¦å¤§äº2ï¼Œä½¿ç”¨PCAé™ç»´å¯è§†åŒ–\n",
    "        if X.shape[1] > 2:\n",
    "            pca = PCA(n_components=2, random_state=42)\n",
    "            X_vis = pca.fit_transform(X)\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "            xlabel = f'PC1 ({explained_variance[0]:.1%} variance)'\n",
    "            ylabel = f'PC2 ({explained_variance[1]:.1%} variance)'\n",
    "        else:\n",
    "            X_vis = X\n",
    "            xlabel = 'ç‰¹å¾ 1'\n",
    "            ylabel = 'ç‰¹å¾ 2'\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # ç»˜åˆ¶èšç±»ç»“æœ\n",
    "        unique_labels = np.unique(cluster_labels)\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for i, label in enumerate(unique_labels):\n",
    "            if label == -1:\n",
    "                # å™ªå£°ç‚¹ç”¨é»‘è‰²è¡¨ç¤º\n",
    "                plt.scatter(X_vis[cluster_labels == label, 0], \n",
    "                           X_vis[cluster_labels == label, 1], \n",
    "                           c='black', marker='x', s=50, label='å™ªå£°ç‚¹')\n",
    "            else:\n",
    "                plt.scatter(X_vis[cluster_labels == label, 0], \n",
    "                           X_vis[cluster_labels == label, 1], \n",
    "                           c=[colors[i]], s=50, label=f'èšç±» {label}')\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(f'{algorithm_name}èšç±»ç»“æœ', fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_clustering_algorithms(self, X, cluster_range: range = range(2, 7)):\n",
    "        \"\"\"æ¯”è¾ƒèšç±»ç®—æ³•æ€§èƒ½\"\"\"\n",
    "        print(f\"\\n   ğŸ“Š æ¯”è¾ƒèšç±»ç®—æ³•æ€§èƒ½:\")\n",
    "        \n",
    "        comparison_results = []\n",
    "        \n",
    "        for n_clusters in cluster_range:\n",
    "            print(f\"\\n   æµ‹è¯•èšç±»æ•°: {n_clusters}\")\n",
    "            \n",
    "            # K-means\n",
    "            kmeans_result = self.kmeans_clustering(X, n_clusters)\n",
    "            comparison_results.append({\n",
    "                'ç®—æ³•': 'K-means',\n",
    "                'èšç±»æ•°': n_clusters,\n",
    "                'è½®å»“ç³»æ•°': kmeans_result['silhouette_score'],\n",
    "                'Calinski-Harabasz': kmeans_result['calinski_harabasz_score'],\n",
    "                'Davies-Bouldin': kmeans_result['davies_bouldin_score']\n",
    "            })\n",
    "            \n",
    "            # å±‚æ¬¡èšç±»\n",
    "            hierarchical_result = self.hierarchical_clustering(X, n_clusters)\n",
    "            comparison_results.append({\n",
    "                'ç®—æ³•': 'å±‚æ¬¡èšç±»',\n",
    "                'èšç±»æ•°': n_clusters,\n",
    "                'è½®å»“ç³»æ•°': hierarchical_result['silhouette_score'],\n",
    "                'Calinski-Harabasz': hierarchical_result['calinski_harabasz_score'],\n",
    "                'Davies-Bouldin': hierarchical_result['davies_bouldin_score']\n",
    "            })\n",
    "            \n",
    "            # é«˜æ–¯æ··åˆæ¨¡å‹\n",
    "            gmm_result = self.gaussian_mixture_clustering(X, n_clusters)\n",
    "            comparison_results.append({\n",
    "                'ç®—æ³•': 'é«˜æ–¯æ··åˆæ¨¡å‹',\n",
    "                'èšç±»æ•°': n_clusters,\n",
    "                'è½®å»“ç³»æ•°': gmm_result['silhouette_score'],\n",
    "                'Calinski-Harabasz': gmm_result['calinski_harabasz_score'],\n",
    "                'Davies-Bouldin': gmm_result['davies_bouldin_score']\n",
    "            })\n",
    "        \n",
    "        # åˆ›å»ºæ¯”è¾ƒè¡¨\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        \n",
    "        # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ\n",
    "        print(f\"\\n   èšç±»ç®—æ³•æ€§èƒ½æ¯”è¾ƒ:\")\n",
    "        print(comparison_df.round(4).to_string(index=False))\n",
    "        \n",
    "        return comparison_df\n",
    "\n",
    "# æ¼”ç¤ºæ— ç›‘ç£å­¦ä¹ \n",
    "print(f\"\\n   ğŸ”§ æ— ç›‘ç£å­¦ä¹ æ¼”ç¤º:\")\n",
    "unsupervised_learning = UnsupervisedLearning(ml_df)\n",
    "\n",
    "# 1. å‡†å¤‡èšç±»æ•°æ®\n",
    "print(f\"\\n   ğŸ“Š å‡†å¤‡èšç±»æ•°æ®:\")\n",
    "clustering_features = ['age', 'income', 'education_years', 'work_experience', \n",
    "                       'credit_score', 'debt_to_income', 'savings_rate', \n",
    "                       'transaction_count', 'account_balance', 'loan_amount']\n",
    "\n",
    "X_clustering = unsupervised_learning.prepare_clustering_data(clustering_features, 'standard')\n",
    "\n",
    "# 2. ç¡®å®šæœ€ä¼˜èšç±»æ•°\n",
    "print(f\"\\n   ğŸ¯ ç¡®å®šæœ€ä¼˜èšç±»æ•°:\")\n",
    "optimal_clusters = unsupervised_learning.optimal_clusters_elbow(X_clustering, max_clusters=8)\n",
    "\n",
    "# 3. æ‰§è¡Œèšç±»ç®—æ³•\n",
    "print(f\"\\n   ğŸ” æ‰§è¡Œèšç±»ç®—æ³•:\")\n",
    "optimal_k = optimal_clusters['optimal_k_silhouette']\n",
    "\n",
    "# K-meansèšç±»\n",
    "kmeans_result = unsupervised_learning.kmeans_clustering(X_clustering, optimal_k)\n",
    "\n",
    "# å±‚æ¬¡èšç±»\n",
    "hierarchical_result = unsupervised_learning.hierarchical_clustering(X_clustering, optimal_k)\n",
    "\n",
    "# DBSCANèšç±»\n",
    "dbscan_result = unsupervised_learning.dbscan_clustering(X_clustering, eps=0.8, min_samples=10)\n",
    "\n",
    "# é«˜æ–¯æ··åˆæ¨¡å‹\n",
    "gmm_result = unsupervised_learning.gaussian_mixture_clustering(X_clustering, optimal_k)\n",
    "\n",
    "# 4. å¯è§†åŒ–èšç±»ç»“æœ\n",
    "print(f\"\\n   ğŸ“ˆ å¯è§†åŒ–èšç±»ç»“æœ:\")\n",
    "\n",
    "# å¯è§†åŒ–K-meansç»“æœ\n",
    "unsupervised_learning.visualize_clustering_results(\n",
    "    X_clustering, kmeans_result['labels'], 'K-means'\n",
    ")\n",
    "\n",
    "# å¯è§†åŒ–å±‚æ¬¡èšç±»ç»“æœ\n",
    "unsupervised_learning.visualize_clustering_results(\n",
    "    X_clustering, hierarchical_result['labels'], 'å±‚æ¬¡èšç±»'\n",
    ")\n",
    "\n",
    "# 5. é™ç»´åˆ†æ\n",
    "print(f\"\\n   ğŸ“‰ é™ç»´åˆ†æ:\")\n",
    "\n",
    "# PCAé™ç»´\n",
    "pca_result = unsupervised_learning.pca_dimensionality_reduction(X_clustering, n_components=2)\n",
    "\n",
    "# t-SNEå¯è§†åŒ–\n",
    "tsne_result = unsupervised_learning.tsne_visualization(X_clustering, n_components=2, perplexity=30)\n",
    "\n",
    "# 6. èšç±»ç®—æ³•æ¯”è¾ƒ\n",
    "print(f\"\\n   ğŸ“Š èšç±»ç®—æ³•æ€§èƒ½æ¯”è¾ƒ:\")\n",
    "clustering_comparison = unsupervised_learning.compare_clustering_algorithms(\n",
    "    X_clustering, range(2, 6)\n",
    ")\n",
    "\n",
    "# 7. åˆ†æèšç±»ç»“æœ\n",
    "print(f\"\\n   ğŸ” åˆ†æèšç±»ç»“æœ:\")\n",
    "\n",
    "# å°†èšç±»æ ‡ç­¾æ·»åŠ åˆ°åŸå§‹æ•°æ®\n",
    "ml_df_clustered = ml_df.copy()\n",
    "ml_df_clustered['kmeans_cluster'] = kmeans_result['labels']\n",
    "\n",
    "# åˆ†æå„èšç±»çš„ç‰¹å¾\n",
    "print(f\"\\n   K-meansèšç±»ç‰¹å¾åˆ†æ:\")\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = ml_df_clustered[ml_df_clustered['kmeans_cluster'] == cluster_id]\n",
    "    print(f\"\\n      èšç±» {cluster_id} (æ ·æœ¬æ•°: {len(cluster_data)}):\")\n",
    "    print(f\"         å¹³å‡å¹´é¾„: {cluster_data['age'].mean():.1f}\")\n",
    "    print(f\"         å¹³å‡æ”¶å…¥: {cluster_data['income'].mean():.0f}\")\n",
    "    print(f\"         å¹³å‡ä¿¡ç”¨è¯„åˆ†: {cluster_data['credit_score'].mean():.0f}\")\n",
    "    print(f\"         å¹³å‡è´¦æˆ·ä½™é¢: {cluster_data['account_balance'].mean():.0f}\")\n",
    "\n",
    "print(f\"\\nâœ… æ— ç›‘ç£å­¦ä¹ ç®—æ³•å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡èšç±»ç®—æ³•çš„åŸºæœ¬åŸç†\")\n",
    "print(f\"   âœ“ ç†è§£é™ç»´æŠ€æœ¯çš„ä½œç”¨\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨å¼‚å¸¸æ£€æµ‹æ–¹æ³•\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿè¯„ä¼°æ— ç›‘ç£å­¦ä¹ çš„æ•ˆæœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å­¦ä¹ æ€»ç»“\n",
    "\n",
    "### âœ… çŸ¥è¯†æ¸…å•è¾¾æˆæƒ…å†µéªŒè¯\n",
    "\n",
    "**5.6 æœºå™¨å­¦ä¹ åŸºç¡€ [â­â­è¿›é˜¶]**\n",
    "- âœ… æŒæ¡æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ\n",
    "- âœ… ç†è§£ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ \n",
    "- âœ… ç†Ÿæ‚‰æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹\n",
    "- âœ… èƒ½å¤Ÿå‡†å¤‡å’Œé¢„å¤„ç†MLæ•°æ®\n",
    "- âœ… æŒæ¡çº¿æ€§æ¨¡å‹çš„åŸºæœ¬åŸç†\n",
    "- âœ… ç†è§£æ ‘æ¨¡å‹å’Œé›†æˆæ–¹æ³•\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨æ”¯æŒå‘é‡æœºå’ŒKè¿‘é‚»\n",
    "- âœ… èƒ½å¤Ÿæ¯”è¾ƒå’Œé€‰æ‹©åˆé€‚çš„ç®—æ³•\n",
    "- âœ… æŒæ¡èšç±»ç®—æ³•çš„åŸºæœ¬åŸç†\n",
    "- âœ… ç†è§£é™ç»´æŠ€æœ¯çš„ä½œç”¨\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨å¼‚å¸¸æ£€æµ‹æ–¹æ³•\n",
    "- âœ… èƒ½å¤Ÿè¯„ä¼°æ— ç›‘ç£å­¦ä¹ çš„æ•ˆæœ\n",
    "- âœ… èƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æœºå™¨å­¦ä¹ ç³»ç»Ÿ\n",
    "\n",
    "### ğŸ¯ ä¸LangChainå­¦ä¹ çš„å…³è”\n",
    "\n",
    "**æœºå™¨å­¦ä¹ é‡è¦æ€§**ï¼š\n",
    "- æœºå™¨å­¦ä¹ æ˜¯LangChainæ™ºèƒ½åº”ç”¨çš„æ ¸å¿ƒ\n",
    "- æ”¯æŒLangChainçš„æ–‡æœ¬åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æ\n",
    "- ä¸ºLangChainçš„æ¨èç³»ç»Ÿæä¾›ç®—æ³•åŸºç¡€\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„é¢„æµ‹å’Œå†³ç­–èƒ½åŠ›\n",
    "- æœºå™¨å­¦ä¹ æ”¯æŒLangChainçš„NLPä»»åŠ¡ä¼˜åŒ–\n",
    "\n",
    "**å®é™…åº”ç”¨åœºæ™¯**ï¼š\n",
    "- LangChainçš„å¯¹è¯æ„å›¾è¯†åˆ«å’Œåˆ†ç±»\n",
    "- LangChainçš„ç”¨æˆ·è¡Œä¸ºæ¨¡å¼èšç±»\n",
    "- LangChainçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿä¼˜åŒ–\n",
    "- LangChainçš„ä¸ªæ€§åŒ–å†…å®¹æ¨è\n",
    "- LangChainçš„çŸ¥è¯†åº“å†…å®¹åˆ†æå’Œç»„ç»‡\n",
    "\n",
    "### ğŸ“š è¿›é˜¶å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹ å»ºè®®**ï¼š\n",
    "   - æ·±å…¥ç»ƒä¹ æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆTensorFlow/PyTorchï¼‰\n",
    "   - æŒæ¡æ›´å¤šé«˜çº§æœºå™¨å­¦ä¹ ç®—æ³•\n",
    "   - å­¦ä¹ AutoMLå’Œæ¨¡å‹è‡ªåŠ¨åŒ–é€‰æ‹©\n",
    "\n",
    "2. **æ‰©å±•å­¦ä¹ **ï¼š\n",
    "   - å­¦ä¹ å¼ºåŒ–å­¦ä¹ ç®—æ³•\n",
    "   - äº†è§£å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰\n",
    "   - æ¢ç´¢è”é‚¦å­¦ä¹ å’Œéšç§ä¿æŠ¤\n",
    "\n",
    "3. **å®é™…åº”ç”¨**ï¼š\n",
    "   - æ„å»ºä¼ä¸šçº§æœºå™¨å­¦ä¹ å¹³å°\n",
    "   - å¼€å‘è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ æµæ°´çº¿\n",
    "   - å®ç°å®æ—¶æ¨¡å‹ç›‘æ§å’Œæ›´æ–°ç³»ç»Ÿ\n",
    "\n",
    "### ğŸ”§ å¸¸è§é”™è¯¯ä¸æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **æ•°æ®æ³„éœ²é—®é¢˜**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåœ¨åˆ†å‰²æ•°æ®å‰è¿›è¡Œç‰¹å¾ç¼©æ”¾\n",
    "   scaler = StandardScaler()\n",
    "   X_scaled = scaler.fit_transform(X)  # ä½¿ç”¨å…¨éƒ¨æ•°æ®\n",
    "   X_train, X_test = train_test_split(X_scaled)  # æ•°æ®æ³„éœ²\n",
    "   \n",
    "   # æ­£ç¡®ï¼šå…ˆåˆ†å‰²æ•°æ®ï¼Œå†åˆ†åˆ«ç¼©æ”¾\n",
    "   X_train, X_test = train_test_split(X)\n",
    "   scaler = StandardScaler()\n",
    "   X_train_scaled = scaler.fit_transform(X_train)\n",
    "   X_test_scaled = scaler.transform(X_test)  # åªä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡\n",
    "   ```\n",
    "\n",
    "2. **è¿‡æ‹Ÿåˆé—®é¢˜**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåªå…³æ³¨è®­ç»ƒé›†æ€§èƒ½\n",
    "   model = DecisionTreeClassifier(random_state=42)\n",
    "   model.fit(X_train, y_train)\n",
    "   train_score = model.score(X_train, y_train)  # å¯èƒ½æ¥è¿‘1.0\n",
    "   print(f\"è®­ç»ƒå‡†ç¡®ç‡: {train_score}\")  # è¯¯å¯¼æ€§æŒ‡æ ‡\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨äº¤å‰éªŒè¯å’Œæµ‹è¯•é›†\n",
    "   model = DecisionTreeClassifier(random_state=42, max_depth=10)  # é™åˆ¶å¤æ‚åº¦\n",
    "   cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "   print(f\"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "   ```\n",
    "\n",
    "3. **ç‰¹å¾é€‰æ‹©åå·®**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåœ¨å…¨éƒ¨æ•°æ®ä¸Šè¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "   selector = SelectKBest(f_classif, k=10)\n",
    "   X_selected = selector.fit_transform(X, y)  # æ•°æ®æ³„éœ²\n",
    "   X_train, X_test = train_test_split(X_selected)\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "   X_train, X_test = train_test_split(X)\n",
    "   selector = SelectKBest(f_classif, k=10)\n",
    "   X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "   X_test_selected = selector.transform(X_test)\n",
    "   ```\n",
    "\n",
    "4. **è¯„ä¼°æŒ‡æ ‡é€‰æ‹©ä¸å½“**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šä½¿ç”¨å‡†ç¡®ç‡\n",
    "   # å‡è®¾æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ä¸º1:99\n",
    "   accuracy = accuracy_score(y_test, y_pred)  # å¯èƒ½å¾ˆé«˜ä½†æ— æ„ä¹‰\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨é€‚åˆçš„è¯„ä¼°æŒ‡æ ‡\n",
    "   precision = precision_score(y_test, y_pred, average='weighted')\n",
    "   recall = recall_score(y_test, y_pred, average='weighted')\n",
    "   f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "   auc = roc_auc_score(y_test, y_pred_proba)\n",
    "   ```\n",
    "\n",
    "5. **è¶…å‚æ•°è°ƒä¼˜è¿‡æ‹Ÿåˆ**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¶…å‚æ•°è°ƒä¼˜\n",
    "   grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "   grid_search.fit(X_train, y_train)\n",
    "   best_model = grid_search.best_estimator_\n",
    "   test_score = best_model.score(X_test, y_test)  # ä¹è§‚åå·®\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨éªŒè¯é›†æˆ–åµŒå¥—äº¤å‰éªŒè¯\n",
    "   X_train, X_val, X_test = np.split(X, [0.6, 0.8])\n",
    "   grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "   grid_search.fit(X_train, y_train)\n",
    "   val_score = grid_search.best_estimator_.score(X_val, y_val)\n",
    "   final_score = grid_search.best_estimator_.score(X_test, y_test)\n",
    "   ```\n",
    "\n",
    "6. **å¿½ç•¥æ•°æ®åˆ†å¸ƒå‡è®¾**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå¿½ç•¥ç®—æ³•å‡è®¾\n",
    "   model = LogisticRegression()  # å‡è®¾çº¿æ€§å¯åˆ†\n",
    "   model.fit(X_nonlinear, y_nonlinear)  # æ•ˆæœå¯èƒ½å¾ˆå·®\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ£€æŸ¥æ•°æ®ç‰¹å¾å¹¶é€‰æ‹©åˆé€‚ç®—æ³•\n",
    "   if is_linearly_separable(X, y):\n",
    "       model = LogisticRegression()\n",
    "   else:\n",
    "       model = RandomForestClassifier()  # éçº¿æ€§æ¨¡å‹\n",
    "   ```\n",
    "\n",
    "### ğŸŒ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "**æœºå™¨å­¦ä¹ æ€§èƒ½ä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨ç‰¹å¾å·¥ç¨‹æé«˜æ¨¡å‹æ€§èƒ½\n",
    "- é‡‡ç”¨é›†æˆæ–¹æ³•å¢å¼ºé¢„æµ‹èƒ½åŠ›\n",
    "- å®ç°è‡ªåŠ¨åŒ–çš„è¶…å‚æ•°ä¼˜åŒ–\n",
    "- ä½¿ç”¨å¹¶è¡Œè®¡ç®—åŠ é€Ÿæ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "**æ¨¡å‹éƒ¨ç½²ä¼˜åŒ–**ï¼š\n",
    "- å»ºç«‹æ ‡å‡†åŒ–çš„æ¨¡å‹è®­ç»ƒæµç¨‹\n",
    "- å®ç°æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶å’Œå›æ»š\n",
    "- è®¾è®¡å®æ—¶é¢„æµ‹æœåŠ¡æ¶æ„\n",
    "- å®ç°æ¨¡å‹æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦\n",
    "\n",
    "**è´¨é‡ä¿è¯ä¼˜åŒ–**ï¼š\n",
    "- å»ºç«‹å®Œæ•´çš„æ¨¡å‹éªŒè¯æµç¨‹\n",
    "- å®ç°æ¨¡å‹å…¬å¹³æ€§å’Œåè§æ£€æµ‹\n",
    "- è®¾è®¡æ¨¡å‹è§£é‡Šæ€§åˆ†æå·¥å…·\n",
    "- å®ç°æ¨¡å‹é²æ£’æ€§æµ‹è¯•æ¡†æ¶\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆæœºå™¨å­¦ä¹ åŸºç¡€å­¦ä¹ ï¼**\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¿›è¡Œç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ï¼Œä¸ºLangChainæ™ºèƒ½åº”ç”¨å¼€å‘æä¾›äº†å¼ºå¤§çš„ç®—æ³•æ”¯æŒã€‚\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ é¢„å‘Š\n",
    "\n",
    "**ç»§ç»­ç¬¬äº”èŠ‚ï¼šæ•°æ®å¤„ç†**\n",
    "- 5.7 æ—¶é—´åºåˆ—åˆ†æ\n",
    "- 5.8 å¤§æ•°æ®å¤„ç†æŠ€æœ¯\n",
    "- 5.9 æ•°æ®å·¥ç¨‹å®è·µ\n",
    "\n",
    "**åç»­ç« èŠ‚é¢„å‘Š**ï¼š\n",
    "- å¼‚æ­¥ç¼–ç¨‹æŠ€æœ¯\n",
    "- Webå¼€å‘æŠ€æœ¯\n",
    "- é¡¹ç›®å·¥ç¨‹å®è·µ\n",
    "\n",
    "ç»§ç»­åŠ æ²¹ï¼Œæœºå™¨å­¦ä¹ æŠ€èƒ½æ­£åœ¨å¿«é€Ÿæå‡ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
