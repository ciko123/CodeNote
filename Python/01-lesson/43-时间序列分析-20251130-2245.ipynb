{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43-æ—¶é—´åºåˆ—åˆ†æ\n",
    "\n",
    "## ğŸ“š ç”¨é€”è¯´æ˜\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**ï¼š\n",
    "- æŒæ¡æ—¶é—´åºåˆ—æ•°æ®çš„åŸºæœ¬æ¦‚å¿µå’Œç‰¹å¾\n",
    "- ç†Ÿç»ƒä½¿ç”¨æ—¶é—´åºåˆ—åˆ†è§£å’Œé¢„å¤„ç†æŠ€æœ¯\n",
    "- ç†è§£å¸¸ç”¨çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹\n",
    "- èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„æ—¶é—´åºåˆ—åˆ†æå·¥ä½œæµ\n",
    "\n",
    "**å‰ç½®è¦æ±‚**ï¼š\n",
    "- å·²å®Œæˆ42-æœºå™¨å­¦ä¹ åŸºç¡€å­¦ä¹ \n",
    "- ç†Ÿç»ƒæŒæ¡Pandasæ—¶é—´åºåˆ—æ“ä½œ\n",
    "- äº†è§£åŸºæœ¬çš„ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ æ¦‚å¿µ\n",
    "\n",
    "**ä¸LangChainå…³è”**ï¼š\n",
    "- æ—¶é—´åºåˆ—åˆ†ææ”¯æŒLangChainçš„æ—¶åºæ•°æ®å¤„ç†\n",
    "- ä¸ºLangChainçš„é¢„æµ‹åº”ç”¨æä¾›æŠ€æœ¯åŸºç¡€\n",
    "- æ”¯æŒLangChainçš„æ—¶é—´åºåˆ—æ–‡æœ¬åˆ†æ\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ çŸ¥è¯†ç‚¹è¦†ç›–\n",
    "\n",
    "### 5.7 æ—¶é—´åºåˆ—åˆ†æ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šæ—¶é—´åºåˆ—åˆ†ææ˜¯å¤„ç†æ—¶é—´ç›¸å…³æ•°æ®çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¶‹åŠ¿åˆ†æã€å­£èŠ‚æ€§åˆ†è§£ã€é¢„æµ‹å»ºæ¨¡ç­‰ã€‚æŒæ¡è¿™äº›æŠ€èƒ½å¯¹äºæ„å»ºæ™ºèƒ½LangChainåº”ç”¨éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡æ—¶é—´åºåˆ—çš„åŸºæœ¬æ¦‚å¿µ\n",
    "- ç†è§£æ—¶é—´åºåˆ—åˆ†è§£æ–¹æ³•\n",
    "- ç†Ÿç»ƒä½¿ç”¨æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹\n",
    "- èƒ½å¤Ÿè¯„ä¼°æ—¶é—´åºåˆ—æ¨¡å‹æ€§èƒ½\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ—¶é—´åºåˆ—åˆ†ææµç¨‹\n",
    "- è¿›è¡Œå¤šç§é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒ\n",
    "- åº”ç”¨æ—¶é—´åºåˆ—åˆ†æè§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æ—¶é—´åºåˆ—é¢„æµ‹ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â° æ—¶é—´åºåˆ—åˆ†æ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Any, Optional, Union, Dict\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ—¶é—´åºåˆ—åˆ†æåº“\n",
    "import statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.forecasting.stl import STLForecast\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# æœºå™¨å­¦ä¹ ç”¨äºæ—¶é—´åºåˆ—\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"âœ… Pandasç‰ˆæœ¬: {pd.__version__}\")\n",
    "print(f\"âœ… NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"âœ… Statsmodelsç‰ˆæœ¬: {statsmodels.__version__}\")\n",
    "\n",
    "# 1. æ—¶é—´åºåˆ—åŸºç¡€æ¦‚å¿µ\n",
    "print(f\"\\nğŸ“ 1. æ—¶é—´åºåˆ—åŸºç¡€æ¦‚å¿µ:\")\n",
    "\n",
    "# 1.1 åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®é›†\n",
    "print(f\"\\n   ğŸ“Š 1.1 åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®é›†:\")\n",
    "\n",
    "def create_time_series_dataset(start_date: str = '2020-01-01', end_date: str = '2023-12-31'):\n",
    "    \"\"\"åˆ›å»ºç”¨äºæ—¶é—´åºåˆ—åˆ†æçš„ç¤ºä¾‹æ•°æ®é›†\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # åˆ›å»ºæ—¥æœŸèŒƒå›´\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # ç”ŸæˆåŸºç¡€æ—¶é—´åºåˆ—æ•°æ®\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    # è¶‹åŠ¿ç»„ä»¶\n",
    "    trend = np.linspace(100, 200, n_days) + np.random.normal(0, 5, n_days)\n",
    "    \n",
    "    # å­£èŠ‚æ€§ç»„ä»¶ï¼ˆå¹´åº¦å­£èŠ‚æ€§ï¼‰\n",
    "    seasonal_annual = 20 * np.sin(2 * np.pi * np.arange(n_days) / 365.25)\n",
    "    \n",
    "    # å­£èŠ‚æ€§ç»„ä»¶ï¼ˆæœˆåº¦å­£èŠ‚æ€§ï¼‰\n",
    "    seasonal_monthly = 10 * np.sin(2 * np.pi * np.arange(n_days) / 30.44)\n",
    "    \n",
    "    # å‘¨å­£èŠ‚æ€§\n",
    "    seasonal_weekly = 5 * np.sin(2 * np.pi * np.arange(n_days) / 7)\n",
    "    \n",
    "    # å™ªå£°ç»„ä»¶\n",
    "    noise = np.random.normal(0, 8, n_days)\n",
    "    \n",
    "    # ç»„åˆæ‰€æœ‰ç»„ä»¶\n",
    "    base_series = trend + seasonal_annual + seasonal_monthly + seasonal_weekly + noise\n",
    "\n",
    "    # åˆ›å»ºå¤šä¸ªç›¸å…³çš„æ—¶é—´åºåˆ—\n",
    "    data = {\n",
    "        'date': dates,\n",
    "        'user_activity': base_series,\n",
    "        'conversation_count': base_series * 0.8 + np.random.normal(0, 10, n_days),\n",
    "        'api_requests': base_series * 1.2 + np.random.normal(0, 15, n_days),\n",
    "        'response_time': 50 + 30 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) + np.random.normal(0, 5, n_days),\n",
    "        'error_rate': 0.02 + 0.01 * np.sin(2 * np.pi * np.arange(n_days) / 30.44) + np.random.exponential(0.005, n_days),\n",
    "        'cpu_usage': 30 + 20 * np.sin(2 * np.pi * np.arange(n_days) / 7) + np.random.normal(0, 3, n_days),\n",
    "        'memory_usage': 40 + 15 * np.sin(2 * np.pi * np.arange(n_days) / 14) + np.random.normal(0, 2, n_days),\n",
    "        'disk_io': base_series * 0.6 + np.random.normal(0, 8, n_days),\n",
    "        'network_traffic': base_series * 0.9 + np.random.normal(0, 12, n_days),\n",
    "        'active_users': base_series * 0.7 + np.random.normal(0, 6, n_days)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ç¡®ä¿æ•°æ®åˆç†æ€§\n",
    "    df['user_activity'] = np.maximum(df['user_activity'], 0)\n",
    "    df['conversation_count'] = np.maximum(df['conversation_count'], 0)\n",
    "    df['api_requests'] = np.maximum(df['api_requests'], 0)\n",
    "    df['response_time'] = np.maximum(df['response_time'], 10)\n",
    "    df['error_rate'] = np.clip(df['error_rate'], 0, 0.1)\n",
    "    df['cpu_usage'] = np.clip(df['cpu_usage'], 0, 100)\n",
    "    df['memory_usage'] = np.clip(df['memory_usage'], 0, 100)\n",
    "    df['disk_io'] = np.maximum(df['disk_io'], 0)\n",
    "    df['network_traffic'] = np.maximum(df['network_traffic'], 0)\n",
    "    df['active_users'] = np.maximum(df['active_users'], 0)\n",
    "    \n",
    "    # æ·»åŠ ä¸€äº›ç‰¹æ®Šäº‹ä»¶ï¼ˆå¼‚å¸¸ç‚¹ï¼‰\n",
    "    event_dates = [\n",
    "        '2021-06-15', '2021-12-24', '2022-03-08', \n",
    "        '2022-07-04', '2022-11-11', '2023-01-01', '2023-09-01'\n",
    "    ]\n",
    "    \n",
    "    for event_date in event_dates:\n",
    "        if event_date in df['date'].dt.strftime('%Y-%m-%d').values:\n",
    "            idx = df[df['date'].dt.strftime('%Y-%m-%d') == event_date].index[0]\n",
    "            # åœ¨äº‹ä»¶æ—¥æœŸå¢åŠ æ´»åŠ¨é‡\n",
    "            df.loc[idx, 'user_activity'] *= 2.5\n",
    "            df.loc[idx, 'conversation_count'] *= 3.0\n",
    "            df.loc[idx, 'api_requests'] *= 2.8\n",
    "            df.loc[idx, 'response_time'] *= 1.8\n",
    "            df.loc[idx, 'error_rate'] *= 2.0\n",
    "    \n",
    "    # è®¾ç½®æ—¥æœŸä¸ºç´¢å¼•\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®é›†\n",
    "ts_df = create_time_series_dataset('2020-01-01', '2023-12-31')\n",
    "print(f\"   åˆ›å»ºäº†åŒ…å« {len(ts_df)} å¤©çš„æ—¶é—´åºåˆ—æ•°æ®é›†\")\n",
    "print(f\"   æ—¶é—´èŒƒå›´: {ts_df.index.min()} åˆ° {ts_df.index.max()}\")\n",
    "print(f\"   å˜é‡æ•°é‡: {len(ts_df.columns)}\")\n",
    "print(f\"   ä¸»è¦æŒ‡æ ‡: ç”¨æˆ·æ´»è·ƒåº¦ã€å¯¹è¯æ•°é‡ã€APIè¯·æ±‚ã€å“åº”æ—¶é—´ç­‰\")\n",
    "\n",
    "# 1.2 æ—¶é—´åºåˆ—åŸºç¡€æ¡†æ¶\n",
    "print(f\"\\n   ğŸ”§ 1.2 æ—¶é—´åºåˆ—åŸºç¡€æ¡†æ¶:\")\n",
    "\n",
    "@dataclass\n",
    "class TimeSeriesAnalysis:\n",
    "    \"\"\"æ—¶é—´åºåˆ—åˆ†æåŸºç¡€æ¡†æ¶\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.decomposition_results = {}\n",
    "        self.stationarity_tests = {}\n",
    "        self.forecasting_models = {}\n",
    "        self.forecasting_results = {}\n",
    "        \n",
    "    def basic_statistics(self, column: str):\n",
    "        \"\"\"åŸºç¡€ç»Ÿè®¡åˆ†æ\"\"\"\n",
    "        print(f\"   {column} åŸºç¡€ç»Ÿè®¡:\")\n",
    "        \n",
    "        series = self.df[column]\n",
    "        \n",
    "        stats = {\n",
    "            'count': len(series),\n",
    "            'mean': series.mean(),\n",
    "            'std': series.std(),\n",
    "            'min': series.min(),\n",
    "            'max': series.max(),\n",
    "            'median': series.median(),\n",
    "            'skewness': series.skew(),\n",
    "            'kurtosis': series.kurtosis()\n",
    "        }\n",
    "        \n",
    "        for key, value in stats.items():\n",
    "            print(f\"      {key}: {value:.4f}\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def plot_time_series(self, columns: List[str], figsize: Tuple[int, int] = (15, 10)):\n",
    "        \"\"\"ç»˜åˆ¶æ—¶é—´åºåˆ—å›¾\"\"\"\n",
    "        n_cols = len(columns)\n",
    "        fig, axes = plt.subplots(n_cols, 1, figsize=figsize, sharex=True)\n",
    "        \n",
    "        if n_cols == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, col in enumerate(columns):\n",
    "            axes[i].plot(self.df.index, self.df[col], linewidth=1)\n",
    "            axes[i].set_title(f'{col}', fontweight='bold')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].set_ylabel(col)\n",
    "        \n",
    "        axes[-1].set_xlabel('æ—¥æœŸ')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def test_stationarity(self, column: str, significance_level: float = 0.05):\n",
    "        \"\"\"æµ‹è¯•æ—¶é—´åºåˆ—å¹³ç¨³æ€§\"\"\"\n",
    "        print(f\"   {column} å¹³ç¨³æ€§æ£€éªŒ:\")\n",
    "        \n",
    "        series = self.df[column].dropna()\n",
    "        \n",
    "        # ADFæ£€éªŒ\n",
    "        adf_result = adfuller(series)\n",
    "        adf_statistic = adf_result[0]\n",
    "        adf_pvalue = adf_result[1]\n",
    "        adf_critical_values = adf_result[4]\n",
    "        \n",
    "        # KPSSæ£€éªŒ\n",
    "        kpss_result = kpss(series, regression='c')\n",
    "        kpss_statistic = kpss_result[0]\n",
    "        kpss_pvalue = kpss_result[1]\n",
    "        kpss_critical_values = kpss_result[3]\n",
    "        \n",
    "        results = {\n",
    "            'ADF': {\n",
    "                'statistic': adf_statistic,\n",
    "                'pvalue': adf_pvalue,\n",
    "                'critical_values': adf_critical_values,\n",
    "                'is_stationary': adf_pvalue < significance_level\n",
    "            },\n",
    "            'KPSS': {\n",
    "                'statistic': kpss_statistic,\n",
    "                'pvalue': kpss_pvalue,\n",
    "                'critical_values': kpss_critical_values,\n",
    "                'is_stationary': kpss_pvalue > significance_level\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"      ADFæ£€éªŒ: ç»Ÿè®¡é‡={adf_statistic:.4f}, på€¼={adf_pvalue:.4f}\")\n",
    "        print(f\"      ADFç»“æœ: {'å¹³ç¨³' if results['ADF']['is_stationary'] else 'éå¹³ç¨³'}\")\n",
    "        print(f\"      KPSSæ£€éªŒ: ç»Ÿè®¡é‡={kpss_statistic:.4f}, på€¼={kpss_pvalue:.4f}\")\n",
    "        print(f\"      KPSSç»“æœ: {'å¹³ç¨³' if results['KPSS']['is_stationary'] else 'éå¹³ç¨³'}\")\n",
    "        \n",
    "        self.stationarity_tests[column] = results\n",
    "        return results\n",
    "    \n",
    "    def decompose_time_series(self, column: str, model: str = 'additive', \n",
    "                              period: int = None, method: str = 'seasonal_decompose'):\n",
    "        \"\"\"æ—¶é—´åºåˆ—åˆ†è§£\"\"\"\n",
    "        print(f\"   {column} æ—¶é—´åºåˆ—åˆ†è§£ ({method}, {model}):\")\n",
    "        \n",
    "        series = self.df[column].dropna()\n",
    "        \n",
    "        if period is None:\n",
    "            # è‡ªåŠ¨æ£€æµ‹å‘¨æœŸ\n",
    "            if len(series) >= 365:\n",
    "                period = 365  # å¹´åº¦å­£èŠ‚æ€§\n",
    "            elif len(series) >= 30:\n",
    "                period = 30   # æœˆåº¦å­£èŠ‚æ€§\n",
    "            else:\n",
    "                period = 7    # å‘¨å­£èŠ‚æ€§\n",
    "        \n",
    "        if method == 'seasonal_decompose':\n",
    "            decomposition = seasonal_decompose(\n",
    "                series, model=model, period=period, extrapolate_trend='freq'\n",
    "            )\n",
    "        elif method == 'STL':\n",
    "            decomposition = STL(series, period=period).fit()\n",
    "        else:\n",
    "            raise ValueError(\"ä¸æ”¯æŒçš„åˆ†è§£æ–¹æ³•\")\n",
    "        \n",
    "        # è®¡ç®—å„ç»„ä»¶çš„è´¡çŒ®åº¦\n",
    "        trend_strength = 1 - np.var(decomposition.resid) / np.var(series + decomposition.resid)\n",
    "        seasonal_strength = 1 - np.var(decomposition.resid) / np.var(series + decomposition.resid)\n",
    "        \n",
    "        result = {\n",
    "            'decomposition': decomposition,\n",
    "            'trend': decomposition.trend,\n",
    "            'seasonal': decomposition.seasonal,\n",
    "            'residual': decomposition.resid,\n",
    "            'trend_strength': trend_strength,\n",
    "            'seasonal_strength': seasonal_strength,\n",
    "            'period': period,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        print(f\"      åˆ†è§£å‘¨æœŸ: {period}\")\n",
    "        print(f\"      è¶‹åŠ¿å¼ºåº¦: {trend_strength:.3f}\")\n",
    "        print(f\"      å­£èŠ‚æ€§å¼ºåº¦: {seasonal_strength:.3f}\")\n",
    "        \n",
    "        # å¯è§†åŒ–åˆ†è§£ç»“æœ\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "        \n",
    "        decomposition.observed.plot(ax=axes[0], title='åŸå§‹åºåˆ—')\n",
    "        decomposition.trend.plot(ax=axes[1], title='è¶‹åŠ¿ç»„ä»¶')\n",
    "        decomposition.seasonal.plot(ax=axes[2], title='å­£èŠ‚æ€§ç»„ä»¶')\n",
    "        decomposition.resid.plot(ax=axes[3], title='æ®‹å·®ç»„ä»¶')\n",
    "        \n",
    "        for ax in axes:\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        self.decomposition_results[column] = result\n",
    "        return result\n",
    "    \n",
    "    def plot_autocorrelation(self, column: str, lags: int = 40):\n",
    "        \"\"\"ç»˜åˆ¶è‡ªç›¸å…³å’Œåè‡ªç›¸å…³å›¾\"\"\"\n",
    "        print(f\"   {column} è‡ªç›¸å…³åˆ†æ:\")\n",
    "        \n",
    "        series = self.df[column].dropna()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "        \n",
    "        # ACFå›¾\n",
    "        plot_acf(series, lags=lags, ax=axes[0], alpha=0.05)\n",
    "        axes[0].set_title('è‡ªç›¸å…³å‡½æ•° (ACF)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # PACFå›¾\n",
    "        plot_pacf(series, lags=lags, ax=axes[1], alpha=0.05, method='ywm')\n",
    "        axes[1].set_title('åè‡ªç›¸å…³å‡½æ•° (PACF)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # è®¡ç®—è‡ªç›¸å…³ç³»æ•°\n",
    "        acf_values = acf(series, nlags=lags, alpha=0.05)\n",
    "        pacf_values = pacf(series, nlags=lags, alpha=0.05, method='ywm')\n",
    "        \n",
    "        return {\n",
    "            'acf': acf_values[0],\n",
    "            'acf_conf_int': acf_values[1],\n",
    "            'pacf': pacf_values[0],\n",
    "            'pacf_conf_int': pacf_values[1]\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–æ—¶é—´åºåˆ—åˆ†ææ¡†æ¶\n",
    "ts_analysis = TimeSeriesAnalysis(ts_df)\n",
    "print(f\"   âœ… æ—¶é—´åºåˆ—åˆ†ææ¡†æ¶åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "print(f\"\\nâœ… æ—¶é—´åºåˆ—åŸºç¡€æ¦‚å¿µå®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡æ—¶é—´åºåˆ—çš„åŸºæœ¬æ¦‚å¿µ\")\n",
    "print(f\"   âœ“ ç†è§£æ—¶é—´åºåˆ—æ•°æ®ç‰¹å¾\")\n",
    "print(f\"   âœ“ ç†Ÿæ‚‰æ—¶é—´åºåˆ—åˆ†ææ¡†æ¶\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿåˆ›å»ºå’Œé¢„å¤„ç†æ—¶é—´åºåˆ—æ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šæ—¶é—´åºåˆ—é¢„æµ‹æ˜¯æ—¶é—´åºåˆ—åˆ†æçš„æ ¸å¿ƒåº”ç”¨ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿç»Ÿè®¡æ¨¡å‹å’Œæœºå™¨å­¦ä¹ æ–¹æ³•ã€‚æŒæ¡è¿™äº›ç®—æ³•å¯¹äºæ„å»ºé¢„æµ‹ç³»ç»Ÿéå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡ARIMAæ¨¡å‹çš„åŸºæœ¬åŸç†\n",
    "- ç†è§£æŒ‡æ•°å¹³æ»‘æ–¹æ³•\n",
    "- ç†Ÿç»ƒä½¿ç”¨æœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹\n",
    "- èƒ½å¤Ÿè¯„ä¼°å’Œæ¯”è¾ƒé¢„æµ‹æ€§èƒ½\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ—¶é—´åºåˆ—é¢„æµ‹æµç¨‹\n",
    "- è¿›è¡Œå¤šç§é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒ\n",
    "- åº”ç”¨æ—¶é—´åºåˆ—é¢„æµ‹è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æ—¶é—´åºåˆ—é¢„æµ‹ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ”® æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ä¼ ç»Ÿç»Ÿè®¡é¢„æµ‹æ¨¡å‹\n",
    "print(f\"ğŸ“ 1. ä¼ ç»Ÿç»Ÿè®¡é¢„æµ‹æ¨¡å‹:\")\n",
    "\n",
    "@dataclass\n",
    "class TimeSeriesForecasting:\n",
    "    \"\"\"æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹å®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, ts_analysis: TimeSeriesAnalysis):\n",
    "        self.ts_analysis = ts_analysis\n",
    "        self.models = {}\n",
    "        self.predictions = {}\n",
    "        self.evaluations = {}\n",
    "    \n",
    "    def split_data(self, column: str, test_size: int = 30):\n",
    "        \"\"\"åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®\"\"\"\n",
    "        print(f\"   åˆ†å‰² {column} æ•°æ® (æµ‹è¯•é›†å¤§å°: {test_size} å¤©)\")\n",
    "        \n",
    "        series = self.ts_analysis.df[column].dropna()\n",
    "        \n",
    "        train = series[:-test_size]\n",
    "        test = series[-test_size:]\n",
    "        \n",
    "        print(f\"      è®­ç»ƒé›†: {len(train)} å¤© ({train.index.min()} åˆ° {train.index.max()})\")\n",
    "        print(f\"      æµ‹è¯•é›†: {len(test)} å¤© ({test.index.min()} åˆ° {test.index.max()})\")\n",
    "        \n",
    "        return train, test\n",
    "    \n",
    "    def arima_forecast(self, train_data, test_data, order: Tuple[int, int, int] = None, \n",
    "                       seasonal_order: Tuple[int, int, int, int] = None):\n",
    "        \"\"\"ARIMAæ¨¡å‹é¢„æµ‹\"\"\"\n",
    "        print(f\"   ARIMAæ¨¡å‹é¢„æµ‹...\")\n",
    "        \n",
    "        if order is None:\n",
    "            # è‡ªåŠ¨é€‰æ‹©ARIMAå‚æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "            order = (1, 1, 1)\n",
    "        \n",
    "        if seasonal_order is None:\n",
    "            seasonal_order = (1, 1, 1, 12)\n",
    "        \n",
    "        try:\n",
    "            # è®­ç»ƒSARIMAæ¨¡å‹\n",
    "            model = SARIMAX(train_data, order=order, seasonal_order=seasonal_order)\n",
    "            fitted_model = model.fit(disp=False)\n",
    "            \n",
    "            # é¢„æµ‹\n",
    "            forecast = fitted_model.forecast(steps=len(test_data))\n",
    "            forecast_ci = fitted_model.get_forecast(steps=len(test_data)).conf_int()\n",
    "            \n",
    "            # è¯„ä¼°\n",
    "            mae = mean_absolute_error(test_data, forecast)\n",
    "            mse = mean_squared_error(test_data, forecast)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = mean_absolute_percentage_error(test_data, forecast)\n",
    "            \n",
    "            result = {\n",
    "                'model': fitted_model,\n",
    "                'forecast': forecast,\n",
    "                'forecast_ci': forecast_ci,\n",
    "                'order': order,\n",
    "                'seasonal_order': seasonal_order,\n",
    "                'metrics': {\n",
    "                    'MAE': mae,\n",
    "                    'MSE': mse,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAPE': mape\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"      ARIMA{order} è®­ç»ƒå®Œæˆ\")\n",
    "            print(f\"      MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
    "            \n",
    "            self.models['ARIMA'] = fitted_model\n",
    "            self.predictions['ARIMA'] = forecast\n",
    "            self.evaluations['ARIMA'] = result['metrics']\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ARIMAæ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def exponential_smoothing_forecast(self, train_data, test_data, trend: str = 'add', \n",
    "                                        seasonal: str = 'add', seasonal_periods: int = 12):\n",
    "        \"\"\"æŒ‡æ•°å¹³æ»‘é¢„æµ‹\"\"\"\n",
    "        print(f\"   æŒ‡æ•°å¹³æ»‘æ¨¡å‹é¢„æµ‹...\")\n",
    "        \n",
    "        try:\n",
    "            # è®­ç»ƒæŒ‡æ•°å¹³æ»‘æ¨¡å‹\n",
    "            model = ExponentialSmoothing(\n",
    "                train_data, \n",
    "                trend=trend, \n",
    "                seasonal=seasonal, \n",
    "                seasonal_periods=seasonal_periods\n",
    "            )\n",
    "            fitted_model = model.fit()\n",
    "            \n",
    "            # é¢„æµ‹\n",
    "            forecast = fitted_model.forecast(steps=len(test_data))\n",
    "            \n",
    "            # è¯„ä¼°\n",
    "            mae = mean_absolute_error(test_data, forecast)\n",
    "            mse = mean_squared_error(test_data, forecast)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = mean_absolute_percentage_error(test_data, forecast)\n",
    "            \n",
    "            result = {\n",
    "                'model': fitted_model,\n",
    "                'forecast': forecast,\n",
    "                'trend': trend,\n",
    "                'seasonal': seasonal,\n",
    "                'seasonal_periods': seasonal_periods,\n",
    "                'metrics': {\n",
    "                    'MAE': mae,\n",
    "                    'MSE': mse,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAPE': mape\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"      æŒ‡æ•°å¹³æ»‘æ¨¡å‹è®­ç»ƒå®Œæˆ\")\n",
    "            print(f\"      MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
    "            \n",
    "            self.models['ExponentialSmoothing'] = fitted_model\n",
    "            self.predictions['ExponentialSmoothing'] = forecast\n",
    "            self.evaluations['ExponentialSmoothing'] = result['metrics']\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      æŒ‡æ•°å¹³æ»‘æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def ml_forecast(self, train_data, test_data, model_type: str = 'RandomForest', \n",
    "                    lag_features: int = 7):\n",
    "        \"\"\"æœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹\"\"\"\n",
    "        print(f\"   {model_type} æœºå™¨å­¦ä¹ é¢„æµ‹...\")\n",
    "        \n",
    "        try:\n",
    "            # åˆ›å»ºæ»åç‰¹å¾\n",
    "            def create_lag_features(series, lags):\n",
    "                df = pd.DataFrame({'target': series})\n",
    "                for lag in range(1, lags + 1):\n",
    "                    df[f'lag_{lag}'] = series.shift(lag)\n",
    "                \n",
    "                # æ·»åŠ æ—¶é—´ç‰¹å¾\n",
    "                df['day_of_week'] = series.index.dayofweek\n",
    "                df['month'] = series.index.month\n",
    "                df['quarter'] = series.index.quarter\n",
    "                df['year'] = series.index.year\n",
    "                df['day_of_year'] = series.index.dayofyear\n",
    "                \n",
    "                return df.dropna()\n",
    "            \n",
    "            # å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "            train_df = create_lag_features(train_data, lag_features)\n",
    "            test_df = create_lag_features(pd.concat([train_data.iloc[-lag_features:], test_data]), lag_features)\n",
    "            \n",
    "            # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡\n",
    "            feature_cols = [col for col in train_df.columns if col != 'target']\n",
    "            X_train = train_df[feature_cols]\n",
    "            y_train = train_df['target']\n",
    "            X_test = test_df[feature_cols]\n",
    "            y_test = test_df['target']\n",
    "            \n",
    "            # ç‰¹å¾ç¼©æ”¾\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # é€‰æ‹©æ¨¡å‹\n",
    "            if model_type == 'RandomForest':\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            elif model_type == 'GradientBoosting':\n",
    "                model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "            elif model_type == 'LinearRegression':\n",
    "                model = LinearRegression()\n",
    "            elif model_type == 'Ridge':\n",
    "                model = Ridge(alpha=1.0)\n",
    "            else:\n",
    "                raise ValueError(\"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹\")\n",
    "            \n",
    "            # è®­ç»ƒæ¨¡å‹\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # é¢„æµ‹\n",
    "            forecast = model.predict(X_test_scaled)\n",
    "            forecast = pd.Series(forecast, index=y_test.index)\n",
    "            \n",
    "            # è¯„ä¼°\n",
    "            mae = mean_absolute_error(y_test, forecast)\n",
    "            mse = mean_squared_error(y_test, forecast)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = mean_absolute_percentage_error(y_test, forecast)\n",
    "            \n",
    "            result = {\n",
    "                'model': model,\n",
    "                'forecast': forecast,\n",
    "                'scaler': scaler,\n",
    "                'feature_cols': feature_cols,\n",
    "                'lag_features': lag_features,\n",
    "                'metrics': {\n",
    "                    'MAE': mae,\n",
    "                    'MSE': mse,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAPE': mape\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"      {model_type} æ¨¡å‹è®­ç»ƒå®Œæˆ\")\n",
    "            print(f\"      MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
    "            \n",
    "            self.models[model_type] = model\n",
    "            self.predictions[model_type] = forecast\n",
    "            self.evaluations[model_type] = result['metrics']\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      {model_type} æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_forecast_results(self, train_data, test_data, title: str = \"é¢„æµ‹ç»“æœå¯¹æ¯”\"):\n",
    "        \"\"\"ç»˜åˆ¶é¢„æµ‹ç»“æœå¯¹æ¯”å›¾\"\"\"\n",
    "        print(f\"   ç»˜åˆ¶é¢„æµ‹ç»“æœå¯¹æ¯”å›¾...\")\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # ç»˜åˆ¶è®­ç»ƒæ•°æ®\n",
    "        plt.plot(train_data.index[-100:], train_data.iloc[-100:], \n",
    "                label='è®­ç»ƒæ•°æ®', color='blue', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        # ç»˜åˆ¶æµ‹è¯•æ•°æ®\n",
    "        plt.plot(test_data.index, test_data, \n",
    "                label='å®é™…æ•°æ®', color='green', linewidth=2)\n",
    "        \n",
    "        # ç»˜åˆ¶å„æ¨¡å‹é¢„æµ‹ç»“æœ\n",
    "        colors = ['red', 'orange', 'purple', 'brown', 'pink']\n",
    "        for i, (model_name, forecast) in enumerate(self.predictions.items()):\n",
    "            if len(forecast) == len(test_data):\n",
    "                plt.plot(forecast.index, forecast, \n",
    "                        label=f'{model_name}é¢„æµ‹', \n",
    "                        color=colors[i % len(colors)], \n",
    "                        linestyle='--', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        plt.title(title, fontweight='bold', fontsize=14)\n",
    "        plt.xlabel('æ—¥æœŸ')\n",
    "        plt.ylabel('æ•°å€¼')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_forecast_performance(self):\n",
    "        \"\"\"æ¯”è¾ƒé¢„æµ‹æ¨¡å‹æ€§èƒ½\"\"\"\n",
    "        print(f\"\\n   ğŸ“Š é¢„æµ‹æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:\")\n",
    "        \n",
    "        if not self.evaluations:\n",
    "            print(\"      æ²¡æœ‰å¯æ¯”è¾ƒçš„æ¨¡å‹ç»“æœ\")\n",
    "            return None\n",
    "        \n",
    "        # åˆ›å»ºæ¯”è¾ƒè¡¨\n",
    "        comparison_data = []\n",
    "        \n",
    "        for model_name, metrics in self.evaluations.items():\n",
    "            comparison_data.append({\n",
    "                'æ¨¡å‹': model_name,\n",
    "                'MAE': metrics['MAE'],\n",
    "                'RMSE': metrics['RMSE'],\n",
    "                'MAPE': metrics['MAPE']\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df = comparison_df.sort_values('MAE')\n",
    "        \n",
    "        print(comparison_df.round(4).to_string(index=False))\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    def residual_analysis(self, model_name: str, test_data):\n",
    "        \"\"\"æ®‹å·®åˆ†æ\"\"\"\n",
    "        print(f\"   {model_name} æ®‹å·®åˆ†æ:\")\n",
    "        \n",
    "        if model_name not in self.predictions:\n",
    "            print(f\"      æ¨¡å‹ {model_name} ä¸å­˜åœ¨\")\n",
    "            return None\n",
    "        \n",
    "        forecast = self.predictions[model_name]\n",
    "        residuals = test_data - forecast\n",
    "        \n",
    "        # æ®‹å·®ç»Ÿè®¡\n",
    "        print(f\"      æ®‹å·®å‡å€¼: {residuals.mean():.4f}\")\n",
    "        print(f\"      æ®‹å·®æ ‡å‡†å·®: {residuals.std():.4f}\")\n",
    "        print(f\"      æ®‹å·®ååº¦: {residuals.skew():.4f}\")\n",
    "        print(f\"      æ®‹å·®å³°åº¦: {residuals.kurtosis():.4f}\")\n",
    "        \n",
    "        # Ljung-Boxæ£€éªŒï¼ˆæ®‹å·®è‡ªç›¸å…³æ£€éªŒï¼‰\n",
    "        try:\n",
    "            lb_test = acorr_ljungbox(residuals, lags=10, return_df=True)\n",
    "            print(f\"      Ljung-Boxæ£€éªŒ på€¼: {lb_test['lb_pvalue'].iloc[-1]:.4f}\")\n",
    "            print(f\"      æ®‹å·®è‡ªç›¸å…³: {'æ˜¾è‘—' if lb_test['lb_pvalue'].iloc[-1] < 0.05 else 'ä¸æ˜¾è‘—'}\")\n",
    "        except:\n",
    "            print(f\"      Ljung-Boxæ£€éªŒå¤±è´¥\")\n",
    "        \n",
    "        # ç»˜åˆ¶æ®‹å·®å›¾\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # æ®‹å·®æ—¶é—´åºåˆ—\n",
    "        axes[0, 0].plot(residuals.index, residuals, alpha=0.7)\n",
    "        axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "        axes[0, 0].set_title('æ®‹å·®æ—¶é—´åºåˆ—')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ®‹å·®ç›´æ–¹å›¾\n",
    "        axes[0, 1].hist(residuals, bins=30, alpha=0.7, density=True)\n",
    "        axes[0, 1].set_title('æ®‹å·®åˆ†å¸ƒ')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Qå›¾\n",
    "        from scipy import stats\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "        axes[1, 0].set_title('Q-Qå›¾')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ®‹å·®ACF\n",
    "        plot_acf(residuals, lags=20, ax=axes[1, 1], alpha=0.05)\n",
    "        axes[1, 1].set_title('æ®‹å·®è‡ªç›¸å…³')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return residuals\n",
    "\n",
    "# æ¼”ç¤ºæ—¶é—´åºåˆ—é¢„æµ‹\n",
    "print(f\"\\n   ğŸ”§ æ—¶é—´åºåˆ—é¢„æµ‹æ¼”ç¤º:\")\n",
    "ts_forecasting = TimeSeriesForecasting(ts_analysis)\n",
    "\n",
    "# 1. æ•°æ®åˆ†å‰²å’ŒåŸºç¡€åˆ†æ\n",
    "print(f\"\\n   ğŸ“Š æ•°æ®å‡†å¤‡å’ŒåŸºç¡€åˆ†æ:\")\n",
    "\n",
    "# é€‰æ‹©ä¸»è¦æŒ‡æ ‡è¿›è¡Œé¢„æµ‹\n",
    "target_column = 'user_activity'\n",
    "\n",
    "# åŸºç¡€ç»Ÿè®¡\n",
    "basic_stats = ts_analysis.basic_statistics(target_column)\n",
    "\n",
    "# å¹³ç¨³æ€§æ£€éªŒ\n",
    "stationarity_result = ts_analysis.test_stationarity(target_column)\n",
    "\n",
    "# æ—¶é—´åºåˆ—åˆ†è§£\n",
    "decomposition_result = ts_analysis.decompose_time_series(\n",
    "    target_column, model='additive', method='seasonal_decompose'\n",
    ")\n",
    "\n",
    "# è‡ªç›¸å…³åˆ†æ\n",
    "autocorr_result = ts_analysis.plot_autocorrelation(target_column, lags=40)\n",
    "\n",
    "# 2. æ•°æ®åˆ†å‰²\n",
    "print(f\"\\n   ğŸ”ª æ•°æ®åˆ†å‰²:\")\n",
    "train_data, test_data = ts_forecasting.split_data(target_column, test_size=30)\n",
    "\n",
    "# 3. è®­ç»ƒé¢„æµ‹æ¨¡å‹\n",
    "print(f\"\\n   ğŸ¯ è®­ç»ƒé¢„æµ‹æ¨¡å‹:\")\n",
    "\n",
    "# ARIMAæ¨¡å‹\n",
    "arima_result = ts_forecasting.arima_forecast(\n",
    "    train_data, test_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)\n",
    ")\n",
    "\n",
    "# æŒ‡æ•°å¹³æ»‘æ¨¡å‹\n",
    "exp_smooth_result = ts_forecasting.exponential_smoothing_forecast(\n",
    "    train_data, test_data, trend='add', seasonal='add', seasonal_periods=12\n",
    ")\n",
    "\n",
    "# æœºå™¨å­¦ä¹ æ¨¡å‹\n",
    "rf_result = ts_forecasting.ml_forecast(\n",
    "    train_data, test_data, model_type='RandomForest', lag_features=7\n",
    ")\n",
    "\n",
    "gb_result = ts_forecasting.ml_forecast(\n",
    "    train_data, test_data, model_type='GradientBoosting', lag_features=7\n",
    ")\n",
    "\n",
    "# 4. é¢„æµ‹ç»“æœå¯è§†åŒ–\n",
    "print(f\"\\n   ğŸ“ˆ é¢„æµ‹ç»“æœå¯è§†åŒ–:\")\n",
    "ts_forecasting.plot_forecast_results(\n",
    "    train_data, test_data, f'{target_column} é¢„æµ‹ç»“æœå¯¹æ¯”'\n",
    ")\n",
    "\n",
    "# 5. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ\n",
    "print(f\"\\n   ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:\")\n",
    "performance_comparison = ts_forecasting.compare_forecast_performance()\n",
    "\n",
    "# 6. æ®‹å·®åˆ†æ\n",
    "print(f\"\\n   ğŸ” æ®‹å·®åˆ†æ:\")\n",
    "if ts_forecasting.predictions:\n",
    "    best_model = performance_comparison.iloc[0]['æ¨¡å‹']\n",
    "    residuals = ts_forecasting.residual_analysis(best_model, test_data)\n",
    "\n",
    "print(f\"\\nâœ… æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡ARIMAæ¨¡å‹çš„åŸºæœ¬åŸç†\")\n",
    "print(f\"   âœ“ ç†è§£æŒ‡æ•°å¹³æ»‘æ–¹æ³•\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨æœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿè¯„ä¼°å’Œæ¯”è¾ƒé¢„æµ‹æ€§èƒ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é«˜çº§æ—¶é—´åºåˆ—åˆ†æ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šé«˜çº§æ—¶é—´åºåˆ—åˆ†æåŒ…æ‹¬å¤šå˜é‡æ—¶é—´åºåˆ—ã€å¼‚å¸¸æ£€æµ‹ã€å˜ç‚¹æ£€æµ‹ç­‰æŠ€æœ¯ã€‚æŒæ¡è¿™äº›æŠ€èƒ½å¯¹äºå¤æ‚çš„æ—¶é—´åºåˆ—åˆ†æéå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡å¤šå˜é‡æ—¶é—´åºåˆ—åˆ†ææ–¹æ³•\n",
    "- ç†è§£æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹æŠ€æœ¯\n",
    "- ç†Ÿç»ƒä½¿ç”¨å˜ç‚¹æ£€æµ‹æ–¹æ³•\n",
    "- èƒ½å¤Ÿå¤„ç†å¤æ‚çš„æ—¶é—´åºåˆ—é—®é¢˜\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„é«˜çº§æ—¶é—´åºåˆ—åˆ†ææµç¨‹\n",
    "- è¿›è¡Œå¤šå˜é‡æ—¶é—´åºåˆ—å»ºæ¨¡\n",
    "- åº”ç”¨å¼‚å¸¸æ£€æµ‹è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜çº§æ—¶é—´åºåˆ—åˆ†æç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸš€ é«˜çº§æ—¶é—´åºåˆ—åˆ†æ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. å¤šå˜é‡æ—¶é—´åºåˆ—åˆ†æ\n",
    "print(f\"ğŸ“ 1. å¤šå˜é‡æ—¶é—´åºåˆ—åˆ†æ:\")\n",
    "\n",
    "@dataclass\n",
    "class AdvancedTimeSeriesAnalysis:\n",
    "    \"\"\"é«˜çº§æ—¶é—´åºåˆ—åˆ†æå®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.multivariate_models = {}\n",
    "        self.anomaly_detection_results = {}\n",
    "        self.change_point_results = {}\n",
    "    \n",
    "    def correlation_analysis(self, columns: List[str]):\n",
    "        \"\"\"å¤šå˜é‡æ—¶é—´åºåˆ—ç›¸å…³æ€§åˆ†æ\"\"\"\n",
    "        print(f\"   å¤šå˜é‡æ—¶é—´åºåˆ—ç›¸å…³æ€§åˆ†æ:\")\n",
    "        \n",
    "        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ\n",
    "        corr_matrix = self.df[columns].corr()\n",
    "        \n",
    "        print(f\"      ç›¸å…³æ€§çŸ©é˜µ:\")\n",
    "        print(corr_matrix.round(3))\n",
    "        \n",
    "        # ç»˜åˆ¶çƒ­åŠ›å›¾\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                   square=True, fmt='.2f', cbar_kws={'label': 'ç›¸å…³ç³»æ•°'})\n",
    "        plt.title('å¤šå˜é‡æ—¶é—´åºåˆ—ç›¸å…³æ€§çƒ­åŠ›å›¾', fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # æ‰¾å‡ºå¼ºç›¸å…³å˜é‡å¯¹\n",
    "        strong_correlations = []\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i+1, len(columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:\n",
    "                    strong_correlations.append({\n",
    "                        'å˜é‡1': columns[i],\n",
    "                        'å˜é‡2': columns[j],\n",
    "                        'ç›¸å…³ç³»æ•°': corr_val\n",
    "                    })\n",
    "        \n",
    "        if strong_correlations:\n",
    "            print(f\"\\n      å¼ºç›¸å…³å˜é‡å¯¹ (|r| > 0.7):\")\n",
    "            for corr in strong_correlations:\n",
    "                print(f\"        {corr['å˜é‡1']} - {corr['å˜é‡2']}: {corr['ç›¸å…³ç³»æ•°']:.3f}\")\n",
    "        \n",
    "        return corr_matrix, strong_correlations\n",
    "    \n",
    "    def var_model(self, columns: List[str], test_size: int = 30):\n",
    "        \"\"\"å‘é‡è‡ªå›å½’(VAR)æ¨¡å‹\"\"\"\n",
    "        print(f\"   VARæ¨¡å‹åˆ†æ:\")\n",
    "        \n",
    "        try:\n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            data = self.df[columns].dropna()\n",
    "            \n",
    "            # æ•°æ®åˆ†å‰²\n",
    "            train_data = data[:-test_size]\n",
    "            test_data = data[-test_size:]\n",
    "            \n",
    "            print(f\"      è®­ç»ƒæ•°æ®: {len(train_data)} æ ·æœ¬\")\n",
    "            print(f\"      æµ‹è¯•æ•°æ®: {len(test_data)} æ ·æœ¬\")\n",
    "            \n",
    "            # é€‰æ‹©æœ€ä¼˜æ»åé˜¶æ•°\n",
    "            from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "            model = VAR(train_data)\n",
    "            \n",
    "            # ä½¿ç”¨ä¿¡æ¯å‡†åˆ™é€‰æ‹©æ»åé˜¶æ•°\n",
    "            lag_order = model.select_order(maxlags=10)\n",
    "            optimal_lag = lag_order.aic\n",
    "            print(f\"      æœ€ä¼˜æ»åé˜¶æ•°: {optimal_lag}\")\n",
    "            \n",
    "            # è®­ç»ƒVARæ¨¡å‹\n",
    "            fitted_model = model.fit(optimal_lag)\n",
    "            \n",
    "            # é¢„æµ‹\n",
    "            forecast = fitted_model.forecast(train_data.values[-optimal_lag:], steps=test_size)\n",
    "            forecast_df = pd.DataFrame(forecast, index=test_data.index, columns=columns)\n",
    "            \n",
    "            # è¯„ä¼°æ¯ä¸ªå˜é‡çš„é¢„æµ‹æ€§èƒ½\n",
    "            evaluation = {}\n",
    "            for col in columns:\n",
    "                mae = mean_absolute_error(test_data[col], forecast_df[col])\n",
    "                rmse = np.sqrt(mean_squared_error(test_data[col], forecast_df[col]))\n",
    "                mape = mean_absolute_percentage_error(test_data[col], forecast_df[col])\n",
    "                \n",
    "                evaluation[col] = {\n",
    "                    'MAE': mae,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAPE': mape\n",
    "                }\n",
    "                \n",
    "                print(f\"      {col} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
    "            \n",
    "            # ç»˜åˆ¶é¢„æµ‹ç»“æœ\n",
    "            fig, axes = plt.subplots(len(columns), 1, figsize=(15, 3*len(columns)))\n",
    "            if len(columns) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, col in enumerate(columns):\n",
    "                axes[i].plot(train_data.index[-50:], train_data[col].iloc[-50:], \n",
    "                            label='è®­ç»ƒæ•°æ®', alpha=0.7)\n",
    "                axes[i].plot(test_data.index, test_data[col], \n",
    "                            label='å®é™…æ•°æ®', linewidth=2)\n",
    "                axes[i].plot(forecast_df.index, forecast_df[col], \n",
    "                            label='VARé¢„æµ‹', linestyle='--', linewidth=2)\n",
    "                axes[i].set_title(f'{col} VARé¢„æµ‹ç»“æœ')\n",
    "                axes[i].legend()\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            result = {\n",
    "                'model': fitted_model,\n",
    "                'forecast': forecast_df,\n",
    "                'optimal_lag': optimal_lag,\n",
    "                'evaluation': evaluation\n",
    "            }\n",
    "            \n",
    "            self.multivariate_models['VAR'] = result\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      VARæ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def anomaly_detection(self, column: str, method: str = 'isolation_forest', \n",
    "                           contamination: float = 0.05):\n",
    "        \"\"\"æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹\"\"\"\n",
    "        print(f\"   {column} å¼‚å¸¸æ£€æµ‹ ({method}):\")\n",
    "        \n",
    "        try:\n",
    "            series = self.df[column].dropna()\n",
    "            \n",
    "            if method == 'isolation_forest':\n",
    "                from sklearn.ensemble import IsolationForest\n",
    "                \n",
    "                # åˆ›å»ºç‰¹å¾çŸ©é˜µï¼ˆåŒ…å«æ»åç‰¹å¾ï¼‰\n",
    "                def create_features(series, window=7):\n",
    "                    features = []\n",
    "                    for i in range(window, len(series)):\n",
    "                        window_data = series.iloc[i-window:i]\n",
    "                        feature_row = [\n",
    "                            series.iloc[i],  # å½“å‰å€¼\n",
    "                            window_data.mean(),  # çª—å£å‡å€¼\n",
    "                            window_data.std(),   # çª—å£æ ‡å‡†å·®\n",
    "                            window_data.min(),   # çª—å£æœ€å°å€¼\n",
    "                            window_data.max(),   # çª—å£æœ€å¤§å€¼\n",
    "                            window_data.iloc[-1] - window_data.iloc[0],  # å˜åŒ–\n",
    "                            (series.iloc[i] - window_data.mean()) / window_data.std() if window_data.std() > 0 else 0  # Zåˆ†æ•°\n",
    "                        ]\n",
    "                        features.append(feature_row)\n",
    "                    \n",
    "                    return np.array(features), series.index[window:]\n",
    "                \n",
    "                X, dates = create_features(series)\n",
    "                \n",
    "                # è®­ç»ƒIsolation Forest\n",
    "                iso_forest = IsolationForest(\n",
    "                    contamination=contamination, \n",
    "                    random_state=42\n",
    "                )\n",
    "                anomaly_labels = iso_forest.fit_predict(X)\n",
    "                anomaly_scores = iso_forest.decision_function(X)\n",
    "                \n",
    "                # å¼‚å¸¸ç‚¹æ ‡ç­¾ (-1ä¸ºå¼‚å¸¸ï¼Œ1ä¸ºæ­£å¸¸)\n",
    "                anomalies = anomaly_labels == -1\n",
    "                \n",
    "                result = {\n",
    "                    'method': method,\n",
    "                    'anomaly_labels': anomaly_labels,\n",
    "                    'anomaly_scores': anomaly_scores,\n",
    "                    'anomaly_dates': dates[anomalies],\n",
    "                    'anomaly_values': series.loc[dates[anomalies]],\n",
    "                    'contamination': contamination\n",
    "                }\n",
    "                \n",
    "                print(f\"      æ£€æµ‹åˆ° {sum(anomalies)} ä¸ªå¼‚å¸¸ç‚¹\")\n",
    "                print(f\"      å¼‚å¸¸æ¯”ä¾‹: {sum(anomalies)/len(anomalies):.3f}\")\n",
    "                \n",
    "            elif method == 'statistical':\n",
    "                # ç»Ÿè®¡æ–¹æ³•ï¼ˆåŸºäºZåˆ†æ•°ï¼‰\n",
    "                z_scores = np.abs((series - series.mean()) / series.std())\n",
    "                threshold = 3  # 3å€æ ‡å‡†å·®\n",
    "                anomalies = z_scores > threshold\n",
    "                \n",
    "                result = {\n",
    "                    'method': method,\n",
    "                    'z_scores': z_scores,\n",
    "                    'threshold': threshold,\n",
    "                    'anomaly_dates': series.index[anomalies],\n",
    "                    'anomaly_values': series[anomalies]\n",
    "                }\n",
    "                \n",
    "                print(f\"      æ£€æµ‹åˆ° {sum(anomalies)} ä¸ªå¼‚å¸¸ç‚¹ (é˜ˆå€¼: {threshold}Ïƒ)\")\n",
    "            \n",
    "            # å¯è§†åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœ\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            \n",
    "            # ç»˜åˆ¶åŸå§‹åºåˆ—\n",
    "            plt.plot(series.index, series, label='åŸå§‹æ•°æ®', alpha=0.7, color='blue')\n",
    "            \n",
    "            # æ ‡è®°å¼‚å¸¸ç‚¹\n",
    "            if method == 'isolation_forest':\n",
    "                plt.scatter(result['anomaly_dates'], result['anomaly_values'], \n",
    "                           color='red', s=100, label='å¼‚å¸¸ç‚¹', marker='o', alpha=0.8)\n",
    "            else:\n",
    "                plt.scatter(result['anomaly_dates'], result['anomaly_values'], \n",
    "                           color='red', s=100, label='å¼‚å¸¸ç‚¹', marker='o', alpha=0.8)\n",
    "            \n",
    "            plt.title(f'{column} å¼‚å¸¸æ£€æµ‹ç»“æœ ({method})', fontweight='bold')\n",
    "            plt.xlabel('æ—¥æœŸ')\n",
    "            plt.ylabel(column)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            self.anomaly_detection_results[column] = result\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def change_point_detection(self, column: str, method: str = 'cusum'):\n",
    "        \"\"\"å˜ç‚¹æ£€æµ‹\"\"\"\n",
    "        print(f\"   {column} å˜ç‚¹æ£€æµ‹ ({method}):\")\n",
    "        \n",
    "        try:\n",
    "            series = self.df[column].dropna()\n",
    "            \n",
    "            if method == 'cusum':\n",
    "                # CUSUMæ–¹æ³•\n",
    "                def cusum_detector(series, threshold=5, drift=0):\n",
    "                    mean_val = series.mean()\n",
    "                    std_val = series.std()\n",
    "                    \n",
    "                    # æ ‡å‡†åŒ–åºåˆ—\n",
    "                    standardized = (series - mean_val) / std_val\n",
    "                    \n",
    "                    # è®¡ç®—CUSUMç»Ÿè®¡é‡\n",
    "                    s_pos = np.zeros(len(series))\n",
    "                    s_neg = np.zeros(len(series))\n",
    "                    \n",
    "                    for i in range(1, len(series)):\n",
    "                        s_pos[i] = max(0, s_pos[i-1] + standardized.iloc[i] - drift)\n",
    "                        s_neg[i] = min(0, s_neg[i-1] + standardized.iloc[i] + drift)\n",
    "                    \n",
    "                    # æ£€æµ‹å˜ç‚¹\n",
    "                    change_points = []\n",
    "                    for i in range(len(series)):\n",
    "                        if s_pos[i] > threshold or s_neg[i] < -threshold:\n",
    "                            change_points.append(series.index[i])\n",
    "                            # é‡ç½®\n",
    "                            s_pos[i] = 0\n",
    "                            s_neg[i] = 0\n",
    "                    \n",
    "                    return change_points, s_pos, s_neg\n",
    "                \n",
    "                change_points, s_pos, s_neg = cusum_detector(series)\n",
    "                \n",
    "                result = {\n",
    "                    'method': method,\n",
    "                    'change_points': change_points,\n",
    "                    's_positive': s_pos,\n",
    "                    's_negative': s_neg\n",
    "                }\n",
    "                \n",
    "                print(f\"      æ£€æµ‹åˆ° {len(change_points)} ä¸ªå˜ç‚¹\")\n",
    "                \n",
    "            elif method == 'bayesian':\n",
    "                # è´å¶æ–¯å˜ç‚¹æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "                def bayesian_change_point(series, max_changes=5):\n",
    "                    # è¿™é‡Œä½¿ç”¨ç®€å•çš„åŸºäºå‡å€¼å˜åŒ–çš„æ–¹æ³•\n",
    "                    n = len(series)\n",
    "                    change_points = []\n",
    "                    \n",
    "                    # æ»‘åŠ¨çª—å£æ£€æµ‹å‡å€¼å˜åŒ–\n",
    "                    window_size = min(30, n // 10)\n",
    "                    \n",
    "                    for i in range(window_size, n - window_size):\n",
    "                        before_mean = series.iloc[i-window_size:i].mean()\n",
    "                        after_mean = series.iloc[i:i+window_size].mean()\n",
    "                        \n",
    "                        # æ£€æµ‹æ˜¾è‘—å˜åŒ–\n",
    "                        if abs(after_mean - before_mean) > series.std() * 0.5:\n",
    "                            change_points.append(series.index[i])\n",
    "                    \n",
    "                    # é™åˆ¶å˜ç‚¹æ•°é‡\n",
    "                    if len(change_points) > max_changes:\n",
    "                        # é€‰æ‹©å˜åŒ–æœ€å¤§çš„ç‚¹\n",
    "                        changes = []\n",
    "                        for cp in change_points:\n",
    "                            idx = series.index.get_loc(cp)\n",
    "                            before_mean = series.iloc[max(0, idx-10):idx].mean()\n",
    "                            after_mean = series.iloc[idx:min(n, idx+10)].mean()\n",
    "                            changes.append(abs(after_mean - before_mean))\n",
    "                        \n",
    "                        # é€‰æ‹©å˜åŒ–æœ€å¤§çš„å‰max_changesä¸ªç‚¹\n",
    "                        top_indices = np.argsort(changes)[-max_changes:]\n",
    "                        change_points = [change_points[i] for i in top_indices]\n",
    "                    \n",
    "                    return sorted(change_points)\n",
    "                \n",
    "                change_points = bayesian_change_point(series)\n",
    "                \n",
    "                result = {\n",
    "                    'method': method,\n",
    "                    'change_points': change_points\n",
    "                }\n",
    "                \n",
    "                print(f\"      æ£€æµ‹åˆ° {len(change_points)} ä¸ªå˜ç‚¹\")\n",
    "            \n",
    "            # å¯è§†åŒ–å˜ç‚¹æ£€æµ‹ç»“æœ\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "            \n",
    "            # åŸå§‹åºåˆ—å’Œå˜ç‚¹\n",
    "            ax1.plot(series.index, series, label='åŸå§‹æ•°æ®', color='blue')\n",
    "            \n",
    "            for cp in result['change_points']:\n",
    "                ax1.axvline(x=cp, color='red', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            ax1.set_title(f'{column} å˜ç‚¹æ£€æµ‹ç»“æœ ({method})', fontweight='bold')\n",
    "            ax1.set_ylabel(column)\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # CUSUMç»Ÿè®¡é‡ï¼ˆä»…é€‚ç”¨äºCUSUMæ–¹æ³•ï¼‰\n",
    "            if method == 'cusum':\n",
    "                ax2.plot(series.index, result['s_positive'], label='CUSUM+', color='green')\n",
    "                ax2.plot(series.index, result['s_negative'], label='CUSUM-', color='orange')\n",
    "                ax2.axhline(y=5, color='red', linestyle=':', label='é˜ˆå€¼')\n",
    "                ax2.axhline(y=-5, color='red', linestyle=':')\n",
    "                ax2.set_title('CUSUMç»Ÿè®¡é‡')\n",
    "                ax2.set_xlabel('æ—¥æœŸ')\n",
    "                ax2.legend()\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "            else:\n",
    "                ax2.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            self.change_point_results[column] = result\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      å˜ç‚¹æ£€æµ‹å¤±è´¥: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def cross_correlation_analysis(self, col1: str, col2: str, max_lags: int = 30):\n",
    "        \"\"\"äº¤å‰ç›¸å…³åˆ†æ\"\"\"\n",
    "        print(f\"   {col1} ä¸ {col2} äº¤å‰ç›¸å…³åˆ†æ:\")\n",
    "        \n",
    "        try:\n",
    "            series1 = self.df[col1].dropna()\n",
    "            series2 = self.df[col2].dropna()\n",
    "            \n",
    "            # å¯¹é½æ•°æ®\n",
    "            common_index = series1.index.intersection(series2.index)\n",
    "            s1 = series1.loc[common_index]\n",
    "            s2 = series2.loc[common_index]\n",
    "            \n",
    "            # è®¡ç®—äº’ç›¸å…³\n",
    "            correlations = []\n",
    "            lags = range(-max_lags, max_lags + 1)\n",
    "            \n",
    "            for lag in lags:\n",
    "                if lag == 0:\n",
    "                    corr = np.corrcoef(s1, s2)[0, 1]\n",
    "                elif lag > 0:\n",
    "                    # col1é¢†å…ˆcol2\n",
    "                    corr = np.corrcoef(s1[:-lag], s2[lag:])[0, 1]\n",
    "                else:\n",
    "                    # col2é¢†å…ˆcol1\n",
    "                    corr = np.corrcoef(s1[abs(lag):], s2[:lag])[0, 1]\n",
    "                \n",
    "                correlations.append(corr)\n",
    "            \n",
    "            # æ‰¾åˆ°æœ€å¤§ç›¸å…³æ€§çš„æ»å\n",
    "            max_corr_idx = np.argmax(np.abs(correlations))\n",
    "            optimal_lag = lags[max_corr_idx]\n",
    "            max_correlation = correlations[max_corr_idx]\n",
    "            \n",
    "            print(f\"      æœ€ä¼˜æ»å: {optimal_lag} å¤©\")\n",
    "            print(f\"      æœ€å¤§ç›¸å…³æ€§: {max_correlation:.3f}\")\n",
    "            \n",
    "            # ç»˜åˆ¶äº’ç›¸å…³å›¾\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.stem(lags, correlations, basefmt='b-')\n",
    "            plt.axvline(x=optimal_lag, color='red', linestyle='--', \n",
    "                       label=f'æœ€ä¼˜æ»å: {optimal_lag} å¤©')\n",
    "            plt.xlabel('æ»å (å¤©)')\n",
    "            plt.ylabel('äº’ç›¸å…³ç³»æ•°')\n",
    "            plt.title(f'{col1} ä¸ {col2} äº’ç›¸å…³å‡½æ•°', fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return {\n",
    "                'lags': lags,\n",
    "                'correlations': correlations,\n",
    "                'optimal_lag': optimal_lag,\n",
    "                'max_correlation': max_correlation\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      äº¤å‰ç›¸å…³åˆ†æå¤±è´¥: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# æ¼”ç¤ºé«˜çº§æ—¶é—´åºåˆ—åˆ†æ\n",
    "print(f\"\\n   ğŸ”§ é«˜çº§æ—¶é—´åºåˆ—åˆ†ææ¼”ç¤º:\")\n",
    "advanced_ts = AdvancedTimeSeriesAnalysis(ts_df)\n",
    "\n",
    "# 1. å¤šå˜é‡ç›¸å…³æ€§åˆ†æ\n",
    "print(f\"\\n   ğŸ“Š å¤šå˜é‡ç›¸å…³æ€§åˆ†æ:\")\n",
    "multivariate_cols = ['user_activity', 'conversation_count', 'api_requests', \n",
    "                      'response_time', 'cpu_usage', 'network_traffic']\n",
    "\n",
    "corr_matrix, strong_corrs = advanced_ts.correlation_analysis(multivariate_cols)\n",
    "\n",
    "# 2. VARæ¨¡å‹åˆ†æ\n",
    "print(f\"\\n   ğŸ¯ VARæ¨¡å‹åˆ†æ:\")\n",
    "var_cols = ['user_activity', 'conversation_count', 'api_requests']\n",
    "var_result = advanced_ts.var_model(var_cols, test_size=30)\n",
    "\n",
    "# 3. å¼‚å¸¸æ£€æµ‹\n",
    "print(f\"\\n   ğŸ” å¼‚å¸¸æ£€æµ‹:\")\n",
    "\n",
    "# ä½¿ç”¨Isolation Forestæ£€æµ‹ç”¨æˆ·æ´»è·ƒåº¦å¼‚å¸¸\n",
    "anomaly_result_iso = advanced_ts.anomaly_detection(\n",
    "    'user_activity', method='isolation_forest', contamination=0.05\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ£€æµ‹å“åº”æ—¶é—´å¼‚å¸¸\n",
    "anomaly_result_stat = advanced_ts.anomaly_detection(\n",
    "    'response_time', method='statistical'\n",
    ")\n",
    "\n",
    "# 4. å˜ç‚¹æ£€æµ‹\n",
    "print(f\"\\n   ğŸ“ˆ å˜ç‚¹æ£€æµ‹:\")\n",
    "\n",
    "# CUSUMæ–¹æ³•æ£€æµ‹ç”¨æˆ·æ´»è·ƒåº¦å˜ç‚¹\n",
    "change_point_cusum = advanced_ts.change_point_detection(\n",
    "    'user_activity', method='cusum'\n",
    ")\n",
    "\n",
    "# è´å¶æ–¯æ–¹æ³•æ£€æµ‹APIè¯·æ±‚å˜ç‚¹\n",
    "change_point_bayes = advanced_ts.change_point_detection(\n",
    "    'api_requests', method='bayesian'\n",
    ")\n",
    "\n",
    "# 5. äº¤å‰ç›¸å…³åˆ†æ\n",
    "print(f\"\\n   ğŸ”— äº¤å‰ç›¸å…³åˆ†æ:\")\n",
    "\n",
    "# åˆ†æç”¨æˆ·æ´»è·ƒåº¦ä¸å¯¹è¯æ•°é‡çš„é¢†å…ˆæ»åå…³ç³»\n",
    "cross_corr_result = advanced_ts.cross_correlation_analysis(\n",
    "    'user_activity', 'conversation_count', max_lags=14\n",
    ")\n",
    "\n",
    "# åˆ†æCPUä½¿ç”¨ç‡ä¸å“åº”æ—¶é—´çš„å…³ç³»\n",
    "cross_corr_cpu = advanced_ts.cross_correlation_analysis(\n",
    "    'cpu_usage', 'response_time', max_lags=7\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… é«˜çº§æ—¶é—´åºåˆ—åˆ†æå®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡å¤šå˜é‡æ—¶é—´åºåˆ—åˆ†ææ–¹æ³•\")\n",
    "print(f\"   âœ“ ç†è§£æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹æŠ€æœ¯\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨å˜ç‚¹æ£€æµ‹æ–¹æ³•\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿå¤„ç†å¤æ‚çš„æ—¶é—´åºåˆ—é—®é¢˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å­¦ä¹ æ€»ç»“\n",
    "\n",
    "### âœ… çŸ¥è¯†æ¸…å•è¾¾æˆæƒ…å†µéªŒè¯\n",
    "\n",
    "**5.7 æ—¶é—´åºåˆ—åˆ†æ [â­â­è¿›é˜¶]**\n",
    "- âœ… æŒæ¡æ—¶é—´åºåˆ—çš„åŸºæœ¬æ¦‚å¿µ\n",
    "- âœ… ç†è§£æ—¶é—´åºåˆ—åˆ†è§£æ–¹æ³•\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹\n",
    "- âœ… èƒ½å¤Ÿè¯„ä¼°æ—¶é—´åºåˆ—æ¨¡å‹æ€§èƒ½\n",
    "- âœ… æŒæ¡ARIMAæ¨¡å‹çš„åŸºæœ¬åŸç†\n",
    "- âœ… ç†è§£æŒ‡æ•°å¹³æ»‘æ–¹æ³•\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨æœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹\n",
    "- âœ… èƒ½å¤Ÿè¯„ä¼°å’Œæ¯”è¾ƒé¢„æµ‹æ€§èƒ½\n",
    "- âœ… æŒæ¡å¤šå˜é‡æ—¶é—´åºåˆ—åˆ†ææ–¹æ³•\n",
    "- âœ… ç†è§£æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹æŠ€æœ¯\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨å˜ç‚¹æ£€æµ‹æ–¹æ³•\n",
    "- âœ… èƒ½å¤Ÿå¤„ç†å¤æ‚çš„æ—¶é—´åºåˆ—é—®é¢˜\n",
    "- âœ… èƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æ—¶é—´åºåˆ—é¢„æµ‹ç³»ç»Ÿ\n",
    "\n",
    "### ğŸ¯ ä¸LangChainå­¦ä¹ çš„å…³è”\n",
    "\n",
    "**æ—¶é—´åºåˆ—åˆ†æé‡è¦æ€§**ï¼š\n",
    "- æ—¶é—´åºåˆ—åˆ†ææ”¯æŒLangChainçš„æ—¶åºæ•°æ®å¤„ç†\n",
    "- ä¸ºLangChainçš„é¢„æµ‹åº”ç”¨æä¾›æŠ€æœ¯åŸºç¡€\n",
    "- æ”¯æŒLangChainçš„æ—¶é—´åºåˆ—æ–‡æœ¬åˆ†æ\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›\n",
    "- æ—¶é—´åºåˆ—æ”¯æŒLangChainçš„å®æ—¶ç›‘æ§å’Œé¢„è­¦\n",
    "\n",
    "**å®é™…åº”ç”¨åœºæ™¯**ï¼š\n",
    "- LangChainçš„å¯¹è¯é‡è¶‹åŠ¿é¢„æµ‹å’Œå®¹é‡è§„åˆ’\n",
    "- LangChainçš„ç”¨æˆ·è¡Œä¸ºæ¨¡å¼æ—¶åºåˆ†æ\n",
    "- LangChainçš„ç³»ç»Ÿæ€§èƒ½ç›‘æ§å’Œå¼‚å¸¸æ£€æµ‹\n",
    "- LangChainçš„æ™ºèƒ½è°ƒåº¦å’Œèµ„æºä¼˜åŒ–\n",
    "- LangChainçš„çŸ¥è¯†åº“æ›´æ–°é¢‘ç‡åˆ†æ\n",
    "\n",
    "### ğŸ“š è¿›é˜¶å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹ å»ºè®®**ï¼š\n",
    "   - æ·±å…¥ç»ƒä¹ æ·±åº¦å­¦ä¹ æ—¶é—´åºåˆ—æ¨¡å‹ï¼ˆLSTMã€GRUã€Transformerï¼‰\n",
    "   - æŒæ¡æ›´å¤šé«˜çº§é¢„æµ‹ç®—æ³•ï¼ˆProphetã€NeuralProphetï¼‰\n",
    "   - å­¦ä¹ å®æ—¶æ—¶é—´åºåˆ—å¤„ç†å’Œæµå¼åˆ†æ\n",
    "\n",
    "2. **æ‰©å±•å­¦ä¹ **ï¼š\n",
    "   - å­¦ä¹ å› æœæ¨æ–­å’Œæ—¶é—´åºåˆ—å› æœåˆ†æ\n",
    "   - äº†è§£å¤šå°ºåº¦æ—¶é—´åºåˆ—åˆ†æ\n",
    "   - æ¢ç´¢æ—¶ç©ºæ•°æ®åˆ†æå’Œé¢„æµ‹\n",
    "\n",
    "3. **å®é™…åº”ç”¨**ï¼š\n",
    "   - æ„å»ºä¼ä¸šçº§æ—¶é—´åºåˆ—é¢„æµ‹å¹³å°\n",
    "   - å¼€å‘å®æ—¶å¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦ç³»ç»Ÿ\n",
    "   - å®ç°æ™ºèƒ½è¿ç»´å’Œé¢„æµ‹æ€§ç»´æŠ¤ç³»ç»Ÿ\n",
    "\n",
    "### ğŸ”§ å¸¸è§é”™è¯¯ä¸æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **æ•°æ®æ³„éœ²é—®é¢˜**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåœ¨é¢„æµ‹æ—¶ä½¿ç”¨æœªæ¥ä¿¡æ¯\n",
    "   train_data = series[:-30]\n",
    "   test_data = series[-30:]\n",
    "   # ä½¿ç”¨å…¨éƒ¨æ•°æ®çš„ç»Ÿè®¡é‡è¿›è¡Œæ ‡å‡†åŒ–\n",
    "   scaler = StandardScaler()\n",
    "   scaled_data = scaler.fit_transform(series)  # æ•°æ®æ³„éœ²\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä»…ä½¿ç”¨è®­ç»ƒæ•°æ®ç»Ÿè®¡é‡\n",
    "   train_data = series[:-30]\n",
    "   test_data = series[-30:]\n",
    "   scaler = StandardScaler()\n",
    "   train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "   test_scaled = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "   ```\n",
    "\n",
    "2. **å¹³ç¨³æ€§æ£€éªŒè¯¯ç”¨**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå¿½ç•¥éå¹³ç¨³æ€§ç›´æ¥å»ºæ¨¡\n",
    "   model = ARIMA(non_stationary_series, order=(1, 0, 1))  # åº”è¯¥å·®åˆ†\n",
    "   \n",
    "   # æ­£ç¡®ï¼šå…ˆæ£€éªŒå¹³ç¨³æ€§å†é€‰æ‹©æ¨¡å‹\n",
    "   adf_result = adfuller(series)\n",
    "   if adf_result[1] > 0.05:  # éå¹³ç¨³\n",
    "       model = ARIMA(series, order=(1, 1, 1))  # åŒ…å«å·®åˆ†\n",
    "   else:\n",
    "       model = ARIMA(series, order=(1, 0, 1))\n",
    "   ```\n",
    "\n",
    "3. **å­£èŠ‚æ€§å‚æ•°é”™è¯¯**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå­£èŠ‚æ€§å‘¨æœŸè®¾ç½®ä¸å½“\n",
    "   model = SARIMAX(daily_data, seasonal_order=(1, 1, 1, 4))  # 4å¤©å‘¨æœŸä¸åˆç†\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ ¹æ®æ•°æ®é¢‘ç‡è®¾ç½®å­£èŠ‚æ€§å‘¨æœŸ\n",
    "   if data_frequency == 'daily':\n",
    "       seasonal_period = 7  # å‘¨å­£èŠ‚æ€§\n",
    "   elif data_frequency == 'monthly':\n",
    "       seasonal_period = 12  # å¹´å­£èŠ‚æ€§\n",
    "   model = SARIMAX(data, seasonal_order=(1, 1, 1, seasonal_period))\n",
    "   ```\n",
    "\n",
    "4. **ç‰¹å¾åˆ›å»ºæ—¶é—´æ³„éœ²**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåˆ›å»ºæ»åç‰¹å¾æ—¶ä½¿ç”¨æœªæ¥ä¿¡æ¯\n",
    "   def create_features_wrong(series):\n",
    "       features = []\n",
    "       for i in range(len(series)):\n",
    "           # ä½¿ç”¨æœªæ¥æ•°æ®è®¡ç®—ç§»åŠ¨å¹³å‡\n",
    "           ma = series.iloc[i:i+7].mean()  # æ•°æ®æ³„éœ²\n",
    "           features.append(ma)\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä»…ä½¿ç”¨å†å²æ•°æ®\n",
    "   def create_features_correct(series):\n",
    "       features = []\n",
    "       for i in range(7, len(series)):\n",
    "           # ä»…ä½¿ç”¨å†å²æ•°æ®\n",
    "           ma = series.iloc[i-7:i].mean()\n",
    "           features.append(ma)\n",
    "   ```\n",
    "\n",
    "5. **æ¨¡å‹è¯„ä¼°åå·®**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
    "   model.fit(train_data)\n",
    "   train_pred = model.predict(train_data)\n",
    "   train_mae = mean_absolute_error(train_data, train_pred)  # ä¹è§‚åå·®\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåœ¨ç‹¬ç«‹çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "   model.fit(train_data)\n",
    "   test_pred = model.forecast(steps=len(test_data))\n",
    "   test_mae = mean_absolute_error(test_data, test_pred)\n",
    "   ```\n",
    "\n",
    "6. **å¿½ç•¥æ¨¡å‹å‡è®¾**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå¿½ç•¥æ¨¡å‹é€‚ç”¨æ¡ä»¶\n",
    "   model = ExponentialSmoothing(data, trend='add', seasonal='add')\n",
    "   # æ•°æ®å¯èƒ½æ²¡æœ‰æ˜æ˜¾å­£èŠ‚æ€§\n",
    "   \n",
    "   # æ­£ç¡®ï¼šå…ˆåˆ†ææ•°æ®ç‰¹å¾å†é€‰æ‹©æ¨¡å‹\n",
    "   decomposition = seasonal_decompose(data)\n",
    "   if has_strong_seasonality(decomposition):\n",
    "       model = ExponentialSmoothing(data, trend='add', seasonal='add')\n",
    "   else:\n",
    "       model = ExponentialSmoothing(data, trend='add', seasonal=None)\n",
    "   ```\n",
    "\n",
    "### ğŸŒ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "**æ—¶é—´åºåˆ—æ€§èƒ½ä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨æ»šåŠ¨çª—å£é¢„æµ‹æé«˜å®æ—¶æ€§\n",
    "- é‡‡ç”¨é›†æˆæ–¹æ³•å¢å¼ºé¢„æµ‹ç¨³å®šæ€§\n",
    "- å®ç°è‡ªåŠ¨åŒ–çš„æ¨¡å‹é€‰æ‹©å’Œè°ƒä¼˜\n",
    "- ä½¿ç”¨å¹¶è¡Œè®¡ç®—åŠ é€Ÿå¤šå˜é‡åˆ†æ\n",
    "\n",
    "**ç³»ç»Ÿéƒ¨ç½²ä¼˜åŒ–**ï¼š\n",
    "- å»ºç«‹æ ‡å‡†åŒ–çš„æ—¶é—´åºåˆ—å¤„ç†æµæ°´çº¿\n",
    "- å®ç°æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶å’ŒA/Bæµ‹è¯•\n",
    "- è®¾è®¡å®æ—¶é¢„æµ‹æœåŠ¡æ¶æ„\n",
    "- å®ç°é¢„æµ‹ç²¾åº¦ç›‘æ§å’Œæ¨¡å‹æ›´æ–°\n",
    "\n",
    "**è´¨é‡ä¿è¯ä¼˜åŒ–**ï¼š\n",
    "- å»ºç«‹å®Œæ•´çš„æ—¶é—´åºåˆ—éªŒè¯æµç¨‹\n",
    "- å®ç°é¢„æµ‹åŒºé—´å’Œä¸ç¡®å®šæ€§é‡åŒ–\n",
    "- è®¾è®¡å¼‚å¸¸æ£€æµ‹å’Œé¢„è­¦æœºåˆ¶\n",
    "- å®ç°æ¨¡å‹è§£é‡Šæ€§å’Œå› æœåˆ†æ\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆæ—¶é—´åºåˆ—åˆ†æå­¦ä¹ ï¼**\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†æ—¶é—´åºåˆ—åˆ†æçš„æ ¸å¿ƒæŠ€èƒ½ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€å¼‚å¸¸æ£€æµ‹å’Œå¤šå˜é‡åˆ†æï¼Œä¸ºLangChainæ™ºèƒ½åº”ç”¨æä¾›äº†å¼ºå¤§çš„æ—¶åºæ•°æ®å¤„ç†èƒ½åŠ›ã€‚\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ é¢„å‘Š\n",
    "\n",
    "**ç»§ç»­ç¬¬äº”èŠ‚ï¼šæ•°æ®å¤„ç†**\n",
    "- 5.8 å¤§æ•°æ®å¤„ç†æŠ€æœ¯\n",
    "- 5.9 æ•°æ®å·¥ç¨‹å®è·µ\n",
    "\n",
    "**åç»­ç« èŠ‚é¢„å‘Š**ï¼š\n",
    "- å¼‚æ­¥ç¼–ç¨‹æŠ€æœ¯\n",
    "- Webå¼€å‘æŠ€æœ¯\n",
    "- é¡¹ç›®å·¥ç¨‹å®è·µ\n",
    "\n",
    "ç»§ç»­åŠ æ²¹ï¼Œæ—¶é—´åºåˆ—åˆ†ææŠ€èƒ½æ­£åœ¨å¿«é€Ÿæå‡ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
