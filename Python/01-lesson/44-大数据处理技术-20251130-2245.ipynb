{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44-å¤§æ•°æ®å¤„ç†æŠ€æœ¯\n",
    "\n",
    "## ğŸ“š ç”¨é€”è¯´æ˜\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**ï¼š\n",
    "- æŒæ¡å¤§æ•°æ®å¤„ç†çš„åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„\n",
    "- ç†Ÿç»ƒä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶å’Œå·¥å…·\n",
    "- ç†è§£å¤§æ•°æ®å­˜å‚¨å’Œå¤„ç†æŠ€æœ¯\n",
    "- èƒ½å¤Ÿæ„å»ºå¯æ‰©å±•çš„æ•°æ®å¤„ç†æµæ°´çº¿\n",
    "\n",
    "**å‰ç½®è¦æ±‚**ï¼š\n",
    "- å·²å®Œæˆ43-æ—¶é—´åºåˆ—åˆ†æå­¦ä¹ \n",
    "- ç†Ÿç»ƒæŒæ¡Pythonæ•°æ®å¤„ç†åŸºç¡€\n",
    "- äº†è§£åŸºæœ¬çš„å¹¶è¡Œç¼–ç¨‹æ¦‚å¿µ\n",
    "\n",
    "**ä¸LangChainå…³è”**ï¼š\n",
    "- å¤§æ•°æ®å¤„ç†æ”¯æŒLangChainçš„æµ·é‡æ–‡æœ¬æ•°æ®å¤„ç†\n",
    "- ä¸ºLangChainçš„åˆ†å¸ƒå¼æ¨ç†æä¾›æŠ€æœ¯åŸºç¡€\n",
    "- æ”¯æŒLangChainçš„å¤§è§„æ¨¡çŸ¥è¯†åº“ç®¡ç†\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„é«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ çŸ¥è¯†ç‚¹è¦†ç›–\n",
    "\n",
    "### 5.8 å¤§æ•°æ®å¤„ç†æŠ€æœ¯ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šå¤§æ•°æ®å¤„ç†æŠ€æœ¯æ˜¯å¤„ç†æµ·é‡æ•°æ®çš„æ ¸å¿ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼è®¡ç®—ã€æµå¼å¤„ç†ã€æ•°æ®å­˜å‚¨ç­‰ã€‚æŒæ¡è¿™äº›æŠ€èƒ½å¯¹äºæ„å»ºä¼ä¸šçº§LangChainåº”ç”¨éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡å¤§æ•°æ®å¤„ç†çš„åŸºæœ¬æ¦‚å¿µ\n",
    "- ç†è§£åˆ†å¸ƒå¼è®¡ç®—åŸç†\n",
    "- ç†Ÿç»ƒä½¿ç”¨å¤§æ•°æ®å¤„ç†å·¥å…·\n",
    "- èƒ½å¤Ÿè®¾è®¡å¯æ‰©å±•çš„æ•°æ®å¤„ç†æ¶æ„\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„å¤§æ•°æ®å¤„ç†æµç¨‹\n",
    "- è¿›è¡Œåˆ†å¸ƒå¼è®¡ç®—çš„æ€§èƒ½ä¼˜åŒ–\n",
    "- åº”ç”¨å¤§æ•°æ®æŠ€æœ¯è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„å¤§æ•°æ®å¤„ç†ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¥ å¤§æ•°æ®å¤„ç†æŠ€æœ¯:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Any, Optional, Union, Dict, Iterator\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å¤§æ•°æ®å¤„ç†ç›¸å…³åº“\n",
    "import itertools\n",
    "import collections\n",
    "from functools import reduce, partial\n",
    "import queue\n",
    "import heapq\n",
    "import hashlib\n",
    "import pickle\n",
    "import sqlite3\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"âœ… Pythonç‰ˆæœ¬: {pd.__version__}\")\n",
    "print(f\"âœ… NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"âœ… CPUæ ¸å¿ƒæ•°: {multiprocessing.cpu_count()}\")\n",
    "\n",
    "# 1. å¤§æ•°æ®å¤„ç†åŸºç¡€æ¦‚å¿µ\n",
    "print(f\"\\nğŸ“ 1. å¤§æ•°æ®å¤„ç†åŸºç¡€æ¦‚å¿µ:\")\n",
    "\n",
    "# 1.1 åˆ›å»ºå¤§æ•°æ®é›†\n",
    "print(f\"\\n   ğŸ“Š 1.1 åˆ›å»ºå¤§æ•°æ®é›†:\")\n",
    "\n",
    "def create_big_dataset(n_records: int = 100000, n_features: int = 20):\n",
    "    \"\"\"åˆ›å»ºç”¨äºå¤§æ•°æ®å¤„ç†çš„ç¤ºä¾‹æ•°æ®é›†\"\"\"\n",
    "    print(f\"   ç”Ÿæˆå¤§æ•°æ®é›†: {n_records:,} æ¡è®°å½•, {n_features} ä¸ªç‰¹å¾\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # ç”ŸæˆåŸºç¡€æ•°æ®\n",
    "    data = {\n",
    "        'id': range(n_records),\n",
    "        'timestamp': pd.date_range('2020-01-01', periods=n_records, freq='1min'),\n",
    "        'user_id': np.random.randint(1, 10000, n_records),\n",
    "        'session_id': np.random.randint(1, 5000, n_records),\n",
    "        'request_type': np.random.choice(['chat', 'completion', 'embedding', 'classification'], n_records),\n",
    "        'model_name': np.random.choice(['gpt-3.5-turbo', 'gpt-4', 'claude-3', 'llama-2'], n_records),\n",
    "        'response_time': np.random.exponential(2.0, n_records) + np.random.normal(0, 0.5, n_records),\n",
    "        'token_count': np.random.poisson(150, n_records),\n",
    "        'cost': np.random.uniform(0.001, 0.1, n_records),\n",
    "        'success_rate': np.random.beta(8, 2, n_records),\n",
    "        'error_code': np.random.choice([None, 'timeout', 'rate_limit', 'invalid_request'], n_records, p=[0.9, 0.05, 0.03, 0.02]),\n",
    "        'latency': np.random.gamma(2, 1, n_records),\n",
    "        'cpu_usage': np.random.beta(2, 5, n_records) * 100,\n",
    "        'memory_usage': np.random.beta(3, 2, n_records) * 100,\n",
    "        'network_io': np.random.exponential(10, n_records),\n",
    "        'disk_io': np.random.exponential(5, n_records),\n",
    "        'queue_length': np.random.poisson(10, n_records),\n",
    "        'concurrent_users': np.random.randint(1, 100, n_records),\n",
    "        'cache_hit_rate': np.random.beta(5, 3, n_records),\n",
    "        'retry_count': np.random.poisson(0.5, n_records)\n",
    "    }\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼ç‰¹å¾\n",
    "    for i in range(n_features - len(data) + 1):\n",
    "        data[f'feature_{i+1}'] = np.random.normal(0, 1, n_records)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ç¡®ä¿æ•°æ®åˆç†æ€§\n",
    "    df['response_time'] = np.maximum(df['response_time'], 0.1)\n",
    "    df['token_count'] = np.maximum(df['token_count'], 1)\n",
    "    df['cost'] = np.maximum(df['cost'], 0.001)\n",
    "    df['success_rate'] = np.clip(df['success_rate'], 0, 1)\n",
    "    df['latency'] = np.maximum(df['latency'], 0.01)\n",
    "    df['cpu_usage'] = np.clip(df['cpu_usage'], 0, 100)\n",
    "    df['memory_usage'] = np.clip(df['memory_usage'], 0, 100)\n",
    "    df['network_io'] = np.maximum(df['network_io'], 0.1)\n",
    "    df['disk_io'] = np.maximum(df['disk_io'], 0.1)\n",
    "    df['queue_length'] = np.maximum(df['queue_length'], 0)\n",
    "    df['concurrent_users'] = np.maximum(df['concurrent_users'], 1)\n",
    "    df['cache_hit_rate'] = np.clip(df['cache_hit_rate'], 0, 1)\n",
    "    df['retry_count'] = np.maximum(df['retry_count'], 0)\n",
    "    \n",
    "    # æ·»åŠ ä¸€äº›ç›¸å…³æ€§\n",
    "    df['total_cost'] = df['cost'] * df['token_count'] / 100\n",
    "    df['efficiency'] = df['token_count'] / (df['response_time'] + 0.1)\n",
    "    df['load_factor'] = (df['cpu_usage'] + df['memory_usage']) / 200\n",
    "    \n",
    "    print(f\"   æ•°æ®é›†åˆ›å»ºå®Œæˆ: {df.shape[0]:,} è¡Œ Ã— {df.shape[1]} åˆ—\")\n",
    "    print(f\"   å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# åˆ›å»ºå¤§æ•°æ®é›†\n",
    "big_df = create_big_dataset(n_records=100000, n_features=20)\n",
    "print(f\"   ä¸»è¦æŒ‡æ ‡: APIè¯·æ±‚ã€æ€§èƒ½æŒ‡æ ‡ã€ç³»ç»Ÿèµ„æºä½¿ç”¨ç­‰\")\n",
    "\n",
    "# 1.2 å¤§æ•°æ®å¤„ç†åŸºç¡€æ¡†æ¶\n",
    "print(f\"\\n   ğŸ”§ 1.2 å¤§æ•°æ®å¤„ç†åŸºç¡€æ¡†æ¶:\")\n",
    "\n",
    "@dataclass\n",
    "class BigDataProcessor:\n",
    "    \"\"\"å¤§æ•°æ®å¤„ç†åŸºç¡€æ¡†æ¶\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, chunk_size: int = 10000):\n",
    "        self.df = df.copy()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.processing_results = {}\n",
    "        self.performance_metrics = {}\n",
    "    \n",
    "    def basic_info(self):\n",
    "        \"\"\"åŸºç¡€ä¿¡æ¯åˆ†æ\"\"\"\n",
    "        print(f\"   å¤§æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:\")\n",
    "        \n",
    "        info = {\n",
    "            'æ€»è¡Œæ•°': len(self.df),\n",
    "            'æ€»åˆ—æ•°': len(self.df.columns),\n",
    "            'å†…å­˜ä½¿ç”¨(MB)': self.df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'æ•°æ®ç±»å‹': self.df.dtypes.value_counts().to_dict(),\n",
    "            'ç¼ºå¤±å€¼': self.df.isnull().sum().sum(),\n",
    "            'é‡å¤è¡Œ': self.df.duplicated().sum()\n",
    "        }\n",
    "        \n",
    "        for key, value in info.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"      {key}:\")\n",
    "                for k, v in value.items():\n",
    "                    print(f\"        {k}: {v}\")\n",
    "            else:\n",
    "                print(f\"      {key}: {value:,}\")\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def chunked_processing(self, chunk_func, *args, **kwargs):\n",
    "        \"\"\"åˆ†å—å¤„ç†å¤§æ•°æ®\"\"\"\n",
    "        print(f\"   åˆ†å—å¤„ç†å¤§æ•°æ® (å—å¤§å°: {self.chunk_size:,}):\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(self.df), self.chunk_size):\n",
    "            chunk = self.df.iloc[i:i+self.chunk_size]\n",
    "            chunk_result = chunk_func(chunk, *args, **kwargs)\n",
    "            results.append(chunk_result)\n",
    "            \n",
    "            if (i // self.chunk_size + 1) % 10 == 0:\n",
    "                print(f\"      å·²å¤„ç† {min(i+self.chunk_size, len(self.df)):,} / {len(self.df):,} è¡Œ\")\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"      å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’\")\n",
    "        \n",
    "        return results, processing_time\n",
    "    \n",
    "    def parallel_processing(self, process_func, n_workers: int = None, \n",
    "                            use_processes: bool = True):\n",
    "        \"\"\"å¹¶è¡Œå¤„ç†å¤§æ•°æ®\"\"\"\n",
    "        print(f\"   å¹¶è¡Œå¤„ç†å¤§æ•°æ® (å·¥ä½œçº¿ç¨‹: {n_workers or multiprocessing.cpu_count()}):\")\n",
    "        \n",
    "        if n_workers is None:\n",
    "            n_workers = multiprocessing.cpu_count()\n",
    "        \n",
    "        # åˆ†å‰²æ•°æ®\n",
    "        chunks = [self.df.iloc[i:i+self.chunk_size] \n",
    "                 for i in range(0, len(self.df), self.chunk_size)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if use_processes:\n",
    "            # ä½¿ç”¨è¿›ç¨‹æ± \n",
    "            with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "                results = list(executor.map(process_func, chunks))\n",
    "        else:\n",
    "            # ä½¿ç”¨çº¿ç¨‹æ± \n",
    "            with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "                results = list(executor.map(process_func, chunks))\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"      å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’\")\n",
    "        \n",
    "        return results, processing_time\n",
    "    \n",
    "    def memory_optimization(self):\n",
    "        \"\"\"å†…å­˜ä¼˜åŒ–\"\"\"\n",
    "        print(f\"   å†…å­˜ä¼˜åŒ–:\")\n",
    "        \n",
    "        original_memory = self.df.memory_usage(deep=True).sum() / 1024**2\n",
    "        print(f\"      ä¼˜åŒ–å‰å†…å­˜ä½¿ç”¨: {original_memory:.2f} MB\")\n",
    "        \n",
    "        # ä¼˜åŒ–æ•°å€¼ç±»å‹\n",
    "        for col in self.df.select_dtypes(include=['int64']).columns:\n",
    "            col_min = self.df[col].min()\n",
    "            col_max = self.df[col].max()\n",
    "            \n",
    "            if col_min >= 0:\n",
    "                if col_max < 255:\n",
    "                    self.df[col] = self.df[col].astype('uint8')\n",
    "                elif col_max < 65535:\n",
    "                    self.df[col] = self.df[col].astype('uint16')\n",
    "                elif col_max < 4294967295:\n",
    "                    self.df[col] = self.df[col].astype('uint32')\n",
    "            else:\n",
    "                if col_min > -128 and col_max < 127:\n",
    "                    self.df[col] = self.df[col].astype('int8')\n",
    "                elif col_min > -32768 and col_max < 32767:\n",
    "                    self.df[col] = self.df[col].astype('int16')\n",
    "                elif col_min > -2147483648 and col_max < 2147483647:\n",
    "                    self.df[col] = self.df[col].astype('int32')\n",
    "        \n",
    "        # ä¼˜åŒ–æµ®ç‚¹ç±»å‹\n",
    "        for col in self.df.select_dtypes(include=['float64']).columns:\n",
    "            self.df[col] = pd.to_numeric(self.df[col], downcast='float')\n",
    "        \n",
    "        # ä¼˜åŒ–å­—ç¬¦ä¸²ç±»å‹\n",
    "        for col in self.df.select_dtypes(include=['object']).columns:\n",
    "            if self.df[col].nunique() / len(self.df) < 0.5:  # å¦‚æœé‡å¤ç‡è¾ƒé«˜\n",
    "                self.df[col] = self.df[col].astype('category')\n",
    "        \n",
    "        optimized_memory = self.df.memory_usage(deep=True).sum() / 1024**2\n",
    "        memory_reduction = (original_memory - optimized_memory) / original_memory * 100\n",
    "        \n",
    "        print(f\"      ä¼˜åŒ–åå†…å­˜ä½¿ç”¨: {optimized_memory:.2f} MB\")\n",
    "        print(f\"      å†…å­˜å‡å°‘: {memory_reduction:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            'original_memory': original_memory,\n",
    "            'optimized_memory': optimized_memory,\n",
    "            'memory_reduction': memory_reduction\n",
    "        }\n",
    "    \n",
    "    def data_sampling(self, method: str = 'random', sample_size: float = 0.1):\n",
    "        \"\"\"æ•°æ®é‡‡æ ·\"\"\"\n",
    "        print(f\"   æ•°æ®é‡‡æ · ({method}, æ¯”ä¾‹: {sample_size}):\")\n",
    "        \n",
    "        n_samples = int(len(self.df) * sample_size)\n",
    "        \n",
    "        if method == 'random':\n",
    "            sample = self.df.sample(n=n_samples, random_state=42)\n",
    "        elif method == 'systematic':\n",
    "            step = len(self.df) // n_samples\n",
    "            sample = self.df.iloc[::step][:n_samples]\n",
    "        elif method == 'stratified':\n",
    "            # æŒ‰request_typeåˆ†å±‚é‡‡æ ·\n",
    "            sample = self.df.groupby('request_type').apply(\n",
    "                lambda x: x.sample(n=min(len(x), n_samples // len(self.df['request_type'].unique())), random_state=42)\n",
    "            ).reset_index(drop=True)\n",
    "        else:\n",
    "            raise ValueError(\"ä¸æ”¯æŒçš„é‡‡æ ·æ–¹æ³•\")\n",
    "        \n",
    "        print(f\"      é‡‡æ ·å®Œæˆ: {len(sample):,} æ¡è®°å½•\")\n",
    "        print(f\"      é‡‡æ ·æ¯”ä¾‹: {len(sample)/len(self.df):.3f}\")\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# åˆå§‹åŒ–å¤§æ•°æ®å¤„ç†æ¡†æ¶\n",
    "big_processor = BigDataProcessor(big_df, chunk_size=10000)\n",
    "print(f\"   âœ… å¤§æ•°æ®å¤„ç†æ¡†æ¶åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "# åŸºç¡€ä¿¡æ¯åˆ†æ\n",
    "basic_info = big_processor.basic_info()\n",
    "\n",
    "print(f\"\\nâœ… å¤§æ•°æ®å¤„ç†åŸºç¡€æ¦‚å¿µå®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡å¤§æ•°æ®å¤„ç†çš„åŸºæœ¬æ¦‚å¿µ\")\n",
    "print(f\"   âœ“ ç†è§£å¤§æ•°æ®å¤„ç†çš„æŒ‘æˆ˜\")\n",
    "print(f\"   âœ“ ç†Ÿæ‚‰å¤§æ•°æ®å¤„ç†æ¡†æ¶\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿåˆ›å»ºå’Œå¤„ç†å¤§æ•°æ®é›†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†å¸ƒå¼è®¡ç®—æŠ€æœ¯ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šåˆ†å¸ƒå¼è®¡ç®—æ˜¯å¤§æ•°æ®å¤„ç†çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬MapReduceã€å¹¶è¡Œå¤„ç†ã€ä»»åŠ¡è°ƒåº¦ç­‰ã€‚æŒæ¡è¿™äº›ç®—æ³•å¯¹äºæ„å»ºé«˜æ€§èƒ½LangChainåº”ç”¨éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡MapReduceçš„åŸºæœ¬åŸç†\n",
    "- ç†è§£å¹¶è¡Œè®¡ç®—æ¨¡å¼\n",
    "- ç†Ÿç»ƒä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—å·¥å…·\n",
    "- èƒ½å¤Ÿä¼˜åŒ–åˆ†å¸ƒå¼è®¡ç®—æ€§èƒ½\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„åˆ†å¸ƒå¼è®¡ç®—æµç¨‹\n",
    "- è¿›è¡Œå¹¶è¡Œè®¡ç®—çš„æ€§èƒ½ä¼˜åŒ–\n",
    "- åº”ç”¨åˆ†å¸ƒå¼è®¡ç®—è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„åˆ†å¸ƒå¼è®¡ç®—ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸš€ åˆ†å¸ƒå¼è®¡ç®—æŠ€æœ¯:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. MapReduceå®ç°\n",
    "print(f\"ğŸ“ 1. MapReduceå®ç°:\")\n",
    "\n",
    "@dataclass\n",
    "class MapReduceFramework:\n",
    "    \"\"\"MapReduceè®¡ç®—æ¡†æ¶å®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Any]):\n",
    "        self.data = data\n",
    "        self.map_results = []\n",
    "        self.reduce_results = {}\n",
    "    \n",
    "    def map_function(self, mapper_func):\n",
    "        \"\"\"Mapé˜¶æ®µ\"\"\"\n",
    "        print(f\"   Mapé˜¶æ®µå¤„ç† {len(self.data):,} æ¡æ•°æ®...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.map_results = []\n",
    "        \n",
    "        for item in self.data:\n",
    "            mapped_item = mapper_func(item)\n",
    "            self.map_results.extend(mapped_item)\n",
    "        \n",
    "        map_time = time.time() - start_time\n",
    "        print(f\"      Mapå®Œæˆï¼Œç”Ÿæˆ {len(self.map_results):,} ä¸ªé”®å€¼å¯¹ï¼Œè€—æ—¶: {map_time:.3f} ç§’\")\n",
    "        \n",
    "        return self.map_results, map_time\n",
    "    \n",
    "    def shuffle_sort(self):\n",
    "        \"\"\"Shuffleå’ŒSorté˜¶æ®µ\"\"\"\n",
    "        print(f\"   Shuffleå’ŒSorté˜¶æ®µ...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # æŒ‰é”®åˆ†ç»„\n",
    "        grouped_data = {}\n",
    "        for key, value in self.map_results:\n",
    "            if key not in grouped_data:\n",
    "                grouped_data[key] = []\n",
    "            grouped_data[key].append(value)\n",
    "        \n",
    "        shuffle_time = time.time() - start_time\n",
    "        print(f\"      Shuffleå®Œæˆï¼Œåˆ†ç»„ä¸º {len(grouped_data):,} ä¸ªé”®ï¼Œè€—æ—¶: {shuffle_time:.3f} ç§’\")\n",
    "        \n",
    "        return grouped_data, shuffle_time\n",
    "    \n",
    "    def reduce_function(self, reducer_func, grouped_data):\n",
    "        \"\"\"Reduceé˜¶æ®µ\"\"\"\n",
    "        print(f\"   Reduceé˜¶æ®µå¤„ç† {len(grouped_data):,} ä¸ªåˆ†ç»„...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.reduce_results = {}\n",
    "        \n",
    "        for key, values in grouped_data.items():\n",
    "            reduced_value = reducer_func(key, values)\n",
    "            self.reduce_results[key] = reduced_value\n",
    "        \n",
    "        reduce_time = time.time() - start_time\n",
    "        print(f\"      Reduceå®Œæˆï¼Œç”Ÿæˆ {len(self.reduce_results):,} ä¸ªç»“æœï¼Œè€—æ—¶: {reduce_time:.3f} ç§’\")\n",
    "        \n",
    "        return self.reduce_results, reduce_time\n",
    "    \n",
    "    def execute(self, mapper_func, reducer_func):\n",
    "        \"\"\"æ‰§è¡Œå®Œæ•´çš„MapReduceæµç¨‹\"\"\"\n",
    "        total_start = time.time()\n",
    "        \n",
    "        # Mapé˜¶æ®µ\n",
    "        map_results, map_time = self.map_function(mapper_func)\n",
    "        \n",
    "        # Shuffleé˜¶æ®µ\n",
    "        grouped_data, shuffle_time = self.shuffle_sort()\n",
    "        \n",
    "        # Reduceé˜¶æ®µ\n",
    "        reduce_results, reduce_time = self.reduce_function(reducer_func, grouped_data)\n",
    "        \n",
    "        total_time = time.time() - total_start\n",
    "        \n",
    "        print(f\"\\n   ğŸ“Š MapReduceæ‰§è¡Œç»Ÿè®¡:\")\n",
    "        print(f\"      æ€»è€—æ—¶: {total_time:.3f} ç§’\")\n",
    "        print(f\"      Mapé˜¶æ®µ: {map_time:.3f} ç§’ ({map_time/total_time*100:.1f}%)\")\n",
    "        print(f\"      Shuffleé˜¶æ®µ: {shuffle_time:.3f} ç§’ ({shuffle_time/total_time*100:.1f}%)\")\n",
    "        print(f\"      Reduceé˜¶æ®µ: {reduce_time:.3f} ç§’ ({reduce_time/total_time*100:.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'results': reduce_results,\n",
    "            'timing': {\n",
    "                'total': total_time,\n",
    "                'map': map_time,\n",
    "                'shuffle': shuffle_time,\n",
    "                'reduce': reduce_time\n",
    "            }\n",
    "        }\n",
    "\n",
    "# 2. å¹¶è¡Œè®¡ç®—æ¡†æ¶\n",
    "print(f\"\\n   ğŸ”§ 2. å¹¶è¡Œè®¡ç®—æ¡†æ¶:\")\n",
    "\n",
    "@dataclass\n",
    "class ParallelComputing:\n",
    "    \"\"\"å¹¶è¡Œè®¡ç®—æ¡†æ¶å®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Any]):\n",
    "        self.data = data\n",
    "        self.n_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    def parallel_map(self, func, chunk_size: int = None):\n",
    "        \"\"\"å¹¶è¡ŒMapæ“ä½œ\"\"\"\n",
    "        print(f\"   å¹¶è¡ŒMapæ“ä½œ (å·¥ä½œè¿›ç¨‹: {self.n_workers}):\")\n",
    "        \n",
    "        if chunk_size is None:\n",
    "            chunk_size = max(1, len(self.data) // self.n_workers)\n",
    "        \n",
    "        # åˆ†å‰²æ•°æ®\n",
    "        chunks = [self.data[i:i+chunk_size] \n",
    "                 for i in range(0, len(self.data), chunk_size)]\n",
    "        \n",
    "        print(f\"      æ•°æ®åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—ï¼Œæ¯å—çº¦ {chunk_size:,} æ¡æ•°æ®\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:\n",
    "            chunk_results = list(executor.map(func, chunks))\n",
    "        \n",
    "        # åˆå¹¶ç»“æœ\n",
    "        results = []\n",
    "        for chunk_result in chunk_results:\n",
    "            results.extend(chunk_result)\n",
    "        \n",
    "        parallel_time = time.time() - start_time\n",
    "        print(f\"      å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶: {parallel_time:.3f} ç§’\")\n",
    "        \n",
    "        return results, parallel_time\n",
    "    \n",
    "    def sequential_map(self, func):\n",
    "        \"\"\"ä¸²è¡ŒMapæ“ä½œï¼ˆç”¨äºæ€§èƒ½å¯¹æ¯”ï¼‰\"\"\"\n",
    "        print(f\"   ä¸²è¡ŒMapæ“ä½œ:\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = func(self.data)\n",
    "        sequential_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"      ä¸²è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶: {sequential_time:.3f} ç§’\")\n",
    "        \n",
    "        return results, sequential_time\n",
    "    \n",
    "    def performance_comparison(self, func, chunk_size: int = None):\n",
    "        \"\"\"æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "        print(f\"\\n   ğŸ“Š å¹¶è¡Œvsä¸²è¡Œæ€§èƒ½å¯¹æ¯”:\")\n",
    "        \n",
    "        # ä¸²è¡Œå¤„ç†\n",
    "        sequential_results, sequential_time = self.sequential_map(func)\n",
    "        \n",
    "        # å¹¶è¡Œå¤„ç†\n",
    "        parallel_results, parallel_time = self.parallel_map(func, chunk_size)\n",
    "        \n",
    "        # éªŒè¯ç»“æœä¸€è‡´æ€§\n",
    "        results_match = len(sequential_results) == len(parallel_results)\n",
    "        \n",
    "        # è®¡ç®—åŠ é€Ÿæ¯”\n",
    "        speedup = sequential_time / parallel_time if parallel_time > 0 else float('inf')\n",
    "        efficiency = speedup / self.n_workers * 100\n",
    "        \n",
    "        print(f\"      æ•°æ®é‡: {len(self.data):,}\")\n",
    "        print(f\"      å·¥ä½œè¿›ç¨‹: {self.n_workers}\")\n",
    "        print(f\"      ä¸²è¡Œæ—¶é—´: {sequential_time:.3f} ç§’\")\n",
    "        print(f\"      å¹¶è¡Œæ—¶é—´: {parallel_time:.3f} ç§’\")\n",
    "        print(f\"      åŠ é€Ÿæ¯”: {speedup:.2f}x\")\n",
    "        print(f\"      å¹¶è¡Œæ•ˆç‡: {efficiency:.1f}%\")\n",
    "        print(f\"      ç»“æœä¸€è‡´æ€§: {'âœ“' if results_match else 'âœ—'}\")\n",
    "        \n",
    "        return {\n",
    "            'sequential_time': sequential_time,\n",
    "            'parallel_time': parallel_time,\n",
    "            'speedup': speedup,\n",
    "            'efficiency': efficiency,\n",
    "            'results_match': results_match\n",
    "        }\n",
    "\n",
    "# 3. æµå¼å¤„ç†æ¡†æ¶\n",
    "print(f\"\\n   ğŸŒŠ 3. æµå¼å¤„ç†æ¡†æ¶:\")\n",
    "\n",
    "@dataclass\n",
    "class StreamProcessor:\n",
    "    \"\"\"æµå¼å¤„ç†æ¡†æ¶å®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_size: int = 1000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = queue.Queue(maxsize=buffer_size)\n",
    "        self.processed_count = 0\n",
    "        self.error_count = 0\n",
    "    \n",
    "    def data_generator(self, data_source, delay: float = 0.001):\n",
    "        \"\"\"æ•°æ®ç”Ÿæˆå™¨\"\"\"\n",
    "        for item in data_source:\n",
    "            time.sleep(delay)  # æ¨¡æ‹Ÿæ•°æ®åˆ°è¾¾å»¶è¿Ÿ\n",
    "            yield item\n",
    "    \n",
    "    def stream_filter(self, predicate):\n",
    "        \"\"\"æµå¼è¿‡æ»¤\"\"\"\n",
    "        def filter_generator(data_gen):\n",
    "            for item in data_gen:\n",
    "                if predicate(item):\n",
    "                    yield item\n",
    "        return filter_generator\n",
    "    \n",
    "    def stream_map(self, transform_func):\n",
    "        \"\"\"æµå¼æ˜ å°„\"\"\"\n",
    "        def map_generator(data_gen):\n",
    "            for item in data_gen:\n",
    "                try:\n",
    "                    transformed = transform_func(item)\n",
    "                    yield transformed\n",
    "                    self.processed_count += 1\n",
    "                except Exception as e:\n",
    "                    self.error_count += 1\n",
    "                    print(f\"      å¤„ç†é”™è¯¯: {e}\")\n",
    "        return map_generator\n",
    "    \n",
    "    def stream_reduce(self, reduce_func, window_size: int = 100):\n",
    "        \"\"\"æµå¼èšåˆï¼ˆæ»‘åŠ¨çª—å£ï¼‰\"\"\"\n",
    "        def reduce_generator(data_gen):\n",
    "            window = []\n",
    "            for item in data_gen:\n",
    "                window.append(item)\n",
    "                if len(window) >= window_size:\n",
    "                    result = reduce_func(window)\n",
    "                    yield result\n",
    "                    window = window[1:]  # æ»‘åŠ¨çª—å£\n",
    "            \n",
    "            # å¤„ç†å‰©ä½™æ•°æ®\n",
    "            if window:\n",
    "                yield reduce_func(window)\n",
    "        \n",
    "        return reduce_generator\n",
    "    \n",
    "    def process_stream(self, data_source, processors):\n",
    "        \"\"\"å¤„ç†æ•°æ®æµ\"\"\"\n",
    "        print(f\"   å¼€å§‹æµå¼å¤„ç† (ç¼“å†²åŒº: {self.buffer_size}):\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨\n",
    "        data_gen = self.data_generator(data_source)\n",
    "        \n",
    "        # åº”ç”¨å¤„ç†å™¨é“¾\n",
    "        for processor in processors:\n",
    "            data_gen = processor(data_gen)\n",
    "        \n",
    "        # æ”¶é›†ç»“æœ\n",
    "        results = []\n",
    "        for result in data_gen:\n",
    "            results.append(result)\n",
    "            \n",
    "            if len(results) % 100 == 0:\n",
    "                print(f\"      å·²å¤„ç† {len(results):,} ä¸ªç»“æœ\")\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"      æµå¼å¤„ç†å®Œæˆ:\")\n",
    "        print(f\"        å¤„ç†æ—¶é—´: {processing_time:.3f} ç§’\")\n",
    "        print(f\"        æˆåŠŸå¤„ç†: {self.processed_count:,}\")\n",
    "        print(f\"        é”™è¯¯æ•°é‡: {self.error_count:,}\")\n",
    "        print(f\"        è¾“å‡ºç»“æœ: {len(results):,}\")\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'processing_time': processing_time,\n",
    "            'processed_count': self.processed_count,\n",
    "            'error_count': self.error_count\n",
    "        }\n",
    "\n",
    "# æ¼”ç¤ºåˆ†å¸ƒå¼è®¡ç®—æŠ€æœ¯\n",
    "print(f\"\\n   ğŸ”§ åˆ†å¸ƒå¼è®¡ç®—æŠ€æœ¯æ¼”ç¤º:\")\n",
    "\n",
    "# 1. å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "print(f\"\\n   ğŸ“Š å‡†å¤‡æµ‹è¯•æ•°æ®:\")\n",
    "test_data = [\n",
    "    {'text': f'langchain is great for ai development {i}', \n",
    "     'category': np.random.choice(['tech', 'business', 'science']), \n",
    "     'score': np.random.uniform(0, 1)}\n",
    "    for i in range(10000)\n",
    "]\n",
    "print(f\"   åˆ›å»ºäº† {len(test_data):,} æ¡æµ‹è¯•æ•°æ®\")\n",
    "\n",
    "# 2. MapReduceæ¼”ç¤º\n",
    "print(f\"\\n   ğŸ¯ MapReduceæ¼”ç¤º:\")\n",
    "\n",
    "# å®šä¹‰mapperå‡½æ•°ï¼šè¯é¢‘ç»Ÿè®¡\n",
    "def word_count_mapper(document):\n",
    "    words = document['text'].lower().split()\n",
    "    return [(word, 1) for word in words]\n",
    "\n",
    "# å®šä¹‰reducerå‡½æ•°ï¼šè¯é¢‘æ±‡æ€»\n",
    "def word_count_reducer(word, counts):\n",
    "    return (word, sum(counts))\n",
    "\n",
    "# æ‰§è¡ŒMapReduce\n",
    "mr_framework = MapReduceFramework(test_data)\n",
    "mr_results = mr_framework.execute(word_count_mapper, word_count_reducer)\n",
    "\n",
    "# æ˜¾ç¤ºå‰10ä¸ªæœ€å¸¸è§è¯\n",
    "top_words = sorted(mr_results['results'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(f\"\\n   ğŸ“ˆ è¯é¢‘ç»Ÿè®¡Top10:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"      {word}: {count}\")\n",
    "\n",
    "# 3. å¹¶è¡Œè®¡ç®—æ¼”ç¤º\n",
    "print(f\"\\n   ğŸš€ å¹¶è¡Œè®¡ç®—æ¼”ç¤º:\")\n",
    "\n",
    "# å®šä¹‰å¤„ç†å‡½æ•°\n",
    "def process_documents_chunk(documents):\n",
    "    results = []\n",
    "    for doc in documents:\n",
    "        # æ¨¡æ‹Ÿå¤æ‚å¤„ç†\n",
    "        processed = {\n",
    "            'id': hash(doc['text']) % 1000000,\n",
    "            'length': len(doc['text']),\n",
    "            'category': doc['category'],\n",
    "            'score_normalized': doc['score'] / 1.0,\n",
    "            'word_count': len(doc['text'].split())\n",
    "        }\n",
    "        results.append(processed)\n",
    "    return results\n",
    "\n",
    "# æ€§èƒ½å¯¹æ¯”\n",
    "parallel_comp = ParallelComputing(test_data)\n",
    "performance_results = parallel_comp.performance_comparison(process_documents_chunk)\n",
    "\n",
    "# 4. æµå¼å¤„ç†æ¼”ç¤º\n",
    "print(f\"\\n   ğŸŒŠ æµå¼å¤„ç†æ¼”ç¤º:\")\n",
    "\n",
    "# å®šä¹‰å¤„ç†å‡½æ•°\n",
    "def extract_features(item):\n",
    "    return {\n",
    "        'category': item['category'],\n",
    "        'score_bucket': int(item['score'] * 5),\n",
    "        'text_length': len(item['text'])\n",
    "    }\n",
    "\n",
    "def aggregate_features(items):\n",
    "    categories = [item['category'] for item in items]\n",
    "    scores = [item['score_bucket'] for item in items]\n",
    "    lengths = [item['text_length'] for item in items]\n",
    "    \n",
    "    return {\n",
    "        'count': len(items),\n",
    "        'avg_score': np.mean(scores),\n",
    "        'avg_length': np.mean(lengths),\n",
    "        'category_distribution': pd.Series(categories).value_counts().to_dict()\n",
    "    }\n",
    "\n",
    "# æµå¼å¤„ç†\n",
    "stream_processor = StreamProcessor(buffer_size=1000)\n",
    "processors = [\n",
    "    stream_processor.stream_filter(lambda x: x['score'] > 0.5),  # è¿‡æ»¤é«˜åˆ†æ•°æ®\n",
    "    stream_processor.stream_map(extract_features),  # ç‰¹å¾æå–\n",
    "    stream_processor.stream_reduce(aggregate_features, window_size=50)  # èšåˆ\n",
    "]\n",
    "\n",
    "stream_results = stream_processor.process_stream(test_data[:1000], processors)\n",
    "\n",
    "print(f\"\\nâœ… åˆ†å¸ƒå¼è®¡ç®—æŠ€æœ¯å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡MapReduceçš„åŸºæœ¬åŸç†\")\n",
    "print(f\"   âœ“ ç†è§£å¹¶è¡Œè®¡ç®—æ¨¡å¼\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—å·¥å…·\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿä¼˜åŒ–åˆ†å¸ƒå¼è®¡ç®—æ€§èƒ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¤§æ•°æ®å­˜å‚¨ä¸æ£€ç´¢ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šå¤§æ•°æ®å­˜å‚¨ä¸æ£€ç´¢æ˜¯å¤§æ•°æ®å¤„ç†çš„åŸºç¡€ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼å­˜å‚¨ã€ç´¢å¼•æŠ€æœ¯ã€ç¼“å­˜ç­–ç•¥ç­‰ã€‚æŒæ¡è¿™äº›æŠ€æœ¯å¯¹äºæ„å»ºé«˜æ€§èƒ½LangChainåº”ç”¨éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡åˆ†å¸ƒå¼å­˜å‚¨çš„åŸºæœ¬åŸç†\n",
    "- ç†è§£æ•°æ®ç´¢å¼•å’Œæ£€ç´¢æŠ€æœ¯\n",
    "- ç†Ÿç»ƒä½¿ç”¨ç¼“å­˜å’Œä¼˜åŒ–ç­–ç•¥\n",
    "- èƒ½å¤Ÿè®¾è®¡é«˜æ•ˆçš„æ•°æ®å­˜å‚¨æ–¹æ¡ˆ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ•°æ®å­˜å‚¨å’Œæ£€ç´¢æµç¨‹\n",
    "- è¿›è¡Œæ•°æ®è®¿é—®çš„æ€§èƒ½ä¼˜åŒ–\n",
    "- åº”ç”¨å­˜å‚¨æŠ€æœ¯è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„æ•°æ®å­˜å‚¨ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ’¾ å¤§æ•°æ®å­˜å‚¨ä¸æ£€ç´¢:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. åˆ†å¸ƒå¼å­˜å‚¨æ¨¡æ‹Ÿ\n",
    "print(f\"ğŸ“ 1. åˆ†å¸ƒå¼å­˜å‚¨æ¨¡æ‹Ÿ:\")\n",
    "\n",
    "@dataclass\n",
    "class DistributedStorage:\n",
    "    \"\"\"åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿæ¨¡æ‹Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, n_nodes: int = 3, replication_factor: int = 2):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.replication_factor = replication_factor\n",
    "        self.nodes = [{} for _ in range(n_nodes)]\n",
    "        self.key_locations = {}\n",
    "        self.access_stats = {i: {'reads': 0, 'writes': 0} for i in range(n_nodes)}\n",
    "    \n",
    "    def hash_key(self, key: str) -> int:\n",
    "        \"\"\"ä¸€è‡´æ€§å“ˆå¸Œ\"\"\"\n",
    "        return int(hashlib.md5(key.encode()).hexdigest(), 16) % self.n_nodes\n",
    "    \n",
    "    def get_replica_nodes(self, primary_node: int) -> List[int]:\n",
    "        \"\"\"è·å–å‰¯æœ¬èŠ‚ç‚¹\"\"\"\n",
    "        replicas = [primary_node]\n",
    "        for i in range(1, self.replication_factor):\n",
    "            replica_node = (primary_node + i) % self.n_nodes\n",
    "            replicas.append(replica_node)\n",
    "        return replicas\n",
    "    \n",
    "    def put(self, key: str, value: Any):\n",
    "        \"\"\"å­˜å‚¨æ•°æ®\"\"\"\n",
    "        primary_node = self.hash_key(key)\n",
    "        replica_nodes = self.get_replica_nodes(primary_node)\n",
    "        \n",
    "        # å­˜å‚¨åˆ°æ‰€æœ‰å‰¯æœ¬èŠ‚ç‚¹\n",
    "        for node_id in replica_nodes:\n",
    "            self.nodes[node_id][key] = value\n",
    "            self.access_stats[node_id]['writes'] += 1\n",
    "        \n",
    "        self.key_locations[key] = replica_nodes\n",
    "        \n",
    "        return replica_nodes\n",
    "    \n",
    "    def get(self, key: str) -> Any:\n",
    "        \"\"\"è·å–æ•°æ®\"\"\"\n",
    "        if key not in self.key_locations:\n",
    "            raise KeyError(f\"Key {key} not found\")\n",
    "        \n",
    "        # ä»ä¸»èŠ‚ç‚¹è¯»å–\n",
    "        primary_node = self.key_locations[key][0]\n",
    "        self.access_stats[primary_node]['reads'] += 1\n",
    "        \n",
    "        return self.nodes[primary_node].get(key)\n",
    "    \n",
    "    def delete(self, key: str):\n",
    "        \"\"\"åˆ é™¤æ•°æ®\"\"\"\n",
    "        if key not in self.key_locations:\n",
    "            return False\n",
    "        \n",
    "        # ä»æ‰€æœ‰å‰¯æœ¬èŠ‚ç‚¹åˆ é™¤\n",
    "        for node_id in self.key_locations[key]:\n",
    "            if key in self.nodes[node_id]:\n",
    "                del self.nodes[node_id][key]\n",
    "        \n",
    "        del self.key_locations[key]\n",
    "        return True\n",
    "    \n",
    "    def get_storage_stats(self):\n",
    "        \"\"\"è·å–å­˜å‚¨ç»Ÿè®¡\"\"\"\n",
    "        stats = {\n",
    "            'total_keys': len(self.key_locations),\n",
    "            'node_stats': []\n",
    "        }\n",
    "        \n",
    "        for i in range(self.n_nodes):\n",
    "            node_stats = {\n",
    "                'node_id': i,\n",
    "                'key_count': len(self.nodes[i]),\n",
    "                'reads': self.access_stats[i]['reads'],\n",
    "                'writes': self.access_stats[i]['writes'],\n",
    "                'memory_usage': len(str(self.nodes[i]))\n",
    "            }\n",
    "            stats['node_stats'].append(node_stats)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def rebalance(self):\n",
    "        \"\"\"æ•°æ®é‡å¹³è¡¡\"\"\"\n",
    "        print(f\"   æ‰§è¡Œæ•°æ®é‡å¹³è¡¡...\")\n",
    "        \n",
    "        # æ”¶é›†æ‰€æœ‰æ•°æ®\n",
    "        all_data = {}\n",
    "        for node in self.nodes:\n",
    "            all_data.update(node)\n",
    "        \n",
    "        # æ¸…ç©ºæ‰€æœ‰èŠ‚ç‚¹\n",
    "        self.nodes = [{} for _ in range(self.n_nodes)]\n",
    "        self.key_locations = {}\n",
    "        \n",
    "        # é‡æ–°åˆ†å¸ƒæ•°æ®\n",
    "        for key, value in all_data.items():\n",
    "            self.put(key, value)\n",
    "        \n",
    "        print(f\"      é‡å¹³è¡¡å®Œæˆï¼Œé‡æ–°åˆ†å¸ƒ {len(all_data)} ä¸ªé”®\")\n",
    "\n",
    "# 2. ç´¢å¼•å’Œæ£€ç´¢ç³»ç»Ÿ\n",
    "print(f\"\\n   ğŸ” 2. ç´¢å¼•å’Œæ£€ç´¢ç³»ç»Ÿ:\")\n",
    "\n",
    "@dataclass\n",
    "class IndexSystem:\n",
    "    \"\"\"ç´¢å¼•å’Œæ£€ç´¢ç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = {}\n",
    "        self.inverted_index = {}  # å€’æ’ç´¢å¼•\n",
    "        self.field_index = {}     # å­—æ®µç´¢å¼•\n",
    "        self.search_stats = {'searches': 0, 'results': 0}\n",
    "    \n",
    "    def add_document(self, doc_id: str, document: Dict[str, Any]):\n",
    "        \"\"\"æ·»åŠ æ–‡æ¡£\"\"\"\n",
    "        self.documents[doc_id] = document\n",
    "        \n",
    "        # æ„å»ºæ–‡æœ¬å€’æ’ç´¢å¼•\n",
    "        if 'text' in document:\n",
    "            words = document['text'].lower().split()\n",
    "            for word in set(words):  # å»é‡\n",
    "                if word not in self.inverted_index:\n",
    "                    self.inverted_index[word] = []\n",
    "                self.inverted_index[word].append(doc_id)\n",
    "        \n",
    "        # æ„å»ºå­—æ®µç´¢å¼•\n",
    "        for field, value in document.items():\n",
    "            if field != 'text':  # æ–‡æœ¬å­—æ®µå·²å•ç‹¬å¤„ç†\n",
    "                if field not in self.field_index:\n",
    "                    self.field_index[field] = {}\n",
    "                \n",
    "                field_value = str(value).lower()\n",
    "                if field_value not in self.field_index[field]:\n",
    "                    self.field_index[field][field_value] = []\n",
    "                self.field_index[field][field_value].append(doc_id)\n",
    "    \n",
    "    def search_text(self, query: str, operator: str = 'OR') -> List[str]:\n",
    "        \"\"\"æ–‡æœ¬æœç´¢\"\"\"\n",
    "        self.search_stats['searches'] += 1\n",
    "        \n",
    "        query_words = query.lower().split()\n",
    "        result_sets = []\n",
    "        \n",
    "        for word in query_words:\n",
    "            if word in self.inverted_index:\n",
    "                result_sets.append(set(self.inverted_index[word]))\n",
    "        \n",
    "        if not result_sets:\n",
    "            return []\n",
    "        \n",
    "        # åˆå¹¶ç»“æœ\n",
    "        if operator == 'OR':\n",
    "            results = set.union(*result_sets)\n",
    "        elif operator == 'AND':\n",
    "            results = set.intersection(*result_sets)\n",
    "        else:\n",
    "            raise ValueError(\"ä¸æ”¯æŒçš„æ“ä½œç¬¦\")\n",
    "        \n",
    "        self.search_stats['results'] += len(results)\n",
    "        return list(results)\n",
    "    \n",
    "    def search_field(self, field: str, value: str) -> List[str]:\n",
    "        \"\"\"å­—æ®µæœç´¢\"\"\"\n",
    "        self.search_stats['searches'] += 1\n",
    "        \n",
    "        if field not in self.field_index:\n",
    "            return []\n",
    "        \n",
    "        search_value = str(value).lower()\n",
    "        results = self.field_index[field].get(search_value, [])\n",
    "        \n",
    "        self.search_stats['results'] += len(results)\n",
    "        return results\n",
    "    \n",
    "    def boolean_search(self, query: str) -> List[str]:\n",
    "        \"\"\"å¸ƒå°”æœç´¢\"\"\"\n",
    "        # ç®€åŒ–çš„å¸ƒå°”æœç´¢å®ç°\n",
    "        # æ”¯æŒæ ¼å¼: \"word1 AND word2\" æˆ– \"word1 OR word2\"\n",
    "        \n",
    "        if ' AND ' in query:\n",
    "            parts = query.split(' AND ')\n",
    "            return self.search_text(' '.join(parts), operator='AND')\n",
    "        elif ' OR ' in query:\n",
    "            parts = query.split(' OR ')\n",
    "            return self.search_text(' '.join(parts), operator='OR')\n",
    "        else:\n",
    "            return self.search_text(query)\n",
    "    \n",
    "    def get_index_stats(self):\n",
    "        \"\"\"è·å–ç´¢å¼•ç»Ÿè®¡\"\"\"\n",
    "        stats = {\n",
    "            'total_documents': len(self.documents),\n",
    "            'inverted_index_size': len(self.inverted_index),\n",
    "            'field_index_size': len(self.field_index),\n",
    "            'search_stats': self.search_stats.copy(),\n",
    "            'avg_results_per_search': (\n",
    "                self.search_stats['results'] / max(1, self.search_stats['searches'])\n",
    "            )\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "# 3. ç¼“å­˜ç³»ç»Ÿ\n",
    "print(f\"\\n   âš¡ 3. ç¼“å­˜ç³»ç»Ÿ:\")\n",
    "\n",
    "@dataclass\n",
    "class CacheSystem:\n",
    "    \"\"\"ç¼“å­˜ç³»ç»Ÿå®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 1000, policy: str = 'LRU'):\n",
    "        self.max_size = max_size\n",
    "        self.policy = policy\n",
    "        self.cache = {}\n",
    "        self.access_order = []  # ç”¨äºLRU\n",
    "        self.access_count = {}  # ç”¨äºLFU\n",
    "        self.stats = {\n",
    "            'hits': 0,\n",
    "            'misses': 0,\n",
    "            'evictions': 0\n",
    "        }\n",
    "    \n",
    "    def get(self, key: str) -> Any:\n",
    "        \"\"\"è·å–ç¼“å­˜å€¼\"\"\"\n",
    "        if key in self.cache:\n",
    "            self.stats['hits'] += 1\n",
    "            \n",
    "            # æ›´æ–°è®¿é—®ä¿¡æ¯\n",
    "            if self.policy == 'LRU':\n",
    "                self.access_order.remove(key)\n",
    "                self.access_order.append(key)\n",
    "            elif self.policy == 'LFU':\n",
    "                self.access_count[key] += 1\n",
    "            \n",
    "            return self.cache[key]\n",
    "        else:\n",
    "            self.stats['misses'] += 1\n",
    "            return None\n",
    "    \n",
    "    def put(self, key: str, value: Any):\n",
    "        \"\"\"å­˜å‚¨ç¼“å­˜å€¼\"\"\"\n",
    "        if key in self.cache:\n",
    "            # æ›´æ–°ç°æœ‰å€¼\n",
    "            self.cache[key] = value\n",
    "            \n",
    "            if self.policy == 'LRU':\n",
    "                self.access_order.remove(key)\n",
    "                self.access_order.append(key)\n",
    "            elif self.policy == 'LFU':\n",
    "                self.access_count[key] += 1\n",
    "        else:\n",
    "            # æ·»åŠ æ–°å€¼\n",
    "            if len(self.cache) >= self.max_size:\n",
    "                self._evict()\n",
    "            \n",
    "            self.cache[key] = value\n",
    "            \n",
    "            if self.policy == 'LRU':\n",
    "                self.access_order.append(key)\n",
    "            elif self.policy == 'LFU':\n",
    "                self.access_count[key] = 1\n",
    "    \n",
    "    def _evict(self):\n",
    "        \"\"\"é©±é€ç¼“å­˜é¡¹\"\"\"\n",
    "        if not self.cache:\n",
    "            return\n",
    "        \n",
    "        if self.policy == 'LRU':\n",
    "            # é©±é€æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„é¡¹\n",
    "            lru_key = self.access_order[0]\n",
    "            del self.cache[lru_key]\n",
    "            self.access_order.remove(lru_key)\n",
    "        elif self.policy == 'LFU':\n",
    "            # é©±é€ä½¿ç”¨é¢‘ç‡æœ€ä½çš„é¡¹\n",
    "            lfu_key = min(self.access_count.keys(), key=lambda k: self.access_count[k])\n",
    "            del self.cache[lfu_key]\n",
    "            del self.access_count[lfu_key]\n",
    "        \n",
    "        self.stats['evictions'] += 1\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"æ¸…ç©ºç¼“å­˜\"\"\"\n",
    "        self.cache.clear()\n",
    "        self.access_order.clear()\n",
    "        self.access_count.clear()\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"è·å–ç¼“å­˜ç»Ÿè®¡\"\"\"\n",
    "        total_access = self.stats['hits'] + self.stats['misses']\n",
    "        hit_rate = self.stats['hits'] / max(1, total_access)\n",
    "        \n",
    "        return {\n",
    "            'size': len(self.cache),\n",
    "            'max_size': self.max_size,\n",
    "            'hit_rate': hit_rate,\n",
    "            'hits': self.stats['hits'],\n",
    "            'misses': self.stats['misses'],\n",
    "            'evictions': self.stats['evictions'],\n",
    "            'policy': self.policy\n",
    "        }\n",
    "\n",
    "# æ¼”ç¤ºå¤§æ•°æ®å­˜å‚¨ä¸æ£€ç´¢\n",
    "print(f\"\\n   ğŸ”§ å¤§æ•°æ®å­˜å‚¨ä¸æ£€ç´¢æ¼”ç¤º:\")\n",
    "\n",
    "# 1. åˆ†å¸ƒå¼å­˜å‚¨æ¼”ç¤º\n",
    "print(f\"\\n   ğŸ’¾ åˆ†å¸ƒå¼å­˜å‚¨æ¼”ç¤º:\")\n",
    "\n",
    "# åˆ›å»ºåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿ\n",
    "dist_storage = DistributedStorage(n_nodes=3, replication_factor=2)\n",
    "\n",
    "# å­˜å‚¨æµ‹è¯•æ•°æ®\n",
    "test_keys = [f'doc_{i}' for i in range(100)]\n",
    "for i, key in enumerate(test_keys):\n",
    "    value = {\n",
    "        'content': f'This is document {i} content',\n",
    "        'timestamp': time.time(),\n",
    "        'size': np.random.randint(100, 1000)\n",
    "    }\n",
    "    dist_storage.put(key, value)\n",
    "\n",
    "# æµ‹è¯•è¯»å†™\n",
    "print(f\"   æµ‹è¯•æ•°æ®è¯»å†™...\")\n",
    "for key in test_keys[:5]:\n",
    "    data = dist_storage.get(key)\n",
    "    print(f\"      {key}: {len(data['content'])} å­—ç¬¦\")\n",
    "\n",
    "# æ˜¾ç¤ºå­˜å‚¨ç»Ÿè®¡\n",
    "storage_stats = dist_storage.get_storage_stats()\n",
    "print(f\"\\n   ğŸ“Š åˆ†å¸ƒå¼å­˜å‚¨ç»Ÿè®¡:\")\n",
    "print(f\"      æ€»é”®æ•°: {storage_stats['total_keys']}\")\n",
    "for node_stat in storage_stats['node_stats']:\n",
    "    print(f\"      èŠ‚ç‚¹{node_stat['node_id']}: {node_stat['key_count']} é”®, \"\n",
    "          f\"è¯»{node_stat['reads']} å†™{node_stat['writes']}\")\n",
    "\n",
    "# 2. ç´¢å¼•ç³»ç»Ÿæ¼”ç¤º\n",
    "print(f\"\\n   ğŸ” ç´¢å¼•ç³»ç»Ÿæ¼”ç¤º:\")\n",
    "\n",
    "# åˆ›å»ºç´¢å¼•ç³»ç»Ÿ\n",
    "index_system = IndexSystem()\n",
    "\n",
    "# æ·»åŠ æ–‡æ¡£\n",
    "documents = [\n",
    "    {'doc_id': f'doc_{i}', \n",
    "     'text': f'LangChain is a framework for building applications powered by language models document {i}',\n",
    "     'category': np.random.choice(['AI', 'ML', 'NLP']),\n",
    "     'author': f'author_{i % 5}'}\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "for doc in documents:\n",
    "    index_system.add_document(doc['doc_id'], doc)\n",
    "\n",
    "# æµ‹è¯•æœç´¢\n",
    "print(f\"   æµ‹è¯•æ–‡æœ¬æœç´¢...\")\n",
    "search_queries = [\n",
    "    'LangChain',\n",
    "    'language models',\n",
    "    'LangChain AND framework',\n",
    "    'AI OR ML'\n",
    "]\n",
    "\n",
    "for query in search_queries:\n",
    "    results = index_system.boolean_search(query)\n",
    "    print(f\"      '{query}': {len(results)} ä¸ªç»“æœ\")\n",
    "\n",
    "# æµ‹è¯•å­—æ®µæœç´¢\n",
    "print(f\"\\n   æµ‹è¯•å­—æ®µæœç´¢...\")\n",
    "field_results = index_system.search_field('category', 'AI')\n",
    "print(f\"      ç±»åˆ«'AI': {len(field_results)} ä¸ªç»“æœ\")\n",
    "\n",
    "# æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡\n",
    "index_stats = index_system.get_index_stats()\n",
    "print(f\"\\n   ğŸ“Š ç´¢å¼•ç³»ç»Ÿç»Ÿè®¡:\")\n",
    "print(f\"      æ€»æ–‡æ¡£æ•°: {index_stats['total_documents']}\")\n",
    "print(f\"      å€’æ’ç´¢å¼•å¤§å°: {index_stats['inverted_index_size']}\")\n",
    "print(f\"      å­—æ®µç´¢å¼•å¤§å°: {index_stats['field_index_size']}\")\n",
    "print(f\"      å¹³å‡æœç´¢ç»“æœæ•°: {index_stats['avg_results_per_search']:.1f}\")\n",
    "\n",
    "# 3. ç¼“å­˜ç³»ç»Ÿæ¼”ç¤º\n",
    "print(f\"\\n   âš¡ ç¼“å­˜ç³»ç»Ÿæ¼”ç¤º:\")\n",
    "\n",
    "# åˆ›å»ºç¼“å­˜ç³»ç»Ÿ\n",
    "cache_lru = CacheSystem(max_size=50, policy='LRU')\n",
    "cache_lfu = CacheSystem(max_size=50, policy='LFU')\n",
    "\n",
    "# æ¨¡æ‹Ÿç¼“å­˜è®¿é—®æ¨¡å¼\n",
    "def simulate_cache_access(cache, access_pattern):\n",
    "    \"\"\"æ¨¡æ‹Ÿç¼“å­˜è®¿é—®\"\"\"\n",
    "    for key in access_pattern:\n",
    "        # å°è¯•è·å–\n",
    "        value = cache.get(key)\n",
    "        \n",
    "        # å¦‚æœæœªå‘½ä¸­ï¼Œåˆ™åŠ è½½å¹¶ç¼“å­˜\n",
    "        if value is None:\n",
    "            value = f'data_for_{key}'\n",
    "            cache.put(key, value)\n",
    "\n",
    "# ç”Ÿæˆè®¿é—®æ¨¡å¼ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰\n",
    "np.random.seed(42)\n",
    "access_pattern = []\n",
    "# 80%çš„è®¿é—®é›†ä¸­åœ¨20%çš„çƒ­ç‚¹æ•°æ®\n",
    "hot_keys = [f'key_{i}' for i in range(20)]\n",
    "cold_keys = [f'key_{i}' for i in range(20, 100)]\n",
    "\n",
    "for _ in range(200):\n",
    "    if np.random.random() < 0.8:\n",
    "        access_pattern.append(np.random.choice(hot_keys))\n",
    "    else:\n",
    "        access_pattern.append(np.random.choice(cold_keys))\n",
    "\n",
    "# æµ‹è¯•LRUç¼“å­˜\n",
    "print(f\"   æµ‹è¯•LRUç¼“å­˜ç­–ç•¥...\")\n",
    "simulate_cache_access(cache_lru, access_pattern)\n",
    "lru_stats = cache_lru.get_stats()\n",
    "\n",
    "# æµ‹è¯•LFUç¼“å­˜\n",
    "print(f\"   æµ‹è¯•LFUç¼“å­˜ç­–ç•¥...\")\n",
    "simulate_cache_access(cache_lfu, access_pattern)\n",
    "lfu_stats = cache_lfu.get_stats()\n",
    "\n",
    "# ç¼“å­˜æ€§èƒ½å¯¹æ¯”\n",
    "print(f\"\\n   ğŸ“Š ç¼“å­˜æ€§èƒ½å¯¹æ¯”:\")\n",
    "print(f\"      ç­–ç•¥     å‘½ä¸­ç‡   å‘½ä¸­æ•°   æœªå‘½ä¸­æ•°   é©±é€æ•°\")\n",
    "print(f\"      LRU      {lru_stats['hit_rate']:.3f}   {lru_stats['hits']:4d}    {lru_stats['misses']:4d}      {lru_stats['evictions']:3d}\")\n",
    "print(f\"      LFU      {lfu_stats['hit_rate']:.3f}   {lfu_stats['hits']:4d}    {lfu_stats['misses']:4d}      {lfu_stats['evictions']:3d}\")\n",
    "\n",
    "print(f\"\\nâœ… å¤§æ•°æ®å­˜å‚¨ä¸æ£€ç´¢å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡åˆ†å¸ƒå¼å­˜å‚¨çš„åŸºæœ¬åŸç†\")\n",
    "print(f\"   âœ“ ç†è§£æ•°æ®ç´¢å¼•å’Œæ£€ç´¢æŠ€æœ¯\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨ç¼“å­˜å’Œä¼˜åŒ–ç­–ç•¥\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿè®¾è®¡é«˜æ•ˆçš„æ•°æ®å­˜å‚¨æ–¹æ¡ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å­¦ä¹ æ€»ç»“\n",
    "\n",
    "### âœ… çŸ¥è¯†æ¸…å•è¾¾æˆæƒ…å†µéªŒè¯\n",
    "\n",
    "**5.8 å¤§æ•°æ®å¤„ç†æŠ€æœ¯ [â­â­è¿›é˜¶]**\n",
    "- âœ… æŒæ¡å¤§æ•°æ®å¤„ç†çš„åŸºæœ¬æ¦‚å¿µ\n",
    "- âœ… ç†è§£åˆ†å¸ƒå¼è®¡ç®—åŸç†\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨å¤§æ•°æ®å¤„ç†å·¥å…·\n",
    "- âœ… èƒ½å¤Ÿè®¾è®¡å¯æ‰©å±•çš„æ•°æ®å¤„ç†æ¶æ„\n",
    "- âœ… æŒæ¡MapReduceçš„åŸºæœ¬åŸç†\n",
    "- âœ… ç†è§£å¹¶è¡Œè®¡ç®—æ¨¡å¼\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—å·¥å…·\n",
    "- âœ… èƒ½å¤Ÿä¼˜åŒ–åˆ†å¸ƒå¼è®¡ç®—æ€§èƒ½\n",
    "- âœ… æŒæ¡åˆ†å¸ƒå¼å­˜å‚¨çš„åŸºæœ¬åŸç†\n",
    "- âœ… ç†è§£æ•°æ®ç´¢å¼•å’Œæ£€ç´¢æŠ€æœ¯\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨ç¼“å­˜å’Œä¼˜åŒ–ç­–ç•¥\n",
    "- âœ… èƒ½å¤Ÿè®¾è®¡é«˜æ•ˆçš„æ•°æ®å­˜å‚¨æ–¹æ¡ˆ\n",
    "- âœ… èƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„å¤§æ•°æ®å¤„ç†ç³»ç»Ÿ\n",
    "\n",
    "### ğŸ¯ ä¸LangChainå­¦ä¹ çš„å…³è”\n",
    "\n",
    "**å¤§æ•°æ®å¤„ç†é‡è¦æ€§**ï¼š\n",
    "- å¤§æ•°æ®å¤„ç†æ”¯æŒLangChainçš„æµ·é‡æ–‡æœ¬æ•°æ®å¤„ç†\n",
    "- ä¸ºLangChainçš„åˆ†å¸ƒå¼æ¨ç†æä¾›æŠ€æœ¯åŸºç¡€\n",
    "- æ”¯æŒLangChainçš„å¤§è§„æ¨¡çŸ¥è¯†åº“ç®¡ç†\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„é«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§\n",
    "- å¤§æ•°æ®å¤„ç†æ”¯æŒLangChainçš„å®æ—¶åˆ†æå’Œå†³ç­–\n",
    "\n",
    "**å®é™…åº”ç”¨åœºæ™¯**ï¼š\n",
    "- LangChainçš„å¤§è§„æ¨¡æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–\n",
    "- LangChainçš„åˆ†å¸ƒå¼æ¨¡å‹æ¨ç†å’Œè®¡ç®—\n",
    "- LangChainçš„å®æ—¶å¯¹è¯æµå¤„ç†å’Œåˆ†æ\n",
    "- LangChainçš„çŸ¥è¯†å›¾è°±æ„å»ºå’ŒæŸ¥è¯¢ä¼˜åŒ–\n",
    "- LangChainçš„ç”¨æˆ·è¡Œä¸ºåˆ†æå’Œä¸ªæ€§åŒ–æ¨è\n",
    "\n",
    "### ğŸ“š è¿›é˜¶å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹ å»ºè®®**ï¼š\n",
    "   - æ·±å…¥ç»ƒä¹ Apache Sparkå’ŒDaskç­‰åˆ†å¸ƒå¼æ¡†æ¶\n",
    "   - æŒæ¡æµå¼å¤„ç†æ¡†æ¶å¦‚Apache Kafkaå’ŒFlink\n",
    "   - å­¦ä¹ äº‘åŸç”Ÿå¤§æ•°æ®å¹³å°å¦‚AWS EMRã€Google Dataflow\n",
    "\n",
    "2. **æ‰©å±•å­¦ä¹ **ï¼š\n",
    "   - å­¦ä¹ åˆ†å¸ƒå¼æ•°æ®åº“å’ŒNoSQLæŠ€æœ¯\n",
    "   - äº†è§£æ•°æ®æ¹–å’Œæ•°æ®ä»“åº“æ¶æ„\n",
    "   - æ¢ç´¢å®æ—¶è®¡ç®—å’Œå¤æ‚äº‹ä»¶å¤„ç†\n",
    "\n",
    "3. **å®é™…åº”ç”¨**ï¼š\n",
    "   - æ„å»ºä¼ä¸šçº§å¤§æ•°æ®å¤„ç†å¹³å°\n",
    "   - å¼€å‘å®æ—¶æ•°æ®æµå¤„ç†ç³»ç»Ÿ\n",
    "   - å®ç°æ™ºèƒ½æ•°æ®ç®¡é“å’ŒETLæµç¨‹\n",
    "\n",
    "### ğŸ”§ å¸¸è§é”™è¯¯ä¸æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **å†…å­˜ç®¡ç†é”™è¯¯**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šä¸€æ¬¡æ€§åŠ è½½å…¨éƒ¨å¤§æ•°æ®\n",
    "   big_data = pd.read_csv('huge_file.csv')  # å†…å­˜æº¢å‡º\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåˆ†å—åŠ è½½å’Œå¤„ç†\n",
    "   chunk_size = 10000\n",
    "   for chunk in pd.read_csv('huge_file.csv', chunksize=chunk_size):\n",
    "       process_chunk(chunk)\n",
    "   ```\n",
    "\n",
    "2. **å¹¶è¡Œè®¡ç®—è¯¯åŒº**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šè¿‡åº¦å¹¶è¡ŒåŒ–å¯¼è‡´å¼€é”€è¿‡å¤§\n",
    "   with ProcessPoolExecutor(max_workers=100) as executor:  # è¶…è¿‡CPUæ ¸å¿ƒæ•°\n",
    "       results = list(executor.map(func, data))\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ ¹æ®ç¡¬ä»¶èµ„æºåˆç†è®¾ç½®å¹¶è¡Œåº¦\n",
    "   n_workers = min(multiprocessing.cpu_count(), len(data) // chunk_size)\n",
    "   with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "       results = list(executor.map(func, data))\n",
    "   ```\n",
    "\n",
    "3. **ç¼“å­˜ç­–ç•¥è¯¯ç”¨**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šç¼“å­˜æ‰€æœ‰æ•°æ®å¯¼è‡´å†…å­˜ä¸è¶³\n",
    "   cache.put('huge_data', massive_dataset)  # ç¼“å­˜è¿‡å¤§æ•°æ®\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåˆç†è®¾ç½®ç¼“å­˜å¤§å°å’Œé©±é€ç­–ç•¥\n",
    "   cache = CacheSystem(max_size=1000, policy='LRU')\n",
    "   # åªç¼“å­˜çƒ­ç‚¹æ•°æ®\n",
    "   if is_hot_data(key):\n",
    "       cache.put(key, value)\n",
    "   ```\n",
    "\n",
    "4. **æ•°æ®ä¸€è‡´æ€§å¿½ç•¥**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåˆ†å¸ƒå¼ç¯å¢ƒä¸‹å¿½ç•¥æ•°æ®ä¸€è‡´æ€§\n",
    "   def update_data(key, value):\n",
    "       node = get_random_node()  # éšæœºé€‰æ‹©èŠ‚ç‚¹\n",
    "       node[key] = value  # å¯èƒ½å¯¼è‡´æ•°æ®ä¸ä¸€è‡´\n",
    "   \n",
    "   # æ­£ç¡®ï¼šç¡®ä¿æ•°æ®ä¸€è‡´æ€§å’Œå‰¯æœ¬åŒæ­¥\n",
    "   def update_data(key, value):\n",
    "       primary_node = hash_key(key)\n",
    "       replica_nodes = get_replica_nodes(primary_node)\n",
    "       for node in replica_nodes:\n",
    "           node[key] = value  # åŒæ­¥æ›´æ–°æ‰€æœ‰å‰¯æœ¬\n",
    "   ```\n",
    "\n",
    "5. **ç´¢å¼•è®¾è®¡ä¸å½“**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šä¸ºæ‰€æœ‰å­—æ®µåˆ›å»ºç´¢å¼•å¯¼è‡´æ€§èƒ½ä¸‹é™\n",
    "   for field in all_fields:\n",
    "       create_index(field)  # è¿‡åº¦ç´¢å¼•\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåªä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µåˆ›å»ºç´¢å¼•\n",
    "   frequent_query_fields = ['title', 'category', 'timestamp']\n",
    "   for field in frequent_query_fields:\n",
    "       if is_queried_frequently(field):\n",
    "           create_index(field)\n",
    "   ```\n",
    "\n",
    "6. **æµå¼å¤„ç†é˜»å¡**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šæµå¼å¤„ç†ä¸­ä½¿ç”¨é˜»å¡æ“ä½œ\n",
    "   def process_stream(data_stream):\n",
    "       for item in data_stream:\n",
    "           result = slow_blocking_operation(item)  # é˜»å¡æ•´ä¸ªæµ\n",
    "           yield result\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨å¼‚æ­¥æˆ–å¹¶è¡Œå¤„ç†é¿å…é˜»å¡\n",
    "   async def process_stream(data_stream):\n",
    "       async for item in data_stream:\n",
    "           result = await async_operation(item)\n",
    "           yield result\n",
    "   ```\n",
    "\n",
    "### ğŸŒ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "**å¤§æ•°æ®å¤„ç†ä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨æ•°æ®åˆ†ç‰‡å’Œåˆ†åŒºæé«˜å¹¶è¡Œåº¦\n",
    "- é‡‡ç”¨å†…å­˜è®¡ç®—å’Œåˆ—å¼å­˜å‚¨ä¼˜åŒ–æ€§èƒ½\n",
    "- å®ç°æ™ºèƒ½ç¼“å­˜å’Œé¢„åŠ è½½ç­–ç•¥\n",
    "- ä½¿ç”¨å‹ç¼©å’Œç¼–ç å‡å°‘å­˜å‚¨å’Œä¼ è¾“å¼€é”€\n",
    "\n",
    "**åˆ†å¸ƒå¼è®¡ç®—ä¼˜åŒ–**ï¼š\n",
    "- ä¼˜åŒ–æ•°æ®å±€éƒ¨æ€§å‡å°‘ç½‘ç»œä¼ è¾“\n",
    "- å®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡å’Œä»»åŠ¡è°ƒåº¦\n",
    "- ä½¿ç”¨å®¹é”™å’Œæ•…éšœæ¢å¤æœºåˆ¶\n",
    "- å®ç°ç›‘æ§å’Œæ€§èƒ½è°ƒä¼˜ç³»ç»Ÿ\n",
    "\n",
    "**å­˜å‚¨æ£€ç´¢ä¼˜åŒ–**ï¼š\n",
    "- è®¾è®¡åˆç†çš„ç´¢å¼•ç­–ç•¥å’ŒæŸ¥è¯¢ä¼˜åŒ–\n",
    "- ä½¿ç”¨åˆ†å±‚å­˜å‚¨å’Œå†·çƒ­æ•°æ®åˆ†ç¦»\n",
    "- å®ç°æ•°æ®å‹ç¼©å’Œå»é‡æŠ€æœ¯\n",
    "- å»ºç«‹å®Œæ•´çš„å¤‡ä»½å’Œæ¢å¤æœºåˆ¶\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆå¤§æ•°æ®å¤„ç†æŠ€æœ¯å­¦ä¹ ï¼**\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†å¤§æ•°æ®å¤„ç†çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¿›è¡Œåˆ†å¸ƒå¼è®¡ç®—ã€å­˜å‚¨ä¼˜åŒ–å’Œæµå¼å¤„ç†ï¼Œä¸ºLangChainæ™ºèƒ½åº”ç”¨æä¾›äº†å¼ºå¤§çš„å¤§æ•°æ®å¤„ç†èƒ½åŠ›ã€‚\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ é¢„å‘Š\n",
    "\n",
    "**ç»§ç»­ç¬¬äº”èŠ‚ï¼šæ•°æ®å¤„ç†**\n",
    "- 5.9 æ•°æ®å·¥ç¨‹å®è·µ\n",
    "\n",
    "**åç»­ç« èŠ‚é¢„å‘Š**ï¼š\n",
    "- å¼‚æ­¥ç¼–ç¨‹æŠ€æœ¯\n",
    "- Webå¼€å‘æŠ€æœ¯\n",
    "- é¡¹ç›®å·¥ç¨‹å®è·µ\n",
    "\n",
    "ç»§ç»­åŠ æ²¹ï¼Œå¤§æ•°æ®å¤„ç†æŠ€èƒ½æ­£åœ¨å¿«é€Ÿæå‡ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
