{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 38-Pandasæ•°æ®å¤„ç†\n",
    "\n",
    "## ğŸ“š ç”¨é€”è¯´æ˜\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**ï¼š\n",
    "- æŒæ¡Pandas DataFrameå’ŒSeriesçš„åŸºç¡€æ“ä½œ\n",
    "- ç†Ÿç»ƒä½¿ç”¨Pandasè¿›è¡Œæ•°æ®æ¸…æ´—ã€è½¬æ¢å’Œåˆ†æ\n",
    "- ç†è§£Pandasçš„æ•°æ®ç»“æ„å’Œç´¢å¼•æœºåˆ¶\n",
    "- èƒ½å¤Ÿä½¿ç”¨Pandaså¤„ç†å®é™…çš„æ•°æ®åˆ†æä»»åŠ¡\n",
    "\n",
    "**å‰ç½®è¦æ±‚**ï¼š\n",
    "- å·²å®Œæˆ37-NumPyæ•°ç»„æ“ä½œå­¦ä¹ \n",
    "- ç†Ÿç»ƒæŒæ¡Pythonæ•°æ®ç»“æ„å’Œå‡½æ•°\n",
    "- äº†è§£åŸºæœ¬çš„æ•°æ®åˆ†ææ¦‚å¿µ\n",
    "\n",
    "**ä¸LangChainå…³è”**ï¼š\n",
    "- Pandasæ˜¯LangChainæ•°æ®å¤„ç†å’Œåˆ†æçš„é‡è¦å·¥å…·\n",
    "- Pandasæ”¯æŒLangChainçš„æ–‡æ¡£åŠ è½½å’Œå¤„ç†\n",
    "- Pandasç”¨äºLangChainçš„æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹\n",
    "- ä¸ºåç»­å­¦ä¹ æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦åšå‡†å¤‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ çŸ¥è¯†ç‚¹è¦†ç›–\n",
    "\n",
    "### 5.2 Pandasæ•°æ®å¤„ç† [â­åŸºç¡€]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šPandasæ˜¯Pythonæ•°æ®åˆ†æçš„æ ¸å¿ƒåº“ï¼Œæä¾›äº†å¼ºå¤§çš„æ•°æ®ç»“æ„å’Œæ•°æ®æ“ä½œåŠŸèƒ½ã€‚æŒæ¡Pandaså¯¹äºæ•°æ®å¤„ç†ã€åˆ†æå’ŒLangChainåº”ç”¨å¼€å‘è‡³å…³é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡Pandas Serieså’ŒDataFrameçš„åˆ›å»ºå’Œæ“ä½œ\n",
    "- ç†è§£æ•°æ®ç´¢å¼•ã€é€‰æ‹©å’Œè¿‡æ»¤\n",
    "- ç†Ÿç»ƒä½¿ç”¨æ•°æ®æ¸…æ´—å’Œè½¬æ¢æ–¹æ³•\n",
    "- èƒ½å¤Ÿè¿›è¡Œæ•°æ®åˆ†ç»„ã€èšåˆå’Œåˆå¹¶\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ•°æ®å¤„ç†åº”ç”¨ç¤ºä¾‹\n",
    "- è¿›è¡Œæ•°æ®æ¸…æ´—å’Œåˆ†æç»ƒä¹ \n",
    "- åº”ç”¨Pandasè§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„æ•°æ®å¤„ç†ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¼ Pandasæ•°æ®å¤„ç†åŸºç¡€:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å®‰è£…æ£€æŸ¥å’Œå¯¼å…¥\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(f\"âœ… Pandasç‰ˆæœ¬: {pd.__version__}\")\n",
    "    print(f\"âœ… NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Pandasæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install pandas numpy\")\n",
    "    exit()\n",
    "\n",
    "import time\n",
    "import random\n",
    "from typing import List, Tuple, Any, Optional, Union, Dict\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 1. Pandasæ•°æ®ç»“æ„\n",
    "print(f\"ğŸ“ 1. Pandasæ•°æ®ç»“æ„:\")\n",
    "\n",
    "# 1.1 SeriesåŸºç¡€æ“ä½œ\n",
    "print(f\"\\n   ğŸ“Š 1.1 SeriesåŸºç¡€æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_series_operations():\n",
    "    \"\"\"æ¼”ç¤ºSeriesåŸºç¡€æ“ä½œ\"\"\"\n",
    "    \n",
    "    # ä»åˆ—è¡¨åˆ›å»ºSeries\n",
    "    data = [10, 20, 30, 40, 50]\n",
    "    series = pd.Series(data)\n",
    "    print(f\"   ä»åˆ—è¡¨åˆ›å»ºSeries:\")\n",
    "    print(f\"   {series}\")\n",
    "    print(f\"   ç±»å‹: {type(series)}\")\n",
    "    print(f\"   å€¼: {series.values}\")\n",
    "    print(f\"   ç´¢å¼•: {series.index}\")\n",
    "    print(f\"   æ•°æ®ç±»å‹: {series.dtype}\")\n",
    "    \n",
    "    # å¸¦è‡ªå®šä¹‰ç´¢å¼•çš„Series\n",
    "    print(f\"\\n   å¸¦è‡ªå®šä¹‰ç´¢å¼•çš„Series:\")\n",
    "    data = ['Apple', 'Banana', 'Orange', 'Grape']\n",
    "    index = ['A', 'B', 'C', 'D']\n",
    "    series_with_index = pd.Series(data, index=index)\n",
    "    print(f\"   {series_with_index}\")\n",
    "    \n",
    "    # ä»å­—å…¸åˆ›å»ºSeries\n",
    "    print(f\"\\n   ä»å­—å…¸åˆ›å»ºSeries:\")\n",
    "    data_dict = {'Math': 90, 'English': 85, 'Science': 88, 'History': 92}\n",
    "    series_from_dict = pd.Series(data_dict)\n",
    "    print(f\"   {series_from_dict}\")\n",
    "    \n",
    "    # Seriesç´¢å¼•å’Œåˆ‡ç‰‡\n",
    "    print(f\"\\n   Seriesç´¢å¼•å’Œåˆ‡ç‰‡:\")\n",
    "    series = pd.Series([100, 200, 300, 400, 500], index=['a', 'b', 'c', 'd', 'e'])\n",
    "    print(f\"   åŸSeries: {series}\")\n",
    "    print(f\"   series['a']: {series['a']}\")\n",
    "    print(f\"   series[['a', 'c', 'e']]:\\n{series[['a', 'c', 'e']]}\")\n",
    "    print(f\"   series[1:4]:\\n{series[1:4]}\")\n",
    "    print(f\"   series['b':'d']:\\n{series['b':'d']}\")\n",
    "    \n",
    "    # Seriesè¿ç®—\n",
    "    print(f\"\\n   Seriesè¿ç®—:\")\n",
    "    s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n",
    "    s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])\n",
    "    print(f\"   s1: {s1}\")\n",
    "    print(f\"   s2: {s2}\")\n",
    "    print(f\"   s1 + s2: {s1 + s2}\")\n",
    "    print(f\"   s1 * 2: {s1 * 2}\")\n",
    "    print(f\"   s1 > 2: {s1 > 2}\")\n",
    "    \n",
    "    # Seriesç»Ÿè®¡æ–¹æ³•\n",
    "    print(f\"\\n   Seriesç»Ÿè®¡æ–¹æ³•:\")\n",
    "    scores = pd.Series([85, 92, 78, 96, 88, 91, 83])\n",
    "    print(f\"   åˆ†æ•°: {scores.values}\")\n",
    "    print(f\"   å¹³å‡åˆ†: {scores.mean():.2f}\")\n",
    "    print(f\"   ä¸­ä½æ•°: {scores.median():.2f}\")\n",
    "    print(f\"   æ ‡å‡†å·®: {scores.std():.2f}\")\n",
    "    print(f\"   æœ€å°å€¼: {scores.min()}\")\n",
    "    print(f\"   æœ€å¤§å€¼: {scores.max()}\")\n",
    "    print(f\"   æ±‚å’Œ: {scores.sum()}\")\n",
    "    print(f\"   è®¡æ•°: {scores.count()}\")\n",
    "\n",
    "demonstrate_series_operations()\n",
    "\n",
    "# 1.2 LangChainæ–‡æ¡£å¤„ç†åº”ç”¨\n",
    "print(f\"\\n   ğŸ“„ 1.2 LangChainæ–‡æ¡£å¤„ç†åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class DocumentProcessor:\n",
    "    \"\"\"æ–‡æ¡£å¤„ç†å™¨ - æ¨¡æ‹ŸLangChainçš„æ–‡æ¡£å¤„ç†åŠŸèƒ½\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "    \n",
    "    def add_document(self, text: str, metadata: Dict[str, Any] = None):\n",
    "        \"\"\"æ·»åŠ æ–‡æ¡£\"\"\"\n",
    "        doc = {\n",
    "            'text': text,\n",
    "            'length': len(text),\n",
    "            'word_count': len(text.split()),\n",
    "            'char_count': len(text.replace(' ', '')),\n",
    "            'created_at': datetime.now(),\n",
    "            **(metadata or {})\n",
    "        }\n",
    "        self.documents.append(doc)\n",
    "    \n",
    "    def get_documents_series(self) -> pd.Series:\n",
    "        \"\"\"è·å–æ–‡æ¡£Series\"\"\"\n",
    "        return pd.Series(self.documents)\n",
    "    \n",
    "    def analyze_text_length(self) -> pd.Series:\n",
    "        \"\"\"åˆ†ææ–‡æœ¬é•¿åº¦\"\"\"\n",
    "        lengths = [doc['length'] for doc in self.documents]\n",
    "        return pd.Series(lengths, name='text_length')\n",
    "    \n",
    "    def analyze_word_frequency(self) -> pd.Series:\n",
    "        \"\"\"åˆ†æè¯é¢‘\"\"\"\n",
    "        all_words = []\n",
    "        for doc in self.documents:\n",
    "            words = doc['text'].lower().split()\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        word_counts = pd.Series(all_words).value_counts()\n",
    "        return word_counts\n",
    "    \n",
    "    def filter_documents_by_length(self, min_length: int, max_length: int) -> List[Dict]:\n",
    "        \"\"\"æ ¹æ®é•¿åº¦è¿‡æ»¤æ–‡æ¡£\"\"\"\n",
    "        filtered = []\n",
    "        for doc in self.documents:\n",
    "            if min_length <= doc['length'] <= max_length:\n",
    "                filtered.append(doc)\n",
    "        return filtered\n",
    "\n",
    "def demonstrate_document_processor():\n",
    "    \"\"\"æ¼”ç¤ºæ–‡æ¡£å¤„ç†å™¨\"\"\"\n",
    "    processor = DocumentProcessor()\n",
    "    \n",
    "    # æ·»åŠ æµ‹è¯•æ–‡æ¡£\n",
    "    test_docs = [\n",
    "        (\"Python is a powerful programming language\", {\"category\": \"programming\", \"difficulty\": \"beginner\"}),\n",
    "        (\"Machine learning algorithms are complex\", {\"category\": \"AI\", \"difficulty\": \"advanced\"}),\n",
    "        (\"Data science combines statistics and programming\", {\"category\": \"data\", \"difficulty\": \"intermediate\"}),\n",
    "        (\"LangChain helps build LLM applications\", {\"category\": \"AI\", \"difficulty\": \"advanced\"}),\n",
    "        (\"NumPy is essential for numerical computing\", {\"category\": \"programming\", \"difficulty\": \"intermediate\"})\n",
    "    ]\n",
    "    \n",
    "    for text, metadata in test_docs:\n",
    "        processor.add_document(text, metadata)\n",
    "    \n",
    "    print(f\"   æ–‡æ¡£å¤„ç†æ¼”ç¤º:\")\n",
    "    print(f\"   æ–‡æ¡£æ•°é‡: {len(processor.documents)}\")\n",
    "    \n",
    "    # æ–‡æ¡£Series\n",
    "    print(f\"\\n   æ–‡æ¡£Series:\")\n",
    "    docs_series = processor.get_documents_series()\n",
    "    print(f\"   Seriesç±»å‹: {type(docs_series)}\")\n",
    "    print(f\"   Seriesé•¿åº¦: {len(docs_series)}\")\n",
    "    \n",
    "    # æ–‡æœ¬é•¿åº¦åˆ†æ\n",
    "    print(f\"\\n   æ–‡æœ¬é•¿åº¦åˆ†æ:\")\n",
    "    length_series = processor.analyze_text_length()\n",
    "    print(f\"   é•¿åº¦Series: {length_series.values}\")\n",
    "    print(f\"   å¹³å‡é•¿åº¦: {length_series.mean():.2f}\")\n",
    "    print(f\"   é•¿åº¦ç»Ÿè®¡:\\n{length_series.describe()}\")\n",
    "    \n",
    "    # è¯é¢‘åˆ†æ\n",
    "    print(f\"\\n   è¯é¢‘åˆ†æ (å‰5ä¸ª):\")\n",
    "    word_freq = processor.analyze_word_frequency()\n",
    "    print(f\"   {word_freq.head()}\")\n",
    "    \n",
    "    # æ–‡æ¡£è¿‡æ»¤\n",
    "    print(f\"\\n   æ–‡æ¡£è¿‡æ»¤ (é•¿åº¦30-50):\")\n",
    "    filtered_docs = processor.filter_documents_by_length(30, 50)\n",
    "    for i, doc in enumerate(filtered_docs, 1):\n",
    "        print(f\"   {i}. {doc['text']} (é•¿åº¦: {doc['length']})\")\n",
    "\n",
    "demonstrate_document_processor()\n",
    "\n",
    "# 2. DataFrameåŸºç¡€æ“ä½œ\n",
    "print(f\"\\nğŸ“ 2. DataFrameåŸºç¡€æ“ä½œ:\")\n",
    "\n",
    "# 2.1 DataFrameåˆ›å»ºå’ŒåŸºç¡€æ“ä½œ\n",
    "print(f\"\\n   ğŸ—‚ï¸ 2.1 DataFrameåˆ›å»ºå’ŒåŸºç¡€æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_dataframe_operations():\n",
    "    \"\"\"æ¼”ç¤ºDataFrameåŸºç¡€æ“ä½œ\"\"\"\n",
    "    \n",
    "    # ä»å­—å…¸åˆ›å»ºDataFrame\n",
    "    print(f\"   ä»å­—å…¸åˆ›å»ºDataFrame:\")\n",
    "    data = {\n",
    "        'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "        'Age': [25, 30, 35, 28],\n",
    "        'City': ['New York', 'Los Angeles', 'Chicago', 'Houston'],\n",
    "        'Salary': [70000, 80000, 90000, 75000]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"   {df}\")\n",
    "    print(f\"   å½¢çŠ¶: {df.shape}\")\n",
    "    print(f\"   åˆ—å: {df.columns.tolist()}\")\n",
    "    print(f\"   ç´¢å¼•: {df.index.tolist()}\")\n",
    "    print(f\"   æ•°æ®ç±»å‹:\\n{df.dtypes}\")\n",
    "    \n",
    "    # ä»åˆ—è¡¨çš„å­—å…¸åˆ›å»º\n",
    "    print(f\"\\n   ä»åˆ—è¡¨çš„å­—å…¸åˆ›å»º:\")\n",
    "    data_list = [\n",
    "        {'Product': 'Laptop', 'Price': 1200, 'Quantity': 5},\n",
    "        {'Product': 'Mouse', 'Price': 25, 'Quantity': 50},\n",
    "        {'Product': 'Keyboard', 'Price': 75, 'Quantity': 20}\n",
    "    ]\n",
    "    df_from_list = pd.DataFrame(data_list)\n",
    "    print(f\"   {df_from_list}\")\n",
    "    \n",
    "    # åŸºç¡€ä¿¡æ¯æŸ¥çœ‹\n",
    "    print(f\"\\n   åŸºç¡€ä¿¡æ¯æŸ¥çœ‹:\")\n",
    "    print(f\"   head(2):\\n{df.head(2)}\")\n",
    "    print(f\"   tail(2):\\n{df.tail(2)}\")\n",
    "    print(f\"   info():\")\n",
    "    df.info()\n",
    "    print(f\"   describe():\\n{df.describe()}\")\n",
    "    \n",
    "    # åˆ—æ“ä½œ\n",
    "    print(f\"\\n   åˆ—æ“ä½œ:\")\n",
    "    print(f\"   å•åˆ—é€‰æ‹© (Name): {df['Name'].tolist()}\")\n",
    "    print(f\"   å¤šåˆ—é€‰æ‹© (Name, Age):\\n{df[['Name', 'Age']]}\")\n",
    "    print(f\"   æ–°å¢åˆ— (Age * 2):\")\n",
    "    df['Age_Double'] = df['Age'] * 2\n",
    "    print(f\"   {df[['Name', 'Age', 'Age_Double']]}\")\n",
    "    \n",
    "    # è¡Œæ“ä½œ\n",
    "    print(f\"\\n   è¡Œæ“ä½œ:\")\n",
    "    print(f\"   loc[0] (ç¬¬ä¸€è¡Œ):\\n{df.loc[0]}\")\n",
    "    print(f\"   iloc[0:2] (å‰ä¸¤è¡Œ):\\n{df.iloc[0:2]}\")\n",
    "    print(f\"   æ¡ä»¶é€‰æ‹© (Age > 28):\\n{df[df['Age'] > 28]}\")\n",
    "    \n",
    "    # ç»Ÿè®¡æ“ä½œ\n",
    "    print(f\"\\n   ç»Ÿè®¡æ“ä½œ:\")\n",
    "    print(f\"   æŒ‰åˆ—æ±‚å’Œ:\\n{df.sum(numeric_only=True)}\")\n",
    "    print(f\"   æŒ‰åˆ—å‡å€¼:\\n{df.mean(numeric_only=True)}\")\n",
    "    print(f\"   æŒ‰åˆ—æœ€å¤§å€¼:\\n{df.max(numeric_only=True)}\")\n",
    "\n",
    "demonstrate_dataframe_operations()\n",
    "\n",
    "# 2.2 LangChainæ•°æ®ç®¡ç†åº”ç”¨\n",
    "print(f\"\\n   ğŸ¤– 2.2 LangChainæ•°æ®ç®¡ç†åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class DataManager:\n",
    "    \"\"\"æ•°æ®ç®¡ç†å™¨ - æ¨¡æ‹ŸLangChainçš„æ•°æ®ç®¡ç†åŠŸèƒ½\"\"\"\n",
    "    \n",
    "    def create_conversation_data(self) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºå¯¹è¯æ•°æ®\"\"\"\n",
    "        conversations = [\n",
    "            {\n",
    "                'conversation_id': 'conv_001',\n",
    "                'user_id': 'user_001',\n",
    "                'message': 'Hello, how are you?',\n",
    "                'response': 'I am fine, thank you!',\n",
    "                'timestamp': datetime.now() - timedelta(hours=2),\n",
    "                'tokens_used': 15,\n",
    "                'model': 'gpt-3.5-turbo',\n",
    "                'satisfaction_score': 4.5\n",
    "            },\n",
    "            {\n",
    "                'conversation_id': 'conv_002',\n",
    "                'user_id': 'user_002',\n",
    "                'message': 'What is Python?',\n",
    "                'response': 'Python is a programming language...',\n",
    "                'timestamp': datetime.now() - timedelta(hours=1),\n",
    "                'tokens_used': 25,\n",
    "                'model': 'gpt-4',\n",
    "                'satisfaction_score': 4.8\n",
    "            },\n",
    "            {\n",
    "                'conversation_id': 'conv_003',\n",
    "                'user_id': 'user_001',\n",
    "                'message': 'Explain machine learning',\n",
    "                'response': 'Machine learning is a subset of AI...',\n",
    "                'timestamp': datetime.now() - timedelta(minutes=30),\n",
    "                'tokens_used': 35,\n",
    "                'model': 'gpt-4',\n",
    "                'satisfaction_score': 4.2\n",
    "            },\n",
    "            {\n",
    "                'conversation_id': 'conv_004',\n",
    "                'user_id': 'user_003',\n",
    "                'message': 'Help with data analysis',\n",
    "                'response': 'I can help you with data analysis...',\n",
    "                'timestamp': datetime.now() - timedelta(minutes=15),\n",
    "                'tokens_used': 40,\n",
    "                'model': 'gpt-3.5-turbo',\n",
    "                'satisfaction_score': 4.6\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return pd.DataFrame(conversations)\n",
    "    \n",
    "    def analyze_usage_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æä½¿ç”¨æ¨¡å¼\"\"\"\n",
    "        analysis = {\n",
    "            'total_conversations': len(df),\n",
    "            'unique_users': df['user_id'].nunique(),\n",
    "            'total_tokens': df['tokens_used'].sum(),\n",
    "            'avg_tokens_per_conversation': df['tokens_used'].mean(),\n",
    "            'avg_satisfaction': df['satisfaction_score'].mean(),\n",
    "            'model_usage': df['model'].value_counts().to_dict(),\n",
    "            'user_activity': df['user_id'].value_counts().to_dict()\n",
    "        }\n",
    "        return analysis\n",
    "    \n",
    "    def filter_by_criteria(self, df: pd.DataFrame, \n",
    "                          min_tokens: Optional[int] = None,\n",
    "                          min_satisfaction: Optional[float] = None,\n",
    "                          model: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"æ ¹æ®æ¡ä»¶è¿‡æ»¤æ•°æ®\"\"\"\n",
    "        filtered_df = df.copy()\n",
    "        \n",
    "        if min_tokens is not None:\n",
    "            filtered_df = filtered_df[filtered_df['tokens_used'] >= min_tokens]\n",
    "        \n",
    "        if min_satisfaction is not None:\n",
    "            filtered_df = filtered_df[filtered_df['satisfaction_score'] >= min_satisfaction]\n",
    "        \n",
    "        if model is not None:\n",
    "            filtered_df = filtered_df[filtered_df['model'] == model]\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def create_summary_report(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ›å»ºæ±‡æ€»æŠ¥å‘Š\"\"\"\n",
    "        # æŒ‰ç”¨æˆ·æ±‡æ€»\n",
    "        user_summary = df.groupby('user_id').agg({\n",
    "            'conversation_id': 'count',\n",
    "            'tokens_used': ['sum', 'mean'],\n",
    "            'satisfaction_score': ['mean', 'min', 'max']\n",
    "        }).round(2)\n",
    "        \n",
    "        # æŒ‰æ¨¡å‹æ±‡æ€»\n",
    "        model_summary = df.groupby('model').agg({\n",
    "            'conversation_id': 'count',\n",
    "            'tokens_used': ['sum', 'mean'],\n",
    "            'satisfaction_score': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        return {'user_summary': user_summary, 'model_summary': model_summary}\n",
    "\n",
    "def demonstrate_data_manager():\n",
    "    \"\"\"æ¼”ç¤ºæ•°æ®ç®¡ç†å™¨\"\"\"\n",
    "    manager = DataManager()\n",
    "    \n",
    "    # åˆ›å»ºå¯¹è¯æ•°æ®\n",
    "    print(f\"   æ•°æ®ç®¡ç†æ¼”ç¤º:\")\n",
    "    df = manager.create_conversation_data()\n",
    "    print(f\"   å¯¹è¯æ•°æ®:\\n{df}\")\n",
    "    \n",
    "    # ä½¿ç”¨æ¨¡å¼åˆ†æ\n",
    "    print(f\"\\n   ä½¿ç”¨æ¨¡å¼åˆ†æ:\")\n",
    "    analysis = manager.analyze_usage_patterns(df)\n",
    "    for key, value in analysis.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # æ•°æ®è¿‡æ»¤\n",
    "    print(f\"\\n   æ•°æ®è¿‡æ»¤æ¼”ç¤º:\")\n",
    "    filtered_high_tokens = manager.filter_by_criteria(df, min_tokens=30)\n",
    "    print(f\"   é«˜tokenä½¿ç”¨ (>=30): {len(filtered_high_tokens)} æ¡è®°å½•\")\n",
    "    \n",
    "    filtered_high_satisfaction = manager.filter_by_criteria(df, min_satisfaction=4.5)\n",
    "    print(f\"   é«˜æ»¡æ„åº¦ (>=4.5): {len(filtered_high_satisfaction)} æ¡è®°å½•\")\n",
    "    \n",
    "    filtered_gpt4 = manager.filter_by_criteria(df, model='gpt-4')\n",
    "    print(f\"   GPT-4æ¨¡å‹: {len(filtered_gpt4)} æ¡è®°å½•\")\n",
    "    \n",
    "    # æ±‡æ€»æŠ¥å‘Š\n",
    "    print(f\"\\n   æ±‡æ€»æŠ¥å‘Š:\")\n",
    "    summary = manager.create_summary_report(df)\n",
    "    print(f\"   ç”¨æˆ·æ±‡æ€»:\\n{summary['user_summary']}\")\n",
    "    print(f\"   æ¨¡å‹æ±‡æ€»:\\n{summary['model_summary']}\")\n",
    "\n",
    "demonstrate_data_manager()\n",
    "\n",
    "print(f\"\\nâœ… Pandasæ•°æ®å¤„ç†åŸºç¡€å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡Pandas Serieså’ŒDataFrameçš„åˆ›å»º\")\n",
    "print(f\"   âœ“ ç†è§£æ•°æ®ç´¢å¼•ã€é€‰æ‹©å’Œè¿‡æ»¤\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨åŸºç¡€æ•°æ®æ“ä½œæ–¹æ³•\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿåº”ç”¨Pandaså¤„ç†å®é™…æ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandasæ•°æ®æ¸…æ´—å’Œè½¬æ¢ [â­åŸºç¡€]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šPandasæä¾›äº†å¼ºå¤§çš„æ•°æ®æ¸…æ´—å’Œè½¬æ¢åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤„ç†ç¼ºå¤±å€¼ã€æ•°æ®ç±»å‹è½¬æ¢ã€å­—ç¬¦ä¸²æ“ä½œç­‰ã€‚æŒæ¡è¿™äº›åŠŸèƒ½å¯¹äºæ•°æ®é¢„å¤„ç†å’ŒLangChainåº”ç”¨å¼€å‘éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡ç¼ºå¤±å€¼æ£€æµ‹å’Œå¤„ç†æ–¹æ³•\n",
    "- ç†è§£æ•°æ®ç±»å‹è½¬æ¢å’ŒéªŒè¯\n",
    "- ç†Ÿç»ƒä½¿ç”¨å­—ç¬¦ä¸²æ“ä½œå’Œæ–‡æœ¬å¤„ç†\n",
    "- èƒ½å¤Ÿè¿›è¡Œæ•°æ®å»é‡å’Œå¼‚å¸¸å€¼å¤„ç†\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ•°æ®æ¸…æ´—æµç¨‹\n",
    "- è¿›è¡Œæ•°æ®è½¬æ¢å’Œæ ‡å‡†åŒ–\n",
    "- åº”ç”¨Pandasè§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„æ•°æ®æ¸…æ´—ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§¹ Pandasæ•°æ®æ¸…æ´—å’Œè½¬æ¢:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Tuple, Any, Optional, Union, Dict\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 1. ç¼ºå¤±å€¼å¤„ç†\n",
    "print(f\"ğŸ“ 1. ç¼ºå¤±å€¼å¤„ç†:\")\n",
    "\n",
    "# 1.1 ç¼ºå¤±å€¼æ£€æµ‹å’Œå¤„ç†\n",
    "print(f\"\\n   ğŸ” 1.1 ç¼ºå¤±å€¼æ£€æµ‹å’Œå¤„ç†:\")\n",
    "\n",
    "def demonstrate_missing_values():\n",
    "    \"\"\"æ¼”ç¤ºç¼ºå¤±å€¼å¤„ç†\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®\n",
    "    data = {\n",
    "        'Name': ['Alice', 'Bob', np.nan, 'David', 'Eve'],\n",
    "        'Age': [25, np.nan, 35, 28, 32],\n",
    "        'Salary': [70000, 80000, np.nan, 75000, 85000],\n",
    "        'Department': ['IT', 'HR', 'Finance', np.nan, 'IT'],\n",
    "        'Experience': [2, 5, 8, np.nan, 4]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"   åŸå§‹æ•°æ® (åŒ…å«ç¼ºå¤±å€¼):\\n{df}\")\n",
    "    \n",
    "    # ç¼ºå¤±å€¼æ£€æµ‹\n",
    "    print(f\"\\n   ç¼ºå¤±å€¼æ£€æµ‹:\")\n",
    "    print(f\"   isnull():\\n{df.isnull()}\")\n",
    "    print(f\"   ç¼ºå¤±å€¼ç»Ÿè®¡: {df.isnull().sum()}\")\n",
    "    print(f\"   ç¼ºå¤±å€¼æ¯”ä¾‹: {df.isnull().sum() / len(df) * 100}\")\n",
    "    \n",
    "    # åˆ é™¤ç¼ºå¤±å€¼\n",
    "    print(f\"\\n   åˆ é™¤ç¼ºå¤±å€¼:\")\n",
    "    df_drop_rows = df.dropna()  # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ\n",
    "    print(f\"   åˆ é™¤ç¼ºå¤±è¡Œå:\\n{df_drop_rows}\")\n",
    "    \n",
    "    df_drop_cols = df.dropna(axis=1)  # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„åˆ—\n",
    "    print(f\"   åˆ é™¤ç¼ºå¤±åˆ—å:\\n{df_drop_cols}\")\n",
    "    \n",
    "    # å¡«å……ç¼ºå¤±å€¼\n",
    "    print(f\"\\n   å¡«å……ç¼ºå¤±å€¼:\")\n",
    "    df_fill_zero = df.fillna(0)  # ç”¨0å¡«å……\n",
    "    print(f\"   ç”¨0å¡«å……:\\n{df_fill_zero}\")\n",
    "    \n",
    "    df_fill_mean = df.copy()\n",
    "    df_fill_mean['Age'] = df_fill_mean['Age'].fillna(df_fill_mean['Age'].mean())\n",
    "    df_fill_mean['Salary'] = df_fill_mean['Salary'].fillna(df_fill_mean['Salary'].mean())\n",
    "    print(f\"   ç”¨å‡å€¼å¡«å……æ•°å€¼åˆ—:\\n{df_fill_mean[['Name', 'Age', 'Salary']]}\\n\")\n",
    "    \n",
    "    # å‰å‘å¡«å……å’Œåå‘å¡«å……\n",
    "    df_ffill = df.fillna(method='ffill')  # å‰å‘å¡«å……\n",
    "    print(f\"   å‰å‘å¡«å……:\\n{df_ffill}\")\n",
    "    \n",
    "    df_bfill = df.fillna(method='bfill')  # åå‘å¡«å……\n",
    "    print(f\"   åå‘å¡«å……:\\n{df_bfill}\")\n",
    "    \n",
    "    # æ’å€¼å¡«å……\n",
    "    df_interpolate = df.copy()\n",
    "    df_interpolate['Age'] = df_interpolate['Age'].interpolate()\n",
    "    print(f\"   æ’å€¼å¡«å……Ageåˆ—:\\n{df_interpolate[['Name', 'Age']]}\")\n",
    "\n",
    "demonstrate_missing_values()\n",
    "\n",
    "# 1.2 LangChainæ•°æ®æ¸…æ´—åº”ç”¨\n",
    "print(f\"\\n   ğŸ¤– 1.2 LangChainæ•°æ®æ¸…æ´—åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class DataCleaner:\n",
    "    \"\"\"æ•°æ®æ¸…æ´—å™¨ - æ¨¡æ‹ŸLangChainçš„æ•°æ®æ¸…æ´—åŠŸèƒ½\"\"\"\n",
    "    \n",
    "    def create_messy_data(self) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºæ··ä¹±çš„æ•°æ®\"\"\"\n",
    "        data = {\n",
    "            'text': [\n",
    "                'Hello world!',\n",
    "                None,\n",
    "                '  Python programming  ',\n",
    "                '',\n",
    "                'Machine learning is awesome',\n",
    "                np.nan,\n",
    "                'Data science',\n",
    "                '  AI  ',\n",
    "                None\n",
    "            ],\n",
    "            'tokens': [15, None, 25, 0, 30, np.nan, 20, 10, None],\n",
    "            'model': ['gpt-3.5', 'gpt-4', None, 'gpt-3.5', '', 'gpt-4', None, 'gpt-3.5', ''],\n",
    "            'satisfaction': [4.5, None, 4.2, 0.0, 4.8, np.nan, 4.6, 4.1, None],\n",
    "            'timestamp': [\n",
    "                datetime.now() - timedelta(hours=2),\n",
    "                None,\n",
    "                datetime.now() - timedelta(hours=1),\n",
    "                datetime.now() - timedelta(minutes=30),\n",
    "                None,\n",
    "                datetime.now() - timedelta(minutes=15),\n",
    "                datetime.now() - timedelta(minutes=5),\n",
    "                None,\n",
    "                datetime.now()\n",
    "            ]\n",
    "        }\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def clean_text_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ¸…æ´—æ–‡æœ¬æ•°æ®\"\"\"\n",
    "        cleaned_df = df.copy()\n",
    "        \n",
    "        # å»é™¤å‰åç©ºæ ¼\n",
    "        cleaned_df['text'] = cleaned_df['text'].str.strip()\n",
    "        \n",
    "        # è½¬æ¢ä¸ºå°å†™\n",
    "        cleaned_df['text'] = cleaned_df['text'].str.lower()\n",
    "        \n",
    "        # ç§»é™¤æ ‡ç‚¹ç¬¦å· - ä¿®å¤è½¬ä¹‰å­—ç¬¦é—®é¢˜\n",
    "        cleaned_df['text'] = cleaned_df['text'].str.replace(r'[^\\\\w\\\\s]', '', regex=True)\n",
    "        \n",
    "        # ç§»é™¤ç©ºå­—ç¬¦ä¸²å’ŒNoneå€¼\n",
    "        cleaned_df = cleaned_df[cleaned_df['text'].notna() & (cleaned_df['text'] != '')]\n",
    "        \n",
    "        return cleaned_df\n",
    "    \n",
    "    def handle_missing_values(self, df: pd.DataFrame, strategy: str = 'drop') -> pd.DataFrame:\n",
    "        \"\"\"å¤„ç†ç¼ºå¤±å€¼\"\"\"\n",
    "        cleaned_df = df.copy()\n",
    "        \n",
    "        if strategy == 'drop':\n",
    "            # åˆ é™¤å…³é”®åˆ—ç¼ºå¤±çš„è¡Œ\n",
    "            cleaned_df = cleaned_df.dropna(subset=['text', 'tokens'])\n",
    "        elif strategy == 'fill':\n",
    "            # å¡«å……ç¼ºå¤±å€¼\n",
    "            cleaned_df['tokens'] = cleaned_df['tokens'].fillna(cleaned_df['tokens'].median())\n",
    "            cleaned_df['model'] = cleaned_df['model'].fillna('unknown')\n",
    "            cleaned_df['satisfaction'] = cleaned_df['satisfaction'].fillna(cleaned_df['satisfaction'].mean())\n",
    "        \n",
    "        return cleaned_df\n",
    "    \n",
    "    def validate_data_types(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"éªŒè¯å’Œè½¬æ¢æ•°æ®ç±»å‹\"\"\"\n",
    "        validated_df = df.copy()\n",
    "        \n",
    "        # ç¡®ä¿tokensæ˜¯æ•°å€¼ç±»å‹\n",
    "        validated_df['tokens'] = pd.to_numeric(validated_df['tokens'], errors='coerce')\n",
    "        \n",
    "        # ç¡®ä¿satisfactionåœ¨åˆç†èŒƒå›´å†…\n",
    "        validated_df['satisfaction'] = validated_df['satisfaction'].clip(0, 5)\n",
    "        \n",
    "        # ç¡®ä¿timestampæ˜¯datetimeç±»å‹\n",
    "        validated_df['timestamp'] = pd.to_datetime(validated_df['timestamp'], errors='coerce')\n",
    "        \n",
    "        return validated_df\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"å»é™¤é‡å¤æ•°æ®\"\"\"\n",
    "        # åŸºäºtextåˆ—å»é™¤é‡å¤\n",
    "        deduplicated_df = df.drop_duplicates(subset=['text'], keep='first')\n",
    "        return deduplicated_df\n",
    "    \n",
    "    def generate_cleaning_report(self, original_df: pd.DataFrame, cleaned_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"ç”Ÿæˆæ¸…æ´—æŠ¥å‘Š\"\"\"\n",
    "        report = {\n",
    "            'original_rows': len(original_df),\n",
    "            'cleaned_rows': len(cleaned_df),\n",
    "            'rows_removed': len(original_df) - len(cleaned_df),\n",
    "            'removal_percentage': (len(original_df) - len(cleaned_df)) / len(original_df) * 100,\n",
    "            'missing_values_before': original_df.isnull().sum().to_dict(),\n",
    "            'missing_values_after': cleaned_df.isnull().sum().to_dict(),\n",
    "            'data_types_before': original_df.dtypes.to_dict(),\n",
    "            'data_types_after': cleaned_df.dtypes.to_dict()\n",
    "        }\n",
    "        return report\n",
    "\n",
    "def demonstrate_data_cleaner():\n",
    "    \"\"\"æ¼”ç¤ºæ•°æ®æ¸…æ´—å™¨\"\"\"\n",
    "    cleaner = DataCleaner()\n",
    "    \n",
    "    # åˆ›å»ºæ··ä¹±æ•°æ®\n",
    "    print(f\"   æ•°æ®æ¸…æ´—æ¼”ç¤º:\")\n",
    "    messy_df = cleaner.create_messy_data()\n",
    "    print(f\"   åŸå§‹æ··ä¹±æ•°æ®:\\n{messy_df}\")\n",
    "    \n",
    "    # æ–‡æœ¬æ¸…æ´—\n",
    "    print(f\"\\n   æ–‡æœ¬æ¸…æ´—:\")\n",
    "    text_cleaned = cleaner.clean_text_data(messy_df)\n",
    "    print(f\"   æ–‡æœ¬æ¸…æ´—å:\\n{text_cleaned}\")\n",
    "    \n",
    "    # ç¼ºå¤±å€¼å¤„ç†\n",
    "    print(f\"\\n   ç¼ºå¤±å€¼å¤„ç†:\")\n",
    "    missing_handled = cleaner.handle_missing_values(text_cleaned, strategy='fill')\n",
    "    print(f\"   ç¼ºå¤±å€¼å¤„ç†å:\\n{missing_handled}\")\n",
    "    \n",
    "    # æ•°æ®ç±»å‹éªŒè¯\n",
    "    print(f\"\\n   æ•°æ®ç±»å‹éªŒè¯:\")\n",
    "    type_validated = cleaner.validate_data_types(missing_handled)\n",
    "    print(f\"   ç±»å‹éªŒè¯å:\\n{type_validated}\")\n",
    "    print(f\"   æ•°æ®ç±»å‹: {type_validated.dtypes.to_dict()}\")\n",
    "    \n",
    "    # å»é‡\n",
    "    print(f\"\\n   å»é‡å¤„ç†:\")\n",
    "    deduplicated = cleaner.remove_duplicates(type_validated)\n",
    "    print(f\"   å»é‡å:\\n{deduplicated}\")\n",
    "    \n",
    "    # æ¸…æ´—æŠ¥å‘Š\n",
    "    print(f\"\\n   æ¸…æ´—æŠ¥å‘Š:\")\n",
    "    report = cleaner.generate_cleaning_report(messy_df, deduplicated)\n",
    "    for key, value in report.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "demonstrate_data_cleaner()\n",
    "\n",
    "# 2. æ•°æ®ç±»å‹è½¬æ¢\n",
    "print(f\"\\nğŸ“ 2. æ•°æ®ç±»å‹è½¬æ¢:\")\n",
    "\n",
    "# 2.1 åŸºç¡€ç±»å‹è½¬æ¢\n",
    "print(f\"\\n   ğŸ”„ 2.1 åŸºç¡€ç±»å‹è½¬æ¢:\")\n",
    "\n",
    "def demonstrate_type_conversion():\n",
    "    \"\"\"æ¼”ç¤ºæ•°æ®ç±»å‹è½¬æ¢\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºæ··åˆç±»å‹æ•°æ®\n",
    "    data = {\n",
    "        'id': ['1', '2', '3', '4', '5'],\n",
    "        'price': ['$10.99', '$25.50', '$5.75', '$100.00', '$50.25'],\n",
    "        'quantity': ['10', '20', '5', '15', '8'],\n",
    "        'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],\n",
    "        'active': ['True', 'False', 'True', 'True', 'False'],\n",
    "        'percentage': ['85.5%', '92.0%', '78.5%', '88.0%', '95.5%']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"   åŸå§‹æ•°æ® (å­—ç¬¦ä¸²ç±»å‹):\\n{df}\")\n",
    "    print(f\"   åŸå§‹æ•°æ®ç±»å‹: {df.dtypes.to_dict()}\")\n",
    "    \n",
    "    # è½¬æ¢æ•°å€¼ç±»å‹\n",
    "    print(f\"\\n   æ•°å€¼ç±»å‹è½¬æ¢:\")\n",
    "    df_converted = df.copy()\n",
    "    \n",
    "    # è½¬æ¢IDä¸ºæ•´æ•°\n",
    "    df_converted['id'] = pd.to_numeric(df_converted['id'])\n",
    "    \n",
    "    # è½¬æ¢ä»·æ ¼ï¼ˆå»é™¤$ç¬¦å·ï¼‰\n",
    "    df_converted['price'] = df_converted['price'].str.replace('$', '').astype(float)\n",
    "    \n",
    "    # è½¬æ¢æ•°é‡ä¸ºæ•´æ•°\n",
    "    df_converted['quantity'] = pd.to_numeric(df_converted['quantity'])\n",
    "    \n",
    "    # è½¬æ¢ç™¾åˆ†æ¯”\n",
    "    df_converted['percentage'] = df_converted['percentage'].str.replace('%', '').astype(float) / 100\n",
    "    \n",
    "    print(f\"   è½¬æ¢åæ•°æ®:\\n{df_converted}\")\n",
    "    print(f\"   è½¬æ¢åæ•°æ®ç±»å‹: {df_converted.dtypes.to_dict()}\")\n",
    "    \n",
    "    # è½¬æ¢æ—¥æœŸç±»å‹\n",
    "    print(f\"\\n   æ—¥æœŸç±»å‹è½¬æ¢:\")\n",
    "    df_converted['date'] = pd.to_datetime(df_converted['date'])\n",
    "    print(f\"   æ—¥æœŸè½¬æ¢å: {df_converted['date'].dtype}\")\n",
    "    print(f\"   æ—¥æœŸç¤ºä¾‹: {df_converted['date'].iloc[0]} (ç±»å‹: {type(df_converted['date'].iloc[0])})\")\n",
    "    \n",
    "    # è½¬æ¢å¸ƒå°”ç±»å‹\n",
    "    print(f\"\\n   å¸ƒå°”ç±»å‹è½¬æ¢:\")\n",
    "    df_converted['active'] = df_converted['active'].astype(bool)\n",
    "    print(f\"   å¸ƒå°”è½¬æ¢å: {df_converted['active'].dtype}\")\n",
    "    print(f\"   å¸ƒå°”å€¼: {df_converted['active'].tolist()}\")\n",
    "    \n",
    "    # åˆ†ç±»æ•°æ®è½¬æ¢\n",
    "    print(f\"\\n   åˆ†ç±»æ•°æ®è½¬æ¢:\")\n",
    "    # æ·»åŠ åˆ†ç±»åˆ—\n",
    "    df_converted['category'] = ['A', 'B', 'A', 'C', 'B']\n",
    "    df_converted['category'] = df_converted['category'].astype('category')\n",
    "    print(f\"   åˆ†ç±»æ•°æ®ç±»å‹: {df_converted['category'].dtype}\")\n",
    "    print(f\"   åˆ†ç±»å€¼: {df_converted['category'].cat.categories.tolist()}\")\n",
    "    \n",
    "    # é”™è¯¯å¤„ç†\n",
    "    print(f\"\\n   é”™è¯¯å¤„ç†æ¼”ç¤º:\")\n",
    "    # åˆ›å»ºåŒ…å«é”™è¯¯çš„æ•°æ®\n",
    "    error_data = {'numbers': ['1', '2', 'abc', '4', '5']}\n",
    "    error_df = pd.DataFrame(error_data)\n",
    "    print(f\"   åŒ…å«é”™è¯¯çš„æ•°æ®: {error_df['numbers'].tolist()}\")\n",
    "    \n",
    "    # ä½¿ç”¨errors='coerce'å¤„ç†é”™è¯¯\n",
    "    error_df['numbers_cleaned'] = pd.to_numeric(error_df['numbers'], errors='coerce')\n",
    "    print(f\"   é”™è¯¯å¤„ç†å: {error_df['numbers_cleaned'].tolist()}\")\n",
    "    print(f\"   ç¼ºå¤±å€¼ä½ç½®: {error_df[error_df['numbers_cleaned'].isna()].index.tolist()}\")\n",
    "\n",
    "demonstrate_type_conversion()\n",
    "\n",
    "# 2.2 LangChainæ•°æ®è½¬æ¢åº”ç”¨\n",
    "print(f\"\\n   ğŸ”„ 2.2 LangChainæ•°æ®è½¬æ¢åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class DataTransformer:\n",
    "    \"\"\"æ•°æ®è½¬æ¢å™¨ - æ¨¡æ‹ŸLangChainçš„æ•°æ®è½¬æ¢åŠŸèƒ½\"\"\"\n",
    "    \n",
    "    def create_raw_data(self) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºåŸå§‹æ•°æ®\"\"\"\n",
    "        data = {\n",
    "            'conversation_id': ['conv_001', 'conv_002', 'conv_003', 'conv_004', 'conv_005'],\n",
    "            'user_message': [\n",
    "                'What is Python?',\n",
    "                'Explain machine learning',\n",
    "                'Help with data analysis',\n",
    "                'LangChain tutorial',\n",
    "                'AI applications'\n",
    "            ],\n",
    "            'bot_response': [\n",
    "                'Python is a programming language...',\n",
    "                'Machine learning is a subset of AI...',\n",
    "                'I can help with data analysis...',\n",
    "                'LangChain is a framework...',\n",
    "                'AI has many applications...'\n",
    "            ],\n",
    "            'tokens_used': ['25', '35', '30', '20', '28'],\n",
    "            'response_time_ms': ['1500', '2000', '1800', '1200', '1600'],\n",
    "            'satisfaction_score': ['4.5', '4.8', '4.2', '4.6', '4.4'],\n",
    "            'timestamp': ['2023-12-01 10:00:00', '2023-12-01 10:05:00', '2023-12-01 10:10:00', \n",
    "                          '2023-12-01 10:15:00', '2023-12-01 10:20:00'],\n",
    "            'cost_usd': ['$0.05', '$0.07', '$0.06', '$0.04', '$0.056'],\n",
    "            'model_version': ['gpt-3.5-turbo', 'gpt-4', 'gpt-3.5-turbo', 'gpt-4', 'gpt-3.5-turbo']\n",
    "        }\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def transform_conversation_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"è½¬æ¢å¯¹è¯æ•°æ®\"\"\"\n",
    "        transformed_df = df.copy()\n",
    "        \n",
    "        # è½¬æ¢æ•°å€¼åˆ—\n",
    "        transformed_df['tokens_used'] = pd.to_numeric(transformed_df['tokens_used'])\n",
    "        transformed_df['response_time_ms'] = pd.to_numeric(transformed_df['response_time_ms'])\n",
    "        transformed_df['satisfaction_score'] = pd.to_numeric(transformed_df['satisfaction_score'])\n",
    "        \n",
    "        # è½¬æ¢æˆæœ¬ï¼ˆå»é™¤$ç¬¦å·ï¼‰\n",
    "        transformed_df['cost_usd'] = transformed_df['cost_usd'].str.replace('$', '').astype(float)\n",
    "        \n",
    "        # è½¬æ¢æ—¶é—´æˆ³\n",
    "        transformed_df['timestamp'] = pd.to_datetime(transformed_df['timestamp'])\n",
    "        \n",
    "        # è½¬æ¢æ¨¡å‹ç‰ˆæœ¬ä¸ºåˆ†ç±»ç±»å‹\n",
    "        transformed_df['model_version'] = transformed_df['model_version'].astype('category')\n",
    "        \n",
    "        # æ·»åŠ è¡ç”Ÿç‰¹å¾\n",
    "        transformed_df['response_time_seconds'] = transformed_df['response_time_ms'] / 1000\n",
    "        transformed_df['cost_per_token'] = transformed_df['cost_usd'] / transformed_df['tokens_used']\n",
    "        transformed_df['tokens_per_second'] = transformed_df['tokens_used'] / transformed_df['response_time_seconds']\n",
    "        \n",
    "        # æ·»åŠ æ—¶é—´ç‰¹å¾\n",
    "        transformed_df['hour'] = transformed_df['timestamp'].dt.hour\n",
    "        transformed_df['day_of_week'] = transformed_df['timestamp'].dt.day_name()\n",
    "        \n",
    "        # æ–‡æœ¬ç‰¹å¾\n",
    "        transformed_df['user_message_length'] = transformed_df['user_message'].str.len()\n",
    "        transformed_df['bot_response_length'] = transformed_df['bot_response'].str.len()\n",
    "        transformed_df['user_word_count'] = transformed_df['user_message'].str.split().str.len()\n",
    "        transformed_df['bot_word_count'] = transformed_df['bot_response'].str.split().str.len()\n",
    "        \n",
    "        return transformed_df\n",
    "    \n",
    "    def categorize_performance(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ€§èƒ½åˆ†ç±»\"\"\"\n",
    "        categorized_df = df.copy()\n",
    "        \n",
    "        # å“åº”æ—¶é—´åˆ†ç±»\n",
    "        categorized_df['response_time_category'] = pd.cut(\n",
    "            categorized_df['response_time_ms'],\n",
    "            bins=[0, 1000, 1500, 2000, float('inf')],\n",
    "            labels=['Fast', 'Normal', 'Slow', 'Very Slow']\n",
    "        )\n",
    "        \n",
    "        # æ»¡æ„åº¦åˆ†ç±»\n",
    "        categorized_df['satisfaction_category'] = pd.cut(\n",
    "            categorized_df['satisfaction_score'],\n",
    "            bins=[0, 3, 4, 4.5, 5],\n",
    "            labels=['Poor', 'Average', 'Good', 'Excellent']\n",
    "        )\n",
    "        \n",
    "        # æˆæœ¬åˆ†ç±»\n",
    "        categorized_df['cost_category'] = pd.cut(\n",
    "            categorized_df['cost_usd'],\n",
    "            bins=[0, 0.04, 0.06, 0.08, float('inf')],\n",
    "            labels=['Low', 'Medium', 'High', 'Very High']\n",
    "        )\n",
    "        \n",
    "        return categorized_df\n",
    "    \n",
    "    def create_performance_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ›å»ºæ€§èƒ½æŒ‡æ ‡\"\"\"\n",
    "        metrics = {\n",
    "            'avg_response_time': df['response_time_ms'].mean(),\n",
    "            'avg_satisfaction': df['satisfaction_score'].mean(),\n",
    "            'avg_cost_per_conversation': df['cost_usd'].mean(),\n",
    "            'total_tokens_used': df['tokens_used'].sum(),\n",
    "            'avg_tokens_per_second': df['tokens_per_second'].mean(),\n",
    "            'performance_distribution': df['response_time_category'].value_counts().to_dict(),\n",
    "            'satisfaction_distribution': df['satisfaction_category'].value_counts().to_dict(),\n",
    "            'model_performance': df.groupby('model_version')['satisfaction_score'].mean().to_dict()\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "def demonstrate_data_transformer():\n",
    "    \"\"\"æ¼”ç¤ºæ•°æ®è½¬æ¢å™¨\"\"\"\n",
    "    transformer = DataTransformer()\n",
    "    \n",
    "    # åˆ›å»ºåŸå§‹æ•°æ®\n",
    "    print(f\"   æ•°æ®è½¬æ¢æ¼”ç¤º:\")\n",
    "    raw_df = transformer.create_raw_data()\n",
    "    print(f\"   åŸå§‹æ•°æ®:\\n{raw_df}\")\n",
    "    print(f\"   åŸå§‹æ•°æ®ç±»å‹: {raw_df.dtypes.to_dict()}\")\n",
    "    \n",
    "    # æ•°æ®è½¬æ¢\n",
    "    print(f\"\\n   æ•°æ®è½¬æ¢:\")\n",
    "    transformed_df = transformer.transform_conversation_data(raw_df)\n",
    "    print(f\"   è½¬æ¢åæ•°æ®ç±»å‹: {transformed_df.dtypes.to_dict()}\")\n",
    "    print(f\"   è½¬æ¢åæ•°æ® (å…³é”®åˆ—):\\n{transformed_df[['conversation_id', 'tokens_used', 'cost_usd', 'response_time_seconds']]}\\n\")\n",
    "    \n",
    "    # æ€§èƒ½åˆ†ç±»\n",
    "    print(f\"\\n   æ€§èƒ½åˆ†ç±»:\")\n",
    "    categorized_df = transformer.categorize_performance(transformed_df)\n",
    "    print(f\"   åˆ†ç±»ç»“æœ:\\n{categorized_df[['conversation_id', 'response_time_category', 'satisfaction_category', 'cost_category']]}\\n\")\n",
    "    \n",
    "    # æ€§èƒ½æŒ‡æ ‡\n",
    "    print(f\"\\n   æ€§èƒ½æŒ‡æ ‡:\")\n",
    "    metrics = transformer.create_performance_metrics(categorized_df)\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "demonstrate_data_transformer()\n",
    "\n",
    "print(f\"\\nâœ… Pandasæ•°æ®æ¸…æ´—å’Œè½¬æ¢å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡ç¼ºå¤±å€¼æ£€æµ‹å’Œå¤„ç†æ–¹æ³•\")\n",
    "print(f\"   âœ“ ç†è§£æ•°æ®ç±»å‹è½¬æ¢å’ŒéªŒè¯\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨å­—ç¬¦ä¸²æ“ä½œå’Œæ–‡æœ¬å¤„ç†\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿè¿›è¡Œæ•°æ®å»é‡å’Œå¼‚å¸¸å€¼å¤„ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandasæ•°æ®åˆ†ç»„å’Œèšåˆ [â­åŸºç¡€]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šPandasæä¾›äº†å¼ºå¤§çš„æ•°æ®åˆ†ç»„å’ŒèšåˆåŠŸèƒ½ï¼ŒåŒ…æ‹¬groupbyã€pivot_tableã€crosstabç­‰ã€‚æŒæ¡è¿™äº›åŠŸèƒ½å¯¹äºæ•°æ®åˆ†æå’ŒLangChainåº”ç”¨å¼€å‘éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡groupbyåˆ†ç»„æ“ä½œ\n",
    "- ç†è§£å„ç§èšåˆå‡½æ•°çš„ä½¿ç”¨\n",
    "- ç†Ÿç»ƒä½¿ç”¨é€è§†è¡¨å’Œäº¤å‰è¡¨\n",
    "- èƒ½å¤Ÿè¿›è¡Œå¤æ‚çš„æ•°æ®åˆ†ææ“ä½œ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ•°æ®åˆ†ææµç¨‹\n",
    "- è¿›è¡Œå¤šç»´æ•°æ®èšåˆåˆ†æ\n",
    "- åº”ç”¨Pandasè§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„æ•°æ®åˆ†æç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š Pandasæ•°æ®åˆ†ç»„å’Œèšåˆ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Any, Optional, Union, Dict\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 1. GroupByåˆ†ç»„æ“ä½œ\n",
    "print(f\"ğŸ“ 1. GroupByåˆ†ç»„æ“ä½œ:\")\n",
    "\n",
    "# 1.1 åŸºç¡€åˆ†ç»„æ“ä½œ\n",
    "print(f\"\\n   ğŸ—‚ï¸ 1.1 åŸºç¡€åˆ†ç»„æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_groupby():\n",
    "    \"\"\"æ¼”ç¤ºGroupByæ“ä½œ\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "    data = {\n",
    "        'Department': ['IT', 'HR', 'IT', 'Finance', 'HR', 'IT', 'Finance', 'IT', 'HR'],\n",
    "        'Employee': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry', 'Ivy'],\n",
    "        'Salary': [70000, 60000, 80000, 75000, 65000, 85000, 78000, 72000, 62000],\n",
    "        'Age': [25, 30, 35, 28, 32, 40, 45, 38, 29],\n",
    "        'Experience': [2, 5, 8, 3, 6, 12, 15, 10, 4],\n",
    "        'Performance': [4.5, 4.2, 4.8, 4.6, 4.3, 4.9, 4.7, 4.4, 4.1]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"   åŸå§‹æ•°æ®:\\n{df}\")\n",
    "    \n",
    "    # åŸºç¡€åˆ†ç»„\n",
    "    print(f\"\\n   æŒ‰éƒ¨é—¨åˆ†ç»„:\")\n",
    "    grouped = df.groupby('Department')\n",
    "    print(f\"   åˆ†ç»„å¯¹è±¡ç±»å‹: {type(grouped)}\")\n",
    "    print(f\"   åˆ†ç»„æ•°é‡: {len(grouped)}\")\n",
    "    print(f\"   åˆ†ç»„é”®: {list(grouped.groups.keys())}\")\n",
    "    \n",
    "    # æŸ¥çœ‹å„åˆ†ç»„\n",
    "    print(f\"\\n   å„åˆ†ç»„æ•°æ®:\")\n",
    "    for name, group in grouped:\n",
    "        print(f\"   {name}ç»„:\\n{group}\")\n",
    "    \n",
    "    # å•ä¸€èšåˆå‡½æ•°\n",
    "    print(f\"\\n   å•ä¸€èšåˆå‡½æ•°:\")\n",
    "    print(f\"   å„éƒ¨é—¨å¹³å‡è–ªèµ„:\\n{grouped['Salary'].mean()}\")\n",
    "    print(f\"   å„éƒ¨é—¨å‘˜å·¥æ•°é‡:\\n{grouped.size()}\")\n",
    "    print(f\"   å„éƒ¨é—¨è–ªèµ„æ€»å’Œ:\\n{grouped['Salary'].sum()}\")\n",
    "    print(f\"   å„éƒ¨é—¨æœ€é«˜è–ªèµ„:\\n{grouped['Salary'].max()}\")\n",
    "    \n",
    "    # å¤šä¸ªèšåˆå‡½æ•°\n",
    "    print(f\"\\n   å¤šä¸ªèšåˆå‡½æ•°:\")\n",
    "    salary_stats = grouped['Salary'].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "    print(f\"   è–ªèµ„ç»Ÿè®¡:\\n{salary_stats}\")\n",
    "    \n",
    "    # å¤šåˆ—èšåˆ\n",
    "    print(f\"\\n   å¤šåˆ—èšåˆ:\")\n",
    "    multi_agg = grouped.agg({\n",
    "        'Salary': ['mean', 'std'],\n",
    "        'Age': 'mean',\n",
    "        'Experience': ['min', 'max'],\n",
    "        'Performance': 'mean'\n",
    "    })\n",
    "    print(f\"   å¤šåˆ—èšåˆç»“æœ:\\n{multi_agg}\")\n",
    "    \n",
    "    # è‡ªå®šä¹‰èšåˆå‡½æ•°\n",
    "    print(f\"\\n   è‡ªå®šä¹‰èšåˆå‡½æ•°:\")\n",
    "    def salary_range(x):\n",
    "        return x.max() - x.min()\n",
    "    \n",
    "    custom_agg = grouped.agg({\n",
    "        'Salary': ['mean', salary_range],\n",
    "        'Performance': lambda x: x.max() - x.min()\n",
    "    })\n",
    "    print(f\"   è‡ªå®šä¹‰èšåˆ:\\n{custom_agg}\")\n",
    "    \n",
    "    # è¿‡æ»¤åˆ†ç»„\n",
    "    print(f\"\\n   è¿‡æ»¤åˆ†ç»„:\")\n",
    "    # åªä¿ç•™å¹³å‡è–ªèµ„è¶…è¿‡70000çš„éƒ¨é—¨\n",
    "    filtered_groups = grouped.filter(lambda x: x['Salary'].mean() > 70000)\n",
    "    print(f\"   é«˜è–ªèµ„éƒ¨é—¨:\\n{filtered_groups}\")\n",
    "    \n",
    "    # è½¬æ¢æ“ä½œ\n",
    "    print(f\"\\n   è½¬æ¢æ“ä½œ:\")\n",
    "    # è®¡ç®—æ¯ä¸ªå‘˜å·¥è–ªèµ„ä¸éƒ¨é—¨å¹³å‡è–ªèµ„çš„å·®å€¼\n",
    "    salary_diff = grouped['Salary'].transform(lambda x: x - x.mean())\n",
    "    df['Salary_Diff_From_Mean'] = salary_diff\n",
    "    print(f\"   è–ªèµ„å·®å€¼:\\n{df[['Department', 'Employee', 'Salary', 'Salary_Diff_From_Mean']]}\")\n",
    "\n",
    "demonstrate_groupby()\n",
    "\n",
    "# 1.2 LangChainæ•°æ®åˆ†æåº”ç”¨\n",
    "print(f\"\\n   ğŸ¤– 1.2 LangChainæ•°æ®åˆ†æåº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class ConversationAnalyzer:\n",
    "    \"\"\"å¯¹è¯åˆ†æå™¨ - æ¨¡æ‹ŸLangChainçš„å¯¹è¯æ•°æ®åˆ†æ\"\"\"\n",
    "    \n",
    "    def create_conversation_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºå¯¹è¯æ•°æ®é›†\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        conversations = []\n",
    "        users = ['user_001', 'user_002', 'user_003', 'user_004', 'user_005']\n",
    "        models = ['gpt-3.5-turbo', 'gpt-4', 'claude-2', 'llama-2']\n",
    "        categories = ['technical', 'general', 'creative', 'analytical']\n",
    "        \n",
    "        for i in range(50):\n",
    "            conv = {\n",
    "                'conversation_id': f'conv_{i:03d}',\n",
    "                'user_id': np.random.choice(users),\n",
    "                'model': np.random.choice(models),\n",
    "                'category': np.random.choice(categories),\n",
    "                'tokens_used': np.random.randint(10, 100),\n",
    "                'response_time_ms': np.random.randint(500, 3000),\n",
    "                'satisfaction_score': np.random.uniform(3.0, 5.0),\n",
    "                'cost_usd': np.random.uniform(0.01, 0.15),\n",
    "                'timestamp': datetime.now() - timedelta(minutes=np.random.randint(0, 1440)),\n",
    "                'success': np.random.choice([True, False], p=[0.9, 0.1])\n",
    "            }\n",
    "            conversations.append(conv)\n",
    "        \n",
    "        return pd.DataFrame(conversations)\n",
    "    \n",
    "    def analyze_by_user(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"æŒ‰ç”¨æˆ·åˆ†æ\"\"\"\n",
    "        user_stats = df.groupby('user_id').agg({\n",
    "            'conversation_id': 'count',\n",
    "            'tokens_used': ['sum', 'mean', 'std'],\n",
    "            'response_time_ms': 'mean',\n",
    "            'satisfaction_score': ['mean', 'min', 'max'],\n",
    "            'cost_usd': 'sum',\n",
    "            'success': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        return user_stats.to_dict()\n",
    "    \n",
    "    def analyze_by_model(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"æŒ‰æ¨¡å‹åˆ†æ\"\"\"\n",
    "        model_stats = df.groupby('model').agg({\n",
    "            'conversation_id': 'count',\n",
    "            'tokens_used': ['mean', 'sum'],\n",
    "            'response_time_ms': ['mean', 'std'],\n",
    "            'satisfaction_score': 'mean',\n",
    "            'cost_usd': ['mean', 'sum'],\n",
    "            'success': ['sum', lambda x: x.sum() / x.count()]  # æˆåŠŸç‡\n",
    "        }).round(2)\n",
    "        \n",
    "        model_stats.columns = ['count', 'avg_tokens', 'total_tokens', 'avg_response_time', \n",
    "                              'response_time_std', 'avg_satisfaction', 'avg_cost', 'total_cost', \n",
    "                              'success_count', 'success_rate']\n",
    "        \n",
    "        return model_stats.to_dict()\n",
    "    \n",
    "    def analyze_by_category(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"æŒ‰ç±»åˆ«åˆ†æ\"\"\"\n",
    "        category_stats = df.groupby('category').agg({\n",
    "            'conversation_id': 'count',\n",
    "            'tokens_used': 'mean',\n",
    "            'response_time_ms': 'mean',\n",
    "            'satisfaction_score': 'mean',\n",
    "            'cost_usd': 'mean',\n",
    "            'success': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        return category_stats.to_dict()\n",
    "    \n",
    "    def analyze_time_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†ææ—¶é—´æ¨¡å¼\"\"\"\n",
    "        df_time = df.copy()\n",
    "        df_time['hour'] = df_time['timestamp'].dt.hour\n",
    "        df_time['day_of_week'] = df_time['timestamp'].dt.day_name()\n",
    "        \n",
    "        hourly_stats = df_time.groupby('hour').agg({\n",
    "            'conversation_id': 'count',\n",
    "            'tokens_used': 'mean',\n",
    "            'satisfaction_score': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        daily_stats = df_time.groupby('day_of_week').agg({\n",
    "            'conversation_id': 'count',\n",
    "            'cost_usd': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        return {\n",
    "            'hourly_patterns': hourly_stats.to_dict(),\n",
    "            'daily_patterns': daily_stats.to_dict()\n",
    "        }\n",
    "    \n",
    "    def create_performance_report(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ›å»ºæ€§èƒ½æŠ¥å‘Š\"\"\"\n",
    "        report = {\n",
    "            'total_conversations': len(df),\n",
    "            'unique_users': df['user_id'].nunique(),\n",
    "            'total_cost': df['cost_usd'].sum(),\n",
    "            'avg_satisfaction': df['satisfaction_score'].mean(),\n",
    "            'overall_success_rate': df['success'].mean(),\n",
    "            'user_analysis': self.analyze_by_user(df),\n",
    "            'model_analysis': self.analyze_by_model(df),\n",
    "            'category_analysis': self.analyze_by_category(df),\n",
    "            'time_analysis': self.analyze_time_patterns(df)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "\n",
    "def demonstrate_conversation_analyzer():\n",
    "    \"\"\"æ¼”ç¤ºå¯¹è¯åˆ†æå™¨\"\"\"\n",
    "    analyzer = ConversationAnalyzer()\n",
    "    \n",
    "    # åˆ›å»ºå¯¹è¯æ•°æ®é›†\n",
    "    print(f\"   å¯¹è¯åˆ†ææ¼”ç¤º:\")\n",
    "    df = analyzer.create_conversation_dataset()\n",
    "    print(f\"   æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n",
    "    print(f\"   æ•°æ®é›†é¢„è§ˆ:\\n{df.head()}\")\n",
    "    \n",
    "    # ç”¨æˆ·åˆ†æ\n",
    "    print(f\"\\n   ç”¨æˆ·åˆ†æ:\")\n",
    "    user_analysis = analyzer.analyze_by_user(df)\n",
    "    for user, stats in user_analysis.items():\n",
    "        print(f\"   {user}: å¯¹è¯æ•°={stats['conversation_id']['count']}, \"\n",
    "              f\"å¹³å‡æ»¡æ„åº¦={stats['satisfaction_score']['mean']}, \"\n",
    "              f\"æ€»æˆæœ¬={stats['cost_usd']['sum']:.2f}\")\n",
    "    \n",
    "    # æ¨¡å‹åˆ†æ\n",
    "    print(f\"\\n   æ¨¡å‹åˆ†æ:\")\n",
    "    model_analysis = analyzer.analyze_by_model(df)\n",
    "    for model, stats in model_analysis.items():\n",
    "        print(f\"   {model}: å¯¹è¯æ•°={stats['count']}, \"\n",
    "              f\"å¹³å‡å“åº”æ—¶é—´={stats['avg_response_time']:.0f}ms, \"\n",
    "              f\"æˆåŠŸç‡={stats['success_rate']:.2%}\")\n",
    "    \n",
    "    # ç±»åˆ«åˆ†æ\n",
    "    print(f\"\\n   ç±»åˆ«åˆ†æ:\")\n",
    "    category_analysis = analyzer.analyze_by_category(df)\n",
    "    for category, stats in category_analysis.items():\n",
    "        print(f\"   {category}: å¯¹è¯æ•°={stats['conversation_id']}, \"\n",
    "              f\"å¹³å‡tokenä½¿ç”¨={stats['tokens_used']:.0f}, \"\n",
    "              f\"å¹³å‡æ»¡æ„åº¦={stats['satisfaction_score']:.2f}\")\n",
    "    \n",
    "    # æ—¶é—´æ¨¡å¼åˆ†æ\n",
    "    print(f\"\\n   æ—¶é—´æ¨¡å¼åˆ†æ (é«˜å³°æ—¶æ®µ):\")\n",
    "    time_analysis = analyzer.analyze_time_patterns(df)\n",
    "    hourly_patterns = time_analysis['hourly_patterns']\n",
    "    \n",
    "    # æ‰¾å‡ºå¯¹è¯é‡æœ€å¤šçš„å°æ—¶\n",
    "    max_hour = max(hourly_patterns['conversation_id']['count'].items(), key=lambda x: x[1])\n",
    "    print(f\"   æœ€ç¹å¿™æ—¶æ®µ: {max_hour[0]}:00 (å¯¹è¯æ•°: {max_hour[1]})\")\n",
    "    \n",
    "    # æ€§èƒ½æŠ¥å‘Šæ‘˜è¦\n",
    "    print(f\"\\n   æ€§èƒ½æŠ¥å‘Šæ‘˜è¦:\")\n",
    "    report = analyzer.create_performance_report(df)\n",
    "    print(f\"   æ€»å¯¹è¯æ•°: {report['total_conversations']}\")\n",
    "    print(f\"   æ´»è·ƒç”¨æˆ·æ•°: {report['unique_users']}\")\n",
    "    print(f\"   æ€»æˆæœ¬: ${report['total_cost']:.2f}\")\n",
    "    print(f\"   å¹³å‡æ»¡æ„åº¦: {report['avg_satisfaction']:.2f}\")\n",
    "    print(f\"   æ•´ä½“æˆåŠŸç‡: {report['overall_success_rate']:.2%}\")\n",
    "\n",
    "demonstrate_conversation_analyzer()\n",
    "\n",
    "# 2. é€è§†è¡¨å’Œäº¤å‰è¡¨\n",
    "print(f\"\\nğŸ“ 2. é€è§†è¡¨å’Œäº¤å‰è¡¨:\")\n",
    "\n",
    "# 2.1 é€è§†è¡¨æ“ä½œ\n",
    "print(f\"\\n   ğŸ“‹ 2.1 é€è§†è¡¨æ“ä½œ:\")\n",
    "\n",
    "def demonstrate_pivot_tables():\n",
    "    \"\"\"æ¼”ç¤ºé€è§†è¡¨æ“ä½œ\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºé”€å”®æ•°æ®\n",
    "    data = {\n",
    "        'Region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North'],\n",
    "        'Product': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'C', 'B'],\n",
    "        'Salesperson': ['John', 'Jane', 'Bob', 'Alice', 'John', 'Jane', 'Bob', 'Alice', 'John'],\n",
    "        'Sales': [100, 150, 200, 120, 80, 180, 160, 90, 140],\n",
    "        'Quantity': [10, 15, 20, 12, 8, 18, 16, 9, 14],\n",
    "        'Month': ['Jan', 'Jan', 'Jan', 'Jan', 'Feb', 'Feb', 'Feb', 'Feb', 'Mar']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"   åŸå§‹é”€å”®æ•°æ®:\\n{df}\")\n",
    "    \n",
    "    # åŸºç¡€é€è§†è¡¨\n",
    "    print(f\"\\n   åŸºç¡€é€è§†è¡¨ (æŒ‰åœ°åŒºæ±‡æ€»é”€å”®é¢):\")\n",
    "    pivot1 = pd.pivot_table(df, values='Sales', index='Region', aggfunc='sum')\n",
    "    print(f\"   {pivot1}\")\n",
    "    \n",
    "    # å¤šæŒ‡æ ‡é€è§†è¡¨\n",
    "    print(f\"\\n   å¤šæŒ‡æ ‡é€è§†è¡¨:\")\n",
    "    pivot2 = pd.pivot_table(df, values=['Sales', 'Quantity'], index='Region', aggfunc='sum')\n",
    "    print(f\"   {pivot2}\")\n",
    "    \n",
    "    # å¤šçº§ç´¢å¼•é€è§†è¡¨\n",
    "    print(f\"\\n   å¤šçº§ç´¢å¼•é€è§†è¡¨ (åœ°åŒº-äº§å“):\")\n",
    "    pivot3 = pd.pivot_table(df, values='Sales', index=['Region', 'Product'], aggfunc='sum')\n",
    "    print(f\"   {pivot3}\")\n",
    "    \n",
    "    # åˆ—é€è§†è¡¨\n",
    "    print(f\"\\n   åˆ—é€è§†è¡¨ (åœ°åŒº-æœˆä»½):\")\n",
    "    pivot4 = pd.pivot_table(df, values='Sales', index='Region', columns='Month', aggfunc='sum', fill_value=0)\n",
    "    print(f\"   {pivot4}\")\n",
    "    \n",
    "    # å¤šèšåˆå‡½æ•°\n",
    "    print(f\"\\n   å¤šèšåˆå‡½æ•°é€è§†è¡¨:\")\n",
    "    pivot5 = pd.pivot_table(df, values='Sales', index='Region', aggfunc=['sum', 'mean', 'count'])\n",
    "    print(f\"   {pivot5}\")\n",
    "    \n",
    "    # äº¤å‰è¡¨\n",
    "    print(f\"\\n   äº¤å‰è¡¨ (åœ°åŒº-äº§å“é¢‘æ¬¡):\")\n",
    "    crosstab1 = pd.crosstab(df['Region'], df['Product'])\n",
    "    print(f\"   {crosstab1}\")\n",
    "    \n",
    "    # å¸¦èšåˆçš„äº¤å‰è¡¨\n",
    "    print(f\"\\n   å¸¦èšåˆçš„äº¤å‰è¡¨ (åœ°åŒº-äº§å“å¹³å‡é”€å”®é¢):\")\n",
    "    crosstab2 = pd.crosstab(df['Region'], df['Product'], values=df['Sales'], aggfunc='mean')\n",
    "    print(f\"   {crosstab2}\")\n",
    "    \n",
    "    # ç™¾åˆ†æ¯”äº¤å‰è¡¨\n",
    "    print(f\"\\n   ç™¾åˆ†æ¯”äº¤å‰è¡¨:\")\n",
    "    crosstab3 = pd.crosstab(df['Region'], df['Product'], normalize='index')\n",
    "    print(f\"   {crosstab3.round(2)}\")\n",
    "\n",
    "demonstrate_pivot_tables()\n",
    "\n",
    "# 2.2 LangChainæ•°æ®é€è§†åº”ç”¨\n",
    "print(f\"\\n   ğŸ“Š 2.2 LangChainæ•°æ®é€è§†åº”ç”¨:\")\n",
    "\n",
    "@dataclass\n",
    "class DataPivotAnalyzer:\n",
    "    \"\"\"æ•°æ®é€è§†åˆ†æå™¨ - æ¨¡æ‹ŸLangChainçš„æ•°æ®é€è§†åˆ†æ\"\"\"\n",
    "    \n",
    "    def create_usage_data(self) -> pd.DataFrame:\n",
    "        \"\"\"åˆ›å»ºä½¿ç”¨æ•°æ®\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        data = []\n",
    "        users = ['user_001', 'user_002', 'user_003', 'user_004']\n",
    "        models = ['gpt-3.5-turbo', 'gpt-4', 'claude-2']\n",
    "        categories = ['technical', 'creative', 'business', 'education']\n",
    "        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "        \n",
    "        for i in range(100):\n",
    "            record = {\n",
    "                'user_id': np.random.choice(users),\n",
    "                'model': np.random.choice(models),\n",
    "                'category': np.random.choice(categories),\n",
    "                'month': np.random.choice(months),\n",
    "                'tokens_used': np.random.randint(20, 150),\n",
    "                'cost_usd': np.random.uniform(0.02, 0.20),\n",
    "                'satisfaction': np.random.uniform(3.5, 5.0),\n",
    "                'response_time_ms': np.random.randint(800, 2500)\n",
    "            }\n",
    "            data.append(record)\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def create_usage_pivot_tables(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"åˆ›å»ºä½¿ç”¨é€è§†è¡¨\"\"\"\n",
    "        pivot_tables = {}\n",
    "        \n",
    "        # ç”¨æˆ·-æ¨¡å‹ä½¿ç”¨é¢‘æ¬¡\n",
    "        pivot_tables['user_model_count'] = pd.pivot_table(\n",
    "            df, values='tokens_used', index='user_id', columns='model', \n",
    "            aggfunc='count', fill_value=0\n",
    "        )\n",
    "        \n",
    "        # ç”¨æˆ·-ç±»åˆ«å¹³å‡æ»¡æ„åº¦\n",
    "        pivot_tables['user_category_satisfaction'] = pd.pivot_table(\n",
    "            df, values='satisfaction', index='user_id', columns='category', \n",
    "            aggfunc='mean', fill_value=0\n",
    "        ).round(2)\n",
    "        \n",
    "        # æ¨¡å‹-æœˆä»½æ€»æˆæœ¬\n",
    "        pivot_tables['model_month_cost'] = pd.pivot_table(\n",
    "            df, values='cost_usd', index='model', columns='month', \n",
    "            aggfunc='sum', fill_value=0\n",
    "        ).round(2)\n",
    "        \n",
    "        # ç±»åˆ«-æœˆä»½å¹³å‡å“åº”æ—¶é—´\n",
    "        pivot_tables['category_month_response'] = pd.pivot_table(\n",
    "            df, values='response_time_ms', index='category', columns='month', \n",
    "            aggfunc='mean', fill_value=0\n",
    "        ).round(0)\n",
    "        \n",
    "        return pivot_tables\n",
    "    \n",
    "    def create_crosstabs(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"åˆ›å»ºäº¤å‰è¡¨\"\"\"\n",
    "        crosstabs = {}\n",
    "        \n",
    "        # ç”¨æˆ·-æ¨¡å‹ä½¿ç”¨é¢‘æ¬¡\n",
    "        crosstabs['user_model_freq'] = pd.crosstab(df['user_id'], df['model'])\n",
    "        \n",
    "        # ç”¨æˆ·-ç±»åˆ«ä½¿ç”¨é¢‘æ¬¡\n",
    "        crosstabs['user_category_freq'] = pd.crosstab(df['user_id'], df['category'])\n",
    "        \n",
    "        # æ¨¡å‹-ç±»åˆ«ä½¿ç”¨é¢‘æ¬¡\n",
    "        crosstabs['model_category_freq'] = pd.crosstab(df['model'], df['category'])\n",
    "        \n",
    "        # ç™¾åˆ†æ¯”äº¤å‰è¡¨\n",
    "        crosstabs['user_model_pct'] = pd.crosstab(\n",
    "            df['user_id'], df['model'], normalize='index'\n",
    "        ).round(2)\n",
    "        \n",
    "        return crosstabs\n",
    "    \n",
    "    def analyze_insights(self, pivot_tables: Dict[str, pd.DataFrame], \n",
    "                        crosstabs: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†ææ´å¯Ÿ\"\"\"\n",
    "        insights = {}\n",
    "        \n",
    "        # æœ€å—æ¬¢è¿çš„æ¨¡å‹\n",
    "        model_usage = pivot_tables['user_model_count'].sum().sort_values(ascending=False)\n",
    "        insights['most_popular_model'] = model_usage.index[0]\n",
    "        insights['model_usage_ranking'] = model_usage.to_dict()\n",
    "        \n",
    "        # æœ€é«˜æ•ˆçš„ç”¨æˆ·ï¼ˆæŒ‰æ»¡æ„åº¦ï¼‰\n",
    "        user_satisfaction = pivot_tables['user_category_satisfaction'].mean(axis=1).sort_values(ascending=False)\n",
    "        insights['most_satisfied_user'] = user_satisfaction.index[0]\n",
    "        insights['user_satisfaction_ranking'] = user_satisfaction.to_dict()\n",
    "        \n",
    "        # æˆæœ¬æœ€é«˜çš„æ¨¡å‹\n",
    "        model_cost = pivot_tables['model_month_cost'].sum(axis=1).sort_values(ascending=False)\n",
    "        insights['highest_cost_model'] = model_cost.index[0]\n",
    "        insights['model_cost_ranking'] = model_cost.to_dict()\n",
    "        \n",
    "        # æœ€ç¹å¿™çš„æœˆä»½\n",
    "        monthly_activity = crosstabs['user_model_freq'].sum().sum()\n",
    "        insights['busiest_month_analysis'] = \"å„æœˆä»½ä½¿ç”¨æƒ…å†µç›¸å¯¹å‡åŒ€\"\n",
    "        \n",
    "        return insights\n",
    "\n",
    "def demonstrate_data_pivot_analyzer():\n",
    "    \"\"\"æ¼”ç¤ºæ•°æ®é€è§†åˆ†æå™¨\"\"\"\n",
    "    analyzer = DataPivotAnalyzer()\n",
    "    \n",
    "    # åˆ›å»ºä½¿ç”¨æ•°æ®\n",
    "    print(f\"   æ•°æ®é€è§†åˆ†ææ¼”ç¤º:\")\n",
    "    df = analyzer.create_usage_data()\n",
    "    print(f\"   æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "    print(f\"   æ•°æ®é¢„è§ˆ:\\n{df.head()}\")\n",
    "    \n",
    "    # åˆ›å»ºé€è§†è¡¨\n",
    "    print(f\"\\n   é€è§†è¡¨åˆ†æ:\")\n",
    "    pivot_tables = analyzer.create_usage_pivot_tables(df)\n",
    "    \n",
    "    print(f\"   ç”¨æˆ·-æ¨¡å‹ä½¿ç”¨é¢‘æ¬¡:\\n{pivot_tables['user_model_count']}\")\n",
    "    print(f\"\\n   ç”¨æˆ·-ç±»åˆ«å¹³å‡æ»¡æ„åº¦:\\n{pivot_tables['user_category_satisfaction']}\")\n",
    "    print(f\"\\n   æ¨¡å‹-æœˆä»½æ€»æˆæœ¬:\\n{pivot_tables['model_month_cost']}\")\n",
    "    \n",
    "    # åˆ›å»ºäº¤å‰è¡¨\n",
    "    print(f\"\\n   äº¤å‰è¡¨åˆ†æ:\")\n",
    "    crosstabs = analyzer.create_crosstabs(df)\n",
    "    \n",
    "    print(f\"   ç”¨æˆ·-æ¨¡å‹ä½¿ç”¨é¢‘æ¬¡:\\n{crosstabs['user_model_freq']}\")\n",
    "    print(f\"\\n   ç”¨æˆ·-æ¨¡å‹ä½¿ç”¨ç™¾åˆ†æ¯”:\\n{crosstabs['user_model_pct']}\")\n",
    "    \n",
    "    # åˆ†ææ´å¯Ÿ\n",
    "    print(f\"\\n   å…³é”®æ´å¯Ÿ:\")\n",
    "    insights = analyzer.analyze_insights(pivot_tables, crosstabs)\n",
    "    for key, value in insights.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "demonstrate_data_pivot_analyzer()\n",
    "\n",
    "print(f\"\\nâœ… Pandasæ•°æ®åˆ†ç»„å’Œèšåˆå®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡groupbyåˆ†ç»„æ“ä½œ\")\n",
    "print(f\"   âœ“ ç†è§£å„ç§èšåˆå‡½æ•°çš„ä½¿ç”¨\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨é€è§†è¡¨å’Œäº¤å‰è¡¨\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿè¿›è¡Œå¤æ‚çš„æ•°æ®åˆ†ææ“ä½œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å­¦ä¹ æ€»ç»“\n",
    "\n",
    "### âœ… çŸ¥è¯†æ¸…å•è¾¾æˆæƒ…å†µéªŒè¯\n",
    "\n",
    "**5.2 Pandasæ•°æ®å¤„ç† [â­åŸºç¡€]**\n",
    "- âœ… æŒæ¡Pandas Serieså’ŒDataFrameçš„åˆ›å»ºå’Œæ“ä½œ\n",
    "- âœ… ç†è§£æ•°æ®ç´¢å¼•ã€é€‰æ‹©å’Œè¿‡æ»¤\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨æ•°æ®æ¸…æ´—å’Œè½¬æ¢æ–¹æ³•\n",
    "- âœ… èƒ½å¤Ÿè¿›è¡Œæ•°æ®åˆ†ç»„ã€èšåˆå’Œåˆå¹¶\n",
    "- âœ… æŒæ¡ç¼ºå¤±å€¼æ£€æµ‹å’Œå¤„ç†æ–¹æ³•\n",
    "- âœ… ç†è§£æ•°æ®ç±»å‹è½¬æ¢å’ŒéªŒè¯\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨å­—ç¬¦ä¸²æ“ä½œå’Œæ–‡æœ¬å¤„ç†\n",
    "- âœ… èƒ½å¤Ÿè¿›è¡Œæ•°æ®å»é‡å’Œå¼‚å¸¸å€¼å¤„ç†\n",
    "- âœ… æŒæ¡groupbyåˆ†ç»„æ“ä½œ\n",
    "- âœ… ç†è§£å„ç§èšåˆå‡½æ•°çš„ä½¿ç”¨\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨é€è§†è¡¨å’Œäº¤å‰è¡¨\n",
    "- âœ… èƒ½å¤Ÿè¿›è¡Œå¤æ‚çš„æ•°æ®åˆ†ææ“ä½œ\n",
    "- âœ… èƒ½ç‹¬ç«‹æ„å»ºé«˜æ•ˆçš„æ•°æ®å¤„ç†ç³»ç»Ÿ\n",
    "\n",
    "### ğŸ¯ ä¸LangChainå­¦ä¹ çš„å…³è”\n",
    "\n",
    "**Pandasé‡è¦æ€§**ï¼š\n",
    "- Pandasæ˜¯LangChainæ•°æ®å¤„ç†å’Œåˆ†æçš„é‡è¦å·¥å…·\n",
    "- Pandasæ”¯æŒLangChainçš„æ–‡æ¡£åŠ è½½å’Œå¤„ç†\n",
    "- Pandasç”¨äºLangChainçš„æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹\n",
    "- Pandasä¸ºLangChainçš„æœºå™¨å­¦ä¹ åŠŸèƒ½æä¾›æ•°æ®åŸºç¡€\n",
    "- Pandasæ”¯æŒLangChainçš„æ‰¹é‡æ•°æ®å¤„ç†å’Œä¼˜åŒ–\n",
    "\n",
    "**å®é™…åº”ç”¨åœºæ™¯**ï¼š\n",
    "- LangChainçš„å¯¹è¯æ•°æ®åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ\n",
    "- LangChainçš„æ–‡æ¡£æ¸…æ´—å’Œé¢„å¤„ç†\n",
    "- LangChainçš„ç”¨æˆ·è¡Œä¸ºåˆ†æå’Œæ¨¡å¼è¯†åˆ«\n",
    "- LangChainçš„æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–åˆ†æ\n",
    "- LangChainçš„æ•°æ®è´¨é‡ç®¡ç†å’ŒéªŒè¯\n",
    "\n",
    "### ğŸ“š è¿›é˜¶å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹ å»ºè®®**ï¼š\n",
    "   - æ·±å…¥ç»ƒä¹ Pandasçš„é«˜çº§ç´¢å¼•å’Œé€‰æ‹©\n",
    "   - æŒæ¡æ›´å¤šçš„æ•°æ®æ¸…æ´—å’Œè½¬æ¢æŠ€å·§\n",
    "   - ç†Ÿç»ƒä½¿ç”¨æ—¶é—´åºåˆ—æ•°æ®å¤„ç†\n",
    "\n",
    "2. **æ‰©å±•å­¦ä¹ **ï¼š\n",
    "   - å­¦ä¹ Pandasçš„æ€§èƒ½ä¼˜åŒ–æŠ€å·§\n",
    "   - äº†è§£Pandasä¸NumPyçš„é›†æˆä½¿ç”¨\n",
    "   - æ¢ç´¢Pandasçš„å¯è§†åŒ–é›†æˆåŠŸèƒ½\n",
    "\n",
    "3. **å®é™…åº”ç”¨**ï¼š\n",
    "   - æ„å»ºå®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“\n",
    "   - å¼€å‘æ•°æ®åˆ†ææŠ¥å‘Šç³»ç»Ÿ\n",
    "   - å®ç°å®æ—¶æ•°æ®ç›‘æ§ä»ªè¡¨æ¿\n",
    "\n",
    "### ğŸ”§ å¸¸è§é”™è¯¯ä¸æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **é“¾å¼ç´¢å¼•è­¦å‘Š**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šé“¾å¼ç´¢å¼•\n",
    "   df[df['Age'] > 30]['Name'] = 'Unknown'  # å¯èƒ½ä¸ç”Ÿæ•ˆ\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨locæˆ–iloc\n",
    "   df.loc[df['Age'] > 30, 'Name'] = 'Unknown'\n",
    "   ```\n",
    "\n",
    "2. **è§†å›¾vså¤åˆ¶æ··æ·†**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šè¯¯è®¤ä¸ºæ˜¯å¤åˆ¶\n",
    "   df_subset = df[df['Age'] > 30]\n",
    "   df_subset['NewColumn'] = 'value'  # å¯èƒ½å½±å“åŸDataFrame\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ˜ç¡®åˆ›å»ºå‰¯æœ¬\n",
    "   df_subset = df[df['Age'] > 30].copy()\n",
    "   df_subset['NewColumn'] = 'value'\n",
    "   ```\n",
    "\n",
    "3. **å†…å­˜ä½¿ç”¨é—®é¢˜**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåˆ›å»ºè¿‡å¤šä¸­é—´DataFrame\n",
    "   for chunk in data_chunks:\n",
    "       df = pd.DataFrame(chunk)\n",
    "       processed = process_dataframe(df)  # å†…å­˜ç´¯ç§¯\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåŠæ—¶é‡Šæ”¾å†…å­˜\n",
    "   for chunk in data_chunks:\n",
    "       df = pd.DataFrame(chunk)\n",
    "       processed = process_dataframe(df)\n",
    "       del df  # åŠæ—¶é‡Šæ”¾\n",
    "   ```\n",
    "\n",
    "4. **æ•°æ®ç±»å‹ä¸ä¸€è‡´**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå¿½ç•¥æ•°æ®ç±»å‹\n",
    "   df['price'] = df['price'].astype(str) + '$'  # å¯èƒ½å¯¼è‡´åç»­è®¡ç®—é”™è¯¯\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä¿æŒæ•°æ®ç±»å‹ä¸€è‡´æ€§\n",
    "   df['price_formatted'] = df['price'].astype(str) + '$'\n",
    "   # ä¿æŒåŸå§‹æ•°å€¼åˆ—ç”¨äºè®¡ç®—\n",
    "   ```\n",
    "\n",
    "5. **åˆ†ç»„æ“ä½œæ€§èƒ½**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šé‡å¤åˆ†ç»„\n",
    "   result1 = df.groupby('category')['value1'].mean()\n",
    "   result2 = df.groupby('category')['value2'].sum()  # é‡å¤è®¡ç®—\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä¸€æ¬¡åˆ†ç»„å¤šæ¬¡èšåˆ\n",
    "   grouped = df.groupby('category')\n",
    "   result1 = grouped['value1'].mean()\n",
    "   result2 = grouped['value2'].sum()\n",
    "   ```\n",
    "\n",
    "6. **ç¼ºå¤±å€¼å¤„ç†ä¸å½“**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šç›²ç›®åˆ é™¤ç¼ºå¤±å€¼\n",
    "   df_clean = df.dropna()  # å¯èƒ½åˆ é™¤è¿‡å¤šæ•°æ®\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ ¹æ®æƒ…å†µå¤„ç†ç¼ºå¤±å€¼\n",
    "   df_clean = df.dropna(subset=['important_column'])  # åªåˆ é™¤å…³é”®åˆ—ç¼ºå¤±\n",
    "   df_clean['numeric_column'] = df_clean['numeric_column'].fillna(df_clean['numeric_column'].median())\n",
    "   ```\n",
    "\n",
    "### ğŸŒ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "**æ•°æ®å¤„ç†ä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯\n",
    "- åˆç†ä½¿ç”¨æ•°æ®ç±»å‹å‡å°‘å†…å­˜å ç”¨\n",
    "- é¿å…é“¾å¼ç´¢å¼•ä½¿ç”¨loc/iloc\n",
    "- åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„DataFrame\n",
    "\n",
    "**å†…å­˜ä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨categoryç±»å‹å¤„ç†é‡å¤å­—ç¬¦ä¸²\n",
    "- åˆ†å—å¤„ç†å¤§æ•°æ®é›†\n",
    "- ä½¿ç”¨infer_objects()ä¼˜åŒ–æ•°æ®ç±»å‹\n",
    "- è€ƒè™‘ä½¿ç”¨daskå¤„ç†è¶…å¤§æ•°æ®\n",
    "\n",
    "**è®¡ç®—ä¼˜åŒ–**ï¼š\n",
    "- ç¼“å­˜åˆ†ç»„ç»“æœé¿å…é‡å¤è®¡ç®—\n",
    "- ä½¿ç”¨eval()è¿›è¡Œå¤æ‚è¡¨è¾¾å¼è®¡ç®—\n",
    "- åˆç†ä½¿ç”¨å¹¶è¡Œå¤„ç†\n",
    "- è€ƒè™‘ä½¿ç”¨numbaåŠ é€Ÿæ•°å€¼è®¡ç®—\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®ŒæˆPandasæ•°æ®å¤„ç†å­¦ä¹ ï¼**\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†Pandasæ•°æ®å¤„ç†çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œèƒ½å¤Ÿç†Ÿç»ƒè¿›è¡ŒSerieså’ŒDataFrameæ“ä½œã€æ•°æ®æ¸…æ´—è½¬æ¢ã€åˆ†ç»„èšåˆç­‰ï¼Œä¸ºåç»­å­¦ä¹ æ•°æ®å¯è§†åŒ–å’Œæœºå™¨å­¦ä¹ å¥ å®šäº†åšå®çš„æ•°æ®å¤„ç†åŸºç¡€ã€‚\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ é¢„å‘Š\n",
    "\n",
    "**ç»§ç»­ç¬¬äº”èŠ‚ï¼šæ•°æ®å¤„ç†**\n",
    "- 5.3 æ•°æ®å¯è§†åŒ–åŸºç¡€\n",
    "- 5.4 æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†\n",
    "- 5.5 ç»Ÿè®¡åˆ†æåº”ç”¨\n",
    "\n",
    "**åç»­ç« èŠ‚é¢„å‘Š**ï¼š\n",
    "- æœºå™¨å­¦ä¹ åŸºç¡€\n",
    "- æ—¶é—´åºåˆ—åˆ†æ\n",
    "- å¤§æ•°æ®å¤„ç†æŠ€æœ¯\n",
    "\n",
    "ç»§ç»­åŠ æ²¹ï¼Œæ•°æ®å¤„ç†æŠ€èƒ½æ­£åœ¨å¿«é€Ÿæå‡ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
