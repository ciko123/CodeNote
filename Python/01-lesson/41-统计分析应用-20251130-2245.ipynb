{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 41-ç»Ÿè®¡åˆ†æåº”ç”¨\n",
    "\n",
    "## ğŸ“š ç”¨é€”è¯´æ˜\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**ï¼š\n",
    "- æŒæ¡æè¿°æ€§ç»Ÿè®¡å’Œæ¨æ–­æ€§ç»Ÿè®¡æ–¹æ³•\n",
    "- ç†Ÿç»ƒä½¿ç”¨Pythonè¿›è¡Œå‡è®¾æ£€éªŒå’Œç½®ä¿¡åŒºé—´ä¼°è®¡\n",
    "- ç†è§£ç›¸å…³æ€§åˆ†æå’Œå›å½’åˆ†ææŠ€æœ¯\n",
    "- èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„ç»Ÿè®¡åˆ†ææµç¨‹\n",
    "\n",
    "**å‰ç½®è¦æ±‚**ï¼š\n",
    "- å·²å®Œæˆ40-æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†å­¦ä¹ \n",
    "- ç†Ÿç»ƒæŒæ¡Pandaså’ŒNumPyæ•°æ®å¤„ç†\n",
    "- äº†è§£åŸºæœ¬çš„ç»Ÿè®¡å­¦æ¦‚å¿µ\n",
    "\n",
    "**ä¸LangChainå…³è”**ï¼š\n",
    "- ç»Ÿè®¡åˆ†ææ˜¯LangChainæ•°æ®åˆ†æçš„åŸºç¡€\n",
    "- æ”¯æŒLangChainçš„ç”¨æˆ·è¡Œä¸ºåˆ†æ\n",
    "- ä¸ºLangChainçš„æœºå™¨å­¦ä¹ åŠŸèƒ½æä¾›ç»Ÿè®¡æ”¯æŒ\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„æ•°æ®æ´å¯Ÿå’Œå†³ç­–æ”¯æŒ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ çŸ¥è¯†ç‚¹è¦†ç›–\n",
    "\n",
    "### 5.5 ç»Ÿè®¡åˆ†æåº”ç”¨ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šç»Ÿè®¡åˆ†ææ˜¯æ•°æ®ç§‘å­¦çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œæ¶‰åŠæè¿°æ€§ç»Ÿè®¡ã€æ¨æ–­æ€§ç»Ÿè®¡ã€å‡è®¾æ£€éªŒç­‰ã€‚æŒæ¡è¿™äº›æŠ€èƒ½å¯¹äºLangChainåº”ç”¨å¼€å‘ä¸­çš„æ•°æ®åˆ†æéå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡æè¿°æ€§ç»Ÿè®¡æ–¹æ³•\n",
    "- ç†è§£æ¨æ–­æ€§ç»Ÿè®¡æŠ€æœ¯\n",
    "- ç†Ÿç»ƒä½¿ç”¨å‡è®¾æ£€éªŒæ–¹æ³•\n",
    "- èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„ç»Ÿè®¡åˆ†æç³»ç»Ÿ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„ç»Ÿè®¡åˆ†æåº”ç”¨ç¤ºä¾‹\n",
    "- è¿›è¡Œå¤šç§ç»Ÿè®¡æ–¹æ³•çš„æ¯”è¾ƒ\n",
    "- åº”ç”¨ç»Ÿè®¡åˆ†æè§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„ç»Ÿè®¡åˆ†æç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š ç»Ÿè®¡åˆ†æåº”ç”¨:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Any, Optional, Union, Dict\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ç»Ÿè®¡ç›¸å…³åº“\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, ttest_ind, ttest_rel, chi2_contingency, f_oneway\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"âœ… Pandasç‰ˆæœ¬: {pd.__version__}\")\n",
    "print(f\"âœ… NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"âœ… SciPyç‰ˆæœ¬: {stats.__version__}\")\n",
    "print(f\"âœ… StatsModelsç‰ˆæœ¬: {sm.__version__}\")\n",
    "\n",
    "# 1. æè¿°æ€§ç»Ÿè®¡åˆ†æ\n",
    "print(f\"\\nğŸ“ 1. æè¿°æ€§ç»Ÿè®¡åˆ†æ:\")\n",
    "\n",
    "# 1.1 åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "print(f\"\\n   ğŸ“Š 1.1 åˆ›å»ºç¤ºä¾‹æ•°æ®:\")\n",
    "\n",
    "def create_statistical_data(num_records: int = 1000):\n",
    "    \"\"\"åˆ›å»ºç”¨äºç»Ÿè®¡åˆ†æçš„ç¤ºä¾‹æ•°æ®\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # åŸºç¡€æ•°æ®\n",
    "    data = {\n",
    "        'user_id': range(1, num_records + 1),\n",
    "        'age': np.random.normal(35, 10, num_records),\n",
    "        'income': np.random.lognormal(10, 0.5, num_records),\n",
    "        'education_years': np.random.normal(16, 3, num_records),\n",
    "        'work_experience': np.random.exponential(5, num_records),\n",
    "        'satisfaction_score': np.random.uniform(1, 10, num_records),\n",
    "        'performance_rating': np.random.choice([1, 2, 3, 4, 5], num_records, p=[0.1, 0.2, 0.3, 0.25, 0.15]),\n",
    "        'department': np.random.choice(['æŠ€æœ¯', 'é”€å”®', 'å¸‚åœº', 'äººäº‹', 'è´¢åŠ¡'], num_records),\n",
    "        'gender': np.random.choice(['ç”·', 'å¥³'], num_records, p=[0.6, 0.4]),\n",
    "        'marital_status': np.random.choice(['å•èº«', 'å·²å©š', 'ç¦»å¼‚'], num_records, p=[0.3, 0.6, 0.1]),\n",
    "        'has_children': np.random.choice([True, False], num_records, p=[0.4, 0.6]),\n",
    "        'training_hours': np.random.poisson(20, num_records),\n",
    "        'projects_completed': np.random.binomial(10, 0.7, num_records),\n",
    "        'overtime_hours': np.random.exponential(2, num_records),\n",
    "        'team_size': np.random.choice([3, 5, 8, 12, 15], num_records),\n",
    "        'remote_work_days': np.random.randint(0, 5, num_records),\n",
    "        'salary': np.random.normal(60000, 15000, num_records),\n",
    "        'bonus': np.random.exponential(5000, num_records),\n",
    "        'vacation_days': np.random.randint(10, 30, num_records)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ç¡®ä¿æ•°æ®åˆç†æ€§\n",
    "    df['age'] = np.clip(df['age'], 18, 65)\n",
    "    df['education_years'] = np.clip(df['education_years'], 8, 25)\n",
    "    df['work_experience'] = np.clip(df['work_experience'], 0, 40)\n",
    "    df['salary'] = np.clip(df['salary'], 25000, 150000)\n",
    "    df['income'] = np.clip(df['income'], 20000, 500000)\n",
    "    \n",
    "    # æ·»åŠ ä¸€äº›ç›¸å…³æ€§\n",
    "    df['salary'] = df['salary'] + df['education_years'] * 2000 + df['work_experience'] * 1000\n",
    "    df['performance_rating'] = df['performance_rating'] + (df['training_hours'] / 20).astype(int)\n",
    "    df['performance_rating'] = np.clip(df['performance_rating'], 1, 5)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# åˆ›å»ºç»Ÿè®¡æ•°æ®\n",
    "stats_df = create_statistical_data(1000)\n",
    "print(f\"   åˆ›å»ºäº†åŒ…å« {len(stats_df)} è¡Œçš„ç»Ÿè®¡æ•°æ®é›†\")\n",
    "\n",
    "# 1.2 æè¿°æ€§ç»Ÿè®¡åˆ†æå™¨\n",
    "print(f\"\\n   ğŸ” 1.2 æè¿°æ€§ç»Ÿè®¡åˆ†æå™¨:\")\n",
    "\n",
    "@dataclass\n",
    "class DescriptiveStatistics:\n",
    "    \"\"\"æè¿°æ€§ç»Ÿè®¡åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.stats_results = {}\n",
    "    \n",
    "    def basic_statistics(self, columns: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"åŸºç¡€ç»Ÿè®¡æè¿°\"\"\"\n",
    "        print(f\"   è®¡ç®—åŸºç¡€ç»Ÿè®¡æè¿°...\")\n",
    "        \n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        basic_stats = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                continue\n",
    "            \n",
    "            stats_dict = {\n",
    "                'count': self.df[col].count(),\n",
    "                'mean': self.df[col].mean(),\n",
    "                'median': self.df[col].median(),\n",
    "                'mode': self.df[col].mode().iloc[0] if not self.df[col].mode().empty else None,\n",
    "                'std': self.df[col].std(),\n",
    "                'var': self.df[col].var(),\n",
    "                'min': self.df[col].min(),\n",
    "                'max': self.df[col].max(),\n",
    "                'range': self.df[col].max() - self.df[col].min(),\n",
    "                'q25': self.df[col].quantile(0.25),\n",
    "                'q75': self.df[col].quantile(0.75),\n",
    "                'iqr': self.df[col].quantile(0.75) - self.df[col].quantile(0.25),\n",
    "                'skewness': self.df[col].skew(),\n",
    "                'kurtosis': self.df[col].kurtosis(),\n",
    "                'cv': self.df[col].std() / self.df[col].mean() if self.df[col].mean() != 0 else None\n",
    "            }\n",
    "            \n",
    "            basic_stats[col] = stats_dict\n",
    "        \n",
    "        self.stats_results['basic_statistics'] = basic_stats\n",
    "        return basic_stats\n",
    "    \n",
    "    def frequency_analysis(self, columns: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"é¢‘ç‡åˆ†æ\"\"\"\n",
    "        print(f\"   è¿›è¡Œé¢‘ç‡åˆ†æ...\")\n",
    "        \n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "        \n",
    "        frequency_stats = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                continue\n",
    "            \n",
    "            value_counts = self.df[col].value_counts()\n",
    "            value_percentages = self.df[col].value_counts(normalize=True) * 100\n",
    "            \n",
    "            freq_dict = {\n",
    "                'value_counts': value_counts.to_dict(),\n",
    "                'value_percentages': value_percentages.to_dict(),\n",
    "                'unique_count': self.df[col].nunique(),\n",
    "                'most_frequent': value_counts.index[0] if not value_counts.empty else None,\n",
    "                'least_frequent': value_counts.index[-1] if not value_counts.empty else None\n",
    "            }\n",
    "            \n",
    "            frequency_stats[col] = freq_dict\n",
    "        \n",
    "        self.stats_results['frequency_analysis'] = frequency_stats\n",
    "        return frequency_stats\n",
    "    \n",
    "    def distribution_analysis(self, columns: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†å¸ƒåˆ†æ\"\"\"\n",
    "        print(f\"   è¿›è¡Œåˆ†å¸ƒåˆ†æ...\")\n",
    "        \n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        distribution_stats = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                continue\n",
    "            \n",
    "            data = self.df[col].dropna()\n",
    "            \n",
    "            # æ­£æ€æ€§æ£€éªŒ\n",
    "            shapiro_stat, shapiro_p = stats.shapiro(data[:5000]) if len(data) <= 5000 else (None, None)\n",
    "            ks_stat, ks_p = stats.kstest(data, 'norm')\n",
    "            \n",
    "            # åˆ†å¸ƒæ‹Ÿåˆ\n",
    "            try:\n",
    "                # å°è¯•æ‹Ÿåˆæ­£æ€åˆ†å¸ƒ\n",
    "                norm_params = stats.norm.fit(data)\n",
    "                # å°è¯•æ‹Ÿåˆå¯¹æ•°æ­£æ€åˆ†å¸ƒ\n",
    "                lognorm_params = stats.lognorm.fit(data)\n",
    "                # å°è¯•æ‹ŸåˆæŒ‡æ•°åˆ†å¸ƒ\n",
    "                exp_params = stats.expon.fit(data)\n",
    "            except:\n",
    "                norm_params = lognorm_params = exp_params = None\n",
    "            \n",
    "            dist_dict = {\n",
    "                'shapiro_test': {'statistic': shapiro_stat, 'p_value': shapiro_p} if shapiro_stat is not None else None,\n",
    "                'ks_test': {'statistic': ks_stat, 'p_value': ks_p},\n",
    "                'normal_fit': norm_params,\n",
    "                'lognormal_fit': lognorm_params,\n",
    "                'exponential_fit': exp_params,\n",
    "                'is_normal': shapiro_p > 0.05 if shapiro_p is not None else None\n",
    "            }\n",
    "            \n",
    "            distribution_stats[col] = dist_dict\n",
    "        \n",
    "        self.stats_results['distribution_analysis'] = distribution_stats\n",
    "        return distribution_stats\n",
    "    \n",
    "    def visualize_distributions(self, columns: List[str] = None, max_plots: int = 6):\n",
    "        \"\"\"å¯è§†åŒ–åˆ†å¸ƒ\"\"\"\n",
    "        print(f\"   å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ...\")\n",
    "        \n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()[:max_plots]\n",
    "        \n",
    "        n_cols = min(3, len(columns))\n",
    "        n_rows = (len(columns) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        \n",
    "        for i, col in enumerate(columns):\n",
    "            if n_rows == 1:\n",
    "                ax = axes[i] if n_cols > 1 else axes\n",
    "            else:\n",
    "                ax = axes[i // n_cols, i % n_cols]\n",
    "            \n",
    "            # ç›´æ–¹å›¾ + å¯†åº¦å›¾\n",
    "            data = self.df[col].dropna()\n",
    "            ax.hist(data, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            \n",
    "            # æ‹Ÿåˆæ­£æ€åˆ†å¸ƒæ›²çº¿\n",
    "            try:\n",
    "                mu, sigma = stats.norm.fit(data)\n",
    "                x = np.linspace(data.min(), data.max(), 100)\n",
    "                ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label=f'Normal(Î¼={mu:.1f}, Ïƒ={sigma:.1f})')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            ax.set_title(f'{col} åˆ†å¸ƒ', fontweight='bold')\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel('å¯†åº¦')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # éšè—å¤šä½™çš„å­å›¾\n",
    "        for i in range(len(columns), n_rows * n_cols):\n",
    "            if n_rows == 1:\n",
    "                if n_cols > 1:\n",
    "                    axes[i].set_visible(False)\n",
    "                else:\n",
    "                    axes.set_visible(False)\n",
    "            else:\n",
    "                axes[i // n_cols, i % n_cols].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_descriptive_report(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆæè¿°æ€§ç»Ÿè®¡æŠ¥å‘Š\"\"\"\n",
    "        report = \"=\" * 60 + \"\\n\"\n",
    "        report += \"æè¿°æ€§ç»Ÿè®¡åˆ†ææŠ¥å‘Š\\n\"\n",
    "        report += \"=\" * 60 + \"\\n\\n\"\n",
    "        \n",
    "        # åŸºç¡€ç»Ÿè®¡\n",
    "        if 'basic_statistics' in self.stats_results:\n",
    "            report += \"ğŸ“Š åŸºç¡€ç»Ÿè®¡ä¿¡æ¯:\\n\"\n",
    "            for col, stats in self.stats_results['basic_statistics'].items():\n",
    "                report += f\"\\n   {col}:\\n\"\n",
    "                report += f\"      æ ·æœ¬é‡: {stats['count']:,}\\n\"\n",
    "                report += f\"      å‡å€¼: {stats['mean']:.2f}\\n\"\n",
    "                report += f\"      ä¸­ä½æ•°: {stats['median']:.2f}\\n\"\n",
    "                report += f\"      æ ‡å‡†å·®: {stats['std']:.2f}\\n\"\n",
    "                report += f\"      å˜å¼‚ç³»æ•°: {stats['cv']:.3f}\\n\"\n",
    "                report += f\"      ååº¦: {stats['skewness']:.3f}\\n\"\n",
    "                report += f\"      å³°åº¦: {stats['kurtosis']:.3f}\\n\"\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # é¢‘ç‡åˆ†æ\n",
    "        if 'frequency_analysis' in self.stats_results:\n",
    "            report += \"ğŸ“ˆ é¢‘ç‡åˆ†æ:\\n\"\n",
    "            for col, stats in self.stats_results['frequency_analysis'].items():\n",
    "                report += f\"\\n   {col}:\\n\"\n",
    "                report += f\"      å”¯ä¸€å€¼æ•°é‡: {stats['unique_count']}\\n\"\n",
    "                report += f\"      æœ€é¢‘ç¹å€¼: {stats['most_frequent']}\\n\"\n",
    "                report += f\"      æœ€å°‘é¢‘å€¼: {stats['least_frequent']}\\n\"\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # åˆ†å¸ƒåˆ†æ\n",
    "        if 'distribution_analysis' in self.stats_results:\n",
    "            report += \"ğŸ“‰ åˆ†å¸ƒåˆ†æ:\\n\"\n",
    "            for col, stats in self.stats_results['distribution_analysis'].items():\n",
    "                report += f\"\\n   {col}:\\n\"\n",
    "                if stats['shapiro_test']:\n",
    "                    report += f\"      Shapiro-Wilkæ£€éªŒ: p={stats['shapiro_test']['p_value']:.4f}\\n\"\n",
    "                    report += f\"      æ˜¯å¦æ­£æ€åˆ†å¸ƒ: {'æ˜¯' if stats['is_normal'] else 'å¦'}\\n\"\n",
    "                report += f\"      K-Sæ£€éªŒ: p={stats['ks_test']['p_value']:.4f}\\n\"\n",
    "            \n",
    "        return report\n",
    "\n",
    "# æ‰§è¡Œæè¿°æ€§ç»Ÿè®¡åˆ†æ\n",
    "print(f\"\\n   ğŸ”§ æè¿°æ€§ç»Ÿè®¡åˆ†ææ¼”ç¤º:\")\n",
    "descriptive_stats = DescriptiveStatistics(stats_df)\n",
    "\n",
    "# åŸºç¡€ç»Ÿè®¡\n",
    "basic_stats = descriptive_stats.basic_statistics(['age', 'income', 'salary', 'education_years', 'work_experience'])\n",
    "print(f\"   âœ… åŸºç¡€ç»Ÿè®¡åˆ†æå®Œæˆ\")\n",
    "\n",
    "# é¢‘ç‡åˆ†æ\n",
    "frequency_stats = descriptive_stats.frequency_analysis(['department', 'gender', 'marital_status', 'performance_rating'])\n",
    "print(f\"   âœ… é¢‘ç‡åˆ†æå®Œæˆ\")\n",
    "\n",
    "# åˆ†å¸ƒåˆ†æ\n",
    "distribution_stats = descriptive_stats.distribution_analysis(['age', 'income', 'salary'])\n",
    "print(f\"   âœ… åˆ†å¸ƒåˆ†æå®Œæˆ\")\n",
    "\n",
    "# å¯è§†åŒ–åˆ†å¸ƒ\n",
    "descriptive_stats.visualize_distributions(['age', 'income', 'salary', 'education_years'])\n",
    "\n",
    "# ç”ŸæˆæŠ¥å‘Š\n",
    "print(f\"\\n{descriptive_stats.generate_descriptive_report()}\")\n",
    "\n",
    "print(f\"\\nâœ… æè¿°æ€§ç»Ÿè®¡åˆ†æå®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡æè¿°æ€§ç»Ÿè®¡æ–¹æ³•\")\n",
    "print(f\"   âœ“ ç†è§£æ•°æ®åˆ†å¸ƒç‰¹å¾\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨ç»Ÿè®¡å›¾è¡¨\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿç”Ÿæˆç»Ÿè®¡åˆ†ææŠ¥å‘Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨æ–­æ€§ç»Ÿè®¡åˆ†æ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šæ¨æ–­æ€§ç»Ÿè®¡æ˜¯ä»æ ·æœ¬æ¨æ–­æ€»ä½“ç‰¹å¾çš„é‡è¦æ–¹æ³•ï¼ŒåŒ…æ‹¬å‡è®¾æ£€éªŒã€ç½®ä¿¡åŒºé—´ç­‰ã€‚æŒæ¡è¿™äº›æŠ€èƒ½å¯¹äºæ•°æ®ç§‘å­¦å†³ç­–éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡å‡è®¾æ£€éªŒçš„åŸºæœ¬åŸç†\n",
    "- ç†è§£ç½®ä¿¡åŒºé—´ä¼°è®¡æ–¹æ³•\n",
    "- ç†Ÿç»ƒä½¿ç”¨å„ç§ç»Ÿè®¡æ£€éªŒæ–¹æ³•\n",
    "- èƒ½å¤Ÿè§£é‡Šç»Ÿè®¡æ£€éªŒç»“æœ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ¨æ–­æ€§ç»Ÿè®¡åˆ†ææµç¨‹\n",
    "- è¿›è¡Œå¤šç§å‡è®¾æ£€éªŒæ–¹æ³•çš„æ¯”è¾ƒ\n",
    "- åº”ç”¨æ¨æ–­æ€§ç»Ÿè®¡è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æ¨æ–­æ€§ç»Ÿè®¡åˆ†æç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ”¬ æ¨æ–­æ€§ç»Ÿè®¡åˆ†æ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. å‡è®¾æ£€éªŒ\n",
    "print(f\"ğŸ“ 1. å‡è®¾æ£€éªŒ:\")\n",
    "\n",
    "@dataclass\n",
    "class InferentialStatistics:\n",
    "    \"\"\"æ¨æ–­æ€§ç»Ÿè®¡åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.inference_results = {}\n",
    "        self.significance_level = 0.05\n",
    "    \n",
    "    def one_sample_t_test(self, column: str, mu0: float, alternative: str = 'two-sided') -> Dict[str, Any]:\n",
    "        \"\"\"å•æ ·æœ¬tæ£€éªŒ\"\"\"\n",
    "        print(f\"   æ‰§è¡Œå•æ ·æœ¬tæ£€éªŒ: {column} vs Î¼={mu0}\")\n",
    "        \n",
    "        data = self.df[column].dropna()\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_1samp(data, mu0, alternative=alternative)\n",
    "        \n",
    "        # è®¡ç®—ç½®ä¿¡åŒºé—´\n",
    "        mean = data.mean()\n",
    "        std_err = stats.sem(data)\n",
    "        dof = len(data) - 1\n",
    "        \n",
    "        t_critical = stats.t.ppf(1 - self.significance_level/2, dof)\n",
    "        margin_error = t_critical * std_err\n",
    "        ci_lower = mean - margin_error\n",
    "        ci_upper = mean + margin_error\n",
    "        \n",
    "        result = {\n",
    "            'test_type': 'one_sample_t_test',\n",
    "            'sample_size': len(data),\n",
    "            'sample_mean': mean,\n",
    "            'hypothesized_mean': mu0,\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'degrees_of_freedom': dof,\n",
    "            'confidence_interval': (ci_lower, ci_upper),\n",
    "            'significance_level': self.significance_level,\n",
    "            'reject_null': p_value < self.significance_level,\n",
    "            'effect_size_cohen_d': (mean - mu0) / data.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"      tç»Ÿè®¡é‡: {t_stat:.4f}\")\n",
    "        print(f\"      på€¼: {p_value:.4f}\")\n",
    "        print(f\"      ç½®ä¿¡åŒºé—´: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "        print(f\"      æ‹’ç»åŸå‡è®¾: {result['reject_null']}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def two_sample_t_test(self, column: str, group_col: str, group1: str, group2: str, \n",
    "                          equal_var: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"ä¸¤æ ·æœ¬tæ£€éªŒ\"\"\"\n",
    "        print(f\"   æ‰§è¡Œä¸¤æ ·æœ¬tæ£€éªŒ: {group1} vs {group2} ({column})\")\n",
    "        \n",
    "        group1_data = self.df[self.df[group_col] == group1][column].dropna()\n",
    "        group2_data = self.df[self.df[group_col] == group2][column].dropna()\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_ind(group1_data, group2_data, equal_var=equal_var)\n",
    "        \n",
    "        # è®¡ç®—æ•ˆåº”é‡\n",
    "        pooled_std = np.sqrt(((len(group1_data) - 1) * group1_data.var() + \n",
    "                              (len(group2_data) - 1) * group2_data.var()) / \n",
    "                             (len(group1_data) + len(group2_data) - 2))\n",
    "        effect_size = (group1_data.mean() - group2_data.mean()) / pooled_std\n",
    "        \n",
    "        result = {\n",
    "            'test_type': 'two_sample_t_test',\n",
    "            'group1_size': len(group1_data),\n",
    "            'group2_size': len(group2_data),\n",
    "            'group1_mean': group1_data.mean(),\n",
    "            'group2_mean': group2_data.mean(),\n",
    "            'group1_std': group1_data.std(),\n",
    "            'group2_std': group2_data.std(),\n",
    "            'mean_difference': group1_data.mean() - group2_data.mean(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'equal_var_assumed': equal_var,\n",
    "            'significance_level': self.significance_level,\n",
    "            'reject_null': p_value < self.significance_level,\n",
    "            'effect_size_cohen_d': effect_size\n",
    "        }\n",
    "        \n",
    "        print(f\"      ç»„1å‡å€¼: {group1_data.mean():.2f} (n={len(group1_data)})\")\n",
    "        print(f\"      ç»„2å‡å€¼: {group2_data.mean():.2f} (n={len(group2_data)})\")\n",
    "        print(f\"      tç»Ÿè®¡é‡: {t_stat:.4f}\")\n",
    "        print(f\"      på€¼: {p_value:.4f}\")\n",
    "        print(f\"      æ•ˆåº”é‡(Cohen's d): {effect_size:.3f}\")\n",
    "        print(f\"      æ‹’ç»åŸå‡è®¾: {result['reject_null']}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def paired_t_test(self, column1: str, column2: str) -> Dict[str, Any]:\n",
    "        \"\"\"é…å¯¹tæ£€éªŒ\"\"\"\n",
    "        print(f\"   æ‰§è¡Œé…å¯¹tæ£€éªŒ: {column1} vs {column2}\")\n",
    "        \n",
    "        # ç§»é™¤ç¼ºå¤±å€¼\n",
    "        paired_data = self.df[[column1, column2]].dropna()\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_rel(paired_data[column1], paired_data[column2])\n",
    "        \n",
    "        differences = paired_data[column1] - paired_data[column2]\n",
    "        effect_size = differences.mean() / differences.std()\n",
    "        \n",
    "        result = {\n",
    "            'test_type': 'paired_t_test',\n",
    "            'sample_size': len(paired_data),\n",
    "            'mean1': paired_data[column1].mean(),\n",
    "            'mean2': paired_data[column2].mean(),\n",
    "            'mean_difference': differences.mean(),\n",
    "            'std_difference': differences.std(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significance_level': self.significance_level,\n",
    "            'reject_null': p_value < self.significance_level,\n",
    "            'effect_size_cohen_d': effect_size\n",
    "        }\n",
    "        \n",
    "        print(f\"      é…å¯¹æ•°: {len(paired_data)}\")\n",
    "        print(f\"      å¹³å‡å·®å¼‚: {differences.mean():.2f}\")\n",
    "        print(f\"      tç»Ÿè®¡é‡: {t_stat:.4f}\")\n",
    "        print(f\"      på€¼: {p_value:.4f}\")\n",
    "        print(f\"      æ‹’ç»åŸå‡è®¾: {result['reject_null']}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def chi_square_test(self, col1: str, col2: str) -> Dict[str, Any]:\n",
    "        \"\"\"å¡æ–¹æ£€éªŒ\"\"\"\n",
    "        print(f\"   æ‰§è¡Œå¡æ–¹æ£€éªŒ: {col1} vs {col2}\")\n",
    "        \n",
    "        # åˆ›å»ºåˆ—è”è¡¨\n",
    "        contingency_table = pd.crosstab(self.df[col1], self.df[col2])\n",
    "        \n",
    "        chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "        \n",
    "        # è®¡ç®—Cramer's Væ•ˆåº”é‡\n",
    "        n = contingency_table.sum().sum()\n",
    "        cramers_v = np.sqrt(chi2_stat / (n * (min(contingency_table.shape) - 1)))\n",
    "        \n",
    "        result = {\n",
    "            'test_type': 'chi_square_test',\n",
    "            'contingency_table': contingency_table.to_dict(),\n",
    "            'chi2_statistic': chi2_stat,\n",
    "            'p_value': p_value,\n",
    "            'degrees_of_freedom': dof,\n",
    "            'expected_frequencies': expected.tolist(),\n",
    "            'significance_level': self.significance_level,\n",
    "            'reject_null': p_value < self.significance_level,\n",
    "            'effect_size_cramers_v': cramers_v\n",
    "        }\n",
    "        \n",
    "        print(f\"      å¡æ–¹ç»Ÿè®¡é‡: {chi2_stat:.4f}\")\n",
    "        print(f\"      på€¼: {p_value:.4f}\")\n",
    "        print(f\"      è‡ªç”±åº¦: {dof}\")\n",
    "        print(f\"      Cramer's V: {cramers_v:.3f}\")\n",
    "        print(f\"      æ‹’ç»åŸå‡è®¾: {result['reject_null']}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def anova_test(self, column: str, group_col: str) -> Dict[str, Any]:\n",
    "        \"\"\"æ–¹å·®åˆ†æ\"\"\"\n",
    "        print(f\"   æ‰§è¡Œæ–¹å·®åˆ†æ: {column} by {group_col}\")\n",
    "        \n",
    "        groups = []\n",
    "        group_names = []\n",
    "        \n",
    "        for group_name in self.df[group_col].unique():\n",
    "            group_data = self.df[self.df[group_col] == group_name][column].dropna()\n",
    "            if len(group_data) > 0:\n",
    "                groups.append(group_data)\n",
    "                group_names.append(group_name)\n",
    "        \n",
    "        f_stat, p_value = f_oneway(*groups)\n",
    "        \n",
    "        # è®¡ç®—ç»„å†…å’Œç»„é—´ç»Ÿè®¡é‡\n",
    "        group_means = [group.mean() for group in groups]\n",
    "        group_stds = [group.std() for group in groups]\n",
    "        group_sizes = [len(group) for group in groups]\n",
    "        \n",
    "        # è®¡ç®—æ•ˆåº”é‡ (eta squared)\n",
    "        total_mean = np.mean(np.concatenate(groups))\n",
    "        ss_between = sum(group_sizes[i] * (group_means[i] - total_mean)**2 for i in range(len(groups)))\n",
    "        ss_total = sum((x - total_mean)**2 for group in groups for x in group)\n",
    "        eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "        \n",
    "        result = {\n",
    "            'test_type': 'anova',\n",
    "            'group_names': group_names,\n",
    "            'group_sizes': group_sizes,\n",
    "            'group_means': group_means,\n",
    "            'group_stds': group_stds,\n",
    "            'f_statistic': f_stat,\n",
    "            'p_value': p_value,\n",
    "            'significance_level': self.significance_level,\n",
    "            'reject_null': p_value < self.significance_level,\n",
    "            'effect_size_eta_squared': eta_squared\n",
    "        }\n",
    "        \n",
    "        print(f\"      Fç»Ÿè®¡é‡: {f_stat:.4f}\")\n",
    "        print(f\"      på€¼: {p_value:.4f}\")\n",
    "        print(f\"      Î·Â²æ•ˆåº”é‡: {eta_squared:.3f}\")\n",
    "        print(f\"      æ‹’ç»åŸå‡è®¾: {result['reject_null']}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def correlation_analysis(self, columns: List[str], method: str = 'pearson') -> Dict[str, Any]:\n",
    "        \"\"\"ç›¸å…³æ€§åˆ†æ\"\"\"\n",
    "        print(f\"   æ‰§è¡Œç›¸å…³æ€§åˆ†æ: {columns} (æ–¹æ³•: {method})\")\n",
    "        \n",
    "        data = self.df[columns].dropna()\n",
    "        \n",
    "        if method == 'pearson':\n",
    "            corr_matrix = data.corr(method='pearson')\n",
    "        elif method == 'spearman':\n",
    "            corr_matrix = data.corr(method='spearman')\n",
    "        elif method == 'kendall':\n",
    "            corr_matrix = data.corr(method='kendall')\n",
    "        \n",
    "        # è®¡ç®—på€¼çŸ©é˜µ\n",
    "        p_values = pd.DataFrame(index=corr_matrix.index, columns=corr_matrix.columns)\n",
    "        \n",
    "        for i, col1 in enumerate(columns):\n",
    "            for j, col2 in enumerate(columns):\n",
    "                if i <= j:\n",
    "                    if method == 'pearson':\n",
    "                        _, p_val = stats.pearsonr(data[col1], data[col2])\n",
    "                    elif method == 'spearman':\n",
    "                        _, p_val = stats.spearmanr(data[col1], data[col2])\n",
    "                    elif method == 'kendall':\n",
    "                        _, p_val = stats.kendalltau(data[col1], data[col2])\n",
    "                    \n",
    "                    p_values.loc[col1, col2] = p_val\n",
    "                    p_values.loc[col2, col1] = p_val\n",
    "        \n",
    "        result = {\n",
    "            'test_type': 'correlation_analysis',\n",
    "            'method': method,\n",
    "            'correlation_matrix': corr_matrix.to_dict(),\n",
    "            'p_value_matrix': p_values.to_dict(),\n",
    "            'sample_size': len(data),\n",
    "            'significant_correlations': {}\n",
    "        }\n",
    "        \n",
    "        # æ‰¾å‡ºæ˜¾è‘—ç›¸å…³æ€§\n",
    "        for i, col1 in enumerate(columns):\n",
    "            for j, col2 in enumerate(columns):\n",
    "                if i < j:\n",
    "                    corr_val = corr_matrix.loc[col1, col2]\n",
    "                    p_val = p_values.loc[col1, col2]\n",
    "                    \n",
    "                    if p_val < self.significance_level:\n",
    "                        result['significant_correlations'][f'{col1}-{col2}'] = {\n",
    "                            'correlation': corr_val,\n",
    "                            'p_value': p_val,\n",
    "                            'strength': self._interpret_correlation_strength(abs(corr_val))\n",
    "                        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _interpret_correlation_strength(self, r: float) -> str:\n",
    "        \"\"\"è§£é‡Šç›¸å…³æ€§å¼ºåº¦\"\"\"\n",
    "        if r < 0.1:\n",
    "            return \"æå¼±\"\n",
    "        elif r < 0.3:\n",
    "            return \"å¼±\"\n",
    "        elif r < 0.5:\n",
    "            return \"ä¸­ç­‰\"\n",
    "        elif r < 0.7:\n",
    "            return \"å¼º\"\n",
    "        else:\n",
    "            return \"æå¼º\"\n",
    "    \n",
    "    def confidence_interval(self, column: str, confidence: float = 0.95) -> Dict[str, Any]:\n",
    "        \"\"\"ç½®ä¿¡åŒºé—´ä¼°è®¡\"\"\"\n",
    "        print(f\"   è®¡ç®—ç½®ä¿¡åŒºé—´: {column} (ç½®ä¿¡æ°´å¹³: {confidence})\")\n",
    "        \n",
    "        data = self.df[column].dropna()\n",
    "        \n",
    "        mean = data.mean()\n",
    "        std_err = stats.sem(data)\n",
    "        dof = len(data) - 1\n",
    "        \n",
    "        t_critical = stats.t.ppf((1 + confidence) / 2, dof)\n",
    "        margin_error = t_critical * std_err\n",
    "        \n",
    "        ci_lower = mean - margin_error\n",
    "        ci_upper = mean + margin_error\n",
    "        \n",
    "        result = {\n",
    "            'test_type': 'confidence_interval',\n",
    "            'sample_size': len(data),\n",
    "            'sample_mean': mean,\n",
    "            'standard_error': std_err,\n",
    "            'confidence_level': confidence,\n",
    "            'critical_value': t_critical,\n",
    "            'margin_of_error': margin_error,\n",
    "            'confidence_interval': (ci_lower, ci_upper),\n",
    "            'interval_width': ci_upper - ci_lower\n",
    "        }\n",
    "        \n",
    "        print(f\"      æ ·æœ¬å‡å€¼: {mean:.2f}\")\n",
    "        print(f\"      æ ‡å‡†è¯¯: {std_err:.4f}\")\n",
    "        print(f\"      ç½®ä¿¡åŒºé—´: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "        print(f\"      åŒºé—´å®½åº¦: {result['interval_width']:.2f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def visualize_test_results(self, test_results: Dict[str, Any]):\n",
    "        \"\"\"å¯è§†åŒ–æ£€éªŒç»“æœ\"\"\"\n",
    "        test_type = test_results.get('test_type')\n",
    "        \n",
    "        if test_type == 'correlation_analysis':\n",
    "            # ç›¸å…³æ€§çƒ­åŠ›å›¾\n",
    "            corr_matrix = pd.DataFrame(test_results['correlation_matrix'])\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                       square=True, fmt='.2f')\n",
    "            plt.title('ç›¸å…³æ€§çŸ©é˜µçƒ­åŠ›å›¾', fontweight='bold', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        elif test_type == 'anova':\n",
    "            # æ–¹å·®åˆ†æç®±çº¿å›¾\n",
    "            group_names = test_results['group_names']\n",
    "            group_means = test_results['group_means']\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # ç®±çº¿å›¾\n",
    "            group_data = []\n",
    "            for group_name in group_names:\n",
    "                # è¿™é‡Œéœ€è¦ä»åŸå§‹æ•°æ®ä¸­è·å–\n",
    "                group_data.append([np.random.normal(mean, 1, 20) for mean in group_means])\n",
    "            \n",
    "            ax1.boxplot(group_data, labels=group_names)\n",
    "            ax1.set_title('å„ç»„åˆ†å¸ƒç®±çº¿å›¾', fontweight='bold')\n",
    "            ax1.set_ylabel('æ•°å€¼')\n",
    "            \n",
    "            # å‡å€¼æ¡å½¢å›¾\n",
    "            ax2.bar(group_names, group_means, alpha=0.7)\n",
    "            ax2.set_title('å„ç»„å‡å€¼æ¯”è¾ƒ', fontweight='bold')\n",
    "            ax2.set_ylabel('å‡å€¼')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# æ¼”ç¤ºæ¨æ–­æ€§ç»Ÿè®¡åˆ†æ\n",
    "print(f\"\\n   ğŸ”§ æ¨æ–­æ€§ç»Ÿè®¡åˆ†ææ¼”ç¤º:\")\n",
    "inferential_stats = InferentialStatistics(stats_df)\n",
    "\n",
    "# 1. å•æ ·æœ¬tæ£€éªŒ\n",
    "print(f\"\\n   ğŸ“Š å•æ ·æœ¬tæ£€éªŒ:\")\n",
    "one_sample_result = inferential_stats.one_sample_t_test('salary', 55000)\n",
    "\n",
    "# 2. ä¸¤æ ·æœ¬tæ£€éªŒ\n",
    "print(f\"\\n   ğŸ“Š ä¸¤æ ·æœ¬tæ£€éªŒ:\")\n",
    "two_sample_result = inferential_stats.two_sample_t_test('salary', 'gender', 'ç”·', 'å¥³')\n",
    "\n",
    "# 3. å¡æ–¹æ£€éªŒ\n",
    "print(f\"\\n   ğŸ“Š å¡æ–¹æ£€éªŒ:\")\n",
    "chi_square_result = inferential_stats.chi_square_test('department', 'gender')\n",
    "\n",
    "# 4. æ–¹å·®åˆ†æ\n",
    "print(f\"\\n   ğŸ“Š æ–¹å·®åˆ†æ:\")\n",
    "anova_result = inferential_stats.anova_test('salary', 'department')\n",
    "\n",
    "# 5. ç›¸å…³æ€§åˆ†æ\n",
    "print(f\"\\n   ğŸ“Š ç›¸å…³æ€§åˆ†æ:\")\n",
    "correlation_result = inferential_stats.correlation_analysis(\n",
    "    ['age', 'education_years', 'work_experience', 'salary', 'performance_rating']\n",
    ")\n",
    "\n",
    "# 6. ç½®ä¿¡åŒºé—´\n",
    "print(f\"\\n   ğŸ“Š ç½®ä¿¡åŒºé—´ä¼°è®¡:\")\n",
    "ci_result = inferential_stats.confidence_interval('salary', 0.95)\n",
    "\n",
    "# å¯è§†åŒ–ç»“æœ\n",
    "print(f\"\\n   ğŸ“ˆ å¯è§†åŒ–æ£€éªŒç»“æœ:\")\n",
    "inferential_stats.visualize_test_results(correlation_result)\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "inferential_stats.inference_results = {\n",
    "    'one_sample_t_test': one_sample_result,\n",
    "    'two_sample_t_test': two_sample_result,\n",
    "    'chi_square_test': chi_square_result,\n",
    "    'anova': anova_result,\n",
    "    'correlation_analysis': correlation_result,\n",
    "    'confidence_interval': ci_result\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… æ¨æ–­æ€§ç»Ÿè®¡åˆ†æå®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡å‡è®¾æ£€éªŒçš„åŸºæœ¬åŸç†\")\n",
    "print(f\"   âœ“ ç†è§£ç½®ä¿¡åŒºé—´ä¼°è®¡æ–¹æ³•\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨å„ç§ç»Ÿè®¡æ£€éªŒæ–¹æ³•\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿè§£é‡Šç»Ÿè®¡æ£€éªŒç»“æœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å›å½’åˆ†æä¸é¢„æµ‹ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šå›å½’åˆ†ææ˜¯é¢„æµ‹å»ºæ¨¡çš„é‡è¦æ–¹æ³•ï¼ŒåŒ…æ‹¬çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ç­‰ã€‚æŒæ¡è¿™äº›æŠ€èƒ½å¯¹äºæ„å»ºé¢„æµ‹æ¨¡å‹éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡çº¿æ€§å›å½’çš„åŸºæœ¬åŸç†\n",
    "- ç†è§£å›å½’æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡\n",
    "- ç†Ÿç»ƒä½¿ç”¨å›å½’åˆ†æè¿›è¡Œé¢„æµ‹\n",
    "- èƒ½å¤Ÿè§£é‡Šå›å½’æ¨¡å‹çš„ç»“æœ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„å›å½’åˆ†ææµç¨‹\n",
    "- è¿›è¡Œå¤šç§å›å½’æ–¹æ³•çš„æ¯”è¾ƒ\n",
    "- åº”ç”¨å›å½’åˆ†æè§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„å›å½’é¢„æµ‹ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“ˆ å›å½’åˆ†æä¸é¢„æµ‹:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. çº¿æ€§å›å½’åˆ†æ\n",
    "print(f\"ğŸ“ 1. çº¿æ€§å›å½’åˆ†æ:\")\n",
    "\n",
    "@dataclass\n",
    "class RegressionAnalysis:\n",
    "    \"\"\"å›å½’åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.regression_results = {}\n",
    "        self.models = {}\n",
    "    \n",
    "    def simple_linear_regression(self, x_col: str, y_col: str) -> Dict[str, Any]:\n",
    "        \"\"\"ç®€å•çº¿æ€§å›å½’\"\"\"\n",
    "        print(f\"   æ‰§è¡Œç®€å•çº¿æ€§å›å½’: {y_col} ~ {x_col}\")\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        data = self.df[[x_col, y_col]].dropna()\n",
    "        X = data[x_col]\n",
    "        y = data[y_col]\n",
    "        \n",
    "        # æ·»åŠ å¸¸æ•°é¡¹\n",
    "        X_with_const = sm.add_constant(X)\n",
    "        \n",
    "        # æ‹Ÿåˆæ¨¡å‹\n",
    "        model = sm.OLS(y, X_with_const).fit()\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        y_pred = model.predict(X_with_const)\n",
    "        \n",
    "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "        residuals = y - y_pred\n",
    "        \n",
    "        result = {\n",
    "            'model_type': 'simple_linear_regression',\n",
    "            'dependent_variable': y_col,\n",
    "            'independent_variable': x_col,\n",
    "            'sample_size': len(data),\n",
    "            'coefficients': {\n",
    "                'intercept': model.params['const'],\n",
    "                'slope': model.params[x_col]\n",
    "            },\n",
    "            'standard_errors': {\n",
    "                'intercept': model.bse['const'],\n",
    "                'slope': model.bse[x_col]\n",
    "            },\n",
    "            'p_values': {\n",
    "                'intercept': model.pvalues['const'],\n",
    "                'slope': model.pvalues[x_col]\n",
    "            },\n",
    "            'r_squared': model.rsquared,\n",
    "            'adj_r_squared': model.rsquared_adj,\n",
    "            'f_statistic': model.fvalue,\n",
    "            'f_pvalue': model.f_pvalue,\n",
    "            'aic': model.aic,\n",
    "            'bic': model.bic,\n",
    "            'rmse': np.sqrt(np.mean(residuals**2)),\n",
    "            'mae': np.mean(np.abs(residuals)),\n",
    "            'mape': np.mean(np.abs(residuals / y)) * 100,\n",
    "            'predictions': y_pred.tolist(),\n",
    "            'residuals': residuals.tolist()\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        self.models[f'{y_col}_~_{x_col}'] = model\n",
    "        \n",
    "        print(f\"      å›å½’æ–¹ç¨‹: {y_col} = {result['coefficients']['intercept']:.2f} + {result['coefficients']['slope']:.2f} * {x_col}\")\n",
    "        print(f\"      RÂ²: {result['r_squared']:.3f}\")\n",
    "        print(f\"      RMSE: {result['rmse']:.2f}\")\n",
    "        print(f\"      æ–œç‡på€¼: {result['p_values']['slope']:.4f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def multiple_linear_regression(self, x_cols: List[str], y_col: str) -> Dict[str, Any]:\n",
    "        \"\"\"å¤šå…ƒçº¿æ€§å›å½’\"\"\"\n",
    "        print(f\"   æ‰§è¡Œå¤šå…ƒçº¿æ€§å›å½’: {y_col} ~ {', '.join(x_cols)}\")\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        all_cols = x_cols + [y_col]\n",
    "        data = self.df[all_cols].dropna()\n",
    "        X = data[x_cols]\n",
    "        y = data[y_col]\n",
    "        \n",
    "        # æ·»åŠ å¸¸æ•°é¡¹\n",
    "        X_with_const = sm.add_constant(X)\n",
    "        \n",
    "        # æ‹Ÿåˆæ¨¡å‹\n",
    "        model = sm.OLS(y, X_with_const).fit()\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        y_pred = model.predict(X_with_const)\n",
    "        \n",
    "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "        residuals = y - y_pred\n",
    "        \n",
    "        result = {\n",
    "            'model_type': 'multiple_linear_regression',\n",
    "            'dependent_variable': y_col,\n",
    "            'independent_variables': x_cols,\n",
    "            'sample_size': len(data),\n",
    "            'coefficients': model.params.to_dict(),\n",
    "            'standard_errors': model.bse.to_dict(),\n",
    "            'p_values': model.pvalues.to_dict(),\n",
    "            'confidence_intervals': model.conf_int().to_dict(),\n",
    "            'r_squared': model.rsquared,\n",
    "            'adj_r_squared': model.rsquared_adj,\n",
    "            'f_statistic': model.fvalue,\n",
    "            'f_pvalue': model.f_pvalue,\n",
    "            'aic': model.aic,\n",
    "            'bic': model.bic,\n",
    "            'rmse': np.sqrt(np.mean(residuals**2)),\n",
    "            'mae': np.mean(np.abs(residuals)),\n",
    "            'mape': np.mean(np.abs(residuals / y)) * 100,\n",
    "            'predictions': y_pred.tolist(),\n",
    "            'residuals': residuals.tolist(),\n",
    "            'significant_variables': []\n",
    "        }\n",
    "        \n",
    "        # æ‰¾å‡ºæ˜¾è‘—å˜é‡\n",
    "        for var, p_val in model.pvalues.items():\n",
    "            if var != 'const' and p_val < 0.05:\n",
    "                result['significant_variables'].append(var)\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        self.models[f'{y_col}_~_multiple'] = model\n",
    "        \n",
    "        print(f\"      RÂ²: {result['r_squared']:.3f}\")\n",
    "        print(f\"      è°ƒæ•´RÂ²: {result['adj_r_squared']:.3f}\")\n",
    "        print(f\"      RMSE: {result['rmse']:.2f}\")\n",
    "        print(f\"      æ˜¾è‘—å˜é‡: {result['significant_variables']}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def polynomial_regression(self, x_col: str, y_col: str, degree: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"å¤šé¡¹å¼å›å½’\"\"\"\n",
    "        print(f\"   æ‰§è¡Œå¤šé¡¹å¼å›å½’: {y_col} ~ {x_col}^{degree}\")\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        data = self.df[[x_col, y_col]].dropna()\n",
    "        X = data[x_col]\n",
    "        y = data[y_col]\n",
    "        \n",
    "        # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾\n",
    "        poly_features = pd.DataFrame()\n",
    "        poly_features['const'] = 1\n",
    "        \n",
    "        for i in range(1, degree + 1):\n",
    "            poly_features[f'{x_col}_{i}'] = X ** i\n",
    "        \n",
    "        # æ‹Ÿåˆæ¨¡å‹\n",
    "        model = sm.OLS(y, poly_features).fit()\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        y_pred = model.predict(poly_features)\n",
    "        \n",
    "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "        residuals = y - y_pred\n",
    "        \n",
    "        result = {\n",
    "            'model_type': 'polynomial_regression',\n",
    "            'dependent_variable': y_col,\n",
    "            'independent_variable': x_col,\n",
    "            'degree': degree,\n",
    "            'sample_size': len(data),\n",
    "            'coefficients': model.params.to_dict(),\n",
    "            'standard_errors': model.bse.to_dict(),\n",
    "            'p_values': model.pvalues.to_dict(),\n",
    "            'r_squared': model.rsquared,\n",
    "            'adj_r_squared': model.rsquared_adj,\n",
    "            'f_statistic': model.fvalue,\n",
    "            'f_pvalue': model.f_pvalue,\n",
    "            'aic': model.aic,\n",
    "            'bic': model.bic,\n",
    "            'rmse': np.sqrt(np.mean(residuals**2)),\n",
    "            'mae': np.mean(np.abs(residuals)),\n",
    "            'mape': np.mean(np.abs(residuals / y)) * 100,\n",
    "            'predictions': y_pred.tolist(),\n",
    "            'residuals': residuals.tolist()\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        self.models[f'{y_col}_~_{x_col}_poly{degree}'] = model\n",
    "        \n",
    "        print(f\"      å¤šé¡¹å¼æ–¹ç¨‹: {y_col} = \", end=\"\")\n",
    "        equation_parts = []\n",
    "        for term, coef in model.params.items():\n",
    "            if term == 'const':\n",
    "                equation_parts.append(f\"{coef:.2f}\")\n",
    "            else:\n",
    "                power = term.split('_')[-1]\n",
    "                equation_parts.append(f\"{coef:.2f}*{x_col}^{power}\")\n",
    "        print(f\" + \".join(equation_parts))\n",
    "        print(f\"      RÂ²: {result['r_squared']:.3f}\")\n",
    "        print(f\"      RMSE: {result['rmse']:.2f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def logistic_regression(self, x_cols: List[str], y_col: str) -> Dict[str, Any]:\n",
    "        \"\"\"é€»è¾‘å›å½’\"\"\"\n",
    "        print(f\"   æ‰§è¡Œé€»è¾‘å›å½’: {y_col} ~ {', '.join(x_cols)}\")\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        all_cols = x_cols + [y_col]\n",
    "        data = self.df[all_cols].dropna()\n",
    "        X = data[x_cols]\n",
    "        y = data[y_col]\n",
    "        \n",
    "        # å¦‚æœyæ˜¯åˆ†ç±»å˜é‡ï¼Œè½¬æ¢ä¸ºäºŒè¿›åˆ¶\n",
    "        if y.dtype == 'object':\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y)\n",
    "        else:\n",
    "            y_encoded = y\n",
    "        \n",
    "        # æ·»åŠ å¸¸æ•°é¡¹\n",
    "        X_with_const = sm.add_constant(X)\n",
    "        \n",
    "        # æ‹Ÿåˆæ¨¡å‹\n",
    "        try:\n",
    "            model = sm.Logit(y_encoded, X_with_const).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"      é€»è¾‘å›å½’æ‹Ÿåˆå¤±è´¥: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # é¢„æµ‹æ¦‚ç‡\n",
    "        y_pred_prob = model.predict(X_with_const)\n",
    "        y_pred_class = (y_pred_prob > 0.5).astype(int)\n",
    "        \n",
    "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "        \n",
    "        accuracy = accuracy_score(y_encoded, y_pred_class)\n",
    "        precision = precision_score(y_encoded, y_pred_class, average='weighted')\n",
    "        recall = recall_score(y_encoded, y_pred_class, average='weighted')\n",
    "        f1 = f1_score(y_encoded, y_pred_class, average='weighted')\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(y_encoded, y_pred_prob)\n",
    "        except:\n",
    "            auc = None\n",
    "        \n",
    "        result = {\n",
    "            'model_type': 'logistic_regression',\n",
    "            'dependent_variable': y_col,\n",
    "            'independent_variables': x_cols,\n",
    "            'sample_size': len(data),\n",
    "            'coefficients': model.params.to_dict(),\n",
    "            'standard_errors': model.bse.to_dict(),\n",
    "            'p_values': model.pvalues.to_dict(),\n",
    "            'confidence_intervals': model.conf_int().to_dict(),\n",
    "            'pseudo_r_squared': model.prsquared,\n",
    "            'log_likelihood': model.llf,\n",
    "            'aic': model.aic,\n",
    "            'bic': model.bic,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc,\n",
    "            'predictions_prob': y_pred_prob.tolist(),\n",
    "            'predictions_class': y_pred_class.tolist()\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        self.models[f'{y_col}_~_logistic'] = model\n",
    "        \n",
    "        print(f\"      å‡†ç¡®ç‡: {accuracy:.3f}\")\n",
    "        print(f\"      ç²¾ç¡®ç‡: {precision:.3f}\")\n",
    "        print(f\"      å¬å›ç‡: {recall:.3f}\")\n",
    "        print(f\"      F1åˆ†æ•°: {f1:.3f}\")\n",
    "        if auc:\n",
    "            print(f\"      AUC: {auc:.3f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def model_diagnostics(self, model_result: Dict[str, Any]):\n",
    "        \"\"\"æ¨¡å‹è¯Šæ–­\"\"\"\n",
    "        print(f\"   æ‰§è¡Œæ¨¡å‹è¯Šæ–­...\")\n",
    "        \n",
    "        model_type = model_result.get('model_type')\n",
    "        residuals = np.array(model_result.get('residuals', []))\n",
    "        predictions = np.array(model_result.get('predictions', []))\n",
    "        \n",
    "        if len(residuals) == 0:\n",
    "            print(f\"      æ— æ®‹å·®æ•°æ®ï¼Œè·³è¿‡è¯Šæ–­\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. æ®‹å·®vsæ‹Ÿåˆå€¼å›¾\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.scatter(predictions, residuals, alpha=0.6)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('æ‹Ÿåˆå€¼')\n",
    "        plt.ylabel('æ®‹å·®')\n",
    "        plt.title('æ®‹å·® vs æ‹Ÿåˆå€¼', fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Q-Qå›¾\n",
    "        plt.subplot(2, 2, 2)\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "        plt.title('æ®‹å·®Q-Qå›¾', fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. æ®‹å·®ç›´æ–¹å›¾\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel('æ®‹å·®')\n",
    "        plt.ylabel('é¢‘æ¬¡')\n",
    "        plt.title('æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾', fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. æ æ†å€¼vsæ ‡å‡†åŒ–æ®‹å·®\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if len(predictions) > 0:\n",
    "            # ç®€åŒ–çš„æ æ†å€¼è®¡ç®—\n",
    "            leverage = np.ones(len(predictions)) * (2 / len(predictions))  # ç®€åŒ–ç‰ˆæœ¬\n",
    "            standardized_residuals = residuals / np.std(residuals)\n",
    "            \n",
    "            plt.scatter(leverage, np.abs(standardized_residuals), alpha=0.6)\n",
    "            plt.axhline(y=2, color='r', linestyle='--', label='é˜ˆå€¼')\n",
    "            plt.xlabel('æ æ†å€¼')\n",
    "            plt.ylabel('|æ ‡å‡†åŒ–æ®‹å·®|')\n",
    "            plt.title('æ æ†å€¼ vs æ ‡å‡†åŒ–æ®‹å·®', fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('æ æ†å€¼ vs æ ‡å‡†åŒ–æ®‹å·®', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_models(self, model_results: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "        \"\"\"æ¯”è¾ƒæ¨¡å‹æ€§èƒ½\"\"\"\n",
    "        print(f\"   æ¯”è¾ƒæ¨¡å‹æ€§èƒ½...\")\n",
    "        \n",
    "        comparison_data = []\n",
    "        \n",
    "        for result in model_results:\n",
    "            if result is None:\n",
    "                continue\n",
    "            \n",
    "            model_type = result.get('model_type')\n",
    "            \n",
    "            comparison_row = {\n",
    "                'æ¨¡å‹ç±»å‹': model_type,\n",
    "                'æ ·æœ¬é‡': result.get('sample_size', 0),\n",
    "                'RÂ²': result.get('r_squared', result.get('pseudo_r_squared', None)),\n",
    "                'è°ƒæ•´RÂ²': result.get('adj_r_squared', None),\n",
    "                'RMSE': result.get('rmse', None),\n",
    "                'MAE': result.get('mae', None),\n",
    "                'MAPE': result.get('mape', None),\n",
    "                'AIC': result.get('aic', None),\n",
    "                'BIC': result.get('bic', None)\n",
    "            }\n",
    "            \n",
    "            # é€»è¾‘å›å½’çš„ç‰¹æ®ŠæŒ‡æ ‡\n",
    "            if model_type == 'logistic_regression':\n",
    "                comparison_row['å‡†ç¡®ç‡'] = result.get('accuracy', None)\n",
    "                comparison_row['ç²¾ç¡®ç‡'] = result.get('precision', None)\n",
    "                comparison_row['å¬å›ç‡'] = result.get('recall', None)\n",
    "                comparison_row['F1åˆ†æ•°'] = result.get('f1_score', None)\n",
    "                comparison_row['AUC'] = result.get('auc', None)\n",
    "            \n",
    "            comparison_data.append(comparison_row)\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # æ˜¾ç¤ºæ¯”è¾ƒè¡¨\n",
    "        print(f\"\\n   ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:\")\n",
    "        print(comparison_df.round(4).to_string(index=False))\n",
    "        \n",
    "        return comparison_df\n",
    "\n",
    "# æ¼”ç¤ºå›å½’åˆ†æ\n",
    "print(f\"\\n   ğŸ”§ å›å½’åˆ†ææ¼”ç¤º:\")\n",
    "regression_analyzer = RegressionAnalysis(stats_df)\n",
    "\n",
    "# 1. ç®€å•çº¿æ€§å›å½’\n",
    "print(f\"\\n   ğŸ“Š ç®€å•çº¿æ€§å›å½’:\")\n",
    "simple_result = regression_analyzer.simple_linear_regression('education_years', 'salary')\n",
    "\n",
    "# 2. å¤šå…ƒçº¿æ€§å›å½’\n",
    "print(f\"\\n   ğŸ“Š å¤šå…ƒçº¿æ€§å›å½’:\")\n",
    "multiple_result = regression_analyzer.multiple_linear_regression(\n",
    "    ['age', 'education_years', 'work_experience', 'performance_rating'], \n",
    "    'salary'\n",
    ")\n",
    "\n",
    "# 3. å¤šé¡¹å¼å›å½’\n",
    "print(f\"\\n   ğŸ“Š å¤šé¡¹å¼å›å½’:\")\n",
    "poly_result = regression_analyzer.polynomial_regression('education_years', 'salary', degree=2)\n",
    "\n",
    "# 4. é€»è¾‘å›å½’ï¼ˆå°†ç»©æ•ˆè¯„åˆ†è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼‰\n",
    "print(f\"\\n   ğŸ“Š é€»è¾‘å›å½’:\")\n",
    "# åˆ›å»ºäºŒåˆ†ç±»ç›®æ ‡å˜é‡\n",
    "stats_df_copy = stats_df.copy()\n",
    "stats_df_copy['high_performance'] = (stats_df_copy['performance_rating'] >= 4).astype(int)\n",
    "logistic_analyzer = RegressionAnalysis(stats_df_copy)\n",
    "logistic_result = logistic_analyzer.logistic_regression(\n",
    "    ['age', 'education_years', 'work_experience', 'training_hours'], \n",
    "    'high_performance'\n",
    ")\n",
    "\n",
    "# 5. æ¨¡å‹è¯Šæ–­\n",
    "print(f\"\\n   ğŸ” æ¨¡å‹è¯Šæ–­:\")\n",
    "regression_analyzer.model_diagnostics(simple_result)\n",
    "\n",
    "# 6. æ¨¡å‹æ¯”è¾ƒ\n",
    "print(f\"\\n   ğŸ“ˆ æ¨¡å‹æ¯”è¾ƒ:\")\n",
    "all_results = [simple_result, multiple_result, poly_result]\n",
    "if logistic_result:\n",
    "    all_results.append(logistic_result)\n",
    "\n",
    "comparison_df = regression_analyzer.compare_models(all_results)\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "regression_analyzer.regression_results = {\n",
    "    'simple_linear': simple_result,\n",
    "    'multiple_linear': multiple_result,\n",
    "    'polynomial': poly_result,\n",
    "    'logistic': logistic_result\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… å›å½’åˆ†æä¸é¢„æµ‹å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡çº¿æ€§å›å½’çš„åŸºæœ¬åŸç†\")\n",
    "print(f\"   âœ“ ç†è§£å›å½’æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨å›å½’åˆ†æè¿›è¡Œé¢„æµ‹\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿè§£é‡Šå›å½’æ¨¡å‹çš„ç»“æœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å­¦ä¹ æ€»ç»“\n",
    "\n",
    "### âœ… çŸ¥è¯†æ¸…å•è¾¾æˆæƒ…å†µéªŒè¯\n",
    "\n",
    "**5.5 ç»Ÿè®¡åˆ†æåº”ç”¨ [â­â­è¿›é˜¶]**\n",
    "- âœ… æŒæ¡æè¿°æ€§ç»Ÿè®¡æ–¹æ³•\n",
    "- âœ… ç†è§£æ•°æ®åˆ†å¸ƒç‰¹å¾\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨ç»Ÿè®¡å›¾è¡¨\n",
    "- âœ… èƒ½å¤Ÿç”Ÿæˆç»Ÿè®¡åˆ†ææŠ¥å‘Š\n",
    "- âœ… æŒæ¡å‡è®¾æ£€éªŒçš„åŸºæœ¬åŸç†\n",
    "- âœ… ç†è§£ç½®ä¿¡åŒºé—´ä¼°è®¡æ–¹æ³•\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨å„ç§ç»Ÿè®¡æ£€éªŒæ–¹æ³•\n",
    "- âœ… èƒ½å¤Ÿè§£é‡Šç»Ÿè®¡æ£€éªŒç»“æœ\n",
    "- âœ… æŒæ¡çº¿æ€§å›å½’çš„åŸºæœ¬åŸç†\n",
    "- âœ… ç†è§£å›å½’æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨å›å½’åˆ†æè¿›è¡Œé¢„æµ‹\n",
    "- âœ… èƒ½å¤Ÿè§£é‡Šå›å½’æ¨¡å‹çš„ç»“æœ\n",
    "- âœ… èƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„ç»Ÿè®¡åˆ†æç³»ç»Ÿ\n",
    "\n",
    "### ğŸ¯ ä¸LangChainå­¦ä¹ çš„å…³è”\n",
    "\n",
    "**ç»Ÿè®¡åˆ†æé‡è¦æ€§**ï¼š\n",
    "- ç»Ÿè®¡åˆ†ææ˜¯LangChainæ•°æ®åˆ†æçš„åŸºç¡€\n",
    "- æ”¯æŒLangChainçš„ç”¨æˆ·è¡Œä¸ºåˆ†æ\n",
    "- ä¸ºLangChainçš„æœºå™¨å­¦ä¹ åŠŸèƒ½æä¾›ç»Ÿè®¡æ”¯æŒ\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„æ•°æ®æ´å¯Ÿå’Œå†³ç­–æ”¯æŒ\n",
    "- ç»Ÿè®¡åˆ†ææ”¯æŒLangChainçš„NLPä»»åŠ¡è¯„ä¼°\n",
    "\n",
    "**å®é™…åº”ç”¨åœºæ™¯**ï¼š\n",
    "- LangChainçš„å¯¹è¯æ•ˆæœç»Ÿè®¡åˆ†æ\n",
    "- LangChainçš„ç”¨æˆ·æ»¡æ„åº¦è°ƒæŸ¥åˆ†æ\n",
    "- LangChainçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œæ¯”è¾ƒ\n",
    "- LangChainçš„A/Bæµ‹è¯•ç»“æœåˆ†æ\n",
    "- LangChainçš„çŸ¥è¯†åº“ä½¿ç”¨æ¨¡å¼åˆ†æ\n",
    "\n",
    "### ğŸ“š è¿›é˜¶å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹ å»ºè®®**ï¼š\n",
    "   - æ·±å…¥ç»ƒä¹ å¤šå…ƒç»Ÿè®¡æ–¹æ³•\n",
    "   - æŒæ¡æ›´å¤šé«˜çº§å›å½’æŠ€æœ¯\n",
    "   - å­¦ä¹ æ—¶é—´åºåˆ—ç»Ÿè®¡åˆ†æ\n",
    "\n",
    "2. **æ‰©å±•å­¦ä¹ **ï¼š\n",
    "   - å­¦ä¹ è´å¶æ–¯ç»Ÿè®¡åˆ†æ\n",
    "   - äº†è§£éå‚æ•°ç»Ÿè®¡æ–¹æ³•\n",
    "   - æ¢ç´¢æœºå™¨å­¦ä¹ ä¸­çš„ç»Ÿè®¡åŸºç¡€\n",
    "\n",
    "3. **å®é™…åº”ç”¨**ï¼š\n",
    "   - æ„å»ºä¼ä¸šçº§ç»Ÿè®¡åˆ†æç³»ç»Ÿ\n",
    "   - å¼€å‘è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆå·¥å…·\n",
    "   - å®ç°å®æ—¶ç»Ÿè®¡åˆ†æä»ªè¡¨æ¿\n",
    "\n",
    "### ğŸ”§ å¸¸è§é”™è¯¯ä¸æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **è¿åç»Ÿè®¡å‡è®¾**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå¿½ç•¥æ­£æ€æ€§å‡è®¾\n",
    "   t_stat, p_value = stats.ttest_1samp(data, mu0)  # æ•°æ®å¯èƒ½éæ­£æ€\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ£€æŸ¥å‡è®¾æ¡ä»¶\n",
    "   if stats.shapiro(data).pvalue > 0.05:\n",
    "       t_stat, p_value = stats.ttest_1samp(data, mu0)\n",
    "   else:\n",
    "       # ä½¿ç”¨éå‚æ•°æ£€éªŒ\n",
    "       stat, p_value = stats.wilcoxon(data - mu0)\n",
    "   ```\n",
    "\n",
    "2. **å¤šé‡æ¯”è¾ƒé—®é¢˜**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå¤šæ¬¡tæ£€éªŒä¸æ ¡æ­£\n",
    "   for group in groups:\n",
    "       ttest_ind(group1, group)  # å¤šé‡æ¯”è¾ƒ\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨ANOVA + äº‹åæ£€éªŒ\n",
    "   f_stat, p_value = f_oneway(*groups)\n",
    "   if p_value < 0.05:\n",
    "       tukey = pairwise_tukeyhsd(data, groups)\n",
    "   ```\n",
    "\n",
    "3. **ç›¸å…³æ€§ä¸å› æœæ··æ·†**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå°†ç›¸å…³æ€§è§£é‡Šä¸ºå› æœ\n",
    "   if correlation > 0.8:\n",
    "       print(\"Xå¯¼è‡´Y\")  # é”™è¯¯ç»“è®º\n",
    "   \n",
    "   # æ­£ç¡®ï¼šè°¨æ…è§£é‡Šç›¸å…³æ€§\n",
    "   if correlation > 0.8:\n",
    "       print(\"Xä¸Yå¼ºç›¸å…³ï¼Œå¯èƒ½å­˜åœ¨å…³è”å…³ç³»\")\n",
    "       print(\"éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å› æœå…³ç³»\")\n",
    "   ```\n",
    "\n",
    "4. **æ ·æœ¬ä»£è¡¨æ€§é—®é¢˜**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šå¿½ç•¥æ ·æœ¬åå·®\n",
    "   sample_mean = df['salary'].mean()\n",
    "   print(f\"æ€»ä½“å¹³å‡è–ªèµ„: {sample_mean}\")  # å¯èƒ½ä¸å‡†ç¡®\n",
    "   \n",
    "   # æ­£ç¡®ï¼šè€ƒè™‘æ ·æœ¬ä»£è¡¨æ€§\n",
    "   if is_representative_sample(df):\n",
    "       sample_mean = df['salary'].mean()\n",
    "       ci = confidence_interval(df['salary'])\n",
    "       print(f\"æ€»ä½“å¹³å‡è–ªèµ„ä¼°è®¡: {sample_mean} (95% CI: {ci})\")\n",
    "   ```\n",
    "\n",
    "5. **è¿‡æ‹Ÿåˆé—®é¢˜**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šè¿‡åº¦å¤æ‚çš„æ¨¡å‹\n",
    "   model = sm.OLS(y, sm.add_constant(X_poly_10)).fit()  # 10æ¬¡å¤šé¡¹å¼\n",
    "   \n",
    "   # æ­£ç¡®ï¼šä½¿ç”¨äº¤å‰éªŒè¯å’Œä¿¡æ¯å‡†åˆ™\n",
    "   best_model = None\n",
    "   best_aic = float('inf')\n",
    "   for degree in range(1, 5):\n",
    "       model = fit_polynomial(X, y, degree)\n",
    "       if model.aic < best_aic:\n",
    "           best_model = model\n",
    "           best_aic = model.aic\n",
    "   ```\n",
    "\n",
    "6. **å¿½ç•¥æ•ˆåº”é‡**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šåªå…³æ³¨på€¼\n",
    "   if p_value < 0.05:\n",
    "       print(\"æ˜¾è‘—å·®å¼‚\")  # ä½†å¯èƒ½æ•ˆåº”é‡å¾ˆå°\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåŒæ—¶è€ƒè™‘på€¼å’Œæ•ˆåº”é‡\n",
    "   if p_value < 0.05 and effect_size > 0.5:\n",
    "       print(\"ç»Ÿè®¡æ˜¾è‘—ä¸”å®é™…æ„ä¹‰å¤§\")\n",
    "   elif p_value < 0.05:\n",
    "       print(\"ç»Ÿè®¡æ˜¾è‘—ä½†æ•ˆåº”é‡å°\")\n",
    "   ```\n",
    "\n",
    "### ğŸŒ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "**ç»Ÿè®¡åˆ†ææ€§èƒ½ä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨å‘é‡åŒ–æ“ä½œæé«˜è®¡ç®—æ•ˆç‡\n",
    "- åˆç†ä½¿ç”¨é‡‡æ ·æŠ€æœ¯å¤„ç†å¤§æ•°æ®é›†\n",
    "- é‡‡ç”¨å¹¶è¡Œè®¡ç®—è¿›è¡Œé‡å¤æ€§åˆ†æ\n",
    "- å®ç°å¢é‡å¼ç»Ÿè®¡æ›´æ–°ç­–ç•¥\n",
    "\n",
    "**åˆ†ææµç¨‹ä¼˜åŒ–**ï¼š\n",
    "- å»ºç«‹æ ‡å‡†åŒ–çš„åˆ†ææ¨¡æ¿\n",
    "- å®ç°è‡ªåŠ¨åŒ–çš„å‡è®¾æ£€éªŒæµç¨‹\n",
    "- æ·»åŠ è¯¦ç»†çš„ç»Ÿè®¡è¯Šæ–­å’ŒéªŒè¯\n",
    "- è®¾è®¡çµæ´»çš„ç»“æœè§£é‡Šå’ŒæŠ¥å‘Šç³»ç»Ÿ\n",
    "\n",
    "**è´¨é‡ä¿è¯ä¼˜åŒ–**ï¼š\n",
    "- å®ç°ç»Ÿè®¡å‡è®¾çš„è‡ªåŠ¨æ£€æŸ¥\n",
    "- å»ºç«‹å¤šé‡æ¯”è¾ƒæ ¡æ­£æœºåˆ¶\n",
    "- è®¾è®¡æ•ˆåº”é‡å’Œå®ç”¨æ„ä¹‰è¯„ä¼°\n",
    "- å®ç°åˆ†æç»“æœçš„å¯é‡ç°æ€§éªŒè¯\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆç»Ÿè®¡åˆ†æåº”ç”¨å­¦ä¹ ï¼**\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†ç»Ÿè®¡åˆ†æçš„æ ¸å¿ƒæŠ€èƒ½ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¿›è¡Œæè¿°æ€§ç»Ÿè®¡ã€æ¨æ–­æ€§ç»Ÿè®¡å’Œå›å½’åˆ†æï¼Œä¸ºLangChainåº”ç”¨å¼€å‘æä¾›äº†å¼ºå¤§çš„æ•°æ®æ´å¯Ÿèƒ½åŠ›ã€‚\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ é¢„å‘Š\n",
    "\n",
    "**ç»§ç»­ç¬¬äº”èŠ‚ï¼šæ•°æ®å¤„ç†**\n",
    "- 5.6 æœºå™¨å­¦ä¹ åŸºç¡€\n",
    "- 5.7 æ—¶é—´åºåˆ—åˆ†æ\n",
    "- 5.8 å¤§æ•°æ®å¤„ç†æŠ€æœ¯\n",
    "\n",
    "**åç»­ç« èŠ‚é¢„å‘Š**ï¼š\n",
    "- æ•°æ®å·¥ç¨‹å®è·µ\n",
    "- å¼‚æ­¥ç¼–ç¨‹æŠ€æœ¯\n",
    "- Webå¼€å‘æŠ€æœ¯\n",
    "\n",
    "ç»§ç»­åŠ æ²¹ï¼Œæ•°æ®åˆ†ææŠ€èƒ½æ­£åœ¨å¿«é€Ÿæå‡ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
