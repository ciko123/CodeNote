{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 45-æ•°æ®å·¥ç¨‹å®è·µ\n",
    "\n",
    "## ğŸ“š ç”¨é€”è¯´æ˜\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**ï¼š\n",
    "- æŒæ¡æ•°æ®å·¥ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µå’Œæ¶æ„è®¾è®¡\n",
    "- ç†Ÿç»ƒæ„å»ºETLæ•°æ®ç®¡é“å’Œå·¥ä½œæµ\n",
    "- ç†è§£æ•°æ®è´¨é‡ç®¡ç†å’Œç›‘æ§ä½“ç³»\n",
    "- èƒ½å¤Ÿå®ç°ä¼ä¸šçº§æ•°æ®å·¥ç¨‹è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "**å‰ç½®è¦æ±‚**ï¼š\n",
    "- å·²å®Œæˆ44-å¤§æ•°æ®å¤„ç†æŠ€æœ¯å­¦ä¹ \n",
    "- ç†Ÿç»ƒæŒæ¡Pythonæ•°æ®å¤„ç†å’Œå¤§æ•°æ®æŠ€æœ¯\n",
    "- äº†è§£åŸºæœ¬çš„è½¯ä»¶å·¥ç¨‹å’Œç³»ç»Ÿè®¾è®¡æ¦‚å¿µ\n",
    "\n",
    "**ä¸LangChainå…³è”**ï¼š\n",
    "- æ•°æ®å·¥ç¨‹æ”¯æŒLangChainçš„æ•°æ®ç®¡é“æ„å»º\n",
    "- ä¸ºLangChainçš„ä¼ä¸šçº§éƒ¨ç½²æä¾›æ•°æ®åŸºç¡€\n",
    "- æ”¯æŒLangChainçš„æ•°æ®æ²»ç†å’Œè´¨é‡ä¿è¯\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„æ•°æ®å¯é æ€§å’Œä¸€è‡´æ€§\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ çŸ¥è¯†ç‚¹è¦†ç›–\n",
    "\n",
    "### 5.9 æ•°æ®å·¥ç¨‹å®è·µ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šæ•°æ®å·¥ç¨‹å®è·µæ˜¯æ„å»ºå¯é æ•°æ®ç³»ç»Ÿçš„æ ¸å¿ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬ETLç®¡é“ã€æ•°æ®è´¨é‡ã€å·¥ä½œæµè°ƒåº¦ã€ç›‘æ§å‘Šè­¦ç­‰ã€‚æŒæ¡è¿™äº›æŠ€èƒ½å¯¹äºæ„å»ºç”Ÿäº§çº§LangChainåº”ç”¨éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡æ•°æ®å·¥ç¨‹çš„åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„\n",
    "- ç†è§£ETLç®¡é“çš„è®¾è®¡å’Œå®ç°\n",
    "- ç†Ÿç»ƒä½¿ç”¨æ•°æ®è´¨é‡ç®¡ç†å·¥å…·\n",
    "- èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„æ•°æ®å·¥ç¨‹è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ•°æ®å·¥ç¨‹æµç¨‹\n",
    "- è¿›è¡Œæ•°æ®è´¨é‡çš„å…¨é¢ç®¡ç†\n",
    "- åº”ç”¨æ•°æ®å·¥ç¨‹è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æ•°æ®å·¥ç¨‹ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ—ï¸ æ•°æ®å·¥ç¨‹å®è·µ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Any, Optional, Union, Dict, Callable, Iterator\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import threading\n",
    "import queue\n",
    "import logging\n",
    "import sqlite3\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import pickle\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "import itertools\n",
    "import collections\n",
    "from functools import wraps\n",
    "import schedule\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"âœ… Pythonç‰ˆæœ¬: {pd.__version__}\")\n",
    "print(f\"âœ… NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"âœ… å½“å‰æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# 1. æ•°æ®å·¥ç¨‹åŸºç¡€æ¦‚å¿µ\n",
    "print(f\"\\nğŸ“ 1. æ•°æ®å·¥ç¨‹åŸºç¡€æ¦‚å¿µ:\")\n",
    "\n",
    "# 1.1 åˆ›å»ºä¼ä¸šçº§æ•°æ®é›†\n",
    "print(f\"\\n   ğŸ“Š 1.1 åˆ›å»ºä¼ä¸šçº§æ•°æ®é›†:\")\n",
    "\n",
    "def create_enterprise_dataset(n_records: int = 50000, n_sources: int = 5):\n",
    "    \"\"\"åˆ›å»ºç”¨äºæ•°æ®å·¥ç¨‹å®è·µçš„ä¼ä¸šçº§æ•°æ®é›†\"\"\"\n",
    "    print(f\"   ç”Ÿæˆä¼ä¸šçº§æ•°æ®é›†: {n_records:,} æ¡è®°å½•, {n_sources} ä¸ªæ•°æ®æº\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå¤šä¸ªæ•°æ®æºçš„æ•°æ®\n",
    "    data_sources = []\n",
    "    \n",
    "    for source_id in range(n_sources):\n",
    "        # æ¯ä¸ªæ•°æ®æºæœ‰ä¸åŒçš„æ•°æ®ç‰¹å¾å’Œè´¨é‡é—®é¢˜\n",
    "        source_name = f'source_{source_id}'\n",
    "        \n",
    "        # åŸºç¡€æ—¶é—´åºåˆ—\n",
    "        dates = pd.date_range('2023-01-01', periods=n_records//n_sources, freq='1H')\n",
    "        \n",
    "        # ä¸åŒæ•°æ®æºçš„æ•°æ®è´¨é‡ç‰¹å¾\n",
    "        missing_rate = 0.05 + source_id * 0.02  # ä¸åŒçš„ç¼ºå¤±ç‡\n",
    "        duplicate_rate = 0.03 + source_id * 0.01  # ä¸åŒçš„é‡å¤ç‡\n",
    "        error_rate = 0.02 + source_id * 0.01  # ä¸åŒçš„é”™è¯¯ç‡\n",
    "        \n",
    "        # ç”Ÿæˆæ•°æ®\n",
    "        data = {\n",
    "            'source_id': [source_name] * len(dates),\n",
    "            'timestamp': dates,\n",
    "            'user_id': np.random.randint(1, 10000, len(dates)),\n",
    "            'session_id': np.random.randint(1, 5000, len(dates)),\n",
    "            'event_type': np.random.choice(['login', 'chat', 'completion', 'embedding', 'search'], len(dates)),\n",
    "            'model_name': np.random.choice(['gpt-3.5-turbo', 'gpt-4', 'claude-3', 'llama-2'], len(dates)),\n",
    "            'request_size': np.random.exponential(1000, len(dates)),\n",
    "            'response_size': np.random.exponential(2000, len(dates)),\n",
    "            'latency_ms': np.random.gamma(2, 50, len(dates)),\n",
    "            'token_count': np.random.poisson(150, len(dates)),\n",
    "            'cost_usd': np.random.uniform(0.001, 0.1, len(dates)),\n",
    "            'success': np.random.choice([True, False], len(dates), p=[0.95, 0.05]),\n",
    "            'error_code': np.random.choice([None, 'timeout', 'rate_limit', 'invalid_request'], len(dates), p=[0.9, 0.05, 0.03, 0.02]),\n",
    "            'region': np.random.choice(['us-east-1', 'us-west-2', 'eu-west-1', 'ap-southeast-1'], len(dates)),\n",
    "            'user_agent': np.random.choice(['web', 'mobile', 'api', 'cli'], len(dates)),\n",
    "            'version': np.random.choice(['v1.0', 'v1.1', 'v2.0'], len(dates))\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # å¼•å…¥æ•°æ®è´¨é‡é—®é¢˜\n",
    "        \n",
    "        # ç¼ºå¤±å€¼\n",
    "        for col in ['latency_ms', 'token_count', 'cost_usd']:\n",
    "            mask = np.random.random(len(df)) < missing_rate\n",
    "            df.loc[mask, col] = np.nan\n",
    "        \n",
    "        # é‡å¤æ•°æ®\n",
    "        n_duplicates = int(len(df) * duplicate_rate)\n",
    "        duplicate_indices = np.random.choice(len(df), n_duplicates, replace=False)\n",
    "        duplicates = df.iloc[duplicate_indices].copy()\n",
    "        df = pd.concat([df, duplicates], ignore_index=True)\n",
    "        \n",
    "        # æ•°æ®é”™è¯¯\n",
    "        # å¼‚å¸¸å€¼\n",
    "        outlier_mask = np.random.random(len(df)) < error_rate\n",
    "        df.loc[outlier_mask, 'latency_ms'] = df.loc[outlier_mask, 'latency_ms'] * 10\n",
    "        \n",
    "        # æ ¼å¼é”™è¯¯\n",
    "        format_error_mask = np.random.random(len(df)) < error_rate * 0.5\n",
    "        df.loc[format_error_mask, 'model_name'] = 'unknown_model'\n",
    "        \n",
    "        # ä¸ä¸€è‡´çš„æ•°æ®\n",
    "        inconsistency_mask = np.random.random(len(df)) < error_rate * 0.3\n",
    "        df.loc[inconsistency_mask, 'success'] = False\n",
    "        df.loc[inconsistency_mask, 'error_code'] = None\n",
    "        \n",
    "        data_sources.append(df)\n",
    "        \n",
    "        print(f\"      {source_name}: {len(df):,} æ¡è®°å½•, ç¼ºå¤±ç‡{missing_rate:.1%}, é‡å¤ç‡{duplicate_rate:.1%}\")\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰æ•°æ®æº\n",
    "    enterprise_df = pd.concat(data_sources, ignore_index=True)\n",
    "    \n",
    "    # æ·»åŠ æ•°æ®è´¨é‡æ ‡è¯†\n",
    "    enterprise_df['data_quality_score'] = np.random.uniform(0.7, 1.0, len(enterprise_df))\n",
    "    enterprise_df['processing_timestamp'] = datetime.datetime.now()\n",
    "    \n",
    "    print(f\"   ä¼ä¸šæ•°æ®é›†åˆ›å»ºå®Œæˆ: {enterprise_df.shape[0]:,} è¡Œ Ã— {enterprise_df.shape[1]} åˆ—\")\n",
    "    print(f\"   æ•°æ®æºæ•°é‡: {n_sources}\")\n",
    "    print(f\"   æ—¶é—´è·¨åº¦: {enterprise_df['timestamp'].min()} åˆ° {enterprise_df['timestamp'].max()}\")\n",
    "    print(f\"   å†…å­˜ä½¿ç”¨: {enterprise_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return enterprise_df\n",
    "\n",
    "# åˆ›å»ºä¼ä¸šæ•°æ®é›†\n",
    "enterprise_df = create_enterprise_dataset(n_records=50000, n_sources=5)\n",
    "print(f\"   ä¸»è¦æŒ‡æ ‡: APIè°ƒç”¨ã€æ€§èƒ½æŒ‡æ ‡ã€ç”¨æˆ·è¡Œä¸ºã€ç³»ç»ŸçŠ¶æ€ç­‰\")\n",
    "\n",
    "# 1.2 æ•°æ®å·¥ç¨‹åŸºç¡€æ¡†æ¶\n",
    "print(f\"\\n   ğŸ”§ 1.2 æ•°æ®å·¥ç¨‹åŸºç¡€æ¡†æ¶:\")\n",
    "\n",
    "class DataQualityLevel(Enum):\n",
    "    \"\"\"æ•°æ®è´¨é‡ç­‰çº§\"\"\"\n",
    "    EXCELLENT = \"excellent\"\n",
    "    GOOD = \"good\"\n",
    "    FAIR = \"fair\"\n",
    "    POOR = \"poor\"\n",
    "\n",
    "@dataclass\n",
    "class DataQualityMetrics:\n",
    "    \"\"\"æ•°æ®è´¨é‡æŒ‡æ ‡\"\"\"\n",
    "    completeness: float  # å®Œæ•´æ€§\n",
    "    accuracy: float     # å‡†ç¡®æ€§\n",
    "    consistency: float  # ä¸€è‡´æ€§\n",
    "    timeliness: float   # åŠæ—¶æ€§\n",
    "    validity: float     # æœ‰æ•ˆæ€§\n",
    "    uniqueness: float   # å”¯ä¸€æ€§\n",
    "    overall_score: float = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.overall_score = (\n",
    "            self.completeness * 0.2 +\n",
    "            self.accuracy * 0.2 +\n",
    "            self.consistency * 0.15 +\n",
    "            self.timeliness * 0.15 +\n",
    "            self.validity * 0.15 +\n",
    "            self.uniqueness * 0.15\n",
    "        )\n",
    "    \n",
    "    def get_quality_level(self) -> DataQualityLevel:\n",
    "        \"\"\"è·å–è´¨é‡ç­‰çº§\"\"\"\n",
    "        if self.overall_score >= 0.9:\n",
    "            return DataQualityLevel.EXCELLENT\n",
    "        elif self.overall_score >= 0.8:\n",
    "            return DataQualityLevel.GOOD\n",
    "        elif self.overall_score >= 0.7:\n",
    "            return DataQualityLevel.FAIR\n",
    "        else:\n",
    "            return DataQualityLevel.POOR\n",
    "\n",
    "@dataclass\n",
    "class DataEngineeringFramework:\n",
    "    \"\"\"æ•°æ®å·¥ç¨‹åŸºç¡€æ¡†æ¶\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.original_df = df.copy()\n",
    "        self.processed_df = df.copy()\n",
    "        self.quality_metrics = {}\n",
    "        self.processing_log = []\n",
    "        self.data_lineage = []\n",
    "        self.performance_metrics = {}\n",
    "    \n",
    "    def log_processing_step(self, step_name: str, description: str, \n",
    "                           input_count: int, output_count: int, \n",
    "                           processing_time: float):\n",
    "        \"\"\"è®°å½•å¤„ç†æ­¥éª¤\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.datetime.now(),\n",
    "            'step_name': step_name,\n",
    "            'description': description,\n",
    "            'input_count': input_count,\n",
    "            'output_count': output_count,\n",
    "            'processing_time': processing_time,\n",
    "            'data_reduction': (input_count - output_count) / input_count * 100\n",
    "        }\n",
    "        self.processing_log.append(log_entry)\n",
    "        \n",
    "        print(f\"   ğŸ“ {step_name}: {input_count:,} -> {output_count:,} è®°å½•, \"\n",
    "              f\"è€—æ—¶ {processing_time:.3f}s, å‡å°‘ {log_entry['data_reduction']:.1f}%\")\n",
    "    \n",
    "    def assess_data_quality(self, df: pd.DataFrame) -> DataQualityMetrics:\n",
    "        \"\"\"è¯„ä¼°æ•°æ®è´¨é‡\"\"\"\n",
    "        print(f\"   è¯„ä¼°æ•°æ®è´¨é‡...\")\n",
    "        \n",
    "        # å®Œæ•´æ€§ï¼šéç©ºå€¼æ¯”ä¾‹\n",
    "        completeness = 1 - (df.isnull().sum().sum() / (len(df) * len(df.columns)))\n",
    "        \n",
    "        # å‡†ç¡®æ€§ï¼šåŸºäºä¸šåŠ¡è§„åˆ™çš„å‡†ç¡®æ€§æ£€æŸ¥\n",
    "        accuracy_score = 0.95  # åŸºç¡€å‡†ç¡®æ€§åˆ†æ•°\n",
    "        \n",
    "        # æ£€æŸ¥æ•°å€¼å­—æ®µçš„åˆç†æ€§\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                # æ£€æŸ¥è´Ÿå€¼æ˜¯å¦åˆç†\n",
    "                if 'size' in col.lower() or 'count' in col.lower() or 'cost' in col.lower():\n",
    "                    negative_ratio = (df[col] < 0).sum() / len(df)\n",
    "                    accuracy_score -= negative_ratio * 0.1\n",
    "        \n",
    "        accuracy = max(0, min(1, accuracy_score))\n",
    "        \n",
    "        # ä¸€è‡´æ€§ï¼šæ£€æŸ¥æ•°æ®æ ¼å¼å’Œé€»è¾‘ä¸€è‡´æ€§\n",
    "        consistency_score = 0.9\n",
    "        \n",
    "        # æ£€æŸ¥æˆåŠŸçŠ¶æ€å’Œé”™è¯¯ä»£ç çš„ä¸€è‡´æ€§\n",
    "        if 'success' in df.columns and 'error_code' in df.columns:\n",
    "            inconsistent = df[(df['success'] == True) & (df['error_code'].notna())].shape[0]\n",
    "            consistency_score -= inconsistent / len(df) * 0.2\n",
    "        \n",
    "        consistency = max(0, min(1, consistency_score))\n",
    "        \n",
    "        # åŠæ—¶æ€§ï¼šæ•°æ®çš„æ–°é²œåº¦\n",
    "        if 'timestamp' in df.columns:\n",
    "            latest_timestamp = df['timestamp'].max()\n",
    "            current_time = datetime.datetime.now()\n",
    "            age_hours = (current_time - latest_timestamp).total_seconds() / 3600\n",
    "            timeliness = max(0, 1 - age_hours / 168)  # ä¸€å‘¨å†…ä¸ºæ»¡åˆ†\n",
    "        else:\n",
    "            timeliness = 0.8\n",
    "        \n",
    "        # æœ‰æ•ˆæ€§ï¼šæ•°æ®æ ¼å¼å’Œå€¼åŸŸçš„æœ‰æ•ˆæ€§\n",
    "        validity_score = 0.9\n",
    "        \n",
    "        # æ£€æŸ¥æšä¸¾å€¼çš„æœ‰æ•ˆæ€§\n",
    "        if 'event_type' in df.columns:\n",
    "            valid_events = {'login', 'chat', 'completion', 'embedding', 'search'}\n",
    "            invalid_events = df[~df['event_type'].isin(valid_events)].shape[0]\n",
    "            validity_score -= invalid_events / len(df) * 0.3\n",
    "        \n",
    "        validity = max(0, min(1, validity_score))\n",
    "        \n",
    "        # å”¯ä¸€æ€§ï¼šæ£€æŸ¥é‡å¤æ•°æ®\n",
    "        duplicate_count = df.duplicated().sum()\n",
    "        uniqueness = 1 - (duplicate_count / len(df))\n",
    "        \n",
    "        metrics = DataQualityMetrics(\n",
    "            completeness=completeness,\n",
    "            accuracy=accuracy,\n",
    "            consistency=consistency,\n",
    "            timeliness=timeliness,\n",
    "            validity=validity,\n",
    "            uniqueness=uniqueness\n",
    "        )\n",
    "        \n",
    "        print(f\"      å®Œæ•´æ€§: {completeness:.3f}\")\n",
    "        print(f\"      å‡†ç¡®æ€§: {accuracy:.3f}\")\n",
    "        print(f\"      ä¸€è‡´æ€§: {consistency:.3f}\")\n",
    "        print(f\"      åŠæ—¶æ€§: {timeliness:.3f}\")\n",
    "        print(f\"      æœ‰æ•ˆæ€§: {validity:.3f}\")\n",
    "        print(f\"      å”¯ä¸€æ€§: {uniqueness:.3f}\")\n",
    "        print(f\"      æ€»ä½“è¯„åˆ†: {metrics.overall_score:.3f} ({metrics.get_quality_level().value})\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"è·å–å¤„ç†æ‘˜è¦\"\"\"\n",
    "        print(f\"\\n   ğŸ“Š æ•°æ®å·¥ç¨‹å¤„ç†æ‘˜è¦:\")\n",
    "        \n",
    "        print(f\"      åŸå§‹æ•°æ®: {len(self.original_df):,} æ¡è®°å½•\")\n",
    "        print(f\"      å¤„ç†åæ•°æ®: {len(self.processed_df):,} æ¡è®°å½•\")\n",
    "        print(f\"      æ•°æ®å‡å°‘: {(len(self.original_df) - len(self.processed_df)) / len(self.original_df) * 100:.1f}%\")\n",
    "        print(f\"      å¤„ç†æ­¥éª¤: {len(self.processing_log)} ä¸ª\")\n",
    "        \n",
    "        total_time = sum(step['processing_time'] for step in self.processing_log)\n",
    "        print(f\"      æ€»å¤„ç†æ—¶é—´: {total_time:.3f} ç§’\")\n",
    "        \n",
    "        if self.quality_metrics:\n",
    "            print(f\"\\n      æ•°æ®è´¨é‡å˜åŒ–:\")\n",
    "            for step, metrics in self.quality_metrics.items():\n",
    "                print(f\"        {step}: {metrics.overall_score:.3f} ({metrics.get_quality_level().value})\")\n",
    "        \n",
    "        return {\n",
    "            'original_count': len(self.original_df),\n",
    "            'processed_count': len(self.processed_df),\n",
    "            'reduction_percentage': (len(self.original_df) - len(self.processed_df)) / len(self.original_df) * 100,\n",
    "            'processing_steps': len(self.processing_log),\n",
    "            'total_processing_time': total_time,\n",
    "            'quality_metrics': self.quality_metrics\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–æ•°æ®å·¥ç¨‹æ¡†æ¶\n",
    "data_engineering = DataEngineeringFramework(enterprise_df)\n",
    "print(f\"   âœ… æ•°æ®å·¥ç¨‹æ¡†æ¶åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "# è¯„ä¼°åˆå§‹æ•°æ®è´¨é‡\n",
    "initial_quality = data_engineering.assess_data_quality(enterprise_df)\n",
    "data_engineering.quality_metrics['initial'] = initial_quality\n",
    "\n",
    "print(f\"\\nâœ… æ•°æ®å·¥ç¨‹åŸºç¡€æ¦‚å¿µå®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡æ•°æ®å·¥ç¨‹çš„åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„\")\n",
    "print(f\"   âœ“ ç†è§£æ•°æ®è´¨é‡è¯„ä¼°ä½“ç³»\")\n",
    "print(f\"   âœ“ ç†Ÿæ‚‰æ•°æ®å·¥ç¨‹æ¡†æ¶è®¾è®¡\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿåˆ›å»ºä¼ä¸šçº§æ•°æ®é›†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETLæ•°æ®ç®¡é“ [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šETLï¼ˆExtract-Transform-Loadï¼‰æ•°æ®ç®¡é“æ˜¯æ•°æ®å·¥ç¨‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä»å¤šä¸ªæ•°æ®æºæå–æ•°æ®ã€è¿›è¡Œè½¬æ¢å¤„ç†ã€åŠ è½½åˆ°ç›®æ ‡ç³»ç»Ÿã€‚æŒæ¡ETLæŠ€æœ¯å¯¹äºæ„å»ºç”Ÿäº§çº§LangChainåº”ç”¨éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡ETLçš„åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„è®¾è®¡\n",
    "- ç†è§£æ•°æ®æå–ã€è½¬æ¢ã€åŠ è½½çš„å„ä¸ªé˜¶æ®µ\n",
    "- ç†Ÿç»ƒä½¿ç”¨ETLå·¥å…·å’Œæ¡†æ¶\n",
    "- èƒ½å¤Ÿæ„å»ºå¯æ‰©å±•çš„ETLç®¡é“\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„ETLæ•°æ®æµç¨‹\n",
    "- è¿›è¡Œæ•°æ®è½¬æ¢å’Œæ¸…æ´—ä¼˜åŒ–\n",
    "- åº”ç”¨ETLè§£å†³å®é™…æ•°æ®é›†æˆé—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„ETLç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ”„ ETLæ•°æ®ç®¡é“:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ETLåŸºç¡€ç»„ä»¶\n",
    "print(f\"ğŸ“ 1. ETLåŸºç¡€ç»„ä»¶:\")\n",
    "\n",
    "class DataSource(ABC):\n",
    "    \"\"\"æ•°æ®æºæŠ½è±¡åŸºç±»\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract(self, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"æå–æ•°æ®\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def validate_connection(self) -> bool:\n",
    "        \"\"\"éªŒè¯è¿æ¥\"\"\"\n",
    "        pass\n",
    "\n",
    "class DataTransformer(ABC):\n",
    "    \"\"\"æ•°æ®è½¬æ¢å™¨æŠ½è±¡åŸºç±»\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"è½¬æ¢æ•°æ®\"\"\"\n",
    "        pass\n",
    "\n",
    "class DataTarget(ABC):\n",
    "    \"\"\"æ•°æ®ç›®æ ‡æŠ½è±¡åŸºç±»\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load(self, df: pd.DataFrame, **kwargs) -> bool:\n",
    "        \"\"\"åŠ è½½æ•°æ®\"\"\"\n",
    "        pass\n",
    "\n",
    "# 2. å…·ä½“å®ç°\n",
    "print(f\"\\n   ğŸ”§ 2. å…·ä½“å®ç°:\")\n",
    "\n",
    "@dataclass\n",
    "class CSVDataSource(DataSource):\n",
    "    \"\"\"CSVæ•°æ®æº\"\"\"\n",
    "    file_path: str\n",
    "    \n",
    "    def extract(self, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"ä»CSVæ–‡ä»¶æå–æ•°æ®\"\"\"\n",
    "        print(f\"   ä»CSVæå–æ•°æ®: {self.file_path}\")\n",
    "        \n",
    "        if not os.path.exists(self.file_path):\n",
    "            # æ¨¡æ‹Ÿåˆ›å»ºCSVæ–‡ä»¶\n",
    "            sample_data = enterprise_df.sample(n=1000, random_state=42)\n",
    "            sample_data.to_csv(self.file_path, index=False)\n",
    "            print(f\"      åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶: {len(sample_data)} æ¡è®°å½•\")\n",
    "        \n",
    "        df = pd.read_csv(self.file_path)\n",
    "        print(f\"      æˆåŠŸæå– {len(df)} æ¡è®°å½•\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def validate_connection(self) -> bool:\n",
    "        \"\"\"éªŒè¯æ–‡ä»¶è¿æ¥\"\"\"\n",
    "        return os.path.exists(self.file_path)\n",
    "\n",
    "@dataclass\n",
    "class DatabaseDataSource(DataSource):\n",
    "    \"\"\"æ•°æ®åº“æ•°æ®æº\"\"\"\n",
    "    db_path: str\n",
    "    table_name: str\n",
    "    \n",
    "    def extract(self, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"ä»æ•°æ®åº“æå–æ•°æ®\"\"\"\n",
    "        print(f\"   ä»æ•°æ®åº“æå–æ•°æ®: {self.db_path}.{self.table_name}\")\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_sql(f\"SELECT * FROM {self.table_name}\", conn)\n",
    "            print(f\"      æˆåŠŸæå– {len(df)} æ¡è®°å½•\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"      è¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®\")\n",
    "            # åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "            sample_data = enterprise_df.sample(n=1000, random_state=42)\n",
    "            sample_data.to_sql(self.table_name, conn, index=False, if_exists='replace')\n",
    "            print(f\"      åˆ›å»ºç¤ºä¾‹è¡¨: {len(sample_data)} æ¡è®°å½•\")\n",
    "            return sample_data\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def validate_connection(self) -> bool:\n",
    "        \"\"\"éªŒè¯æ•°æ®åº“è¿æ¥\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            conn.close()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "@dataclass\n",
    "class DataCleaningTransformer(DataTransformer):\n",
    "    \"\"\"æ•°æ®æ¸…æ´—è½¬æ¢å™¨\"\"\"\n",
    "    remove_duplicates: bool = True\n",
    "    fill_missing: bool = True\n",
    "    remove_outliers: bool = True\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ•°æ®æ¸…æ´—è½¬æ¢\"\"\"\n",
    "        print(f\"   æ‰§è¡Œæ•°æ®æ¸…æ´—è½¬æ¢...\")\n",
    "        \n",
    "        original_count = len(df)\n",
    "        cleaned_df = df.copy()\n",
    "        \n",
    "        # ç§»é™¤é‡å¤æ•°æ®\n",
    "        if self.remove_duplicates:\n",
    "            cleaned_df = cleaned_df.drop_duplicates()\n",
    "            print(f\"      ç§»é™¤é‡å¤æ•°æ®: {original_count - len(cleaned_df)} æ¡\")\n",
    "        \n",
    "        # å¡«å……ç¼ºå¤±å€¼\n",
    "        if self.fill_missing:\n",
    "            numeric_cols = cleaned_df.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_cols:\n",
    "                if cleaned_df[col].isnull().sum() > 0:\n",
    "                    median_val = cleaned_df[col].median()\n",
    "                    cleaned_df[col].fillna(median_val, inplace=True)\n",
    "            \n",
    "            categorical_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
    "            for col in categorical_cols:\n",
    "                if cleaned_df[col].isnull().sum() > 0:\n",
    "                    mode_val = cleaned_df[col].mode()[0] if not cleaned_df[col].mode().empty else 'unknown'\n",
    "                    cleaned_df[col].fillna(mode_val, inplace=True)\n",
    "            \n",
    "            print(f\"      å¡«å……ç¼ºå¤±å€¼å®Œæˆ\")\n",
    "        \n",
    "        # ç§»é™¤å¼‚å¸¸å€¼\n",
    "        if self.remove_outliers:\n",
    "            numeric_cols = ['latency_ms', 'token_count', 'cost_usd']\n",
    "            for col in numeric_cols:\n",
    "                if col in cleaned_df.columns:\n",
    "                    Q1 = cleaned_df[col].quantile(0.25)\n",
    "                    Q3 = cleaned_df[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - 1.5 * IQR\n",
    "                    upper_bound = Q3 + 1.5 * IQR\n",
    "                    \n",
    "                    outlier_mask = (cleaned_df[col] < lower_bound) | (cleaned_df[col] > upper_bound)\n",
    "                    outlier_count = outlier_mask.sum()\n",
    "                    \n",
    "                    if outlier_count > 0:\n",
    "                        cleaned_df = cleaned_df[~outlier_mask]\n",
    "                        print(f\"      ç§»é™¤ {col} å¼‚å¸¸å€¼: {outlier_count} æ¡\")\n",
    "        \n",
    "        print(f\"      æ¸…æ´—å®Œæˆ: {original_count:,} -> {len(cleaned_df):,} è®°å½•\")\n",
    "        \n",
    "        return cleaned_df\n",
    "\n",
    "@dataclass\n",
    "class DataValidationTransformer(DataTransformer):\n",
    "    \"\"\"æ•°æ®éªŒè¯è½¬æ¢å™¨\"\"\"\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ•°æ®éªŒè¯è½¬æ¢\"\"\"\n",
    "        print(f\"   æ‰§è¡Œæ•°æ®éªŒè¯è½¬æ¢...\")\n",
    "        \n",
    "        validated_df = df.copy()\n",
    "        validation_errors = []\n",
    "        \n",
    "        # éªŒè¯æ•°å€¼å­—æ®µ\n",
    "        numeric_validations = {\n",
    "            'latency_ms': (0, 10000),  # å»¶è¿Ÿåº”è¯¥åœ¨0-10ç§’ä¹‹é—´\n",
    "            'token_count': (1, 10000), # tokenæ•°é‡åº”è¯¥åˆç†\n",
    "            'cost_usd': (0.001, 1.0)   # æˆæœ¬åº”è¯¥åˆç†\n",
    "        }\n",
    "        \n",
    "        for col, (min_val, max_val) in numeric_validations.items():\n",
    "            if col in validated_df.columns:\n",
    "                invalid_mask = (validated_df[col] < min_val) | (validated_df[col] > max_val)\n",
    "                invalid_count = invalid_mask.sum()\n",
    "                \n",
    "                if invalid_count > 0:\n",
    "                    validation_errors.append(f\"{col}: {invalid_count} æ¡è®°å½•è¶…å‡ºèŒƒå›´ [{min_val}, {max_val}]\")\n",
    "                    # å°†å¼‚å¸¸å€¼è®¾ç½®ä¸ºè¾¹ç•Œå€¼\n",
    "                    validated_df.loc[validated_df[col] < min_val, col] = min_val\n",
    "                    validated_df.loc[validated_df[col] > max_val, col] = max_val\n",
    "        \n",
    "        # éªŒè¯åˆ†ç±»å­—æ®µ\n",
    "        if 'event_type' in validated_df.columns:\n",
    "            valid_events = {'login', 'chat', 'completion', 'embedding', 'search'}\n",
    "            invalid_events = validated_df[~validated_df['event_type'].isin(valid_events)]\n",
    "            if len(invalid_events) > 0:\n",
    "                validation_errors.append(f\"event_type: {len(invalid_events)} æ¡è®°å½•åŒ…å«æ— æ•ˆå€¼\")\n",
    "                validated_df.loc[~validated_df['event_type'].isin(valid_events), 'event_type'] = 'unknown'\n",
    "        \n",
    "        # éªŒè¯é€»è¾‘ä¸€è‡´æ€§\n",
    "        if 'success' in validated_df.columns and 'error_code' in validated_df.columns:\n",
    "            inconsistent = validated_df[(validated_df['success'] == True) & (validated_df['error_code'].notna())]\n",
    "            if len(inconsistent) > 0:\n",
    "                validation_errors.append(f\"é€»è¾‘ä¸€è‡´æ€§: {len(inconsistent)} æ¡è®°å½•æˆåŠŸä½†æœ‰é”™è¯¯ä»£ç \")\n",
    "                validated_df.loc[(validated_df['success'] == True) & (validated_df['error_code'].notna()), 'error_code'] = None\n",
    "        \n",
    "        # æ·»åŠ éªŒè¯ç»“æœ\n",
    "        validated_df['validation_passed'] = len(validation_errors) == 0\n",
    "        \n",
    "        print(f\"      éªŒè¯å®Œæˆï¼Œå‘ç° {len(validation_errors)} ä¸ªé—®é¢˜:\")\n",
    "        for error in validation_errors:\n",
    "            print(f\"        - {error}\")\n",
    "        \n",
    "        return validated_df\n",
    "\n",
    "@dataclass\n",
    "class DataEnrichmentTransformer(DataTransformer):\n",
    "    \"\"\"æ•°æ®å¢å¼ºè½¬æ¢å™¨\"\"\"\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ•°æ®å¢å¼ºè½¬æ¢\"\"\"\n",
    "        print(f\"   æ‰§è¡Œæ•°æ®å¢å¼ºè½¬æ¢...\")\n",
    "        \n",
    "        enriched_df = df.copy()\n",
    "        \n",
    "        # è®¡ç®—æ´¾ç”Ÿå­—æ®µ\n",
    "        if 'request_size' in enriched_df.columns and 'response_size' in enriched_df.columns:\n",
    "            enriched_df['data_ratio'] = enriched_df['response_size'] / (enriched_df['request_size'] + 1)\n",
    "        \n",
    "        if 'latency_ms' in enriched_df.columns and 'token_count' in enriched_df.columns:\n",
    "            enriched_df['tokens_per_second'] = enriched_df['token_count'] / (enriched_df['latency_ms'] / 1000 + 0.001)\n",
    "        \n",
    "        if 'cost_usd' in enriched_df.columns and 'token_count' in enriched_df.columns:\n",
    "            enriched_df['cost_per_token'] = enriched_df['cost_usd'] / enriched_df['token_count']\n",
    "        \n",
    "        # æ·»åŠ æ—¶é—´ç›¸å…³å­—æ®µ\n",
    "        if 'timestamp' in enriched_df.columns:\n",
    "            enriched_df['hour'] = pd.to_datetime(enriched_df['timestamp']).dt.hour\n",
    "            enriched_df['day_of_week'] = pd.to_datetime(enriched_df['timestamp']).dt.dayofweek\n",
    "            enriched_df['is_weekend'] = enriched_df['day_of_week'].isin([5, 6])\n",
    "        \n",
    "        # æ·»åŠ æ€§èƒ½åˆ†ç±»\n",
    "        if 'latency_ms' in enriched_df.columns:\n",
    "            enriched_df['performance_category'] = pd.cut(\n",
    "                enriched_df['latency_ms'],\n",
    "                bins=[0, 100, 500, float('inf')],\n",
    "                labels=['fast', 'medium', 'slow']\n",
    "            )\n",
    "        \n",
    "        print(f\"      å¢å¼ºå®Œæˆï¼Œæ–°å¢ {len(enriched_df.columns) - len(df.columns)} ä¸ªå­—æ®µ\")\n",
    "        \n",
    "        return enriched_df\n",
    "\n",
    "@dataclass\n",
    "class CSVDataTarget(DataTarget):\n",
    "    \"\"\"CSVæ•°æ®ç›®æ ‡\"\"\"\n",
    "    file_path: str\n",
    "    \n",
    "    def load(self, df: pd.DataFrame, **kwargs) -> bool:\n",
    "        \"\"\"åŠ è½½åˆ°CSVæ–‡ä»¶\"\"\"\n",
    "        print(f\"   åŠ è½½æ•°æ®åˆ°CSV: {self.file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "            os.makedirs(os.path.dirname(self.file_path), exist_ok=True)\n",
    "            \n",
    "            df.to_csv(self.file_path, index=False)\n",
    "            print(f\"      æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"      åŠ è½½å¤±è´¥: {e}\")\n",
    "            return False\n",
    "\n",
    "@dataclass\n",
    "class DatabaseDataTarget(DataTarget):\n",
    "    \"\"\"æ•°æ®åº“æ•°æ®ç›®æ ‡\"\"\"\n",
    "    db_path: str\n",
    "    table_name: str\n",
    "    \n",
    "    def load(self, df: pd.DataFrame, **kwargs) -> bool:\n",
    "        \"\"\"åŠ è½½åˆ°æ•°æ®åº“\"\"\"\n",
    "        print(f\"   åŠ è½½æ•°æ®åˆ°æ•°æ®åº“: {self.db_path}.{self.table_name}\")\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            df.to_sql(self.table_name, conn, index=False, if_exists='replace')\n",
    "            conn.close()\n",
    "            print(f\"      æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"      åŠ è½½å¤±è´¥: {e}\")\n",
    "            return False\n",
    "\n",
    "# 3. ETLç®¡é“\n",
    "print(f\"\\n   ğŸ”„ 3. ETLç®¡é“:\")\n",
    "\n",
    "@dataclass\n",
    "class ETLPipeline:\n",
    "    \"\"\"ETLæ•°æ®ç®¡é“\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.sources: List[DataSource] = []\n",
    "        self.transformers: List[DataTransformer] = []\n",
    "        self.targets: List[DataTarget] = []\n",
    "        self.execution_log = []\n",
    "    \n",
    "    def add_source(self, source: DataSource):\n",
    "        \"\"\"æ·»åŠ æ•°æ®æº\"\"\"\n",
    "        self.sources.append(source)\n",
    "        print(f\"   æ·»åŠ æ•°æ®æº: {source.__class__.__name__}\")\n",
    "    \n",
    "    def add_transformer(self, transformer: DataTransformer):\n",
    "        \"\"\"æ·»åŠ è½¬æ¢å™¨\"\"\"\n",
    "        self.transformers.append(transformer)\n",
    "        print(f\"   æ·»åŠ è½¬æ¢å™¨: {transformer.__class__.__name__}\")\n",
    "    \n",
    "    def add_target(self, target: DataTarget):\n",
    "        \"\"\"æ·»åŠ æ•°æ®ç›®æ ‡\"\"\"\n",
    "        self.targets.append(target)\n",
    "        print(f\"   æ·»åŠ æ•°æ®ç›®æ ‡: {target.__class__.__name__}\")\n",
    "    \n",
    "    def execute(self) -> Dict[str, Any]:\n",
    "        \"\"\"æ‰§è¡ŒETLç®¡é“\"\"\"\n",
    "        print(f\"\\n   ğŸš€ æ‰§è¡ŒETLç®¡é“: {self.name}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        execution_results = {\n",
    "            'pipeline_name': self.name,\n",
    "            'start_time': datetime.datetime.now(),\n",
    "            'extraction_results': [],\n",
    "            'transformation_results': [],\n",
    "            'loading_results': [],\n",
    "            'total_records_processed': 0,\n",
    "            'success': False,\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Extracté˜¶æ®µ\n",
    "            print(f\"\\n      ğŸ“¥ Extracté˜¶æ®µ:\")\n",
    "            extracted_data = []\n",
    "            \n",
    "            for i, source in enumerate(self.sources):\n",
    "                step_start = time.time()\n",
    "                \n",
    "                if not source.validate_connection():\n",
    "                    raise Exception(f\"æ•°æ®æºè¿æ¥å¤±è´¥: {source.__class__.__name__}\")\n",
    "                \n",
    "                df = source.extract()\n",
    "                step_time = time.time() - step_start\n",
    "                \n",
    "                extracted_data.append(df)\n",
    "                \n",
    "                execution_results['extraction_results'].append({\n",
    "                    'source': source.__class__.__name__,\n",
    "                    'records': len(df),\n",
    "                    'time': step_time\n",
    "                })\n",
    "                \n",
    "                print(f\"         æº{i+1}: æå– {len(df)} æ¡è®°å½•ï¼Œè€—æ—¶ {step_time:.3f}s\")\n",
    "            \n",
    "            # åˆå¹¶æ‰€æœ‰æå–çš„æ•°æ®\n",
    "            if len(extracted_data) > 1:\n",
    "                combined_df = pd.concat(extracted_data, ignore_index=True)\n",
    "                print(f\"         åˆå¹¶æ•°æ®: {len(combined_df)} æ¡è®°å½•\")\n",
    "            else:\n",
    "                combined_df = extracted_data[0]\n",
    "            \n",
    "            # Transformé˜¶æ®µ\n",
    "            print(f\"\\n      ğŸ”„ Transformé˜¶æ®µ:\")\n",
    "            current_df = combined_df\n",
    "            \n",
    "            for i, transformer in enumerate(self.transformers):\n",
    "                step_start = time.time()\n",
    "                input_count = len(current_df)\n",
    "                \n",
    "                current_df = transformer.transform(current_df)\n",
    "                step_time = time.time() - step_start\n",
    "                \n",
    "                execution_results['transformation_results'].append({\n",
    "                    'transformer': transformer.__class__.__name__,\n",
    "                    'input_records': input_count,\n",
    "                    'output_records': len(current_df),\n",
    "                    'time': step_time\n",
    "                })\n",
    "                \n",
    "                print(f\"         è½¬æ¢{i+1}: {input_count:,} -> {len(current_df):,} è®°å½•ï¼Œè€—æ—¶ {step_time:.3f}s\")\n",
    "            \n",
    "            # Loadé˜¶æ®µ\n",
    "            print(f\"\\n      ğŸ“¤ Loadé˜¶æ®µ:\")\n",
    "            \n",
    "            for i, target in enumerate(self.targets):\n",
    "                step_start = time.time()\n",
    "                \n",
    "                success = target.load(current_df)\n",
    "                step_time = time.time() - step_start\n",
    "                \n",
    "                execution_results['loading_results'].append({\n",
    "                    'target': target.__class__.__name__,\n",
    "                    'records': len(current_df),\n",
    "                    'time': step_time,\n",
    "                    'success': success\n",
    "                })\n",
    "                \n",
    "                print(f\"         ç›®æ ‡{i+1}: åŠ è½½ {len(current_df)} æ¡è®°å½•ï¼Œè€—æ—¶ {step_time:.3f}sï¼ŒçŠ¶æ€: {'æˆåŠŸ' if success else 'å¤±è´¥'}\")\n",
    "            \n",
    "            execution_results['total_records_processed'] = len(current_df)\n",
    "            execution_results['success'] = True\n",
    "            execution_results['end_time'] = datetime.datetime.now()\n",
    "            execution_results['total_time'] = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\n   âœ… ETLç®¡é“æ‰§è¡ŒæˆåŠŸ!\")\n",
    "            print(f\"      æ€»å¤„ç†è®°å½•: {execution_results['total_records_processed']:,}\")\n",
    "            print(f\"      æ€»è€—æ—¶: {execution_results['total_time']:.3f} ç§’\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_results['success'] = False\n",
    "            execution_results['errors'].append(str(e))\n",
    "            execution_results['end_time'] = datetime.datetime.now()\n",
    "            execution_results['total_time'] = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\n   âŒ ETLç®¡é“æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "        \n",
    "        self.execution_log.append(execution_results)\n",
    "        return execution_results\n",
    "    \n",
    "    def get_execution_summary(self):\n",
    "        \"\"\"è·å–æ‰§è¡Œæ‘˜è¦\"\"\"\n",
    "        if not self.execution_log:\n",
    "            return \"æš‚æ— æ‰§è¡Œè®°å½•\"\n",
    "        \n",
    "        latest_execution = self.execution_log[-1]\n",
    "        \n",
    "        print(f\"\\n   ğŸ“Š ETLç®¡é“æ‰§è¡Œæ‘˜è¦:\")\n",
    "        print(f\"      ç®¡é“åç§°: {latest_execution['pipeline_name']}\")\n",
    "        print(f\"      æ‰§è¡Œæ—¶é—´: {latest_execution['start_time']} - {latest_execution['end_time']}\")\n",
    "        print(f\"      æ‰§è¡ŒçŠ¶æ€: {'æˆåŠŸ' if latest_execution['success'] else 'å¤±è´¥'}\")\n",
    "        print(f\"      å¤„ç†è®°å½•: {latest_execution['total_records_processed']:,}\")\n",
    "        print(f\"      æ€»è€—æ—¶: {latest_execution['total_time']:.3f} ç§’\")\n",
    "        \n",
    "        if latest_execution['errors']:\n",
    "            print(f\"      é”™è¯¯ä¿¡æ¯:\")\n",
    "            for error in latest_execution['errors']:\n",
    "                print(f\"        - {error}\")\n",
    "        \n",
    "        return latest_execution\n",
    "\n",
    "# æ¼”ç¤ºETLç®¡é“\n",
    "print(f\"\\n   ğŸ”§ ETLç®¡é“æ¼”ç¤º:\")\n",
    "\n",
    "# åˆ›å»ºETLç®¡é“\n",
    "etl_pipeline = ETLPipeline(\"LangChainæ•°æ®å¤„ç†ç®¡é“\")\n",
    "\n",
    "# æ·»åŠ æ•°æ®æº\n",
    "csv_source = CSVDataSource(\"./data/source_data.csv\")\n",
    "db_source = DatabaseDataSource(\"./data/pipeline.db\", \"raw_events\")\n",
    "\n",
    "etl_pipeline.add_source(csv_source)\n",
    "etl_pipeline.add_source(db_source)\n",
    "\n",
    "# æ·»åŠ è½¬æ¢å™¨\n",
    "cleaning_transformer = DataCleaningTransformer()\n",
    "validation_transformer = DataValidationTransformer()\n",
    "enrichment_transformer = DataEnrichmentTransformer()\n",
    "\n",
    "etl_pipeline.add_transformer(cleaning_transformer)\n",
    "etl_pipeline.add_transformer(validation_transformer)\n",
    "etl_pipeline.add_transformer(enrichment_transformer)\n",
    "\n",
    "# æ·»åŠ æ•°æ®ç›®æ ‡\n",
    "csv_target = CSVDataTarget(\"./data/processed_data.csv\")\n",
    "db_target = DatabaseDataTarget(\"./data/pipeline.db\", \"processed_events\")\n",
    "\n",
    "etl_pipeline.add_target(csv_target)\n",
    "etl_pipeline.add_target(db_target)\n",
    "\n",
    "# æ‰§è¡ŒETLç®¡é“\n",
    "etl_results = etl_pipeline.execute()\n",
    "\n",
    "# æ˜¾ç¤ºæ‰§è¡Œæ‘˜è¦\n",
    "etl_pipeline.get_execution_summary()\n",
    "\n",
    "print(f\"\\nâœ… ETLæ•°æ®ç®¡é“å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡ETLçš„åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„è®¾è®¡\")\n",
    "print(f\"   âœ“ ç†è§£æ•°æ®æå–ã€è½¬æ¢ã€åŠ è½½çš„å„ä¸ªé˜¶æ®µ\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨ETLå·¥å…·å’Œæ¡†æ¶\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿæ„å»ºå¯æ‰©å±•çš„ETLç®¡é“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®è´¨é‡ç®¡ç† [â­â­è¿›é˜¶]\n",
    "**çŸ¥è¯†ç‚¹è¯´æ˜**ï¼šæ•°æ®è´¨é‡ç®¡ç†æ˜¯æ•°æ®å·¥ç¨‹çš„æ ¸å¿ƒç¯èŠ‚ï¼ŒåŒ…æ‹¬æ•°æ®è´¨é‡è¯„ä¼°ã€ç›‘æ§ã€æ”¹è¿›å’Œæ²»ç†ã€‚æŒæ¡æ•°æ®è´¨é‡ç®¡ç†æŠ€æœ¯å¯¹äºç¡®ä¿LangChainåº”ç”¨çš„æ•°æ®å¯é æ€§éå¸¸é‡è¦ã€‚\n",
    "\n",
    "**å­¦ä¹ è¦æ±‚**ï¼š\n",
    "- æŒæ¡æ•°æ®è´¨é‡è¯„ä¼°æ–¹æ³•å’ŒæŒ‡æ ‡\n",
    "- ç†è§£æ•°æ®è´¨é‡ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶\n",
    "- ç†Ÿç»ƒä½¿ç”¨æ•°æ®è´¨é‡ç®¡ç†å·¥å…·\n",
    "- èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„æ•°æ®è´¨é‡ä½“ç³»\n",
    "\n",
    "**æ¡ˆä¾‹è¦æ±‚**ï¼š\n",
    "- å®ç°å®Œæ•´çš„æ•°æ®è´¨é‡ç®¡ç†æµç¨‹\n",
    "- è¿›è¡Œæ•°æ®è´¨é‡çš„å…¨é¢ç›‘æ§\n",
    "- åº”ç”¨è´¨é‡ç®¡ç†è§£å†³å®é™…é—®é¢˜\n",
    "- éªŒè¯ç‚¹ï¼šèƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æ•°æ®è´¨é‡ç®¡ç†ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ” æ•°æ®è´¨é‡ç®¡ç†:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. æ•°æ®è´¨é‡è§„åˆ™å¼•æ“\n",
    "print(f\"ğŸ“ 1. æ•°æ®è´¨é‡è§„åˆ™å¼•æ“:\")\n",
    "\n",
    "class QualityRuleType(Enum):\n",
    "    \"\"\"è´¨é‡è§„åˆ™ç±»å‹\"\"\"\n",
    "    NOT_NULL = \"not_null\"\n",
    "    RANGE_CHECK = \"range_check\"\n",
    "    FORMAT_CHECK = \"format_check\"\n",
    "    UNIQUENESS_CHECK = \"uniqueness_check\"\n",
    "    REFERENTIAL_INTEGRITY = \"referential_integrity\"\n",
    "    BUSINESS_RULE = \"business_rule\"\n",
    "\n",
    "@dataclass\n",
    "class QualityRule:\n",
    "    \"\"\"æ•°æ®è´¨é‡è§„åˆ™\"\"\"\n",
    "    name: str\n",
    "    rule_type: QualityRuleType\n",
    "    column: str\n",
    "    description: str\n",
    "    severity: str = \"medium\"  # low, medium, high, critical\n",
    "    parameters: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"éªŒè¯è§„åˆ™\"\"\"\n",
    "        total_records = len(df)\n",
    "        violations = 0\n",
    "        violation_details = []\n",
    "        \n",
    "        if self.rule_type == QualityRuleType.NOT_NULL:\n",
    "            null_count = df[self.column].isnull().sum()\n",
    "            violations = null_count\n",
    "            if violations > 0:\n",
    "                violation_details = [f\"åˆ— {self.column} æœ‰ {violations} ä¸ªç©ºå€¼\"]\n",
    "        \n",
    "        elif self.rule_type == QualityRuleType.RANGE_CHECK:\n",
    "            min_val = self.parameters.get('min_value')\n",
    "            max_val = self.parameters.get('max_value')\n",
    "            \n",
    "            if min_val is not None and max_val is not None:\n",
    "                out_of_range = ((df[self.column] < min_val) | (df[self.column] > max_val)).sum()\n",
    "                violations = out_of_range\n",
    "                if violations > 0:\n",
    "                    violation_details = [f\"åˆ— {self.column} æœ‰ {violations} ä¸ªå€¼è¶…å‡ºèŒƒå›´ [{min_val}, {max_val}]\"]\n",
    "        \n",
    "        elif self.rule_type == QualityRuleType.FORMAT_CHECK:\n",
    "            pattern = self.parameters.get('pattern')\n",
    "            if pattern:\n",
    "                import re\n",
    "                invalid_format = ~df[self.column].astype(str).str.match(pattern, na=False)\n",
    "                violations = invalid_format.sum()\n",
    "                if violations > 0:\n",
    "                    violation_details = [f\"åˆ— {self.column} æœ‰ {violations} ä¸ªå€¼ä¸ç¬¦åˆæ ¼å¼è¦æ±‚\"]\n",
    "        \n",
    "        elif self.rule_type == QualityRuleType.UNIQUENESS_CHECK:\n",
    "            duplicate_count = df[self.column].duplicated().sum()\n",
    "            violations = duplicate_count\n",
    "            if violations > 0:\n",
    "                violation_details = [f\"åˆ— {self.column} æœ‰ {violations} ä¸ªé‡å¤å€¼\"]\n",
    "        \n",
    "        elif self.rule_type == QualityRuleType.BUSINESS_RULE:\n",
    "            # è‡ªå®šä¹‰ä¸šåŠ¡è§„åˆ™\n",
    "            rule_function = self.parameters.get('rule_function')\n",
    "            if rule_function and callable(rule_function):\n",
    "                violations = rule_function(df)\n",
    "                if violations > 0:\n",
    "                    violation_details = [f\"ä¸šåŠ¡è§„åˆ™ {self.name} è¿å {violations} æ¬¡\"]\n",
    "        \n",
    "        violation_rate = violations / total_records if total_records > 0 else 0\n",
    "        passed = violations == 0\n",
    "        \n",
    "        return {\n",
    "            'rule_name': self.name,\n",
    "            'column': self.column,\n",
    "            'total_records': total_records,\n",
    "            'violations': violations,\n",
    "            'violation_rate': violation_rate,\n",
    "            'passed': passed,\n",
    "            'severity': self.severity,\n",
    "            'details': violation_details\n",
    "        }\n",
    "\n",
    "# 2. æ•°æ®è´¨é‡ç›‘æ§å™¨\n",
    "print(f\"\\n   ğŸ” 2. æ•°æ®è´¨é‡ç›‘æ§å™¨:\")\n",
    "\n",
    "@dataclass\n",
    "class DataQualityMonitor:\n",
    "    \"\"\"æ•°æ®è´¨é‡ç›‘æ§å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules: List[QualityRule] = []\n",
    "        self.monitoring_history = []\n",
    "        self.alert_thresholds = {\n",
    "            'critical': 0.1,    # 10%ä»¥ä¸Šè¿è§„ä¸ºä¸¥é‡\n",
    "            'high': 0.05,       # 5%ä»¥ä¸Šè¿è§„ä¸ºé«˜çº§\n",
    "            'medium': 0.02,     # 2%ä»¥ä¸Šè¿è§„ä¸ºä¸­çº§\n",
    "            'low': 0.01         # 1%ä»¥ä¸Šè¿è§„ä¸ºä½çº§\n",
    "        }\n",
    "    \n",
    "    def add_rule(self, rule: QualityRule):\n",
    "        \"\"\"æ·»åŠ è´¨é‡è§„åˆ™\"\"\"\n",
    "        self.rules.append(rule)\n",
    "        print(f\"   æ·»åŠ è´¨é‡è§„åˆ™: {rule.name} ({rule.rule_type.value})\")\n",
    "    \n",
    "    def run_quality_check(self, df: pd.DataFrame, check_name: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"è¿è¡Œè´¨é‡æ£€æŸ¥\"\"\"\n",
    "        if check_name is None:\n",
    "            check_name = f\"è´¨é‡æ£€æŸ¥_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        print(f\"\\n   æ‰§è¡Œè´¨é‡æ£€æŸ¥: {check_name}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        check_results = {\n",
    "            'check_name': check_name,\n",
    "            'timestamp': datetime.datetime.now(),\n",
    "            'total_records': len(df),\n",
    "            'total_rules': len(self.rules),\n",
    "            'rule_results': [],\n",
    "            'summary': {\n",
    "                'passed_rules': 0,\n",
    "                'failed_rules': 0,\n",
    "                'total_violations': 0,\n",
    "                'overall_score': 0.0\n",
    "            },\n",
    "            'alerts': [],\n",
    "            'processing_time': 0.0\n",
    "        }\n",
    "        \n",
    "        # æ‰§è¡Œæ‰€æœ‰è§„åˆ™\n",
    "        for rule in self.rules:\n",
    "            if rule.column in df.columns:\n",
    "                result = rule.validate(df)\n",
    "                check_results['rule_results'].append(result)\n",
    "                \n",
    "                if result['passed']:\n",
    "                    check_results['summary']['passed_rules'] += 1\n",
    "                else:\n",
    "                    check_results['summary']['failed_rules'] += 1\n",
    "                    check_results['summary']['total_violations'] += result['violations']\n",
    "                    \n",
    "                    # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦\n",
    "                    threshold = self.alert_thresholds.get(result['severity'], 0.05)\n",
    "                    if result['violation_rate'] > threshold:\n",
    "                        alert = {\n",
    "                            'rule_name': rule.name,\n",
    "                            'severity': result['severity'],\n",
    "                            'violation_rate': result['violation_rate'],\n",
    "                            'message': f\"è´¨é‡è§„åˆ™ {rule.name} è¿è§„ç‡ {result['violation_rate']:.2%} è¶…è¿‡é˜ˆå€¼ {threshold:.2%}\"\n",
    "                        }\n",
    "                        check_results['alerts'].append(alert)\n",
    "                \n",
    "                print(f\"      {rule.name}: {'âœ“' if result['passed'] else 'âœ—'} \"\n",
    "                      f\"({result['violations']} è¿è§„, {result['violation_rate']:.2%})\")\n",
    "            else:\n",
    "                print(f\"      {rule.name}: è·³è¿‡ (åˆ—ä¸å­˜åœ¨)\")\n",
    "        \n",
    "        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°\n",
    "        if check_results['total_rules'] > 0:\n",
    "            check_results['summary']['overall_score'] = (\n",
    "                check_results['summary']['passed_rules'] / check_results['total_rules']\n",
    "            )\n",
    "        \n",
    "        check_results['processing_time'] = time.time() - start_time\n",
    "        \n",
    "        # ä¿å­˜æ£€æŸ¥å†å²\n",
    "        self.monitoring_history.append(check_results)\n",
    "        \n",
    "        # æ˜¾ç¤ºæ‘˜è¦\n",
    "        print(f\"\\n      ğŸ“Š è´¨é‡æ£€æŸ¥æ‘˜è¦:\")\n",
    "        print(f\"        æ€»è§„åˆ™æ•°: {check_results['total_rules']}\")\n",
    "        print(f\"        é€šè¿‡è§„åˆ™: {check_results['summary']['passed_rules']}\")\n",
    "        print(f\"        å¤±è´¥è§„åˆ™: {check_results['summary']['failed_rules']}\")\n",
    "        print(f\"        æ€»è¿è§„æ•°: {check_results['summary']['total_violations']}\")\n",
    "        print(f\"        è´¨é‡åˆ†æ•°: {check_results['summary']['overall_score']:.3f}\")\n",
    "        print(f\"        å‘Šè­¦æ•°é‡: {len(check_results['alerts'])}\")\n",
    "        print(f\"        æ£€æŸ¥è€—æ—¶: {check_results['processing_time']:.3f} ç§’\")\n",
    "        \n",
    "        if check_results['alerts']:\n",
    "            print(f\"\\n      ğŸš¨ è´¨é‡å‘Šè­¦:\")\n",
    "            for alert in check_results['alerts']:\n",
    "                print(f\"        [{alert['severity'].upper()}] {alert['message']}\")\n",
    "        \n",
    "        return check_results\n",
    "    \n",
    "    def get_quality_trend(self, days: int = 7) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–è´¨é‡è¶‹åŠ¿\"\"\"\n",
    "        if len(self.monitoring_history) < 2:\n",
    "            return \"æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿\"\n",
    "        \n",
    "        # è·å–æœ€è¿‘å‡ å¤©çš„æ£€æŸ¥ç»“æœ\n",
    "        recent_checks = self.monitoring_history[-days:]\n",
    "        \n",
    "        timestamps = [check['timestamp'] for check in recent_checks]\n",
    "        scores = [check['summary']['overall_score'] for check in recent_checks]\n",
    "        violations = [check['summary']['total_violations'] for check in recent_checks]\n",
    "        \n",
    "        # è®¡ç®—è¶‹åŠ¿\n",
    "        if len(scores) >= 2:\n",
    "            score_trend = scores[-1] - scores[0]\n",
    "            violation_trend = violations[-1] - violations[0]\n",
    "        else:\n",
    "            score_trend = 0\n",
    "            violation_trend = 0\n",
    "        \n",
    "        print(f\"\\n   ğŸ“ˆ è´¨é‡è¶‹åŠ¿åˆ†æ (æœ€è¿‘{days}æ¬¡æ£€æŸ¥):\")\n",
    "        print(f\"      è´¨é‡åˆ†æ•°è¶‹åŠ¿: {score_trend:+.3f}\")\n",
    "        print(f\"      è¿è§„æ•°é‡è¶‹åŠ¿: {violation_trend:+d}\")\n",
    "        print(f\"      å¹³å‡è´¨é‡åˆ†æ•°: {np.mean(scores):.3f}\")\n",
    "        print(f\"      æœ€é«˜è´¨é‡åˆ†æ•°: {max(scores):.3f}\")\n",
    "        print(f\"      æœ€ä½è´¨é‡åˆ†æ•°: {min(scores):.3f}\")\n",
    "        \n",
    "        # å¯è§†åŒ–è¶‹åŠ¿\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # è´¨é‡åˆ†æ•°è¶‹åŠ¿\n",
    "        ax1.plot(timestamps, scores, 'b-o', linewidth=2, markersize=6)\n",
    "        ax1.set_title('æ•°æ®è´¨é‡åˆ†æ•°è¶‹åŠ¿')\n",
    "        ax1.set_ylabel('è´¨é‡åˆ†æ•°')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # è¿è§„æ•°é‡è¶‹åŠ¿\n",
    "        ax2.plot(timestamps, violations, 'r-o', linewidth=2, markersize=6)\n",
    "        ax2.set_title('è¿è§„æ•°é‡è¶‹åŠ¿')\n",
    "        ax2.set_ylabel('è¿è§„æ•°é‡')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'score_trend': score_trend,\n",
    "            'violation_trend': violation_trend,\n",
    "            'average_score': np.mean(scores),\n",
    "            'max_score': max(scores),\n",
    "            'min_score': min(scores),\n",
    "            'data_points': len(scores)\n",
    "        }\n",
    "    \n",
    "    def generate_quality_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç”Ÿæˆè´¨é‡æŠ¥å‘Š\"\"\"\n",
    "        if not self.monitoring_history:\n",
    "            return \"æš‚æ— ç›‘æ§æ•°æ®\"\n",
    "        \n",
    "        latest_check = self.monitoring_history[-1]\n",
    "        \n",
    "        report = {\n",
    "            'report_time': datetime.datetime.now(),\n",
    "            'latest_check': latest_check['check_name'],\n",
    "            'overall_status': 'è‰¯å¥½' if latest_check['summary']['overall_score'] >= 0.8 else 'éœ€è¦æ”¹è¿›',\n",
    "            'key_metrics': {\n",
    "                'total_records': latest_check['total_records'],\n",
    "                'quality_score': latest_check['summary']['overall_score'],\n",
    "                'passed_rules': latest_check['summary']['passed_rules'],\n",
    "                'failed_rules': latest_check['summary']['failed_rules'],\n",
    "                'total_violations': latest_check['summary']['total_violations'],\n",
    "                'active_alerts': len(latest_check['alerts'])\n",
    "            },\n",
    "            'rule_performance': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # åˆ†æè§„åˆ™æ€§èƒ½\n",
    "        rule_stats = {}\n",
    "        for rule_result in latest_check['rule_results']:\n",
    "            rule_name = rule_result['rule_name']\n",
    "            if rule_name not in rule_stats:\n",
    "                rule_stats[rule_name] = {\n",
    "                    'violations': 0,\n",
    "                    'violation_rate': 0,\n",
    "                    'severity': rule_result['severity']\n",
    "                }\n",
    "            rule_stats[rule_name]['violations'] += rule_result['violations']\n",
    "            rule_stats[rule_name]['violation_rate'] += rule_result['violation_rate']\n",
    "        \n",
    "        report['rule_performance'] = rule_stats\n",
    "        \n",
    "        # ç”Ÿæˆæ”¹è¿›å»ºè®®\n",
    "        if latest_check['summary']['overall_score'] < 0.8:\n",
    "            report['recommendations'].append(\"æ•´ä½“è´¨é‡åˆ†æ•°åä½ï¼Œå»ºè®®å…¨é¢æ£€æŸ¥æ•°æ®æµç¨‹\")\n",
    "        \n",
    "        high_violation_rules = [\n",
    "            rule_name for rule_name, stats in rule_stats.items()\n",
    "            if stats['violation_rate'] > 0.1\n",
    "        ]\n",
    "        \n",
    "        if high_violation_rules:\n",
    "            report['recommendations'].append(f\"ä»¥ä¸‹è§„åˆ™è¿è§„ç‡è¾ƒé«˜ï¼Œéœ€è¦ä¼˜å…ˆå¤„ç†: {', '.join(high_violation_rules)}\")\n",
    "        \n",
    "        if latest_check['alerts']:\n",
    "            report['recommendations'].append(\"å­˜åœ¨æ´»è·ƒå‘Šè­¦ï¼Œå»ºè®®ç«‹å³å¤„ç†æ•°æ®è´¨é‡é—®é¢˜\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæŠ¥å‘Š\n",
    "        print(f\"\\n   ğŸ“‹ æ•°æ®è´¨é‡æŠ¥å‘Š:\")\n",
    "        print(f\"      æŠ¥å‘Šæ—¶é—´: {report['report_time']}\")\n",
    "        print(f\"      æ•´ä½“çŠ¶æ€: {report['overall_status']}\")\n",
    "        print(f\"      è´¨é‡åˆ†æ•°: {report['key_metrics']['quality_score']:.3f}\")\n",
    "        print(f\"      é€šè¿‡è§„åˆ™: {report['key_metrics']['passed_rules']}/{latest_check['total_rules']}\")\n",
    "        print(f\"      è¿è§„æ•°é‡: {report['key_metrics']['total_violations']}\")\n",
    "        print(f\"      æ´»è·ƒå‘Šè­¦: {report['key_metrics']['active_alerts']}\")\n",
    "        \n",
    "        if report['recommendations']:\n",
    "            print(f\"\\n      ğŸ’¡ æ”¹è¿›å»ºè®®:\")\n",
    "            for i, rec in enumerate(report['recommendations'], 1):\n",
    "                print(f\"        {i}. {rec}\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "# 3. æ•°æ®è´¨é‡æ”¹è¿›å™¨\n",
    "print(f\"\\n   ğŸ”§ 3. æ•°æ®è´¨é‡æ”¹è¿›å™¨:\")\n",
    "\n",
    "@dataclass\n",
    "class DataQualityImprover:\n",
    "    \"\"\"æ•°æ®è´¨é‡æ”¹è¿›å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.improvement_history = []\n",
    "    \n",
    "    def auto_fix_data(self, df: pd.DataFrame, quality_results: Dict[str, Any]) -> pd.DataFrame:\n",
    "        \"\"\"è‡ªåŠ¨ä¿®å¤æ•°æ®è´¨é‡é—®é¢˜\"\"\"\n",
    "        print(f\"   æ‰§è¡Œè‡ªåŠ¨æ•°æ®ä¿®å¤...\")\n",
    "        \n",
    "        improved_df = df.copy()\n",
    "        fixes_applied = []\n",
    "        \n",
    "        for rule_result in quality_results['rule_results']:\n",
    "            if not rule_result['passed']:\n",
    "                rule_name = rule_result['rule_name']\n",
    "                column = rule_result['column']\n",
    "                violations = rule_result['violations']\n",
    "                \n",
    "                if 'not_null' in rule_name.lower():\n",
    "                    # ä¿®å¤ç©ºå€¼\n",
    "                    if improved_df[column].dtype in ['int64', 'float64']:\n",
    "                        fill_value = improved_df[column].median()\n",
    "                    else:\n",
    "                        fill_value = improved_df[column].mode()[0] if not improved_df[column].mode().empty else 'unknown'\n",
    "                    \n",
    "                    null_count = improved_df[column].isnull().sum()\n",
    "                    improved_df[column].fillna(fill_value, inplace=True)\n",
    "                    fixes_applied.append(f\"å¡«å…… {column} åˆ—çš„ {null_count} ä¸ªç©ºå€¼\")\n",
    "                \n",
    "                elif 'range' in rule_name.lower():\n",
    "                    # ä¿®å¤èŒƒå›´é”™è¯¯\n",
    "                    if column in improved_df.columns:\n",
    "                        # å‡è®¾çš„åˆç†èŒƒå›´ï¼ˆæ ¹æ®ä¸šåŠ¡é€»è¾‘ï¼‰\n",
    "                        if 'latency' in column.lower():\n",
    "                            min_val, max_val = 0, 30000  # 0-30ç§’\n",
    "                        elif 'cost' in column.lower():\n",
    "                            min_val, max_val = 0.001, 1.0\n",
    "                        elif 'token' in column.lower():\n",
    "                            min_val, max_val = 1, 10000\n",
    "                        else:\n",
    "                            min_val, max_val = improved_df[column].quantile([0.01, 0.99])\n",
    "                        \n",
    "                        out_of_range_mask = (improved_df[column] < min_val) | (improved_df[column] > max_val)\n",
    "                        out_of_range_count = out_of_range_mask.sum()\n",
    "                        \n",
    "                        if out_of_range_count > 0:\n",
    "                            improved_df.loc[improved_df[column] < min_val, column] = min_val\n",
    "                            improved_df.loc[improved_df[column] > max_val, column] = max_val\n",
    "                            fixes_applied.append(f\"ä¿®æ­£ {column} åˆ—çš„ {out_of_range_count} ä¸ªè¶…å‡ºèŒƒå›´çš„å€¼\")\n",
    "                \n",
    "                elif 'uniqueness' in rule_name.lower():\n",
    "                    # å¤„ç†é‡å¤å€¼ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰\n",
    "                    duplicate_count = improved_df[column].duplicated().sum()\n",
    "                    if duplicate_count > 0:\n",
    "                        improved_df.drop_duplicates(subset=[column], keep='first', inplace=True)\n",
    "                        fixes_applied.append(f\"ç§»é™¤ {column} åˆ—çš„ {duplicate_count} ä¸ªé‡å¤å€¼\")\n",
    "        \n",
    "        # è®°å½•ä¿®å¤å†å²\n",
    "        improvement_record = {\n",
    "            'timestamp': datetime.datetime.now(),\n",
    "            'original_records': len(df),\n",
    "            'improved_records': len(improved_df),\n",
    "            'fixes_applied': fixes_applied,\n",
    "            'fixes_count': len(fixes_applied)\n",
    "        }\n",
    "        \n",
    "        self.improvement_history.append(improvement_record)\n",
    "        \n",
    "        print(f\"      è‡ªåŠ¨ä¿®å¤å®Œæˆ:\")\n",
    "        print(f\"        ä¿®å¤å‰è®°å½•: {improvement_record['original_records']:,}\")\n",
    "        print(f\"        ä¿®å¤åè®°å½•: {improvement_record['improved_records']:,}\")\n",
    "        print(f\"        åº”ç”¨ä¿®å¤: {improvement_record['fixes_count']} é¡¹\")\n",
    "        \n",
    "        for fix in fixes_applied:\n",
    "            print(f\"          - {fix}\")\n",
    "        \n",
    "        return improved_df\n",
    "    \n",
    "    def suggest_improvements(self, quality_results: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"å»ºè®®æ”¹è¿›æªæ–½\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        for rule_result in quality_results['rule_results']:\n",
    "            if not rule_result['passed']:\n",
    "                rule_name = rule_result['rule_name']\n",
    "                column = rule_result['column']\n",
    "                violation_rate = rule_result['violation_rate']\n",
    "                \n",
    "                if violation_rate > 0.2:\n",
    "                    suggestions.append(f\"åˆ— {column} çš„ {rule_name} è¿è§„ç‡è¿‡é«˜ ({violation_rate:.1%})ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æº\")\n",
    "                elif violation_rate > 0.1:\n",
    "                    suggestions.append(f\"åˆ— {column} çš„ {rule_name} éœ€è¦æ”¹è¿›ï¼Œå»ºè®®å¢å¼ºæ•°æ®éªŒè¯\")\n",
    "                else:\n",
    "                    suggestions.append(f\"åˆ— {column} çš„ {rule_name} æœ‰è½»å¾®é—®é¢˜ï¼Œå»ºè®®å®šæœŸç›‘æ§\")\n",
    "        \n",
    "        # é€šç”¨å»ºè®®\n",
    "        overall_score = quality_results['summary']['overall_score']\n",
    "        if overall_score < 0.7:\n",
    "            suggestions.append(\"æ•´ä½“æ•°æ®è´¨é‡åä½ï¼Œå»ºè®®å»ºç«‹å®Œå–„çš„æ•°æ®æ²»ç†ä½“ç³»\")\n",
    "        elif overall_score < 0.9:\n",
    "            suggestions.append(\"æ•°æ®è´¨é‡æœ‰æ”¹è¿›ç©ºé—´ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®å¤„ç†æµç¨‹\")\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "# æ¼”ç¤ºæ•°æ®è´¨é‡ç®¡ç†\n",
    "print(f\"\\n   ğŸ”§ æ•°æ®è´¨é‡ç®¡ç†æ¼”ç¤º:\")\n",
    "\n",
    "# åˆ›å»ºè´¨é‡ç›‘æ§å™¨\n",
    "quality_monitor = DataQualityMonitor()\n",
    "\n",
    "# æ·»åŠ è´¨é‡è§„åˆ™\n",
    "rules = [\n",
    "    QualityRule(\n",
    "        name=\"ç”¨æˆ·IDä¸èƒ½ä¸ºç©º\",\n",
    "        rule_type=QualityRuleType.NOT_NULL,\n",
    "        column=\"user_id\",\n",
    "        description=\"æ£€æŸ¥ç”¨æˆ·IDå­—æ®µæ˜¯å¦æœ‰ç©ºå€¼\",\n",
    "        severity=\"high\"\n",
    "    ),\n",
    "    QualityRule(\n",
    "        name=\"å»¶è¿Ÿæ—¶é—´èŒƒå›´æ£€æŸ¥\",\n",
    "        rule_type=QualityRuleType.RANGE_CHECK,\n",
    "        column=\"latency_ms\",\n",
    "        description=\"æ£€æŸ¥å»¶è¿Ÿæ—¶é—´æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…\",\n",
    "        severity=\"medium\",\n",
    "        parameters={'min_value': 0, 'max_value': 30000}\n",
    "    ),\n",
    "    QualityRule(\n",
    "        name=\"Tokenæ•°é‡èŒƒå›´æ£€æŸ¥\",\n",
    "        rule_type=QualityRuleType.RANGE_CHECK,\n",
    "        column=\"token_count\",\n",
    "        description=\"æ£€æŸ¥Tokenæ•°é‡æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…\",\n",
    "        severity=\"medium\",\n",
    "        parameters={'min_value': 1, 'max_value': 10000}\n",
    "    ),\n",
    "    QualityRule(\n",
    "        name=\"äº‹ä»¶ç±»å‹æ ¼å¼æ£€æŸ¥\",\n",
    "        rule_type=QualityRuleType.BUSINESS_RULE,\n",
    "        column=\"event_type\",\n",
    "        description=\"æ£€æŸ¥äº‹ä»¶ç±»å‹æ˜¯å¦ä¸ºæœ‰æ•ˆå€¼\",\n",
    "        severity=\"low\",\n",
    "        parameters={\n",
    "            'rule_function': lambda df: (~df['event_type'].isin(['login', 'chat', 'completion', 'embedding', 'search'])).sum()\n",
    "        }\n",
    "    ),\n",
    "    QualityRule(\n",
    "        name=\"æˆåŠŸçŠ¶æ€ä¸€è‡´æ€§æ£€æŸ¥\",\n",
    "        rule_type=QualityRuleType.BUSINESS_RULE,\n",
    "        column=\"success\",\n",
    "        description=\"æ£€æŸ¥æˆåŠŸçŠ¶æ€ä¸é”™è¯¯ä»£ç çš„ä¸€è‡´æ€§\",\n",
    "        severity=\"medium\",\n",
    "        parameters={\n",
    "            'rule_function': lambda df: ((df['success'] == True) & (df['error_code'].notna())).sum()\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "for rule in rules:\n",
    "    quality_monitor.add_rule(rule)\n",
    "\n",
    "# è¿è¡Œè´¨é‡æ£€æŸ¥\n",
    "quality_results = quality_monitor.run_quality_check(enterprise_df, \"åˆå§‹è´¨é‡æ£€æŸ¥\")\n",
    "\n",
    "# åˆ›å»ºè´¨é‡æ”¹è¿›å™¨\n",
    "quality_improver = DataQualityImprover()\n",
    "\n",
    "# è·å–æ”¹è¿›å»ºè®®\n",
    "suggestions = quality_improver.suggest_improvements(quality_results)\n",
    "print(f\"\\n   ğŸ’¡ æ•°æ®è´¨é‡æ”¹è¿›å»ºè®®:\")\n",
    "for i, suggestion in enumerate(suggestions, 1):\n",
    "    print(f\"      {i}. {suggestion}\")\n",
    "\n",
    "# è‡ªåŠ¨ä¿®å¤æ•°æ®\n",
    "improved_df = quality_improver.auto_fix_data(enterprise_df, quality_results)\n",
    "\n",
    "# å†æ¬¡æ£€æŸ¥ä¿®å¤åçš„è´¨é‡\n",
    "print(f\"\\n   ğŸ” æ£€æŸ¥ä¿®å¤åçš„æ•°æ®è´¨é‡...\")\n",
    "improved_quality_results = quality_monitor.run_quality_check(improved_df, \"ä¿®å¤åè´¨é‡æ£€æŸ¥\")\n",
    "\n",
    "# ç”Ÿæˆè´¨é‡è¶‹åŠ¿åˆ†æ\n",
    "print(f\"\\n   ğŸ“ˆ è´¨é‡è¶‹åŠ¿åˆ†æ:\")\n",
    "trend_analysis = quality_monitor.get_quality_trend(days=2)\n",
    "\n",
    "# ç”Ÿæˆè´¨é‡æŠ¥å‘Š\n",
    "print(f\"\\n   ğŸ“‹ ç”Ÿæˆè´¨é‡æŠ¥å‘Š:\")\n",
    "quality_report = quality_monitor.generate_quality_report()\n",
    "\n",
    "print(f\"\\nâœ… æ•°æ®è´¨é‡ç®¡ç†å®Œæˆ\")\n",
    "print(f\"ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆ:\")\n",
    "print(f\"   âœ“ æŒæ¡æ•°æ®è´¨é‡è¯„ä¼°æ–¹æ³•å’ŒæŒ‡æ ‡\")\n",
    "print(f\"   âœ“ ç†è§£æ•°æ®è´¨é‡ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶\")\n",
    "print(f\"   âœ“ ç†Ÿç»ƒä½¿ç”¨æ•°æ®è´¨é‡ç®¡ç†å·¥å…·\")\n",
    "print(f\"   âœ“ èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„æ•°æ®è´¨é‡ä½“ç³»\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å­¦ä¹ æ€»ç»“\n",
    "\n",
    "### âœ… çŸ¥è¯†æ¸…å•è¾¾æˆæƒ…å†µéªŒè¯\n",
    "\n",
    "**5.9 æ•°æ®å·¥ç¨‹å®è·µ [â­â­è¿›é˜¶]**\n",
    "- âœ… æŒæ¡æ•°æ®å·¥ç¨‹çš„åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„\n",
    "- âœ… ç†è§£ETLç®¡é“çš„è®¾è®¡å’Œå®ç°\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨æ•°æ®è´¨é‡ç®¡ç†å·¥å…·\n",
    "- âœ… èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„æ•°æ®å·¥ç¨‹è§£å†³æ–¹æ¡ˆ\n",
    "- âœ… æŒæ¡ETLçš„åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„è®¾è®¡\n",
    "- âœ… ç†è§£æ•°æ®æå–ã€è½¬æ¢ã€åŠ è½½çš„å„ä¸ªé˜¶æ®µ\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨ETLå·¥å…·å’Œæ¡†æ¶\n",
    "- âœ… èƒ½å¤Ÿæ„å»ºå¯æ‰©å±•çš„ETLç®¡é“\n",
    "- âœ… æŒæ¡æ•°æ®è´¨é‡è¯„ä¼°æ–¹æ³•å’ŒæŒ‡æ ‡\n",
    "- âœ… ç†è§£æ•°æ®è´¨é‡ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶\n",
    "- âœ… ç†Ÿç»ƒä½¿ç”¨æ•°æ®è´¨é‡ç®¡ç†å·¥å…·\n",
    "- âœ… èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„æ•°æ®è´¨é‡ä½“ç³»\n",
    "- âœ… èƒ½ç‹¬ç«‹æ„å»ºæœ‰æ•ˆçš„æ•°æ®å·¥ç¨‹ç³»ç»Ÿ\n",
    "\n",
    "### ğŸ¯ ä¸LangChainå­¦ä¹ çš„å…³è”\n",
    "\n",
    "**æ•°æ®å·¥ç¨‹é‡è¦æ€§**ï¼š\n",
    "- æ•°æ®å·¥ç¨‹æ”¯æŒLangChainçš„æ•°æ®ç®¡é“æ„å»º\n",
    "- ä¸ºLangChainçš„ä¼ä¸šçº§éƒ¨ç½²æä¾›æ•°æ®åŸºç¡€\n",
    "- æ”¯æŒLangChainçš„æ•°æ®æ²»ç†å’Œè´¨é‡ä¿è¯\n",
    "- ç¡®ä¿LangChainåº”ç”¨çš„æ•°æ®å¯é æ€§å’Œä¸€è‡´æ€§\n",
    "- æ•°æ®å·¥ç¨‹æ”¯æŒLangChainçš„å®æ—¶æ•°æ®å¤„ç†å’Œåˆ†æ\n",
    "\n",
    "**å®é™…åº”ç”¨åœºæ™¯**ï¼š\n",
    "- LangChainçš„å¤§è§„æ¨¡æ–‡æ¡£ETLå¤„ç†å’Œå‘é‡åŒ–\n",
    "- LangChainçš„å¤šæºæ•°æ®é›†æˆå’Œæ ‡å‡†åŒ–\n",
    "- LangChainçš„å®æ—¶æ•°æ®æµå¤„ç†å’Œè´¨é‡ç›‘æ§\n",
    "- LangChainçš„çŸ¥è¯†å›¾è°±æ•°æ®å·¥ç¨‹æ„å»º\n",
    "- LangChainçš„ç”¨æˆ·è¡Œä¸ºæ•°æ®åˆ†æå’Œæ´å¯Ÿ\n",
    "\n",
    "### ğŸ“š è¿›é˜¶å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **ç»ƒä¹ å»ºè®®**ï¼š\n",
    "   - æ·±å…¥ç»ƒä¹ Apache Airflowå·¥ä½œæµè°ƒåº¦\n",
    "   - æŒæ¡Apache Kafkaæµå¼æ•°æ®å¤„ç†\n",
    "   - å­¦ä¹ äº‘åŸç”Ÿæ•°æ®å·¥ç¨‹å¹³å°å¦‚AWS Glueã€Google Dataflow\n",
    "\n",
    "2. **æ‰©å±•å­¦ä¹ **ï¼š\n",
    "   - å­¦ä¹ æ•°æ®æ¹–å’Œæ•°æ®ä»“åº“æ¶æ„è®¾è®¡\n",
    "   - äº†è§£å®æ—¶è®¡ç®—å’Œå¤æ‚äº‹ä»¶å¤„ç†\n",
    "   - æ¢ç´¢æ•°æ®æ²»ç†å’Œå…ƒæ•°æ®ç®¡ç†\n",
    "\n",
    "3. **å®é™…åº”ç”¨**ï¼š\n",
    "   - æ„å»ºä¼ä¸šçº§æ•°æ®å·¥ç¨‹å¹³å°\n",
    "   - å¼€å‘å®æ—¶æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿ\n",
    "   - å®ç°æ™ºèƒ½æ•°æ®ç®¡é“å’Œè‡ªåŠ¨åŒ–ETL\n",
    "\n",
    "### ğŸ”§ å¸¸è§é”™è¯¯ä¸æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **ETLè®¾è®¡é”™è¯¯**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šETLç®¡é“ç¼ºä¹é”™è¯¯å¤„ç†\n",
    "   def extract_data():\n",
    "       return pd.read_csv('data.csv')  # å¯èƒ½å¤±è´¥ä½†æ²¡æœ‰å¤„ç†\n",
    "   \n",
    "   # æ­£ç¡®ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶\n",
    "   def extract_data_with_retry(max_retries=3):\n",
    "       for attempt in range(max_retries):\n",
    "           try:\n",
    "               return pd.read_csv('data.csv')\n",
    "           except Exception as e:\n",
    "               if attempt == max_retries - 1:\n",
    "                   raise e\n",
    "               time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿\n",
    "   ```\n",
    "\n",
    "2. **æ•°æ®è´¨é‡ç›‘æ§è¯¯åŒº**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šè´¨é‡è§„åˆ™è¿‡äºä¸¥æ ¼å¯¼è‡´å¤§é‡è¯¯æŠ¥\n",
    "   strict_rule = QualityRule(\n",
    "       name=\"ä¸¥æ ¼æ£€æŸ¥\",\n",
    "       rule_type=QualityRuleType.RANGE_CHECK,\n",
    "       column=\"value\",\n",
    "       parameters={'min_value': 100, 'max_value': 101}  # èŒƒå›´è¿‡çª„\n",
    "   )\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ ¹æ®ä¸šåŠ¡å®é™…è®¾ç½®åˆç†çš„è´¨é‡é˜ˆå€¼\n",
    "   reasonable_rule = QualityRule(\n",
    "       name=\"åˆç†æ£€æŸ¥\",\n",
    "       rule_type=QualityRuleType.RANGE_CHECK,\n",
    "       column=\"value\",\n",
    "       parameters={'min_value': 0, 'max_value': 1000}  # åŸºäºä¸šåŠ¡ç»Ÿè®¡\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **æ•°æ®ç®¡é“æ€§èƒ½é—®é¢˜**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šä¸²è¡Œå¤„ç†å¤§é‡æ•°æ®å¯¼è‡´æ€§èƒ½ä½ä¸‹\n",
    "   def process_large_dataset(data):\n",
    "       results = []\n",
    "       for item in data:  # é€æ¡å¤„ç†\n",
    "           result = complex_transformation(item)\n",
    "           results.append(result)\n",
    "       return results\n",
    "   \n",
    "   # æ­£ç¡®ï¼šæ‰¹é‡å¹¶è¡Œå¤„ç†æé«˜æ€§èƒ½\n",
    "   def process_large_dataset_optimized(data, chunk_size=1000):\n",
    "       with ProcessPoolExecutor() as executor:\n",
    "           chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "           results = list(executor.map(process_chunk, chunks))\n",
    "       return [item for sublist in results for item in sublist]\n",
    "   ```\n",
    "\n",
    "4. **æ•°æ®ä¸€è‡´æ€§å¿½ç•¥**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šETLè¿‡ç¨‹ä¸­å¿½ç•¥æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥\n",
    "   def transform_data(df):\n",
    "       df['new_field'] = df['field_a'] / df['field_b']  # å¯èƒ½é™¤é›¶\n",
    "       return df\n",
    "   \n",
    "   # æ­£ç¡®ï¼šç¡®ä¿æ•°æ®è½¬æ¢çš„ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§\n",
    "   def transform_data_safe(df):\n",
    "       df = df.copy()\n",
    "       # é¿å…é™¤é›¶é”™è¯¯\n",
    "       df['new_field'] = df['field_a'] / df['field_b'].replace(0, np.nan)\n",
    "       # æ£€æŸ¥ä¸šåŠ¡é€»è¾‘ä¸€è‡´æ€§\n",
    "       df['is_consistent'] = df['field_a'] >= df['field_b']\n",
    "       return df\n",
    "   ```\n",
    "\n",
    "5. **ç›‘æ§å‘Šè­¦è¿‡åº¦**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šè¿‡äºé¢‘ç¹çš„è´¨é‡æ£€æŸ¥å¯¼è‡´ç³»ç»Ÿè´Ÿè½½è¿‡é«˜\n",
    "   while True:\n",
    "       run_quality_check(data)  # æ¯ç§’æ£€æŸ¥\n",
    "       time.sleep(1)\n",
    "   \n",
    "   # æ­£ç¡®ï¼šåˆç†çš„ç›‘æ§é¢‘ç‡å’Œæ™ºèƒ½å‘Šè­¦\n",
    "   def smart_quality_monitoring(data, check_interval=300):  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡\n",
    "       last_quality_score = 1.0\n",
    "       while True:\n",
    "           current_score = run_quality_check(data)['summary']['overall_score']\n",
    "           # åªåœ¨è´¨é‡æ˜¾è‘—ä¸‹é™æ—¶å‘Šè­¦\n",
    "           if current_score < last_quality_score - 0.1:\n",
    "               send_alert(f\"æ•°æ®è´¨é‡ä¸‹é™: {last_quality_score:.3f} -> {current_score:.3f}\")\n",
    "           last_quality_score = current_score\n",
    "           time.sleep(check_interval)\n",
    "   ```\n",
    "\n",
    "6. **æ•°æ®è¡€ç¼˜è¿½è¸ªç¼ºå¤±**ï¼š\n",
    "   ```python\n",
    "   # é”™è¯¯ï¼šç¼ºä¹æ•°æ®è¡€ç¼˜è¿½è¸ª\n",
    "   def process_data(data):\n",
    "       # å¤æ‚çš„æ•°æ®è½¬æ¢\n",
    "       return transformed_data\n",
    "   \n",
    "   # æ­£ç¡®ï¼šå®Œæ•´çš„æ•°æ®è¡€ç¼˜è¿½è¸ª\n",
    "   def process_data_with_lineage(data, pipeline_id):\n",
    "       lineage_record = {\n",
    "           'pipeline_id': pipeline_id,\n",
    "           'source_data': data.shape,\n",
    "           'transformations': [],\n",
    "           'timestamp': datetime.now()\n",
    "       }\n",
    "       \n",
    "       # è®°å½•æ¯ä¸ªè½¬æ¢æ­¥éª¤\n",
    "       data = step1_transform(data)\n",
    "       lineage_record['transformations'].append('step1_transform')\n",
    "       \n",
    "       data = step2_transform(data)\n",
    "       lineage_record['transformations'].append('step2_transform')\n",
    "       \n",
    "       save_lineage_record(lineage_record)\n",
    "       return data\n",
    "   ```\n",
    "\n",
    "### ğŸŒ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "**ETLç®¡é“ä¼˜åŒ–**ï¼š\n",
    "- ä½¿ç”¨å¹¶è¡Œå¤„ç†å’Œæ‰¹é‡æ“ä½œæé«˜ååé‡\n",
    "- å®ç°å¢é‡å¤„ç†å’Œå˜æ›´æ•°æ®æ•è·(CDC)\n",
    "- é‡‡ç”¨å†…å­˜è®¡ç®—å’Œåˆ—å¼å­˜å‚¨ä¼˜åŒ–æ€§èƒ½\n",
    "- å»ºç«‹æ™ºèƒ½è°ƒåº¦å’Œè´Ÿè½½å‡è¡¡æœºåˆ¶\n",
    "\n",
    "**æ•°æ®è´¨é‡ä¼˜åŒ–**ï¼š\n",
    "- è®¾è®¡åˆ†å±‚è´¨é‡æ£€æŸ¥ç­–ç•¥é¿å…è¿‡åº¦æ£€æŸ¥\n",
    "- ä½¿ç”¨é‡‡æ ·å’Œç»Ÿè®¡æ–¹æ³•è¿›è¡Œå¤§è§„æ¨¡è´¨é‡è¯„ä¼°\n",
    "- å®ç°å®æ—¶è´¨é‡ç›‘æ§å’Œå¼‚å¸¸æ£€æµ‹\n",
    "- å»ºç«‹è‡ªé€‚åº”è´¨é‡é˜ˆå€¼å’Œå‘Šè­¦æœºåˆ¶\n",
    "\n",
    "**æ•°æ®å·¥ç¨‹æ¶æ„ä¼˜åŒ–**ï¼š\n",
    "- é‡‡ç”¨å¾®æœåŠ¡æ¶æ„æé«˜ç³»ç»Ÿå¯æ‰©å±•æ€§\n",
    "- å®ç°æ•°æ®ç®¡é“çš„å¯è§‚æµ‹æ€§å’Œç›‘æ§\n",
    "- ä½¿ç”¨å®¹å™¨åŒ–å’Œç¼–æ’ç®€åŒ–éƒ¨ç½²ç®¡ç†\n",
    "- å»ºç«‹å®Œæ•´çš„å¤‡ä»½å’Œç¾éš¾æ¢å¤æœºåˆ¶\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆæ•°æ®å·¥ç¨‹å®è·µå­¦ä¹ ï¼**\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†æ•°æ®å·¥ç¨‹çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¿›è¡ŒETLç®¡é“è®¾è®¡ã€æ•°æ®è´¨é‡ç®¡ç†å’Œå·¥ç¨‹å®è·µï¼Œä¸ºLangChainæ™ºèƒ½åº”ç”¨æä¾›äº†å¼ºå¤§çš„æ•°æ®å·¥ç¨‹åŸºç¡€ã€‚\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ é¢„å‘Š\n",
    "\n",
    "**ç¬¬äº”èŠ‚ï¼šæ•°æ®å¤„ç† - å…¨éƒ¨å®Œæˆï¼** âœ…\n",
    "- 5.1 NumPyæ•°ç»„æ“ä½œ âœ…\n",
    "- 5.2 Pandasæ•°æ®å¤„ç† âœ…\n",
    "- 5.3 æ•°æ®å¯è§†åŒ–åŸºç¡€ âœ…\n",
    "- 5.4 æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç† âœ…\n",
    "- 5.5 ç»Ÿè®¡åˆ†æåº”ç”¨ âœ…\n",
    "- 5.6 æœºå™¨å­¦ä¹ åŸºç¡€ âœ…\n",
    "- 5.7 æ—¶é—´åºåˆ—åˆ†æ âœ…\n",
    "- 5.8 å¤§æ•°æ®å¤„ç†æŠ€æœ¯ âœ…\n",
    "- 5.9 æ•°æ®å·¥ç¨‹å®è·µ âœ…\n",
    "\n",
    "**ç»§ç»­ç¬¬å…­èŠ‚ï¼šå¼‚æ­¥ç¼–ç¨‹**\n",
    "- 6.1 å¼‚æ­¥ç¼–ç¨‹åŸºç¡€\n",
    "- 6.2 åç¨‹ä¸äº‹ä»¶å¾ªç¯\n",
    "- 6.3 å¼‚æ­¥I/Oæ“ä½œ\n",
    "- 6.4 å¼‚æ­¥ç½‘ç»œç¼–ç¨‹\n",
    "- 6.5 å¼‚æ­¥Webæ¡†æ¶\n",
    "- 6.6 å¹¶å‘ç¼–ç¨‹æ¨¡å¼\n",
    "- 6.7 å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—\n",
    "- 6.8 å¼‚æ­¥æ€§èƒ½ä¼˜åŒ–\n",
    "- 6.9 å¼‚æ­¥æœ€ä½³å®è·µ\n",
    "\n",
    "**åç»­ç« èŠ‚é¢„å‘Š**ï¼š\n",
    "- Webå¼€å‘æŠ€æœ¯\n",
    "- é¡¹ç›®å·¥ç¨‹å®è·µ\n",
    "\n",
    "ç»§ç»­åŠ æ²¹ï¼Œæ•°æ®å¤„ç†æŠ€èƒ½å·²ç»å…¨é¢æŒæ¡ï¼å‡†å¤‡è¿›å…¥å¼‚æ­¥ç¼–ç¨‹çš„å­¦ä¹ ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
